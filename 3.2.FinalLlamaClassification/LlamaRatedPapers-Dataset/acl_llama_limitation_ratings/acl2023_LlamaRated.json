[
    {
        "title": "Detecting and Mitigating Hallucinations in Machine Translation: Model Internal Workings Alone Do Well, Sentence Similarity Even Better",
        "authors": [
            "David Dale",
            "Elena Voita",
            "Loic Barrault",
            "Marta R. Costa-jussà"
        ],
        "published": "2023",
        "summary": "While the problem of hallucinations in neural machine translation has long been recognized, so far the progress on its alleviation is very little. Indeed, recently it turned out that without artificially encouraging models to hallucinate, previously existing methods fall short and even the standard sequence log-probability is more informative. It means that internal characteristics of the model can give much more information than we expect, and before using external models and measures, we first need to ask: how far can we go if we use nothing but the translation model itself ? We propose to use a method that evaluates the percentage of the source contribution to a generated translation. Intuitively, hallucinations are translations “detached” from the source, hence they can be identified by low source contribution. This method improves detection accuracy for the most severe hallucinations by a factor of 2 and is able to alleviate hallucinations at test time on par with the previous best approach that relies on external models. Next, if we move away from internal model characteristics and allow external tools, we show that using sentence similarity from cross-lingual embeddings further improves these results. We release the code of our experiments.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.3.pdf",
        "keywords": [
            "machine translation",
            "translations",
            "neural machine translation",
            "similarity"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "EM Pre-training for Multi-party Dialogue Response Generation",
        "authors": [
            "Yiyang Li",
            "Hai Zhao"
        ],
        "published": "2023",
        "summary": "Dialogue response generation requires an agent to generate a response according to the current dialogue history, in terms of which two-party dialogues have been well studied, but leaving a great gap for multi-party dialogues at the same time. Different from two-party dialogues where each response is a direct reply to its previous utterance, the addressee of a response utterance should be specified before it is generated in the multi-party scenario. Thanks to the huge amount of two-party conversational data, various pre-trained language models for two-party dialogue response generation have been proposed. However, due to the lack of annotated addressee labels in multi-party dialogue datasets, it is hard to use them to pre-train a response generation model for multi-party dialogues. To tackle this obstacle, we propose an Expectation-Maximization (EM) approach that iteratively performs the expectation steps to generate addressee labels, and the maximization steps to optimize a response generation model. Theoretical analyses and extensive experiments have justified the feasibility and effectiveness of our proposed method. The official implementation of this paper is available at https://github.com/EricLee8/MPDRG.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.7.pdf",
        "keywords": [
            "dialogue response",
            "dialogues",
            "dialogue response generation",
            "em pre training",
            "maximization"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, due to the lack of annotated addressee labels in multi-party dialogue datasets, it is hard to use them to pre-train a response generation model for multi-party dialogues.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, due to the lack of annotated addressee labels in multi-party dialogue datasets, it is hard to use them to pre-train a response generation model for multi-party dialogues.\""
    },
    {
        "title": "ACLM: A Selective-Denoising based Generative Data Augmentation Approach for Low-Resource Complex NER",
        "authors": [
            "Sreyan Ghosh",
            "Utkarsh Tyagi",
            "Manan Suri",
            "Sonal Kumar",
            "Ramaneswaran S",
            "Dinesh Manocha"
        ],
        "published": "2023",
        "summary": "Complex Named Entity Recognition (NER) is the task of detecting linguistically complex named entities in low-context text. In this paper, we present ACLM Attention-map aware keyword selection for Conditional Language Model fine-tuning), a novel data augmentation approach based on conditional generation, to address the data scarcity problem in low-resource complex NER. ACLM alleviates the context-entity mismatch issue, a problem existing NER data augmentation techniques suffer from and often generates incoherent augmentations by placing complex named entities in the wrong context. ACLM builds on BART and is optimized on a novel text reconstruction or denoising task - we use selective masking (aided by attention maps) to retain the named entities and certain keywords in the input sentence that provide contextually relevant additional knowledge or hints about the named entities. Compared with other data augmentation strategies, ACLM can generate more diverse and coherent augmentations preserving the true word sense of complex entities in the sentence. We demonstrate the effectiveness of ACLM both qualitatively and quantitatively on monolingual, cross-lingual, and multilingual complex NER across various low-resource settings. ACLM outperforms all our neural baselines by a significant margin (1%-36%). In addition, we demonstrate the application of ACLM to other domains that suffer from data scarcity (e.g., biomedical). In practice, ACLM generates more effective and factual augmentations for these domains than prior methods.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.8.pdf",
        "keywords": [
            "data augmentation",
            "named entity recognition",
            "aclm",
            "generative data augmentation",
            "conditional generation",
            "conditional language",
            "aclm attention map",
            "selective masking",
            "selective denoising",
            "text reconstruction",
            "keyword selection"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"a problem existing NER data augmentation techniques suffer from and often generates incoherent augmentations by placing complex named entities in the wrong context.\"\n\nThis paper mentions a limitation of existing data augmentation techniques for LLMs in passing, but the primary focus is on the proposed solution and its effectiveness.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"a problem existing NER data augmentation techniques suffer from and often generates incoherent augmentations by placing complex named entities in the wrong context.\"\n\nThis paper mentions a limitation of existing data augmentation techniques for LLMs in passing, but the primary focus is on the proposed solution and its effectiveness."
    },
    {
        "title": "Natural Language to Code Generation in Interactive Data Science Notebooks",
        "authors": [
            "Pengcheng Yin",
            "Wen-Ding Li",
            "Kefan Xiao",
            "Abhishek Rao",
            "Yeming Wen",
            "Kensen Shi",
            "Joshua Howland",
            "Paige Bailey",
            "Michele Catasta",
            "Henryk Michalewski",
            "Oleksandr Polozov",
            "Charles Sutton"
        ],
        "published": "2023",
        "summary": "Computational notebooks, such as Jupyter notebooks, are interactive computing environments that are ubiquitous among data scientists to perform data wrangling and analytic tasks. To measure the performance of AI pair programmers that automatically synthesize programs for those tasks given natural language (NL) intents from users, we build ARCADE, a benchmark of 1078 code generation problems using the pandas data analysis framework in data science notebooks. ARCADE features multiple rounds of NL-to-code problems from the same notebook. It requires a model to understand rich multi-modal contexts, such as existing notebook cells and their execution states as well as previous turns of interaction. To establish a strong baseline on this challenging task, we develop PaChiNCo, a 62B code language model (LM) for Python computational notebooks, which significantly outperforms public code LMs. Finally, we explore few-shot prompting strategies to elicit better code with step-by-step decomposition and NL explanation, showing the potential to improve the diversity and explainability of model predictions. Arcade is publicly available at https://github.com/google-research/arcade-nl2code/.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.9.pdf",
        "keywords": [
            "natural language",
            "interactive",
            "interactive data science notebooks",
            "code generation"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation of LLMs is mentioned, but \"To establish a strong baseline on this challenging task\" implies that the task is challenging for LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation of LLMs is mentioned, but \"To establish a strong baseline on this challenging task\" implies that the task is challenging for LLMs."
    },
    {
        "title": "MIL-Decoding: Detoxifying Language Models at Token-Level via Multiple Instance Learning",
        "authors": [
            "Xu Zhang",
            "Xiaojun Wan"
        ],
        "published": "2023",
        "summary": "Despite advances in large pre-trained neural language models, they are prone to generating toxic language, which brings security risks to their applications. We introduce MIL-Decoding, which detoxifies language models at token-level by interpolating it with a trained multiple instance learning (MIL) network.MIL model is trained on a corpus with a toxicity label for each text to predict the overall toxicity and the toxicity of each token in its context. Intuitively, the MIL network computes a toxicity distribution over next tokens according to the generated context which supplements the original language model to avoid toxicity. We evaluate MIL-Decoding with automatic metrics and human evaluation, where MIL-Decoding outperforms other baselines in detoxification while it only hurts generation fluency a little bit.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.11.pdf",
        "keywords": [
            "mil decoding",
            "multiple instance learning",
            "language models",
            "trained multiple instance learning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite advances in large pre-trained neural language models, they are prone to generating toxic language, which brings security risks to their applications.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite advances in large pre-trained neural language models, they are prone to generating toxic language, which brings security risks to their applications.\""
    },
    {
        "title": "Dependency resolution at the syntax-semantics interface: psycholinguistic and computational insights on control dependencies",
        "authors": [
            "Iria de-Dios-Flores",
            "Juan Garcia Amboage",
            "Marcos Garcia"
        ],
        "published": "2023",
        "summary": "Using psycholinguistic and computational experiments we compare the ability of humans and several pre-trained masked language models to correctly identify control dependencies in Spanish sentences such as ‘José le prometió/ordenó a María ser ordenado/a’ (‘Joseph promised/ordered Mary to be tidy’). These structures underlie complex anaphoric and agreement relations at the interface of syntax and semantics, allowing us to study lexically-guided antecedent retrieval processes. Our results show that while humans correctly identify the (un)acceptability of the strings, language models often fail to identify the correct antecedent in non-adjacent dependencies, showing their reliance on linearity. Additional experiments on Galician reinforce these conclusions. Our findings are equally valuable for the evaluation of language models’ ability to capture linguistic generalizations, as well as for psycholinguistic theories of anaphor resolution.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.12.pdf",
        "keywords": [
            "anaphor resolution",
            "control dependencies",
            "agreement relations",
            "psycholinguistic"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our results show that while humans correctly identify the (un)acceptability of the strings, language models often fail to identify the correct antecedent in non-adjacent dependencies, showing their reliance on linearity.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Our results show that while humans correctly identify the (un)acceptability of the strings, language models often fail to identify the correct antecedent in non-adjacent dependencies, showing their reliance on linearity.\""
    },
    {
        "title": "Open-ended Long Text Generation via Masked Language Modeling",
        "authors": [
            "Xiaobo Liang",
            "Zecheng Tang",
            "Juntao Li",
            "Min Zhang"
        ],
        "published": "2023",
        "summary": "Pre-trained autoregressive (AR) language models such as BART and GPTs have dominated OPen-ended Long Text Generation (Open-LTG).However, the AR nature will decrease the inference efficiency along with the increase of generation length, which hinder their application in Open-LTG.To improve inference efficiency, we alternatively explore the potential of the pre-trained masked language models (MLMs) along with a representative iterative non-autoregressive (NAR) decoding strategy for Open-LTG.Our preliminary study shows that pre-trained MLMs can merely generate short text and will collapse for long text modeling. To enhance the long text generation capability of MLMs, we introduce two simple yet effective strategies for the iterative NAR model: dynamic sliding window attention (DSWA) and linear temperature decay (LTD). It can alleviate long-distance collapse problems and achieve longer text generation with a flexible trade-off between performance and inference speedup. Experiments on the storytelling and multi-paragraph opinionated article writing tasks show that pre-trained MLMs can achieve more than 3 × → 13 × speedup with better performance than strong AR models.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.13.pdf",
        "keywords": [
            "long text generation",
            "sliding window attention",
            "masked"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the AR nature will decrease the inference efficiency along with the increase of generation length, which hinder their application in Open-LTG.\"; \"Our preliminary study shows that pre-trained MLMs can merely generate short text and will collapse for long text modeling.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, the AR nature will decrease the inference efficiency along with the increase of generation length, which hinder their application in Open-LTG.\"; \"Our preliminary study shows that pre-trained MLMs can merely generate short text and will collapse for long text modeling.\""
    },
    {
        "title": "A Method for Studying Semantic Construal in Grammatical Constructions with Interpretable Contextual Embedding Spaces",
        "authors": [
            "Gabriella Chronis",
            "Kyle Mahowald",
            "Katrin Erk"
        ],
        "published": "2023",
        "summary": "We study semantic construal in grammatical constructions using large language models. First, we project contextual word embeddings into three interpretable semantic spaces, each defined by a different set of psycholinguistic feature norms. We validate these interpretable spaces and then use them to automatically derive semantic characterizations of lexical items in two grammatical constructions: nouns in subject or object position within the same sentence, and the AANN construction (e.g., ‘a beautiful three days’). We show that a word in subject position is interpreted as more agentive than the very same word in object position, and that the nouns in the AANN construction are interpreted as more measurement-like than when in the canonical alternation. Our method can probe the distributional meaning of syntactic constructions at a templatic level, abstracted away from specific lexemes.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.14.pdf",
        "keywords": [
            "grammatical constructions",
            "syntactic constructions",
            "contextual word",
            "semantic spaces"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but \"We study semantic construal in grammatical constructions using large language models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but \"We study semantic construal in grammatical constructions using large language models.\""
    },
    {
        "title": "Prompts Can Play Lottery Tickets Well: Achieving Lifelong Information Extraction via Lottery Prompt Tuning",
        "authors": [
            "Zujie Liang",
            "Feng Wei",
            "Yin Jie",
            "Yuxi Qian",
            "Zhenghong Hao",
            "Bing Han"
        ],
        "published": "2023",
        "summary": "Thanks to the recent success of Pre-trained Language Models (PLMs), it has become a promising research direction to develop a universal model (UIE) that can solve all typical information extraction tasks within one generative framework. Nonetheless, in real-world scenarios of UIE applications, new data of different IE tasks and domains usually come in a stream over time. A desirable UIE system should be capable of continually learning new tasks without forgetting old ones, thereby allowing knowledge and functionalities expansion without re-training the whole system. In this paper, we study the UIE system under a more challenging yet practical scenario, i.e., “lifelong learning” settings, to evaluate its abilities in three aspects, including knowledge sharing and expansion, catastrophic forgetting prevention, and rapid generalization on few-shot and unseen tasks. To achieve these three goals, we present a novel parameter- and deployment-efficient prompt tuning method namely Lottery Prompt Tuning (LPT).LPT freezes the PLM’s parameters and sequentially learns compact pruned prompt vectors for each task leveraging a binary prompt mask, while keeping the prompt parameters selected by the previous tasks insusceptible. Furthermore, we use a simple yet effective method to perform mask selection and show the powerful transferability of Lottery Prompts to novel tasks. Extensive experiments demonstrate that LPT consistently sets state-of-the-art performance on multiple lifelong learning settings of UIE, including task-incremental setting on seen tasks, few-shot adaptation, and zero-shot generalization on novel tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.16.pdf",
        "keywords": [
            "lottery tickets",
            "lottery",
            "mask selection",
            "tuning",
            "lottery prompt tuning",
            "rapid generalization",
            "lifelong learning",
            "prompts"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"A desirable UIE system should be capable of continually learning new tasks without forgetting old ones, thereby allowing knowledge and functionalities expansion without re-training the whole system.\"\n\nThis rating is based on the fact that the abstract mentions the limitation of catastrophic forgetting in the context of lifelong learning for UIE systems, but does not explore this limitation in depth and instead focuses on the proposed solution, Lottery Prompt",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"A desirable UIE system should be capable of continually learning new tasks without forgetting old ones, thereby allowing knowledge and functionalities expansion without re-training the whole system.\"\n\nThis rating is based on the fact that the abstract mentions the limitation of catastrophic forgetting in the context of lifelong learning for UIE systems, but does not explore this limitation in depth and instead focuses on the proposed solution, Lottery Prompt"
    },
    {
        "title": "WeCheck: Strong Factual Consistency Checker via Weakly Supervised Learning",
        "authors": [
            "Wenhao Wu",
            "Wei Li",
            "Xinyan Xiao",
            "Jiachen Liu",
            "Sujian Li",
            "Yajuan Lyu"
        ],
        "published": "2023",
        "summary": "A crucial issue of current text generation models is that they often uncontrollably generate text that is factually inconsistent with inputs. Due to lack of annotated data, existing factual consistency metrics usually train evaluation models on synthetic texts or directly transfer from other related tasks, such as question answering (QA) and natural language inference (NLI).Bias in synthetic text or upstream tasks makes them perform poorly on text actually generated by language models, especially for general evaluation for various tasks. To alleviate this problem, we propose a weakly supervised framework named WeCheck that is directly trained on actual generated samples from language models with weakly annotated labels.WeCheck first utilizes a generative model to infer the factual labels of generated samples by aggregating weak labels from multiple resources.Next, we train a simple noise-aware classification model as the target metric using the inferred weakly supervised information.Comprehensive experiments on various tasks demonstrate the strong performance of WeCheck, achieving an average absolute improvement of 3.3% on the TRUE benchmark over 11B state-of-the-art methods using only 435M parameters.Furthermore, it is up to 30 times faster than previous evaluation methods, greatly improving the accuracy and efficiency of factual consistency evaluation.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.18.pdf",
        "keywords": [
            "factual consistency",
            "weakly supervised",
            "inferred weakly supervised",
            "weakly supervised learning",
            "text generation",
            "synthetic text",
            "natural language inference",
            "wecheck"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"A crucial issue of current text generation models is that they often uncontrollably generate text that is factually inconsistent with inputs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"A crucial issue of current text generation models is that they often uncontrollably generate text that is factually inconsistent with inputs.\""
    },
    {
        "title": "Text Adversarial Purification as Defense against Adversarial Attacks",
        "authors": [
            "Linyang Li",
            "Demin Song",
            "Xipeng Qiu"
        ],
        "published": "2023",
        "summary": "Adversarial purification is a successful defense mechanism against adversarial attacks without requiring knowledge of the form of the incoming attack. Generally, adversarial purification aims to remove the adversarial perturbations therefore can make correct predictions based on the recovered clean samples. Despite the success of adversarial purification in the computer vision field that incorporates generative models such as energy-based models and diffusion models,using purification as a defense strategy against textual adversarial attacks is rarely explored. In this work, we introduce a novel adversarial purification method that focuses on defending against textual adversarial attacks. With the help of language models, we can inject noise by masking input texts and reconstructing the masked texts based on the masked language models. In this way, we construct an adversarial purification process for textual models against the most widely used word-substitution adversarial attacks. We test our proposed adversarial purification method on several strong adversarial attack methods including Textfooler and BERT-Attack and experimental results indicate that the purification algorithm can successfully defend against strong word-substitution attacks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.20.pdf",
        "keywords": [
            "adversarial purification",
            "adversarial attack",
            "word substitution adversarial attacks",
            "word substitution attacks"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"With the help of language models, we can inject noise by masking input texts and reconstructing the masked texts based on the masked language models.\"\n\nThis abstract mentions the use of language models but does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"With the help of language models, we can inject noise by masking input texts and reconstructing the masked texts based on the masked language models.\"\n\nThis abstract mentions the use of language models but does not discuss any limitations of LLMs."
    },
    {
        "title": "Tailor: A Soft-Prompt-Based Approach to Attribute-Based Controlled Text Generation",
        "authors": [
            "Kexin Yang",
            "Dayiheng Liu",
            "Wenqiang Lei",
            "Baosong Yang",
            "Mingfeng Xue",
            "Boxing Chen",
            "Jun Xie"
        ],
        "published": "2023",
        "summary": "Attribute-based Controlled Text Generation (CTG) refers to generating sentences that satisfy desirable attributes (e.g., emotions and topics). Existing work usually utilize fine-tuning or resort to extra attribute classifiers, yet suffer from increases in storage and inference time. To address these concerns, we explore attribute-based CTG in a parameter-efficient manner. In short, the proposed Tailor represents each attribute as a pre-trained continuous vector i.e., single-attribute prompt), which guides the generation of a fixed pre-trained language model (PLM) to satisfy a pre-specified attribute. These prompts can be simply concatenated as a whole for multi-attribute CTG without any re-training. Nevertheless, this may raise problems of fluency downgrading and position sensitivity. To solve this, Tailor provides two solutions to enhance the combination. The former contains a multi-attribute prompt mask and a re-indexing position sequence to bridge the gap between the training (one single-attribute prompt for each task) and the testing stage (concatenating two prompts). The latter introduces a trainable prompt connector to further enhance the combinations. Experiments demonstrate that, only requiring 0.08% extra training parameters of the GPT-2, Tailor can achieve effective and general improvements on eleven attribute-specific generation tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.25.pdf",
        "keywords": [
            "attribute specific generation",
            "tailor",
            "parameter efficient"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Nevertheless, this may raise problems of fluency downgrading and position sensitivity.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Nevertheless, this may raise problems of fluency downgrading and position sensitivity.\""
    },
    {
        "title": "Knowledge of cultural moral norms in large language models",
        "authors": [
            "Aida Ramezani",
            "Yang Xu"
        ],
        "published": "2023",
        "summary": "Moral norms vary across cultures. A recent line of work suggests that English large language models contain human-like moral biases, but these studies typically do not examine moral variation in a diverse cultural setting. We investigate the extent to which monolingual English language models contain knowledge about moral norms in different countries. We consider two levels of analysis: 1) whether language models capture fine-grained moral variation across countries over a variety of topics such as “homosexuality” and “divorce”; 2) whether language models capture cultural diversity and shared tendencies in which topics people around the globe tend to diverge or agree on in their moral judgment. We perform our analyses with two public datasets from the World Values Survey (across 55 countries) and PEW global surveys (across 40 countries) on morality. We find that pre-trained English language models predict empirical moral norms across countries worse than the English moral norms reported previously. However, fine-tuning language models on the survey data improves inference across countries at the expense of a less accurate estimate of the English moral norms. We discuss the relevance and challenges of incorporating cultural knowledge into the automated inference of moral norms.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.26.pdf",
        "keywords": [
            "moral norms",
            "cultural moral norms",
            "morality",
            "language",
            "language models",
            "large language models",
            "cultural knowledge",
            "moral variation",
            "shared tendencies",
            "diverge"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that pre-trained English language models predict empirical moral norms across countries worse than the English moral norms reported previously.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We find that pre-trained English language models predict empirical moral norms across countries worse than the English moral norms reported previously.\""
    },
    {
        "title": "Revealing Single Frame Bias for Video-and-Language Learning",
        "authors": [
            "Jie Lei",
            "Tamara Berg",
            "Mohit Bansal"
        ],
        "published": "2023",
        "summary": "Training an effective video-and-language model intuitively requires multiple frames as model inputs. However, it is unclear whether using multiple frames is beneficial to downstream tasks, and if yes, whether the performance gain is worth the drastically-increased computation and memory costs resulting from using more frames. In this work, we explore single-frame models for video-and-language learning. On a diverse set of video-and-language tasks (including text-to-video retrieval and video question answering), we show the surprising result that, with large-scale pre-training and a proper frame ensemble strategy at inference time, a single-frame trained model that does not consider temporal information can achieve better performance than existing methods that use multiple frames for training. This result reveals the existence of a strong “static appearance bias” in popular video-and-language datasets. Therefore, to allow for a more comprehensive evaluation of video-and-language models, we propose two new retrieval tasks based on existing fine-grained action recognition datasets that encourage temporal modeling. Our code is available at https://github.com/jayleicn/singularity.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.29.pdf",
        "keywords": [
            "action recognition",
            "video",
            "temporal modeling",
            "bias",
            "language models",
            "language",
            "appearance bias",
            "language learning",
            "text to video"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it is unclear whether using multiple frames is beneficial to downstream tasks, and if yes, whether the performance gain is worth the drastically-increased computation and memory costs resulting from using more frames.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, it is unclear whether using multiple frames is beneficial to downstream tasks, and if yes, whether the performance gain is worth the drastically-increased computation and memory costs resulting from using more frames.\""
    },
    {
        "title": "World-to-Words: Grounded Open Vocabulary Acquisition through Fast Mapping in Vision-Language Models",
        "authors": [
            "Ziqiao Ma",
            "Jiayi Pan",
            "Joyce Chai"
        ],
        "published": "2023",
        "summary": "The ability to connect language units to their referents in the physical world, referred to as grounding, is crucial to learning and understanding grounded meanings of words. While humans demonstrate fast mapping in new word learning, it remains unclear whether modern vision-language models can truly represent language with their grounded meanings, and how grounding may further bootstrap new word learning. To this end, we introduce Grounded Open Vocabulary Acquisition (GOVA) to examine grounding and bootstrapping in open-world language learning. As an initial attempt, we propose World-to-Words (W2W), a novel visually-grounded language model by pre-training on image-text pairs highlighting grounding as an objective. Through extensive experiments and analysis, we demonstrate that W2W is a more coherent and fast grounded word learner, and that the grounding ability acquired during pre-training helps the model to learn unseen words more rapidly and robustly.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.31.pdf",
        "keywords": [
            "grounding",
            "grounded meanings",
            "grounded language model",
            "grounded open vocabulary acquisition",
            "open vocabulary",
            "world to words",
            "fast grounded word",
            "vision language models",
            "language units",
            "bootstrapping"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations of LLMs are mentioned, but the paper implies that modern vision-language models may struggle with truly representing language with grounded meanings.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations of LLMs are mentioned, but the paper implies that modern vision-language models may struggle with truly representing language with grounded meanings."
    },
    {
        "title": "A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models",
        "authors": [
            "Alessandro Stolfo",
            "Zhijing Jin",
            "Kumar Shridhar",
            "Bernhard Schoelkopf",
            "Mrinmaya Sachan"
        ],
        "published": "2023",
        "summary": "We have recently witnessed a number of impressive results on hard mathematical reasoning problems with language models. At the same time, the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution. Building on the idea of behavioral testing, we propose a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands, and math operators on the output solution. By grounding the behavioral analysis in a causal graph describing an intuitive reasoning process, we study the behavior of language models in terms of robustness and sensitivity to direct interventions in the input space. We apply our framework on a test bed of math word problems. Our analysis shows that robustness does not appear to continuously improve as a function of size, but the GPT-3 Davinci models (175B) achieve a dramatic improvement in both robustness and sensitivity compared to all other GPT variants.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.32.pdf",
        "keywords": [
            "robustness",
            "reasoning",
            "mathematical reasoning",
            "language models",
            "causal framework",
            "behavioral testing",
            "behavioral analysis",
            "effect"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution.\""
    },
    {
        "title": "Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions",
        "authors": [
            "John Chung",
            "Ece Kamar",
            "Saleema Amershi"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) can be used to generate text data for training and evaluating other models. However, creating high-quality datasets with LLMs can be challenging. In this work, we explore human-AI partnerships to facilitate high diversity and accuracy in LLM-based text data generation. We first examine two approaches to diversify text generation: 1) logit suppression, which minimizes the generation of languages that have already been frequently generated, and 2) temperature sampling, which flattens the token sampling probability. We found that diversification approaches can increase data diversity but often at the cost of data accuracy (i.e., text and labels being appropriate for the target domain). To address this issue, we examined two human interventions, 1) label replacement (LR), correcting misaligned labels, and 2) out-of-scope filtering (OOSF), removing instances that are out of the user’s domain of interest or to which no considered label applies. With oracle studies, we found that LR increases the absolute accuracy of models trained with diversified datasets by 14.4%. Moreover, we found that some models trained with data generated with LR interventions outperformed LLM-based few-shot classification. In contrast, OOSF was not effective in increasing model accuracy, implying the need for future work in human-in-the-loop text data generation.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.34.pdf",
        "keywords": [
            "generation",
            "data generated",
            "text data generation",
            "generate text",
            "text generation",
            "text",
            "data diversity",
            "diversity",
            "language models",
            "large language models",
            "diversification"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, creating high-quality datasets with LLMs can be challenging... diversification approaches can increase data diversity but often at the cost of data accuracy (i.e., text and labels being appropriate for the target domain).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, creating high-quality datasets with LLMs can be challenging... diversification approaches can increase data diversity but often at the cost of data accuracy (i.e., text and labels being appropriate for the target domain).\""
    },
    {
        "title": "Pruning Pre-trained Language Models Without Fine-Tuning",
        "authors": [
            "Ting Jiang",
            "Deqing Wang",
            "Fuzhen Zhuang",
            "Ruobing Xie",
            "Feng Xia"
        ],
        "published": "2023",
        "summary": "To overcome the overparameterized problem in Pre-trained Language Models (PLMs), pruning is widely used as a simple and straightforward compression method by directly removing unimportant weights. Previous first-order methods successfully compress PLMs to extremely high sparsity with little performance drop. These methods, such as movement pruning, use first-order information to prune PLMs while fine-tuning the remaining weights. In this work, we argue fine-tuning is redundant for first-order pruning, since first-order pruning is sufficient to converge PLMs to downstream tasks without fine-tuning. Under this motivation, we propose Static Model Pruning (SMP), which only uses first-order pruning to adapt PLMs to downstream tasks while achieving the target sparsity level. In addition, we also design a new masking function and training objective to further improve SMP. Extensive experiments at various sparsity levels show SMP has significant improvements over first-order and zero-order methods. Unlike previous first-order methods, SMP is also applicable to low sparsity and outperforms zero-order methods. Meanwhile, SMP is more parameter efficient than other methods due to it does not require fine-tuning.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.35.pdf",
        "keywords": [
            "pruning",
            "pre trained language models",
            "masking",
            "static model pruning",
            "fine tuning",
            "sparsity"
        ],
        "Rate_Llama-3.1-70b": "not extracted",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"To overcome the overparameterized problem in Pre-trained Language Models (PLMs)\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs:  \nEvidence: \"To overcome the overparameterized problem in Pre-trained Language Models (PLMs)\""
    },
    {
        "title": "Do Androids Laugh at Electric Sheep? Humor “Understanding” Benchmarks from The New Yorker Caption Contest",
        "authors": [
            "Jack Hessel",
            "Ana Marasovic",
            "Jena D. Hwang",
            "Lillian Lee",
            "Jeff Da",
            "Rowan Zellers",
            "Robert Mankoff",
            "Yejin Choi"
        ],
        "published": "2023",
        "summary": "Large neural networks can now generate jokes, but do they really “understand” humor? We challenge AI models with three tasks derived from the New Yorker Cartoon Caption Contest: matching a joke to a cartoon, identifying a winning caption, and explaining why a winning caption is funny. These tasks encapsulate progressively more sophisticated aspects of “understanding” a cartoon; key elements are the complex, often surprising relationships between images and captions and the frequent inclusion of indirect and playful allusions to human experience and culture. We investigate both multimodal and language-only models: the former are challenged with the cartoon images directly, while the latter are given multifaceted descriptions of the visual scene to simulate human-level visual understanding. We find that both types of models struggle at all three tasks. For example, our best multimodal models fall 30 accuracy points behind human performance on the matching task, and, even when provided ground-truth visual scene descriptors, human-authored explanations are preferred head-to-head over the best machine-authored ones (few-shot GPT-4) in more than 2/3 of cases. We release models, code, leaderboard, and corpus, which includes newly-gathered annotations describing the image’s locations/entities, what’s unusual in the scene, and an explanation of the joke.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.41.pdf",
        "keywords": [
            "cartoon caption contest",
            "cartoon",
            "benchmarks",
            "new yorker caption contest",
            "neural networks",
            "electric",
            "humor",
            "androids laugh",
            "visual scene descriptors",
            "language"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that both types of models struggle at all three tasks. For example, our best multimodal models fall 30 accuracy points behind human performance on the matching task, and, even when provided ground-truth visual scene descriptors, human-authored explanations are preferred head-to-head over the best machine-authored ones (few-shot GPT-4) in more than 2/3 of",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We find that both types of models struggle at all three tasks. For example, our best multimodal models fall 30 accuracy points behind human performance on the matching task, and, even when provided ground-truth visual scene descriptors, human-authored explanations are preferred head-to-head over the best machine-authored ones (few-shot GPT-4) in more than 2/3 of"
    },
    {
        "title": "CLCL: Non-compositional Expression Detection with Contrastive Learning and Curriculum Learning",
        "authors": [
            "Jianing Zhou",
            "Ziheng Zeng",
            "Suma Bhat"
        ],
        "published": "2023",
        "summary": "Non-compositional expressions present a substantial challenge for natural language processing (NLP) systems, necessitating more intricate processing compared to general language tasks, even with large pre-trained language models. Their non-compositional nature and limited availability of data resources further compound the difficulties in accurately learning their representations. This paper addresses both of these challenges. By leveraging contrastive learning techniques to build improved representations it tackles the non-compositionality challenge. Additionally, we propose a dynamic curriculum learning framework specifically designed to take advantage of the scarce available data for modeling non-compositionality. Our framework employs an easy-to-hard learning strategy, progressively optimizing the model’s performance by effectively utilizing available training data. Moreover, we integrate contrastive learning into the curriculum learning approach to maximize its benefits. Experimental results demonstrate the gradual improvement in the model’s performance on idiom usage recognition and metaphor detection tasks. Our evaluation encompasses six datasets, consistently affirming the effectiveness of the proposed framework. Our models available at https://github.com/zhjjn/CLCL.git.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.43.pdf",
        "keywords": [
            "non compositional expression detection",
            "non compositionality",
            "curriculum learning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"even with large pre-trained language models...Their non-compositional nature and limited availability of data resources further compound the difficulties in accurately learning their representations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"even with large pre-trained language models...Their non-compositional nature and limited availability of data resources further compound the difficulties in accurately learning their representations.\""
    },
    {
        "title": "Self-Edit: Fault-Aware Code Editor for Code Generation",
        "authors": [
            "Kechi Zhang",
            "Zhuo Li",
            "Jia Li",
            "Ge Li",
            "Zhi Jin"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have demonstrated an impressive ability to generate codes on competitive programming tasks. However, with limited sample numbers, LLMs still suffer from poor accuracy. Inspired by the process of human programming, we propose a generate-and-edit approach named Self-Edit that utilizes execution results of the generated code from LLMs to improve the code quality on the competitive programming task. We execute the generated code on the example test case provided in the question and wrap execution results into a supplementary comment. Utilizing this comment as guidance, our fault-aware code editor is employed to correct errors in the generated code. We perform extensive evaluations across two competitive programming datasets with nine different LLMs. Compared to directly generating from LLMs, our approach can improve the average of pass@1 by 89% on APPS-dev, 31% on APPS-test, and 48% on HumanEval over nine popular code generation LLMs with parameter sizes ranging from 110M to 175B. Compared to other post-processing methods, our method demonstrates superior accuracy and efficiency.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.45.pdf",
        "keywords": [
            "code generation",
            "fault aware code editor",
            "self edit",
            "code quality",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, with limited sample numbers, LLMs still suffer from poor accuracy.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (poor accuracy with limited sample numbers), but it is not the primary focus of the paper and is used to justify the proposed approach.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, with limited sample numbers, LLMs still suffer from poor accuracy.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (poor accuracy with limited sample numbers), but it is not the primary focus of the paper and is used to justify the proposed approach."
    },
    {
        "title": "How About Kind of Generating Hedges using End-to-End Neural Models?",
        "authors": [
            "Alafate Abulimiti",
            "Chloé Clavel",
            "Justine Cassell"
        ],
        "published": "2023",
        "summary": "Hedging is a strategy for softening the impact of a statement in conversation. In reducing the strength of an expression, it may help to avoid embarrassment (more technically, “face threat”) to one’s listener. For this reason, it is often found in contexts of instruction, such as tutoring. In this work, we develop a model of hedge generation based on i) fine-tuning state-of-the-art language models trained on human-human tutoring data, followed by ii) reranking to select the candidate that best matches the expected hedging strategy within a candidate pool using a hedge classifier. We apply this method to a natural peer-tutoring corpus containing a significant number of disfluencies, repetitions, and repairs. The results show that generation in this noisy environment is feasible with reranking. By conducting an error analysis for both approaches, we reveal the challenges faced by systems attempting to accomplish both social and task-oriented goals in conversation.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.50.pdf",
        "keywords": [
            "hedge classifier",
            "generating hedges",
            "hedge generation",
            "hedging strategy",
            "models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"By conducting an error analysis for both approaches, we reveal the challenges faced by systems attempting to accomplish both social and task-oriented goals in conversation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"By conducting an error analysis for both approaches, we reveal the challenges faced by systems attempting to accomplish both social and task-oriented goals in conversation.\""
    },
    {
        "title": "Does GPT-3 Grasp Metaphors? Identifying Metaphor Mappings with Generative Language Models",
        "authors": [
            "Lennart Wachowiak",
            "Dagmar Gromann"
        ],
        "published": "2023",
        "summary": "Conceptual metaphors present a powerful cognitive vehicle to transfer knowledge structures from a source to a target domain. Prior neural approaches focus on detecting whether natural language sequences are metaphoric or literal. We believe that to truly probe metaphoric knowledge in pre-trained language models, their capability to detect this transfer should be investigated. To this end, this paper proposes to probe the ability of GPT-3 to detect metaphoric language and predict the metaphor’s source domain without any pre-set domains. We experiment with different training sample configurations for fine-tuning and few-shot prompting on two distinct datasets. When provided 12 few-shot samples in the prompt, GPT-3 generates the correct source domain for a new sample with an accuracy of 65.15% in English and 34.65% in Spanish. GPT’s most common error is a hallucinated source domain for which no indicator is present in the sentence. Other common errors include identifying a sequence as literal even though a metaphor is present and predicting the wrong source domain based on specific words in the sequence that are not metaphorically related to the target domain.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.58.pdf",
        "keywords": [
            "grasp metaphors",
            "metaphor mappings",
            "language",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"GPT’s most common error is a hallucinated source domain for which no indicator is present in the sentence. Other common errors include identifying a sequence as literal even though a metaphor is present and predicting the wrong source domain based on specific words in the sequence that are not metaphorically related to the target domain.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"GPT’s most common error is a hallucinated source domain for which no indicator is present in the sentence. Other common errors include identifying a sequence as literal even though a metaphor is present and predicting the wrong source domain based on specific words in the sequence that are not metaphorically related to the target domain.\""
    },
    {
        "title": "ALERT: Adapt Language Models to Reasoning Tasks",
        "authors": [
            "Ping Yu",
            "Tianlu Wang",
            "Olga Golovneva",
            "Badr AlKhamissi",
            "Siddharth Verma",
            "Zhijing Jin",
            "Gargi Ghosh",
            "Mona Diab",
            "Asli Celikyilmaz"
        ],
        "published": "2023",
        "summary": "Recent advancements in large language models have enabled them to perform well on complex tasks that require step-by-step reasoning with few-shot learning. However, it is unclear whether these models are applying reasoning skills they have learnt during pre-training , or if they are simply memorizing their training corpus at finer granularity and have learnt to better understand their context. To address this question, we introduce {pasted macro ‘OUR’}model, a benchmark and suite of analyses for evaluating reasoning skills of language models. {pasted macro ‘OUR’}model enables comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. Our benchmark provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. By using {pasted macro ‘OUR’}model we further investigate the role of finetuning. Our extensive empirical analysis shows that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during the finetuning stage compared to pretraining stage. However, we also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.60.pdf",
        "keywords": [
            "adapt language models",
            "language models",
            "models",
            "analogical reasoning",
            "abductive",
            "language models learn"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it is unclear whether these models are applying reasoning skills they have learnt during pre-training, or if they are simply memorizing their training corpus at finer granularity and have learnt to better understand their context.\"; \"However, we also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, it is unclear whether these models are applying reasoning skills they have learnt during pre-training, or if they are simply memorizing their training corpus at finer granularity and have learnt to better understand their context.\"; \"However, we also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems"
    },
    {
        "title": "Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages",
        "authors": [
            "Ayyoob Imani",
            "Peiqin Lin",
            "Amir Hossein Kargaran",
            "Silvia Severini",
            "Masoud Jalili Sabet",
            "Nora Kassner",
            "Chunlan Ma",
            "Helmut Schmid",
            "André Martins",
            "François Yvon",
            "Hinrich Schütze"
        ],
        "published": "2023",
        "summary": "The NLP community has mainly focused on scaling Large Language Models (LLMs) vertically, i.e., making them better for about 100 languages. We instead scale LLMs horizontally: we create, through continued pretraining, Glot500-m, an LLM that covers 511 predominantly low-resource languages. An important part of this effort is to collect and clean Glot500-c, a corpus that covers these 511 languages and allows us to train Glot500-m. We evaluate Glot500-m on five diverse tasks across these languages. We observe large improvements for both high-resource and low-resource languages compared to an XLM-R baseline. Our analysis shows that no single factor explains the quality of multilingual LLM representations. Rather, a combination of factors determines quality including corpus size, script, “help” from related languages and the total capacity of the model. Our work addresses an important goal of NLP research: we should notlimit NLP to a small fraction of the world’s languages and instead strive to support as many languages as possible to bring the benefits of NLP technology to all languages and cultures. Code, data and models are available at https://github.com/cisnlp/Glot500.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.61.pdf",
        "keywords": [
            "corpus",
            "glot500",
            "languages",
            "language models",
            "corpora",
            "nlp",
            "resource languages",
            "s languages"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Rather, a combination of factors determines quality including corpus size, script, “help” from related languages and the total capacity of the model.\"\n\nNote that the abstract mentions a limitation of LLMs in the sense that a single factor does not explain the quality of multilingual LLM representations, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Rather, a combination of factors determines quality including corpus size, script, “help” from related languages and the total capacity of the model.\"\n\nNote that the abstract mentions a limitation of LLMs in the sense that a single factor does not explain the quality of multilingual LLM representations, but it is not the primary focus of the paper."
    },
    {
        "title": "Pretrained Bidirectional Distillation for Machine Translation",
        "authors": [
            "Yimeng Zhuang",
            "Mei Tu"
        ],
        "published": "2023",
        "summary": "Knowledge transfer can boost neural machine translation (NMT), for example, by finetuning a pretrained masked language model (LM). However, it may suffer from the forgetting problem and the structural inconsistency between pretrained LMs and NMT models. Knowledge distillation (KD) may be a potential solution to alleviate these issues, but few studies have investigated language knowledge transfer from pretrained language models to NMT models through KD. In this paper, we propose Pretrained Bidirectional Distillation (PBD) for NMT, which aims to efficiently transfer bidirectional language knowledge from masked language pretraining to NMT models. Its advantages are reflected in efficiency and effectiveness through a globally defined and bidirectional context-aware distillation objective. Bidirectional language knowledge of the entire sequence is transferred to an NMT model concurrently during translation training. Specifically, we propose self-distilled masked language pretraining to obtain the PBD objective. We also design PBD losses to efficiently distill the language knowledge, in the form of token probabilities, to the encoder and decoder of an NMT model using the PBD objective. Extensive experiments reveal that pretrained bidirectional distillation can significantly improve machine translation performance and achieve competitive or even better results than previous pretrain-finetune or unified multilingual translation methods in supervised, unsupervised, and zero-shot scenarios. Empirically, it is concluded that pretrained bidirectional distillation is an effective and efficient method for transferring language knowledge from pretrained language models to NMT models.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.63.pdf",
        "keywords": [
            "knowledge distillation",
            "machine translation",
            "knowledge transfer",
            "bidirectional distillation",
            "neural machine translation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it may suffer from the forgetting problem and the structural inconsistency between pretrained LMs and NMT models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, it may suffer from the forgetting problem and the structural inconsistency between pretrained LMs and NMT models.\""
    },
    {
        "title": "ThinkSum: Probabilistic reasoning over sets using large language models",
        "authors": [
            "Batu Ozturkler",
            "Nikolay Malkin",
            "Zhen Wang",
            "Nebojsa Jojic"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have a substantial capacity for high-level analogical reasoning: reproducing patterns in linear text that occur in their training data (zero-shot evaluation) or in the provided context (few-shot in-context learning). However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions. We propose a two-stage probabilistic inference paradigm, ThinkSum, which reasons over sets of objects or facts in a structured manner. In the first stage (Think – retrieval of associations), a LLM is queried in parallel over a set of phrases extracted from the prompt or an auxiliary model call. In the second stage (Sum – probabilistic inference or reasoning), the results of these queries are aggregated to make the final prediction. We demonstrate the possibilities and advantages of ThinkSum on the BIG-bench suite of LLM evaluation tasks, achieving improvements over the state of the art using GPT-family models on thirteen difficult tasks, often with far smaller model variants. We also compare and contrast ThinkSum with other proposed modifications to direct prompting of LLMs, such as variants of chain-of-thought prompting. Our results suggest that because the probabilistic inference in ThinkSum is performed outside of calls to the LLM, ThinkSum is less sensitive to prompt design, yields more interpretable predictions, and can be flexibly combined with latent variable models to extract structured knowledge from LLMs. Overall, our proposed paradigm represents a promising approach for enhancing the reasoning capabilities of LLMs.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.68.pdf",
        "keywords": [
            "language models",
            "models",
            "reasons",
            "probabilistic reasoning",
            "large language models",
            "probabilistic inference"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions.\""
    },
    {
        "title": "Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe",
        "authors": [
            "Xiang Yue",
            "Huseyin Inan",
            "Xuechen Li",
            "Girish Kumar",
            "Julia McAnallen",
            "Hoda Shajari",
            "Huan Sun",
            "David Levitan",
            "Robert Sim"
        ],
        "published": "2023",
        "summary": "Privacy concerns have attracted increasing attention in data-driven products due to the tendency of machine learning models to memorize sensitive training data. Generating synthetic versions of such data with a formal privacy guarantee, such as differential privacy (DP), provides a promising path to mitigating these privacy concerns, but previous approaches in this direction have typically failed to produce synthetic data of high quality. In this work, we show that a simple and practical recipe in the text domain is effective: simply fine-tuning a pretrained generative language model with DP enables the model to generate useful synthetic text with strong privacy protection. Through extensive empirical analyses on both benchmark and private customer data, we demonstrate that our method produces synthetic text that is competitive in terms of utility with its non-private counterpart, meanwhile providing strong protection against potential privacy leakages.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.74.pdf",
        "keywords": [],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation of LLMs is mentioned, but it discusses a general limitation of machine learning models to memorize sensitive training data.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation of LLMs is mentioned, but it discusses a general limitation of machine learning models to memorize sensitive training data."
    },
    {
        "title": "A Close Look into the Calibration of Pre-trained Language Models",
        "authors": [
            "Yangyi Chen",
            "Lifan Yuan",
            "Ganqu Cui",
            "Zhiyuan Liu",
            "Heng Ji"
        ],
        "published": "2023",
        "summary": "Pre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty. We take a close look into this problem, aiming to answer two questions: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? For the first question, we conduct fine-grained control experiments to study the dynamic change in PLMs’ calibration performance in training. We consider six factors as control variables, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. We observe a consistent change in calibration performance across six factors. We find that PLMs don’t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not. We highlight that our finding somewhat contradicts two established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining improves model calibration. Next, we study the effectiveness of existing calibration methods in mitigating the overconfidence issue. Besides unlearnable calibration methods (e.g., label smoothing), we adapt and extend two recently proposed learnable methods that directly collect data to train models to have reasonable confidence estimations. Experimental results show that learnable methods significantly reduce PLMs’ confidence in wrong predictions.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.75.pdf",
        "keywords": [
            "calibrated",
            "language models",
            "trained language models",
            "pretraining"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Pre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty... We find that PLMs don’t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Pre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty... We find that PLMs don’t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not.\""
    },
    {
        "title": "Diverse Demonstrations Improve In-context Compositional Generalization",
        "authors": [
            "Itay Levy",
            "Ben Bogin",
            "Jonathan Berant"
        ],
        "published": "2023",
        "summary": "In-context learning has shown great success in i.i.d semantic parsing splits, where the training and test sets are drawn from the same distribution. In this setup, models are typically prompted with demonstrations that are similar to the input utterance. However, in the setup of compositional generalization, where models are tested on outputs with structures that are absent from the training set, selecting similar demonstrations is insufficient, as often no example will be similar enough to the input. In this work, we propose a method to select diverse demonstrations that aims to collectively cover all of the structures required in the output program, in order to encourage the model to generalize to new structures from these demonstrations. We empirically show that combining diverse demonstrations with in-context learning substantially improves performance across three compositional generalization semantic parsing datasets in the pure in-context learning setup and when combined with finetuning.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.78.pdf",
        "keywords": [
            "generalization semantic parsing"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, in the setup of compositional generalization, where models are tested on outputs with structures that are absent from the training set, selecting similar demonstrations is insufficient, as often no example will be similar enough to the input.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, in the setup of compositional generalization, where models are tested on outputs with structures that are absent from the training set, selecting similar demonstrations is insufficient, as often no example will be similar enough to the input.\""
    },
    {
        "title": "Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering",
        "authors": [
            "Zhiyong Wu",
            "Yaoxiang Wang",
            "Jiacheng Ye",
            "Lingpeng Kong"
        ],
        "published": "2023",
        "summary": "Despite the surprising few-shot performance of in-context learning (ICL), it is still a common practice to randomly sample examples to serve as context. This paper advocates a new principle for ICL: self-adaptive in-context learning. The self-adaption mechanism is introduced to help each sample find an in-context example organization (i.e., selection and permutation) that can derive the correct prediction, thus maximizing performance. To validate the effectiveness of self-adaptive ICL, we propose a general select-then-rank framework and instantiate it with new selection and ranking algorithms. Upon extensive evaluation on eight different NLP datasets, our self-adaptive ICL method achieves a 40% relative improvement over the common practice setting. Further analysis reveals the enormous potential of self-adaptive ICL that it might be able to close the gap between ICL and finetuning given more advanced algorithms. Our code will be released to facilitate future research.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.79.pdf",
        "keywords": [
            "self adaptive",
            "in context learning",
            "in context example selection",
            "information compression",
            "advanced algorithms"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Cross-Domain Data Augmentation with Domain-Adaptive Language Modeling for Aspect-Based Sentiment Analysis",
        "authors": [
            "Jianfei Yu",
            "Qiankun Zhao",
            "Rui Xia"
        ],
        "published": "2023",
        "summary": "Cross-domain Aspect-Based Sentiment Analysis (ABSA) aims to leverage the useful knowledge from a source domain to identify aspect-sentiment pairs in sentences from a target domain. To tackle the task, several recent works explore a new unsupervised domain adaptation framework, i.e., Cross-Domain Data Augmentation (CDDA), aiming to directly generate much labeled target-domain data based on the labeled source-domain data. However, these CDDA methods still suffer from several issues: 1) preserving many source-specific attributes such as syntactic structures; 2) lack of fluency and coherence; 3) limiting the diversity of generated data. To address these issues, we propose a new cross-domain Data Augmentation approach based on Domain-Adaptive Language Modeling named DA2LM, which contains three stages: 1) assigning pseudo labels to unlabeled target-domain data; 2) unifying the process of token generation and labeling with a Domain-Adaptive Language Model (DALM) to learn the shared context and annotation across domains; 3) using the trained DALM to generate labeled target-domain data. Experiments show that DA2LM consistently outperforms previous feature adaptation and CDDA methods on both ABSA and Aspect Extraction tasks. The source code is publicly released at https://github.com/NUSTM/DALM.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.81.pdf",
        "keywords": [
            "cross domain data augmentation",
            "aspect based sentiment analysis",
            "domain adaptive language modeling",
            "aspect extraction"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these CDDA methods still suffer from several issues: 1) preserving many source-specific attributes such as syntactic structures; 2) lack of fluency and coherence; 3) limiting the diversity of generated data.\"\n\nNote that the paper discusses limitations of Cross-Domain Data Augmentation (CDDA) methods, which involve language modeling, but the limitations are",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these CDDA methods still suffer from several issues: 1) preserving many source-specific attributes such as syntactic structures; 2) lack of fluency and coherence; 3) limiting the diversity of generated data.\"\n\nNote that the paper discusses limitations of Cross-Domain Data Augmentation (CDDA) methods, which involve language modeling, but the limitations are"
    },
    {
        "title": "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models",
        "authors": [
            "Myra Cheng",
            "Esin Durmus",
            "Dan Jurafsky"
        ],
        "published": "2023",
        "summary": "To recognize and mitigate harms from large language models (LLMs), we need to understand the prevalence and nuances of stereotypes in LLM outputs. Toward this end, we present Marked Personas, a prompt-based method to measure stereotypes in LLMs for intersectional demographic groups without any lexicon or data labeling. Grounded in the sociolinguistic concept of markedness (which characterizes explicitly linguistically marked categories versus unmarked defaults), our proposed method is twofold: 1) prompting an LLM to generate personas, i.e., natural language descriptions, of the target demographic group alongside personas of unmarked, default groups; 2) identifying the words that significantly distinguish personas of the target group from corresponding unmarked ones. We find that the portrayals generated by GPT-3.5 and GPT-4 contain higher rates of racial stereotypes than human-written portrayals using the same prompts. The words distinguishing personas of marked (non-white, non-male) groups reflect patterns of othering and exoticizing these demographics. An intersectional lens further reveals tropes that dominate portrayals of marginalized groups, such as tropicalism and the hypersexualization of minoritized women. These representational harms have concerning implications for downstream applications like story generation.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.84.pdf",
        "keywords": [
            "stereotypes",
            "natural language",
            "language models",
            "story generation",
            "racial stereotypes",
            "prompts"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that the portrayals generated by GPT-3.5 and GPT-4 contain higher rates of racial stereotypes than human-written portrayals using the same prompts. The words distinguishing personas of marked (non-white, non-male) groups reflect patterns of othering and exoticizing these demographics. An intersectional lens further reveals tropes that dominate portrayals of marginalized groups",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We find that the portrayals generated by GPT-3.5 and GPT-4 contain higher rates of racial stereotypes than human-written portrayals using the same prompts. The words distinguishing personas of marked (non-white, non-male) groups reflect patterns of othering and exoticizing these demographics. An intersectional lens further reveals tropes that dominate portrayals of marginalized groups"
    },
    {
        "title": "Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information",
        "authors": [
            "Sunjae Kwon",
            "Rishabh Garodia",
            "Minhwa Lee",
            "Zhichao Yang",
            "Hong Yu"
        ],
        "published": "2023",
        "summary": "Visual Word Sense Disambiguation (VWSD) is a task to find the image that most accurately depicts the correct sense of the target word for the given context. Previously, image-text matching models often suffered from recognizing polysemous words. This paper introduces an unsupervised VWSD approach that uses gloss information of an external lexical knowledge-base, especially the sense definitions. Specifically, we suggest employing Bayesian inference to incorporate the sense definitions when sense information of the answer is not provided. In addition, to ameliorate the out-of-dictionary (OOD) issue, we propose a context-aware definition generation with GPT-3. Experimental results show that the VWSD performance significantly increased with our Bayesian inference-based approach. In addition, our context-aware definition generation achieved prominent performance improvement in OOD examples exhibiting better performance than the existing definition generation method.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.88.pdf",
        "keywords": [
            "out of dictionary",
            "gloss information",
            "context aware",
            "word sense disambiguation",
            "visual word sense disambiguation",
            "bayesian inference",
            "sense definitions"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Previously, image-text matching models often suffered from recognizing polysemous words.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Previously, image-text matching models often suffered from recognizing polysemous words.\""
    },
    {
        "title": "Elaboration-Generating Commonsense Question Answering at Scale",
        "authors": [
            "Wenya Wang",
            "Vivek Srikumar",
            "Hannaneh Hajishirzi",
            "Noah A. Smith"
        ],
        "published": "2023",
        "summary": "In question answering requiring common sense, language models (e.g., GPT-3) have been used to generate text expressing background knowledge that helps improve performance. Yet the cost of working with such models is very high; in this work, we finetune smaller language models to generate useful intermediate context, referred to here as elaborations. Our framework alternates between updating two language models—an elaboration generator and an answer predictor—allowing each to influence the other. Using less than 0.5% of the parameters of GPT-3, our model outperforms alternatives with similar sizes and closes the gap with GPT-3 on four commonsense question answering benchmarks. Human evaluations show that the quality of the generated elaborations is high.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.90.pdf",
        "keywords": [
            "commonsense question answering",
            "commonsense question",
            "generator",
            "language models",
            "influence"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Yet the cost of working with such models is very high;\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Yet the cost of working with such models is very high;\""
    },
    {
        "title": "Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces of Pre-trained Language Models",
        "authors": [
            "Zhong Zhang",
            "Bang Liu",
            "Junming Shao"
        ],
        "published": "2023",
        "summary": "Pre-trained language models (PLMs) are known to be overly parameterized and have significant redundancy, indicating a small degree of freedom of the PLMs. Motivated by the observation, in this paper, we study the problem of re-parameterizing and fine-tuning PLMs from a new perspective: Discovery of intrinsic task-specific subspace. Specifically, by exploiting the dynamics of the fine-tuning process for a given task, the parameter optimization trajectory is learned to uncover its intrinsic task-specific subspace. A key finding is that PLMs can be effectively fine-tuned in the subspace with a small number of free parameters. Beyond, we observe some outlier dimensions emerging during fine-tuning in the subspace. Disabling these dimensions degrades the model performance significantly. This suggests that these dimensions are crucial to induce task-specific knowledge to downstream tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.95.pdf",
        "keywords": [
            "pre trained language models",
            "outlier dimensions"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Pre-trained language models (PLMs) are known to be overly parameterized and have significant redundancy, indicating a small degree of freedom of the PLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Pre-trained language models (PLMs) are known to be overly parameterized and have significant redundancy, indicating a small degree of freedom of the PLMs.\""
    },
    {
        "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels",
        "authors": [
            "Luyu Gao",
            "Xueguang Ma",
            "Jimmy Lin",
            "Jamie Callan"
        ],
        "published": "2023",
        "summary": "While dense retrieval has been shown to be effective and efficient across tasks and languages, it remains difficult to create effective fully zero-shot dense retrieval systems when no relevance labels are available. In this paper, we recognize the difficulty of zero-shot learning and encoding relevance. Instead, we propose to pivot through Hypothetical Document Embeddings (HyDE). Given a query, HyDE first zero-shot prompts an instruction-following language model (e.g., InstructGPT) to generate a hypothetical document. The document captures relevance patterns but is “fake” and may contain hallucinations. Then, an unsupervised contrastively learned encoder (e.g., Contriever) encodes the document into an embedding vector. This vector identifies a neighborhood in the corpus embedding space, from which similar real documents are retrieved based on vector similarity. This second step grounds the generated document to the actual corpus, with the encoder’s dense bottleneck filtering out the hallucinations. Our experiments show that HyDE significantly outperforms the state-of-the-art unsupervised dense retriever Contriever and shows strong performance comparable to fine-tuned retrievers across various tasks (e.g. web search, QA, fact verification) and in non-English languages (e.g., sw, ko, ja, bn).",
        "pdf_link": "https://aclanthology.org/2023.acl-long.99.pdf",
        "keywords": [
            "dense retrieval",
            "relevance",
            "vector similarity",
            "zero shot learning"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Given a query, HyDE first zero-shot prompts an instruction-following language model (e.g., InstructGPT) to generate a hypothetical document. The document captures relevance patterns but is “fake” and may contain hallucinations.\"\n\nThis paper mentions a limitation of LLMs (hallucinations) but does not explore it in depth.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Given a query, HyDE first zero-shot prompts an instruction-following language model (e.g., InstructGPT) to generate a hypothetical document. The document captures relevance patterns but is “fake” and may contain hallucinations.\"\n\nThis paper mentions a limitation of LLMs (hallucinations) but does not explore it in depth."
    },
    {
        "title": "White-Box Multi-Objective Adversarial Attack on Dialogue Generation",
        "authors": [
            "Yufei Li",
            "Zexin Li",
            "Yingfan Gao",
            "Cong Liu"
        ],
        "published": "2023",
        "summary": "Pre-trained transformers are popular in state-of-the-art dialogue generation (DG) systems. Such language models are, however, vulnerable to various adversarial samples as studied in traditional tasks such as text classification, which inspires our curiosity about their robustness in DG systems. One main challenge of attacking DG models is that perturbations on the current sentence can hardly degrade the response accuracy because the unchanged chat histories are also considered for decision-making. Instead of merely pursuing pitfalls of performance metrics such as BLEU, ROUGE, we observe that crafting adversarial samples to force longer generation outputs benefits attack effectiveness—the generated responses are typically irrelevant, lengthy, and repetitive. To this end, we propose a white-box multi-objective attack method called DGSlow. Specifically, DGSlow balances two objectives—generation accuracy and length, via a gradient-based multi-objective optimizer and applies an adaptive searching mechanism to iteratively craft adversarial samples with only a few modifications. Comprehensive experiments on four benchmark datasets demonstrate that DGSlow could significantly degrade state-of-the-art DG models with a higher success rate than traditional accuracy-based methods. Besides, our crafted sentences also exhibit strong transferability in attacking other models.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.100.pdf",
        "keywords": [
            "dialogue generation",
            "decision making",
            "multi objective adversarial attack",
            "adaptive searching"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Such language models are, however, vulnerable to various adversarial samples... One main challenge of attacking DG models is that perturbations on the current sentence can hardly degrade the response accuracy because the unchanged chat histories are also considered for decision-making.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Such language models are, however, vulnerable to various adversarial samples... One main challenge of attacking DG models is that perturbations on the current sentence can hardly degrade the response accuracy because the unchanged chat histories are also considered for decision-making.\""
    },
    {
        "title": "Few-shot Adaptation Works with UnpredicTable Data",
        "authors": [
            "Jun Shern Chan",
            "Michael Pieler",
            "Jonathan Jao",
            "Jérémy Scheurer",
            "Ethan Perez"
        ],
        "published": "2023",
        "summary": "Prior work on language models (LMs) shows that training on a large number of diverse tasks improves few-shot learning (FSL) performance on new tasks. We take this to the extreme, automatically extracting 413,299 tasks from internet tables - orders of magnitude more than the next-largest public datasets. Finetuning on the resulting dataset leads to improved FSL performance on Natural Language Processing (NLP) tasks, but not proportionally to dataset scale. In fact, we find that narrow subsets of our dataset sometimes outperform more diverse datasets. For example, finetuning on software documentation from support.google.com raises FSL performance by a mean of +7.5% on 52 downstream tasks, which beats training on 40 human-curated NLP datasets (+6.7%). Finetuning on various narrow datasets leads to similar broad improvements across test tasks, suggesting that the gains are not from domain adaptation but adapting to FSL in general. We do not observe clear patterns between the datasets that lead to FSL gains, leaving open questions about why certain data helps with FSL.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.102.pdf",
        "keywords": [
            "finetuning",
            "natural language processing",
            "language models",
            "domain adaptation",
            "shot learning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Finetuning on the resulting dataset leads to improved FSL performance on Natural Language Processing (NLP) tasks, but not proportionally to dataset scale.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Finetuning on the resulting dataset leads to improved FSL performance on Natural Language Processing (NLP) tasks, but not proportionally to dataset scale.\""
    },
    {
        "title": "Do language models have coherent mental models of everyday things?",
        "authors": [
            "Yuling Gu",
            "Bhavana Dalvi Mishra",
            "Peter Clark"
        ],
        "published": "2023",
        "summary": "When people think of everyday things like an egg, they typically have a mental image associated with it. This allows them to correctly judge, for example, that “the yolk surrounds the shell” is a false statement. Do language models similarly have a coherent picture of such everyday things? To investigate this, we propose a benchmark dataset consisting of 100 everyday things, their parts, and the relationships between these parts, expressed as 11,720 “X relation Y?” true/false questions. Using these questions as probes, we observe that state-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw have fragments of knowledge about these everyday things, but do not have fully coherent “parts mental models” (54-59% accurate, 19-43% conditional constraint violation). We propose an extension where we add a constraint satisfaction layer on top of the LM’s raw predictions to apply commonsense constraints. As well as removing inconsistencies, we find that this also significantly improves accuracy (by 16-20%), suggesting how the incoherence of the LM’s pictures of everyday things can be significantly reduced.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.106.pdf",
        "keywords": [
            "mental models",
            "everyday things",
            "language models",
            "mental image",
            "commonsense constraints",
            "accuracy",
            "pre trained language models"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"we observe that state-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw have fragments of knowledge about these everyday things, but do not have fully coherent “parts mental models” (54-59% accurate, 19-43% conditional constraint violation).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"we observe that state-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw have fragments of knowledge about these everyday things, but do not have fully coherent “parts mental models” (54-59% accurate, 19-43% conditional constraint violation).\""
    },
    {
        "title": "Instruction Induction: From Few Examples to Natural Language Task Descriptions",
        "authors": [
            "Or Honovich",
            "Uri Shaham",
            "Samuel R. Bowman",
            "Omer Levy"
        ],
        "published": "2023",
        "summary": "Large language models are able to perform a task by conditioning on a few input-output demonstrations - a paradigm known as in-context learning. We show that language models can explicitly infer an underlying task from a few demonstrations by prompting them to generate a natural language instruction that fits the examples. To explore this ability, we introduce the instruction induction challenge, compile a dataset consisting of 24 tasks, and define a novel evaluation metric based on executing the generated instruction. We discover that, to a large extent, the ability to generate instructions does indeed emerge when using a model that is both large enough and aligned to follow instructions; InstructGPT achieves 65.7% of human performance in our execution-based metric, while the original GPT-3 model reaches only 9.8% of human performance. This surprising result suggests that instruction induction might be a viable learning paradigm in and of itself, where instead of fitting a set of latent continuous parameters to the data, one searches for the best description in the natural language hypothesis space.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.108.pdf",
        "keywords": [
            "performance",
            "instruction induction",
            "natural language instruction",
            "natural language",
            "natural language task descriptions",
            "language models",
            "human performance"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"while the original GPT-3 model reaches only 9.8% of human performance.\"\n\nThis evidence suggests a limitation of the original GPT-3 model in instruction induction, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"while the original GPT-3 model reaches only 9.8% of human performance.\"\n\nThis evidence suggests a limitation of the original GPT-3 model in instruction induction, but it is not the primary focus of the paper."
    },
    {
        "title": "In-Context Analogical Reasoning with Pre-Trained Language Models",
        "authors": [
            "Xiaoyang Hu",
            "Shane Storks",
            "Richard Lewis",
            "Joyce Chai"
        ],
        "published": "2023",
        "summary": "Analogical reasoning is a fundamental capacity of human cognition that allows us to reason abstractly about novel situations by relating them to past experiences. While it is thought to be essential for robust reasoning in AI systems, conventional approaches require significant training and/or hard-coding of domain knowledge to be applied to benchmark tasks. Inspired by cognitive science research that has found connections between human language and analogy-making, we explore the use of intuitive language-based abstractions to support analogy in AI systems. Specifically, we apply large pre-trained language models (PLMs) to visual Raven’s Progressive Matrices (RPM), a common relational reasoning test. By simply encoding the perceptual features of the problem into language form, we find that PLMs exhibit a striking capacity for zero-shot relational reasoning, exceeding human performance and nearing supervised vision-based methods. We explore different encodings that vary the level of abstraction over task features, finding that higher-level abstractions further strengthen PLMs’ analogical reasoning. Our detailed analysis reveals insights on the role of model complexity, in-context learning, and prior knowledge in solving RPM tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.109.pdf",
        "keywords": [
            "analogy",
            "analogical reasoning",
            "reasoning",
            "language",
            "pre trained language models",
            "context analogical reasoning",
            "abstractions",
            "context",
            "language based abstractions",
            "human language"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the paper discusses the capacity of large pre-trained language models (PLMs) for zero-shot relational reasoning without mentioning any limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the paper discusses the capacity of large pre-trained language models (PLMs) for zero-shot relational reasoning without mentioning any limitations."
    },
    {
        "title": "Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering",
        "authors": [
            "Avi Caciularu",
            "Matthew Peters",
            "Jacob Goldberger",
            "Ido Dagan",
            "Arman Cohan"
        ],
        "published": "2023",
        "summary": "The integration of multi-document pre-training objectives into language models has resulted in remarkable improvements in multi-document downstream tasks. In this work, we propose extending this idea by pre-training a generic multi-document model from a novel cross-document question answering pre-training objective. To that end, given a set (or cluster) of topically-related documents, we systematically generate semantically-oriented questions from a salient sentence in one document and challenge the model, during pre-training, to answer these questions while “peeking” into other topically-related documents. In a similar manner, the model is also challenged to recover the sentence from which the question was generated, again while leveraging cross-document information. This novel multi-document QA formulation directs the model to better recover cross-text informational relations, and introduces a natural augmentation that artificially increases the pre-training data. Further, unlike prior multi-document models that focus on either classification or summarization tasks, our pre-training objective formulation enables the model to perform tasks that involve both short text generation (e.g., QA) and long text generation (e.g., summarization).Following this scheme, we pre-train our model - termed QAmden - and evaluate its performance across several multi-document tasks, including multi-document QA, summarization, and query-focused summarization, yielding improvements of up to 7%, and significantly outperforms zero-shot GPT-3.5 and GPT-4.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.110.pdf",
        "keywords": [
            "document",
            "cross document",
            "multi document",
            "multi document modeling",
            "prior multi document models",
            "cross document question answering"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "REV: Information-Theoretic Evaluation of Free-Text Rationales",
        "authors": [
            "Hanjie Chen",
            "Faeze Brahman",
            "Xiang Ren",
            "Yangfeng Ji",
            "Yejin Choi",
            "Swabha Swayamdipta"
        ],
        "published": "2023",
        "summary": "Generating free-text rationales is a promising step towards explainable NLP, yet evaluating such rationales remains a challenge. Existing metrics have mostly focused on measuring the association between the rationale and a given label. We argue that an ideal metric should focus on the new information uniquely provided in the rationale that is otherwise not provided in the input or the label. We investigate this research problem from an information-theoretic perspective using conditional V-information (Hewitt et al., 2021). More concretely, we propose a metric called REV (Rationale Evaluation with conditional V-information), to quantify the amount of new, label-relevant information in a rationale beyond the information already available in the input or the label. Experiments across four benchmarks with reasoning tasks, including chain-of-thought, demonstrate the effectiveness of REV in evaluating rationale-label pairs, compared to existing metrics. We further demonstrate REV is consistent with human judgments on rationale evaluations and provides more sensitive measurements of new information in free-text rationales. When used alongside traditional performance metrics, REV provides deeper insights into models’ reasoning and prediction processes.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.112.pdf",
        "keywords": [
            "rationales",
            "rationale label",
            "rationale evaluation",
            "v information",
            "conditional v information",
            "free text rationales"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "ELQA: A Corpus of Metalinguistic Questions and Answers about English",
        "authors": [
            "Shabnam Behzad",
            "Keisuke Sakaguchi",
            "Nathan Schneider",
            "Amir Zeldes"
        ],
        "published": "2023",
        "summary": "We present ELQA, a corpus of questions and answers in and about the English language. Collected from two online forums, the >70k questions (from English learners and others) cover wide-ranging topics including grammar, meaning, fluency, and etymology. The answers include descriptions of general properties of English vocabulary and grammar as well as explanations about specific (correct and incorrect) usage examples. Unlike most NLP datasets, this corpus is metalinguistic—it consists of language about language. As such, it can facilitate investigations of the metalinguistic capabilities of NLU models, as well as educational applications in the language learning domain. To study this, we define a free-form question answering task on our dataset and conduct evaluations on multiple LLMs (Large Language Models) to analyze their capacity to generate metalinguistic answers.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.113.pdf",
        "keywords": [
            "question answering"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"To study this, we define a free-form question answering task on our dataset and conduct evaluations on multiple LLMs (Large Language Models) to analyze their capacity to generate metalinguistic answers.\"\n\nThis rating is given because the abstract mentions the evaluation of LLMs' capacity to generate metalinguistic answers, which implies a potential limitation, but it is not the primary",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"To study this, we define a free-form question answering task on our dataset and conduct evaluations on multiple LLMs (Large Language Models) to analyze their capacity to generate metalinguistic answers.\"\n\nThis rating is given because the abstract mentions the evaluation of LLMs' capacity to generate metalinguistic answers, which implies a potential limitation, but it is not the primary"
    },
    {
        "title": "Schema-Guided User Satisfaction Modeling for Task-Oriented Dialogues",
        "authors": [
            "Yue Feng",
            "Yunlong Jiao",
            "Animesh Prasad",
            "Nikolaos Aletras",
            "Emine Yilmaz",
            "Gabriella Kazai"
        ],
        "published": "2023",
        "summary": "User Satisfaction Modeling (USM) is one of the popular choices for task-oriented dialogue systems evaluation, where user satisfaction typically depends on whether the user’s task goals were fulfilled by the system. Task-oriented dialogue systems use task schema, which is a set of task attributes, to encode the user’s task goals. Existing studies on USM neglect explicitly modeling the user’s task goals fulfillment using the task schema. In this paper, we propose SG-USM, a novel schema-guided user satisfaction modeling framework. It explicitly models the degree to which the user’s preferences regarding the task attributes are fulfilled by the system for predicting the user’s satisfaction level. SG-USM employs a pre-trained language model for encoding dialogue context and task attributes. Further, it employs a fulfillment representation layer for learning how many task attributes have been fulfilled in the dialogue, an importance predictor component for calculating the importance of task attributes. Finally, it predicts the user satisfaction based on task attribute fulfillment and task attribute importance. Experimental results on benchmark datasets (i.e. MWOZ, SGD, ReDial, and JDDC) show that SG-USM consistently outperforms competitive existing methods. Our extensive analysis demonstrates that SG-USM can improve the interpretability of user satisfaction modeling, has good scalability as it can effectively deal with unseen tasks and can also effectively work in low-resource settings by leveraging unlabeled data. Code is available at https://github.com/amzn/user-satisfaction-modeling.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.116.pdf",
        "keywords": [
            "user satisfaction modeling",
            "user satisfaction",
            "dialogue",
            "task schema",
            "task oriented dialogues",
            "task attributes",
            "task attribute fulfillment",
            "language model",
            "task goals",
            "goals"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"It employs a pre-trained language model for encoding dialogue context and task attributes.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"It employs a pre-trained language model for encoding dialogue context and task attributes.\""
    },
    {
        "title": "Robust Multi-bit Natural Language Watermarking through Invariant Features",
        "authors": [
            "KiYoon Yoo",
            "Wonhyuk Ahn",
            "Jiho Jang",
            "Nojun Kwak"
        ],
        "published": "2023",
        "summary": "Recent years have witnessed a proliferation of valuable original natural language contents found in subscription-based media outlets, web novel platforms, and outputs of large language models. However, these contents are susceptible to illegal piracy and potential misuse without proper security measures. This calls for a secure watermarking system to guarantee copyright protection through leakage tracing or ownership identification. To effectively combat piracy and protect copyrights, a multi-bit watermarking framework should be able to embed adequate bits of information and extract the watermarks in a robust manner despite possible corruption. In this work, we explore ways to advance both payload and robustness by following a well-known proposition from image watermarking and identify features in natural language that are invariant to minor corruption. Through a systematic analysis of the possible sources of errors, we further propose a corruption-resistant infill model. Our full method improves upon the previous work on robustness by +16.8% point on average on four datasets, three corruption types, and two corruption ratios",
        "pdf_link": "https://aclanthology.org/2023.acl-long.117.pdf",
        "keywords": [
            "multi bit watermarking",
            "corruption resistant",
            "copyright protection",
            "natural language watermarking",
            "piracy",
            "secure watermarking"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"outputs of large language models\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"outputs of large language models\""
    },
    {
        "title": "KALM: Knowledge-Aware Integration of Local, Document, and Global Contexts for Long Document Understanding",
        "authors": [
            "Shangbin Feng",
            "Zhaoxuan Tan",
            "Wenqian Zhang",
            "Zhenyu Lei",
            "Yulia Tsvetkov"
        ],
        "published": "2023",
        "summary": "With the advent of pre-trained language models (LMs), increasing research efforts have been focusing on infusing commonsense and domain-specific knowledge to prepare LMs for downstream tasks. These works attempt to leverage knowledge graphs, the de facto standard of symbolic knowledge representation, along with pre-trained LMs. While existing approaches leverage external knowledge, it remains an open question how to jointly incorporate knowledge graphs represented in varying contexts — from local (e.g., sentence), document-level, to global knowledge, to enable knowledge-rich and interpretable exchange across contexts. In addition, incorporating varying contexts can especially benefit long document understanding tasks that leverage pre-trained LMs, typically bounded by the input sequence length. In light of these challenges, we propose KALM, a language model that jointly leverages knowledge in local, document-level, and global contexts for long document understanding. KALM firstly encodes long documents and knowledge graphs into the three knowledge-aware context representations. KALM then processes each context with context-specific layers. These context-specific layers are followed by a ContextFusion layer that facilitates knowledge exchange to derive an overarching document representation. Extensive experiments demonstrate that KALM achieves state-of-the-art performance on three long document understanding tasks across 6 datasets/settings. Further analyses reveal that the three knowledge-aware contexts are complementary and they all contribute to model performance, while the importance and information exchange patterns of different contexts vary on different tasks and datasets.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.118.pdf",
        "keywords": [
            "knowledge",
            "knowledge graphs",
            "knowledge aware context",
            "document",
            "knowledge aware integration",
            "document understanding",
            "global contexts",
            "symbolic knowledge representation",
            "knowledge exchange",
            "local"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"incorporating varying contexts can especially benefit long document understanding tasks that leverage pre-trained LMs, typically bounded by the input sequence length.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs (input sequence length) in passing, but it is not the primary focus of the paper. The paper mainly proposes a solution (KALM) to address",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"incorporating varying contexts can especially benefit long document understanding tasks that leverage pre-trained LMs, typically bounded by the input sequence length.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs (input sequence length) in passing, but it is not the primary focus of the paper. The paper mainly proposes a solution (KALM) to address"
    },
    {
        "title": "SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval",
        "authors": [
            "Liang Wang",
            "Nan Yang",
            "Xiaolong Huang",
            "Binxing Jiao",
            "Linjun Yang",
            "Daxin Jiang",
            "Rangan Majumder",
            "Furu Wei"
        ],
        "published": "2023",
        "summary": "In this paper, we propose SimLM (Similarity matching with Language Model pre-training), a simple yet effective pre-training method for dense passage retrieval. It employs a simple bottleneck architecture that learns to compress the passage information into a dense vector through self-supervised pre-training. We use a replaced language modeling objective, which is inspired by ELECTRA (Clark et al., 2020), to improve the sample efficiency and reduce the mismatch of the input distribution between pre-training and fine-tuning. SimLM only requires access to an unlabeled corpus and is more broadly applicable when there are no labeled data or queries. We conduct experiments on several large-scale passage retrieval datasets and show substantial improvements over strong baselines under various settings. Remarkably, SimLM even outperforms multi-vector approaches such as ColBERTv2 (Santhanam et al., 2021) which incurs significantly more storage cost. Our code and model checkpoints are available at https://github.com/microsoft/unilm/tree/master/simlm .",
        "pdf_link": "https://aclanthology.org/2023.acl-long.125.pdf",
        "keywords": [
            "passage retrieval",
            "dense passage retrieval",
            "similarity matching",
            "pre training",
            "language model pre training",
            "representation",
            "replaced language modeling",
            "bottleneck"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the paper discusses a pre-training method for dense passage retrieval and does not mention any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the paper discusses a pre-training method for dense passage retrieval and does not mention any limitations of LLMs."
    },
    {
        "title": "What Makes Pre-trained Language Models Better Zero-shot Learners?",
        "authors": [
            "Jinghui Lu",
            "Dongsheng Zhu",
            "Weidong Han",
            "Rui Zhao",
            "Brian Mac Namee",
            "Fei Tan"
        ],
        "published": "2023",
        "summary": "Current methods for prompt learning in zero-shot scenarios widely rely on a development set with sufficient human-annotated data to select the best-performing prompt template a posteriori. This is not ideal because in a real-world zero-shot scenario of practical relevance, no labelled data is available. Thus, we propose a simple yet effective method for screening reasonable prompt templates in zero-shot text classification: Perplexity Selection (Perplection). We hypothesize that language discrepancy can be used to measure the efficacy of prompt templates, and thereby develop a substantiated perplexity-based scheme allowing for forecasting the performance of prompt templates in advance. Experiments show that our method leads to improved prediction performance in a realistic zero-shot setting, eliminating the need for any labelled examples.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.128.pdf",
        "keywords": [
            "perplexity",
            "language"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Current methods for prompt learning in zero-shot scenarios widely rely on a development set with sufficient human-annotated data to select the best-performing prompt template a posteriori. This is not ideal because in a real-world zero-shot scenario of practical relevance, no labelled data is available.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Current methods for prompt learning in zero-shot scenarios widely rely on a development set with sufficient human-annotated data to select the best-performing prompt template a posteriori. This is not ideal because in a real-world zero-shot scenario of practical relevance, no labelled data is available.\""
    },
    {
        "title": "Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations",
        "authors": [
            "Xinxi Lyu",
            "Sewon Min",
            "Iz Beltagy",
            "Luke Zettlemoyer",
            "Hannaneh Hajishirzi"
        ],
        "published": "2023",
        "summary": "Although large language models can be prompted for both zero- and few-shot learning, performance drops significantly when no demonstrations are available. In this paper, we introduce Z-ICL, a new zero-shot method that closes the gap by constructing pseudo-demonstrations for a given test input using a raw text corpus. Concretely, pseudo-demonstrations are constructed by (1) finding the nearest neighbors to the test input from the corpus and pairing them with random task labels, and (2) applying a set of techniques to reduce the amount of direct copying the model does from the resulting demonstrations. Evaluation on nine classification datasets shows that Z-ICL outperforms previous zero-shot methods by a significant margin, and is on par with in-context learning with labeled training data in the few-shot setting. Overall, Z-ICL provides a significantly higher estimate of the zero-shot performance levels of a model, and supports future efforts to develop better pseudo-demonstrations that further improve zero-shot results.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.129.pdf",
        "keywords": [
            "pseudo demonstrations"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although large language models can be prompted for both zero- and few-shot learning, performance drops significantly when no demonstrations are available.\"\n\nThis paper mentions a limitation of LLMs (poor performance in zero-shot learning without demonstrations) but does not explore it in depth and focuses on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although large language models can be prompted for both zero- and few-shot learning, performance drops significantly when no demonstrations are available.\"\n\nThis paper mentions a limitation of LLMs (poor performance in zero-shot learning without demonstrations) but does not explore it in depth and focuses on the proposed solution."
    },
    {
        "title": "Better Simultaneous Translation with Monotonic Knowledge Distillation",
        "authors": [
            "Shushu Wang",
            "Jing Wu",
            "Kai Fan",
            "Wei Luo",
            "Jun Xiao",
            "Zhongqiang Huang"
        ],
        "published": "2023",
        "summary": "Simultaneous machine translation (SiMT) presents a unique challenge as it requires generating target tokens before the source sentence is fully consumed. This can lead to the hallucination problem, where target tokens are generated without support from the source sentence. The prefix-to-prefix training data used to train SiMT models are not always parallel, due to divergent word order between the source and target languages, and can contribute to the problem. In this paper, we propose a novel approach that leverages traditional translation models as teachers and employs a two-stage beam search algorithm to generate monotonic yet accurate reference translations for sequence-level knowledge distillation. Experimental results demonstrate the significant improvements achieved by our approach over multiple strong SiMT baselines, leading to new state-of-the-art performance across various language pairs. Notably, when evaluated on a monotonic version of the WMT15 De-En test set, which includes references generated in a more monotonic style by professional translators, our approach achieves even more substantial improvement over the baselines. The source code and data are publicly available for further exploration.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.131.pdf",
        "keywords": [
            "knowledge distillation",
            "translation",
            "simultaneous machine translation"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In",
        "authors": [
            "Zichun Yu",
            "Chenyan Xiong",
            "Shi Yu",
            "Zhiyuan Liu"
        ],
        "published": "2023",
        "summary": "Retrieval augmentation can aid language models (LMs) in knowledge-intensive tasks by supplying them with external information. Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target LMs that may not be known beforehand or are unable to be fine-tuned together. To retrieve useful documents for unseen target LMs, we propose augmentation-adapted retriever (AAR), which learns LM’s preferences obtained from a known source LM. Experiments on the MMLU and PopQA datasets demonstrate that our AAR trained with a small source LM is able to significantly improve the zero-shot generalization of larger target LMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates that the preferences of different LMs overlap, enabling AAR trained with a single source LM to serve as a generic plug-in for various target LMs. Our code is open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.136.pdf",
        "keywords": [
            "plug in",
            "adapted retriever",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled.\""
    },
    {
        "title": "Cold-Start Data Selection for Better Few-shot Language Model Fine-tuning: A Prompt-based Uncertainty Propagation Approach",
        "authors": [
            "Yue Yu",
            "Rongzhi Zhang",
            "Ran Xu",
            "Jieyu Zhang",
            "Jiaming Shen",
            "Chao Zhang"
        ],
        "published": "2023",
        "summary": "We present PATRON, a prompt-based data selection method for pre-trained language model fine-tuning under cold-start scenarios, i.e., no initial labeled data are available. In PATRON, we design (1) a prompt-based uncertainty propagation approach to estimate the importance of data points and (2) a partition-then-rewrite (PTR) strategy to promote sample diversity when querying for annotations. Experiments on six text classification datasets show that PATRON outperforms the strongest cold-start data selection baselines by up to 6.9%. Besides, with 128 labels only, PATRON achieves 91.0% and 92.1% of the fully supervised performance based on vanilla fine-tuning and prompt-based learning respectively. Our implementation of PATRON will be published upon acceptance.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.141.pdf",
        "keywords": [
            "cold start",
            "cold start data selection",
            "data selection",
            "sample diversity",
            "tuning"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"under cold-start scenarios, i.e., no initial labeled data are available.\"\n\n(Note: Although the paper discusses a limitation of LLMs, specifically the cold-start scenario, it does not elaborate on this limitation and instead focuses on proposing a solution to address this issue.)",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"under cold-start scenarios, i.e., no initial labeled data are available.\"\n\n(Note: Although the paper discusses a limitation of LLMs, specifically the cold-start scenario, it does not elaborate on this limitation and instead focuses on proposing a solution to address this issue.)"
    },
    {
        "title": "Bi-Phone: Modeling Inter Language Phonetic Influences in Text",
        "authors": [
            "Abhirut Gupta",
            "Ananya B. Sai",
            "Richard Sproat",
            "Yuri Vasilevski",
            "James Ren",
            "Ambarish Jash",
            "Sukhdeep Sodhi",
            "Aravindan Raghuveer"
        ],
        "published": "2023",
        "summary": "A large number of people are forced to use the Web in a language they have low literacy in due to technology asymmetries. Written text in the second language (L2) from such users often contains a large number of errors that are influenced by their native language (L1).We propose a method to mine phoneme confusions (sounds in L2 that an L1 speaker is likely to conflate) for pairs of L1 and L2.These confusions are then plugged into a generative model (Bi-Phone) for synthetically producing corrupted L2 text. Through human evaluations, we show that Bi-Phone generates plausible corruptions that differ across L1s and also have widespread coverage on the Web.We also corrupt the popular language understanding benchmark SuperGLUE with our technique (FunGLUE for Phonetically Noised GLUE) and show that SoTA language understating models perform poorly. We also introduce a new phoneme prediction pre-training task which helps byte models to recover performance close to SuperGLUE. Finally, we also release the SuperGLUE benchmark to promote further research in phonetically robust language models. To the best of our knowledge, FunGLUE is the first benchmark to introduce L1-L2 interactions in text.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.145.pdf",
        "keywords": [
            "literacy",
            "sounds",
            "text",
            "phone",
            "bi phone",
            "phoneme prediction",
            "language",
            "superglue"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We also corrupt the popular language understanding benchmark SuperGLUE with our technique (FunGLUE for Phonetically Noised GLUE) and show that SoTA language understating models perform poorly.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We also corrupt the popular language understanding benchmark SuperGLUE with our technique (FunGLUE for Phonetically Noised GLUE) and show that SoTA language understating models perform poorly.\""
    },
    {
        "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models",
        "authors": [
            "Lei Wang",
            "Wanyu Xu",
            "Yihuai Lan",
            "Zhiqiang Hu",
            "Yunshi Lan",
            "Roy Ka-Wei Lee",
            "Ee-Peng Lim"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with “Let’s think step by step” as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.147.pdf",
        "keywords": [
            "prompting",
            "shot prompting",
            "shot cot prompting",
            "language models",
            "large language models",
            "shot chain of thought",
            "thought reasoning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors.\""
    },
    {
        "title": "RetroMAE-2: Duplex Masked Auto-Encoder For Pre-Training Retrieval-Oriented Language Models",
        "authors": [
            "Zheng Liu",
            "Shitao Xiao",
            "Yingxia Shao",
            "Zhao Cao"
        ],
        "published": "2023",
        "summary": "To better support information retrieval tasks such as web search and open-domain question answering, growing effort is made to develop retrieval-oriented language models, e.g., RetroMAE and many others. Most of the existing works focus on improving the semantic representation capability for the contextualized embedding of the [CLS] token. However, recent study shows that the ordinary tokens besides [CLS] may provide extra information, which help to produce a better representation effect. As such, it’s necessary to extend the current methods where all contextualized embeddings can be jointly pre-trained for the retrieval tasks. In this work, we propose a novel pre-training method called Duplex Masked Auto-Encoder, a.k.a. DupMAE. It is designed to improve the quality of semantic representation where all contextualized embeddings of the pre-trained model can be leveraged. It takes advantage of two complementary auto-encoding tasks: one reconstructs the input sentence on top of the [CLS] embedding; the other one predicts the bag-of-words feature of the input sentence based on the ordinary tokens’ embeddings. The two tasks are jointly conducted to train a unified encoder, where the whole contextualized embeddings are aggregated in a compact way to produce the final semantic representation. DupMAE is simple but empirically competitive: it substantially improves the pre-trained model’s representation capability and transferability, where superior retrieval performances can be achieved on popular benchmarks, like MS MARCO and BEIR. We make our code publicly available at https://github.com/staoxiao/RetroMAE.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.148.pdf",
        "keywords": [
            "retromae",
            "retromae 2",
            "auto encoder",
            "masked auto encoder",
            "retrieval"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, recent study shows that the ordinary tokens besides [CLS] may provide extra information, which help to produce a better representation effect.\"\n\nThis abstract mentions a limitation of existing retrieval-oriented language models in that they only focus on improving the semantic representation capability for the contextualized embedding of the [CLS] token, but does not elaborate on this limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, recent study shows that the ordinary tokens besides [CLS] may provide extra information, which help to produce a better representation effect.\"\n\nThis abstract mentions a limitation of existing retrieval-oriented language models in that they only focus on improving the semantic representation capability for the contextualized embedding of the [CLS] token, but does not elaborate on this limitation."
    },
    {
        "title": "Symbolic Chain-of-Thought Distillation: Small Models Can Also “Think” Step-by-Step",
        "authors": [
            "Liunian Harold Li",
            "Jack Hessel",
            "Youngjae Yu",
            "Xiang Ren",
            "Kai-Wei Chang",
            "Yejin Choi"
        ],
        "published": "2023",
        "summary": "Chain-of-thought prompting (e.g., “Let’s think step-by-ste”) primes large language models to verbalize rationalization for their predictions. While chain-of-thought can lead to dramatic performance gains, benefits appear to emerge only for sufficiently large models (beyond 50B parameters). We show that orders-of-magnitude smaller models (125M—1.3B parameters) can still benefit from chain-of-thought prompting. To achieve this, we introduce Symbolic Chain-of-Thought Distillation (SCoTD), a method to train a smaller student model on rationalizations sampled from a significantly larger teacher model. Experiments across several commonsense benchmarks show that: 1) SCoTD enhances the performance of the student model in both supervised and few-shot settings, and especially for challenge sets; 2) sampling many reasoning chains per instance from the teacher is paramount; and 3) after distillation, student chain-of-thoughts are judged by humans as comparable to the teacher, despite orders of magnitude fewer parameters. We test several hypotheses regarding what properties of chain-of-thought samples are important, e.g., diversity vs. teacher likelihood vs. open-endedness. We release our corpus of chain-of-thought samples and code.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.150.pdf",
        "keywords": [
            "thought distillation",
            "thought prompting",
            "commonsense benchmarks",
            "thought samples",
            "chain",
            "reasoning",
            "sampling",
            "symbolic",
            "step"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"benefits appear to emerge only for sufficiently large models (beyond 50B parameters)\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"benefits appear to emerge only for sufficiently large models (beyond 50B parameters)\""
    },
    {
        "title": "Generating EDU Extracts for Plan-Guided Summary Re-Ranking",
        "authors": [
            "Griffin Adams",
            "Alex Fabbri",
            "Faisal Ladhak",
            "Noémie Elhadad",
            "Kathleen McKeown"
        ],
        "published": "2023",
        "summary": "Two-step approaches, in which summary candidates are generated-then-reranked to return a single summary, can improve ROUGE scores over the standard single-step approach. Yet, standard decoding methods (i.e., beam search, nucleus sampling, and diverse beam search) produce candidates with redundant, and often low quality, content. In this paper, we design a novel method to generate candidates for re-ranking that addresses these issues. We ground each candidate abstract on its own unique content plan and generate distinct plan-guided abstracts using a model’s top beam. More concretely, a standard language model (a BART LM) auto-regressively generates elemental discourse unit (EDU) content plans with an extractive copy mechanism. The top K beams from the content plan generator are then used to guide a separate LM, which produces a single abstractive candidate for each distinct plan. We apply an existing re-ranker (BRIO) to abstractive candidates generated from our method, as well as baseline decoding methods. We show large relevance improvements over previously published methods on widely used single document news article corpora, with ROUGE-2 F1 gains of 0.88, 2.01, and 0.38 on CNN / Dailymail, NYT, and Xsum, respectively. A human evaluation on CNN / DM validates these results. Similarly, on 1k samples from CNN / DM, we show that prompting GPT-3 to follow EDU plans outperforms sampling-based methods by by 1.05 ROUGE-2 F1 points. Code to generate and realize plans is available at https://github.com/griff4692/edu-sum.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.151.pdf",
        "keywords": [
            "sampling",
            "plan",
            "edu plans",
            "beam search",
            "standard language model"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Yet, standard decoding methods (i.e., beam search, nucleus sampling, and diverse beam search) produce candidates with redundant, and often low quality, content.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Yet, standard decoding methods (i.e., beam search, nucleus sampling, and diverse beam search) produce candidates with redundant, and often low quality, content.\""
    },
    {
        "title": "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters",
        "authors": [
            "Boshi Wang",
            "Sewon Min",
            "Xiang Deng",
            "Jiaming Shen",
            "You Wu",
            "Luke Zettlemoyer",
            "Huan Sun"
        ],
        "published": "2023",
        "summary": "Chain-of-Thought (CoT) prompting can dramatically improve the multi-step reasoning abilities of large language models (LLMs). CoT explicitly encourages the LLM to generate intermediate rationales for solving a problem, by providing a series of reasoning steps in the demonstrations. Despite its success, there is still little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance. In this paper, we show that CoT reasoning is possible even with invalid demonstrations - prompting with invalid reasoning steps can achieve over 80-90% of the performance obtained using CoT under various metrics, while still generating coherent lines of reasoning during inference. Further experiments show that other aspects of the rationales, such as being relevant to the query and correctly ordering the reasoning steps, are much more important for effective CoT reasoning. Overall, these findings both deepen our understanding of CoT prompting, and open up new questions regarding LLMs’ capability to learn to reason in context.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.153.pdf",
        "keywords": [
            "prompting",
            "cot prompting",
            "language models",
            "demonstrations prompting",
            "empirical study",
            "cot reasoning"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"despite its success, there is still little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"despite its success, there is still little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance.\""
    },
    {
        "title": "Gradient-based Intra-attention Pruning on Pre-trained Language Models",
        "authors": [
            "Ziqing Yang",
            "Yiming Cui",
            "Xin Yao",
            "Shijin Wang"
        ],
        "published": "2023",
        "summary": "Pre-trained language models achieve superior performance but are computationally expensive. Techniques such as pruning and knowledge distillation have been developed to reduce their sizes and latencies. In this work, we propose a structured pruning method GRAIN (gradient-based intra-attention pruning), which performs task-specific pruning with knowledge distillation and yields highly effective models. Different from common approaches that prune each attention head as a whole, GRAIN inspects and prunes intra-attention structures, which greatly expands the structure search space and enables more flexible models. We also propose a gradient separation strategy that reduces the interference of distillation on pruning for a better combination of the two approaches. Experiments on GLUE, SQuAD, and CoNLL 2003 show that GRAIN notably outperforms other methods, especially in the high sparsity regime, and achieves 6 7x speedups while maintaining 93% 99% performance. Under extreme compression where only 3% transformer weights remain, the pruned model is still competitive compared to larger models.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.156.pdf",
        "keywords": [
            "pruning",
            "intra attention pruning",
            "language",
            "language models",
            "knowledge distillation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Pre-trained language models achieve superior performance but are computationally expensive.\"\n\nThis paper mentions a limitation of pre-trained language models (computational expense) but does not explore it in depth and focuses on proposing a solution to this limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Pre-trained language models achieve superior performance but are computationally expensive.\"\n\nThis paper mentions a limitation of pre-trained language models (computational expense) but does not explore it in depth and focuses on proposing a solution to this limitation."
    },
    {
        "title": "DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control for Empathetic Response Generation",
        "authors": [
            "Guanqun Bi",
            "Lei Shen",
            "Yanan Cao",
            "Meng Chen",
            "Yuqiang Xie",
            "Zheng Lin",
            "Xiaodong He"
        ],
        "published": "2023",
        "summary": "Empathy is a crucial factor in open-domain conversations, which naturally shows one’s caring and understanding to others. Though several methods have been proposed to generate empathetic responses, existing works often lead to monotonous empathy that refers to generic and safe expressions. In this paper, we propose to use explicit control to guide the empathy expression and design a framework DiffusEmp based on conditional diffusion language model to unify the utilization of dialogue context and attribute-oriented control signals. Specifically, communication mechanism, intent, and semantic frame are imported as multi-grained signals that control the empathy realization from coarse to fine levels. We then design a specific masking strategy to reflect the relationship between multi-grained signals and response tokens, and integrate it into the diffusion model to influence the generative process. Experimental results on a benchmark dataset EmpatheticDialogue show that our framework outperforms competitive baselines in terms of controllability, informativeness, and diversity without the loss of context-relatedness.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.158.pdf",
        "keywords": [
            "empathy",
            "diffusion",
            "conditional diffusion",
            "diffusion model",
            "masking",
            "response generation"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions \"existing works often lead to monotonous empathy that refers to generic and safe expressions\", which might imply a limitation in generating empathetic responses, but it is not explicitly stated as a limitation of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions \"existing works often lead to monotonous empathy that refers to generic and safe expressions\", which might imply a limitation in generating empathetic responses, but it is not explicitly stated as a limitation of LLMs."
    },
    {
        "title": "Faithful Low-Resource Data-to-Text Generation through Cycle Training",
        "authors": [
            "Zhuoer Wang",
            "Marcus Collins",
            "Nikhita Vedula",
            "Simone Filice",
            "Shervin Malmasi",
            "Oleg Rokhlenko"
        ],
        "published": "2023",
        "summary": "Methods to generate text from structured data have advanced significantly in recent years, primarily due to fine-tuning of pre-trained language models on large datasets. However, such models can fail to produce output faithful to the input data, particularly on out-of-domain data. Sufficient annotated data is often not available for specific domains, leading us to seek an unsupervised approach to improve the faithfulness of output text. Since the problem is fundamentally one of consistency between the representations of the structured data and text, we evaluate the effectiveness of cycle training in this work. Cycle training uses two models which are inverses of each other: one that generates text from structured data, and one which generates the structured data from natural language text. We show that cycle training, when initialized with a small amount of supervised data (100 samples in our case), achieves nearly the same performance as fully supervised approaches for the data-to-text generation task on the WebNLG, E2E, WTQ, and WSQL datasets. We perform extensive empirical analysis with automated evaluation metrics and a newly designed human evaluation schema to reveal different cycle training strategies’ effectiveness of reducing various types of generation errors. Our code is publicly available at https://github.com/Edillower/CycleNLG.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.160.pdf",
        "keywords": [
            "cycle training",
            "perform",
            "generation errors",
            "faithfulness",
            "text generation",
            "low resource data",
            "evaluation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, such models can fail to produce output faithful to the input data, particularly on out-of-domain data.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of pre-trained language models (failing to produce faithful output, particularly on out-of-domain data), but it is not the primary focus of the paper. The paper aims to propose a solution (cycle training",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, such models can fail to produce output faithful to the input data, particularly on out-of-domain data.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of pre-trained language models (failing to produce faithful output, particularly on out-of-domain data), but it is not the primary focus of the paper. The paper aims to propose a solution (cycle training"
    },
    {
        "title": "Dynamic and Efficient Inference for Text Generation via BERT Family",
        "authors": [
            "Xiaobo Liang",
            "Juntao Li",
            "Lijun Wu",
            "Ziqiang Cao",
            "Min Zhang"
        ],
        "published": "2023",
        "summary": "Despite the excellent performance of Pre-trained Language Models on many text generation tasks, they suffer from inefficient inference on computation and memory due to their large-scale parameters and the universal autoregressive decoding paradigm. In this work, we propose a novel fine-tuning method DEER, which can make a single pre-trained model support Dynamic and Efficient infERence and achieve an adaptive trade-off between model performance and latency. In particular, our critical insight is to jointly utilize the non-autoregressive (NAR) generation and dynamic parameter pruning techniques, which can flexibly control the decoding iteration steps and model sizes according to memory and latency limitations. Besides, we also explore the effectiveness of the pre-trained MLMs (i.e., the BERT family) for text generation tasks since their bidirectional attention nature is more suitable for the NAR training objective. Extensive experiments on both monolingual and multilingual pre-trained MLMs demonstrate the effectiveness of our proposed DEER method by consistently achieving (1) higher BLEU scores than the strong autoregressive Transformer model on three neural machine translation tasks with 3 → 12 times speedup, (2) competitive performance (but with much faster inference speed) compared with the BART model on four GLGE benchmark tasks. Our code will be publicly available at GitHub https://github.com/dropreg/DEER.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.162.pdf",
        "keywords": [
            "bert family",
            "bert",
            "text generation",
            "bleu",
            "bart model"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the excellent performance of Pre-trained Language Models on many text generation tasks, they suffer from inefficient inference on computation and memory due to their large-scale parameters and the universal autoregressive decoding paradigm.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite the excellent performance of Pre-trained Language Models on many text generation tasks, they suffer from inefficient inference on computation and memory due to their large-scale parameters and the universal autoregressive decoding paradigm.\""
    },
    {
        "title": "Hierarchical Verbalizer for Few-Shot Hierarchical Text Classification",
        "authors": [
            "Ke Ji",
            "Yixin Lian",
            "Jingsheng Gao",
            "Baoyuan Wang"
        ],
        "published": "2023",
        "summary": "Due to the complex label hierarchy and intensive labeling cost in practice, the hierarchical text classification (HTC) suffers a poor performance especially when low-resource or few-shot settings are considered. Recently, there is a growing trend of applying prompts on pre-trained language models (PLMs), which has exhibited effectiveness in the few-shot flat text classification tasks. However, limited work has studied the paradigm of prompt-based learning in the HTC problem when the training data is extremely scarce. In this work, we define a path-based few-shot setting and establish a strict path-based evaluation metric to further explore few-shot HTC tasks. To address the issue, we propose the hierarchical verbalizer (“HierVerb”), a multi-verbalizer framework treating HTC as a single- or multi-label classification problem at multiple layers and learning vectors as verbalizers constrained by hierarchical structure and hierarchical contrastive learning. In this manner, HierVerb fuses label hierarchy knowledge into verbalizers and remarkably outperforms those who inject hierarchy through graph encoders, maximizing the benefits of PLMs. Extensive experiments on three popular HTC datasets under the few-shot settings demonstrate that prompt with HierVerb significantly boosts the HTC performance, meanwhile indicating an elegant way to bridge the gap between the large pre-trained model and downstream hierarchical classification tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.164.pdf",
        "keywords": [
            "verbalizer",
            "hierarchical text classification",
            "hierarchical verbalizer",
            "hierarchical classification",
            "multi verbalizer framework",
            "htc",
            "label classification",
            "graph encoders"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, limited work has studied the paradigm of prompt-based learning in the HTC problem when the training data is extremely scarce.\"\n\nThis paper discusses the limitations of pre-trained language models (PLMs) in the context of hierarchical text classification (HTC) tasks, particularly when training data is scarce.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, limited work has studied the paradigm of prompt-based learning in the HTC problem when the training data is extremely scarce.\"\n\nThis paper discusses the limitations of pre-trained language models (PLMs) in the context of hierarchical text classification (HTC) tasks, particularly when training data is scarce."
    },
    {
        "title": "StoryWars: A Dataset and Instruction Tuning Baselines for Collaborative Story Understanding and Generation",
        "authors": [
            "Yulun Du",
            "Lydia Chilton"
        ],
        "published": "2023",
        "summary": "Collaborative stories, which are texts created through the collaborative efforts of multiple authors with different writing styles and intentions, pose unique challenges for NLP models. Understanding and generating such stories remains an underexplored area due to the lack of open-domain corpora. To address this, we introduce StoryWars, a new dataset of over 40,000 collaborative stories written by 9,400 different authors from an online platform. We design 12 task types, comprising 7 understanding and 5 generation task types, on {pasted macro ‘STORYWARS’}, deriving 101 diverse story-related tasks in total as a multi-task benchmark covering all fully-supervised, few-shot, and zero-shot scenarios. Furthermore, we present our instruction-tuned model, InstructStory, for the story tasks showing that instruction tuning, in addition to achieving superior results in zero-shot and few-shot scenarios, can also obtain the best performance on the fully-supervised tasks in StoryWars, establishing strong multi-task benchmark performances on StoryWars.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.171.pdf",
        "keywords": [
            "storywars",
            "story understanding",
            "corpora",
            "instruction tuning baselines",
            "instruction tuning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Understanding and generating such stories remains an underexplored area due to the lack of open-domain corpora.\"\n\nThis evidence suggests that the paper mentions a limitation of LLMs, specifically the lack of open-domain corpora for understanding and generating collaborative stories, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Understanding and generating such stories remains an underexplored area due to the lack of open-domain corpora.\"\n\nThis evidence suggests that the paper mentions a limitation of LLMs, specifically the lack of open-domain corpora for understanding and generating collaborative stories, but it is not the primary focus of the paper."
    },
    {
        "title": "Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning",
        "authors": [
            "Fan Yin",
            "Jesse Vig",
            "Philippe Laban",
            "Shafiq Joty",
            "Caiming Xiong",
            "Chien-Sheng Wu"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have shown impressive performance in following natural language instructions to solve unseen tasks. However, it remains unclear whether models truly understand task definitions and whether the human-written definitions are optimal. In this paper, we systematically study the role of task definitions in instruction learning. We first conduct an ablation analysis informed by human annotations to understand which parts of a task definition are most important, and find that model performance only drops substantially when removing contents describing the task output, in particular label information. Next, we propose an automatic algorithm to compress task definitions to a minimal supporting set of tokens, and find that 60% of tokens can be removed while maintaining or even improving model performance. Based on these results, we propose two strategies to help models better leverage task instructions: (1) providing only key information for tasks in a common structured format, and (2) adding a meta-tuning stage to help the model better understand the definitions. With these two strategies, we achieve a 4.2 Rouge-L improvement over 119 unseen test tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.172.pdf",
        "keywords": [
            "task definitions",
            "instruction learning",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it remains unclear whether models truly understand task definitions and whether the human-written definitions are optimal.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, it remains unclear whether models truly understand task definitions and whether the human-written definitions are optimal.\""
    },
    {
        "title": "Do PLMs Know and Understand Ontological Knowledge?",
        "authors": [
            "Weiqi Wu",
            "Chengyue Jiang",
            "Yong Jiang",
            "Pengjun Xie",
            "Kewei Tu"
        ],
        "published": "2023",
        "summary": "Ontological knowledge, which comprises classes and properties and their relationships, is integral to world knowledge. It is significant to explore whether Pretrained Language Models (PLMs) know and understand such knowledge. However, existing PLM-probing studies focus mainly on factual knowledge, lacking a system- atic probing of ontological knowledge. In this paper, we focus on probing whether PLMs store ontological knowledge and have a semantic un- derstanding of the knowledge rather than rote memorization of the surface form. To probe whether PLMs know ontological knowledge, we investigate how well PLMs memorize: (1) types of entities; (2) hierarchical relationships among classes and properties, e.g., Person is a subclass of Animal and Member of Sports Team is a subproperty of Member of ; (3) domain and range constraints of properties, e.g., the subject of Member of Sports Team should be a Person and the object should be a Sports Team. To further probe whether PLMs truly understand ontological knowledge beyond memorization, we comprehensively study whether they can reliably perform logical reasoning with given knowledge according to ontological entailment rules. Our probing results show that PLMs can memorize certain ontological knowledge and utilize implicit knowledge in reasoning. How- ever, both the memorizing and reasoning per- formances are less than perfect, indicating in- complete knowledge and understanding.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.173.pdf",
        "keywords": [
            "ontological",
            "ontological knowledge",
            "plms",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, both the memorizing and reasoning per- formances are less than perfect, indicating in- complete knowledge and understanding.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, both the memorizing and reasoning per- formances are less than perfect, indicating in- complete knowledge and understanding.\""
    },
    {
        "title": "Interpretable Word Sense Representations via Definition Generation: The Case of Semantic Change Analysis",
        "authors": [
            "Mario Giulianelli",
            "Iris Luden",
            "Raquel Fernandez",
            "Andrey Kutuzov"
        ],
        "published": "2023",
        "summary": "We propose using automatically generated natural language definitions of contextualised word usages as interpretable word and word sense representations. Given a collection of usage examples for a target word, and the corresponding data-driven usage clusters (i.e., word senses), a definition is generated for each usage with a specialised Flan-T5 language model, and the most prototypical definition in a usage cluster is chosen as the sense label. We demonstrate how the resulting sense labels can make existing approaches to semantic change analysis more interpretable, and how they can allow users — historical linguists, lexicographers, or social scientists — to explore and intuitively explain diachronic trajectories of word meaning. Semantic change analysis is only one of many possible applications of the ‘definitions as representations’ paradigm. Beyond being human-readable, contextualised definitions also outperform token or usage sentence embeddings in word-in-context semantic similarity judgements, making them a new promising type of lexical representation for NLP.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.176.pdf",
        "keywords": [
            "semantic change analysis",
            "semantic similarity"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"a specialised Flan-T5 language model\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"a specialised Flan-T5 language model\""
    },
    {
        "title": "An Invariant Learning Characterization of Controlled Text Generation",
        "authors": [
            "Carolina Zheng",
            "Claudia Shi",
            "Keyon Vafa",
            "Amir Feder",
            "David Blei"
        ],
        "published": "2023",
        "summary": "Controlled generation refers to the problem of creating text that contains stylistic or semantic attributes of interest. Many approaches reduce this problem to training a predictor of the desired attribute. For example, researchers hoping to deploy a large language model to produce non-toxic content may use a toxicity classifier to filter generated text. In practice, the generated text to classify, which is determined by user prompts, may come from a wide range of distributions. In this paper, we show that the performance of controlled generation may be poor if the distributions of text in response to user prompts differ from the distribution the predictor was trained on. To address this problem, we cast controlled generation under distribution shift as an invariant learning problem: the most effective predictor should be invariant across multiple text environments. We then discuss a natural solution that arises from this characterization and propose heuristics for selecting natural environments. We study this characterization and the proposed method empirically using both synthetic and real data. Experiments demonstrate both the challenge of distribution shift in controlled generation and the potential of invariance methods in this setting.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.179.pdf",
        "keywords": [
            "language model",
            "invariance",
            "invariant learning",
            "controlled generation",
            "distribution shift",
            "text generation",
            "text"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"In practice, the generated text to classify, which is determined by user prompts, may come from a wide range of distributions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"In practice, the generated text to classify, which is determined by user prompts, may come from a wide range of distributions.\""
    },
    {
        "title": "HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation",
        "authors": [
            "Hongyi Yuan",
            "Zheng Yuan",
            "Chuanqi Tan",
            "Fei Huang",
            "Songfang Huang"
        ],
        "published": "2023",
        "summary": "Language models with the Transformers structure have shown great performance in natural language processing. However, there still poses problems when fine-tuning pre-trained language models on downstream tasks, such as over-fitting or representation collapse. In this work, we propose HyPe, a simple yet effective fine-tuning technique to alleviate such problems by perturbing hidden representations of Transformers layers. Unlike previous works that only add noise to inputs or parameters, we argue that the hidden representations of Transformers layers convey more diverse and meaningful language information. Therefore, making the Transformers layers more robust to hidden representation perturbations can further benefit the fine-tuning of PLMs en bloc. We conduct extensive experiments and analyses on GLUE and other natural language inference datasets. Results demonstrate that HyPe outperforms vanilla fine-tuning and enhances generalization of hidden representations from different layers. In addition, HyPe acquires negligible computational overheads, and is better than and compatible with previous state-of-the-art fine-tuning techniques.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.182.pdf",
        "keywords": [
            "fine tuning",
            "hidden representations",
            "hidden representation perturbation",
            "language model fine tuning",
            "models",
            "natural language",
            "transformers"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, there still poses problems when fine-tuning pre-trained language models on downstream tasks, such as over-fitting or representation collapse.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, there still poses problems when fine-tuning pre-trained language models on downstream tasks, such as over-fitting or representation collapse.\""
    },
    {
        "title": "Word sense extension",
        "authors": [
            "Lei Yu",
            "Yang Xu"
        ],
        "published": "2023",
        "summary": "Humans often make creative use of words to expressnovel senses. A long-standing effort in natural language processing hasbeen focusing on word sense disambiguation (WSD), but little has been explored about how the sense inventory of a word may be extended toward novel meanings. We present a paradigm of word sense extension (WSE) thatenables words to spawn new senses toward novel context. We develop a framework that simulates novel word sense extension by first partitioning a polysemous word type into two pseudo-tokens that mark its different senses, and then inferring whether the meaning of a pseudo-token can be extended to convey the sense denoted by the token partitioned from the same word type. Our framework combines cognitivemodels of chaining with a learning scheme that transforms a language model embedding space to supportvarious types of word sense extension. We evaluate our frameworkagainst several competitive baselines and show that it is superior in predicting plausible novel senses for over 7,500 English words. Furthermore, we show that our WSE framework improves performance over a range of transformer-based WSD models in predicting rare word senses with few or zero mentions in the training data.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.184.pdf",
        "keywords": [
            "language model",
            "word sense",
            "word sense extension",
            "sense disambiguation",
            "learning"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We evaluate our framework against several competitive baselines and show that it is superior in predicting plausible novel senses for over 7,500 English words.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"We evaluate our framework against several competitive baselines and show that it is superior in predicting plausible novel senses for over 7,500 English words.\""
    },
    {
        "title": "Decoding Symbolism in Language Models",
        "authors": [
            "Meiqi Guo",
            "Rebecca Hwa",
            "Adriana Kovashka"
        ],
        "published": "2023",
        "summary": "This work explores the feasibility of eliciting knowledge from language models (LMs) to decode symbolism, recognizing something (e.g.,roses) as a stand-in for another (e.g., love). We present our evaluative framework, Symbolism Analysis (SymbA), which compares LMs (e.g., RoBERTa, GPT-J) on different types of symbolism and analyze the outcomes along multiple metrics. Our findings suggest that conventional symbols are more reliably elicited from LMs while situated symbols are more challenging. Results also reveal the negative impact of the bias in pre-trained corpora. We further demonstrate that a simple re-ranking strategy can mitigate the bias and significantly improve model performances to be on par with human performances in some cases.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.186.pdf",
        "keywords": [
            "language models",
            "symbolism",
            "symbolism analysis",
            "knowledge"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Results also reveal the negative impact of the bias in pre-trained corpora.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Results also reveal the negative impact of the bias in pre-trained corpora.\""
    },
    {
        "title": "A Survey on Zero Pronoun Translation",
        "authors": [
            "Longyue Wang",
            "Siyou Liu",
            "Mingzhou Xu",
            "Linfeng Song",
            "Shuming Shi",
            "Zhaopeng Tu"
        ],
        "published": "2023",
        "summary": "Zero pronouns (ZPs) are frequently omitted in pro-drop languages (e.g. Chinese, Hungarian, and Hindi), but should be recalled in non-pro-drop languages (e.g. English). This phenomenon has been studied extensively in machine translation (MT), as it poses a significant challenge for MT systems due to the difficulty in determining the correct antecedent for the pronoun. This survey paper highlights the major works that have been undertaken in zero pronoun translation (ZPT) after the neural revolution so that researchers can recognize the current state and future directions of this field. We provide an organization of the literature based on evolution, dataset, method, and evaluation. In addition, we compare and analyze competing models and evaluation metrics on different benchmarks. We uncover a number of insightful findings such as: 1) ZPT is in line with the development trend of large language model; 2) data limitation causes learning bias in languages and domains; 3) performance improvements are often reported on single benchmarks, but advanced methods are still far from real-world use; 4) general-purpose metrics are not reliable on nuances and complexities of ZPT, emphasizing the necessity of targeted metrics; 5) apart from commonly-cited errors, ZPs will cause risks of gender bias.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.187.pdf",
        "keywords": [
            "pronoun translation",
            "survey",
            "machine translation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"data limitation causes learning bias in languages and domains\"; \"performance improvements are often reported on single benchmarks, but advanced methods are still far from real-world use\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"data limitation causes learning bias in languages and domains\"; \"performance improvements are often reported on single benchmarks, but advanced methods are still far from real-world use\""
    },
    {
        "title": "We Understand Elliptical Sentences, and Language Models should Too: A New Dataset for Studying Ellipsis and its Interaction with Thematic Fit",
        "authors": [
            "Davide Testa",
            "Emmanuele Chersoni",
            "Alessandro Lenci"
        ],
        "published": "2023",
        "summary": "Ellipsis is a linguistic phenomenon characterized by the omission of one or more sentence elements. Solving such a linguistic construction is not a trivial issue in natural language processing since it involves the retrieval of non-overtly expressed verbal material, which might in turn require the model to integrate human-like syntactic and semantic knowledge. In this paper, we explored the issue of how the prototypicality of event participants affects the ability of Language Models (LMs) to handle elliptical sentences and to identify the omitted arguments at different degrees of thematic fit, ranging from highly typical participants to semantically anomalous ones. With this purpose in mind, we built ELLie, the first dataset composed entirely of utterances containing different types of elliptical constructions, and structurally suited for evaluating the effect of argument thematic fit in solving ellipsis and reconstructing the missing element. Our tests demonstrated that the probability scores assigned by the models are higher for typical events than for atypical and impossible ones in different elliptical contexts, confirming the influence of prototypicality of the event participants in interpreting such linguistic structures. Finally, we conducted a retrieval task of the elided verb in the sentence in which the low performance of LMs highlighted a considerable difficulty in reconstructing the correct event.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.188.pdf",
        "keywords": [
            "ellipsis",
            "elliptical sentences",
            "language models",
            "thematic fit",
            "elliptical contexts",
            "linguistic",
            "event"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Finally, we conducted a retrieval task of the elided verb in the sentence in which the low performance of LMs highlighted a considerable difficulty in reconstructing the correct event.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Finally, we conducted a retrieval task of the elided verb in the sentence in which the low performance of LMs highlighted a considerable difficulty in reconstructing the correct event.\""
    },
    {
        "title": "A Gradient Control Method for Backdoor Attacks on Parameter-Efficient Tuning",
        "authors": [
            "Naibin Gu",
            "Peng Fu",
            "Xiyu Liu",
            "Zhengxiao Liu",
            "Zheng Lin",
            "Weiping Wang"
        ],
        "published": "2023",
        "summary": "Parameter-Efficient Tuning (PET) has shown remarkable performance by fine-tuning only a small number of parameters of the pre-trained language models (PLMs) for the downstream tasks, while it is also possible to construct backdoor attacks due to the vulnerability of pre-trained weights. However, a large reduction in the number of attackable parameters in PET will cause the user’s fine-tuning to greatly affect the effectiveness of backdoor attacks, resulting in backdoor forgetting. We find that the backdoor injection process can be regarded as multi-task learning, which has a convergence imbalance problem between the training of clean and poisoned data. And this problem might result in forgetting the backdoor. Based on this finding, we propose a gradient control method to consolidate the attack effect, comprising two strategies. One controls the gradient magnitude distribution cross layers within one task and the other prevents the conflict of gradient directions between tasks. Compared with previous backdoor attack methods in the scenario of PET, our method improve the effect of the attack on sentiment classification and spam detection respectively, which shows that our method is widely applicable to different tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.194.pdf",
        "keywords": [
            "backdoor",
            "backdoor attacks",
            "tuning",
            "parameter efficient tuning",
            "backdoor injection",
            "spam detection",
            "multi task learning",
            "gradient control",
            "classification",
            "trained"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, a large reduction in the number of attackable parameters in PET will cause the user’s fine-tuning to greatly affect the effectiveness of backdoor attacks, resulting in backdoor forgetting.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, a large reduction in the number of attackable parameters in PET will cause the user’s fine-tuning to greatly affect the effectiveness of backdoor attacks, resulting in backdoor forgetting.\""
    },
    {
        "title": "From the One, Judge of the Whole: Typed Entailment Graph Construction with Predicate Generation",
        "authors": [
            "Zhibin Chen",
            "Yansong Feng",
            "Dongyan Zhao"
        ],
        "published": "2023",
        "summary": "Entailment Graphs (EGs) have been constructed based on extracted corpora as a strong and explainable form to indicate context-independent entailment relation in natural languages. However, EGs built by previous methods often suffer from the severe sparsity issues, due to limited corpora available and the long-tail phenomenon of predicate distributions. In this paper, we propose a multi-stage method, Typed Predicate-Entailment Graph Generator (TP-EGG), to tackle this problem. Given several seed predicates, TP-EGG builds the graphs by generating new predicates and detecting entailment relations among them. The generative nature of TP-EGG helps us leverage the recent advances from large pretrained language models (PLMs), while avoiding the reliance on carefully prepared corpora. Experiments on benchmark datasets show that TP-EGG can generate high-quality and scale-controllable entailment graphs, achieving significant in-domain improvement over state-of-the-art EGs and boosting the performance of down-stream inference tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.196.pdf",
        "keywords": [
            "entailment",
            "language models",
            "predicate generation",
            "entailment graph",
            "corpora"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"leverage the recent advances from large pretrained language models (PLMs)\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"leverage the recent advances from large pretrained language models (PLMs)\""
    },
    {
        "title": "Alleviating Over-smoothing for Unsupervised Sentence Representation",
        "authors": [
            "Nuo Chen",
            "Linjun Shou",
            "Jian Pei",
            "Ming Gong",
            "Bowen Cao",
            "Jianhui Chang",
            "Jia Li",
            "Daxin Jiang"
        ],
        "published": "2023",
        "summary": "Currently, learning better unsupervised sentence representations is the pursuit of many natural language processing communities. Lots of approaches based on pre-trained language models (PLMs) and contrastive learning have achieved promising results on this task. Experimentally, we observe that the over-smoothing problem reduces the capacity of these powerful PLMs, leading to sub-optimal sentence representations. In this paper, we present a Simple method named Self-Contrastive Learning (SSCL) to alleviate this issue, which samples negatives from PLMs intermediate layers, improving the quality of the sentence representation. Our proposed method is quite simple and can be easily extended to various state-of-the-art models for performance boosting, which can be seen as a plug-and-play contrastive framework for learning unsupervised sentence representation. Extensive results prove that SSCL brings the superior performance improvements of different strong baselines (e.g., BERT and SimCSE) on Semantic Textual Similarity and Transfer datasets",
        "pdf_link": "https://aclanthology.org/2023.acl-long.197.pdf",
        "keywords": [
            "pre trained language models",
            "unsupervised sentence",
            "unsupervised sentence representation",
            "smoothing",
            "boosting"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Experimentally, we observe that the over-smoothing problem reduces the capacity of these powerful PLMs, leading to sub-optimal sentence representations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Experimentally, we observe that the over-smoothing problem reduces the capacity of these powerful PLMs, leading to sub-optimal sentence representations.\""
    },
    {
        "title": "From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding",
        "authors": [
            "Li Sun",
            "Florian Luisier",
            "Kayhan Batmanghelich",
            "Dinei Florencio",
            "Cha Zhang"
        ],
        "published": "2023",
        "summary": "Current state-of-the-art models for natural language understanding require a preprocessing step to convert raw text into discrete tokens. This process known as tokenization relies on a pre-built vocabulary of words or sub-word morphemes. This fixed vocabulary limits the model’s robustness to spelling errors and its capacity to adapt to new domains. In this work, we introduce a novel open-vocabulary language model that adopts a hierarchical two-level approach: one at the word level and another at the sequence level. Concretely, we design an intra-word module that uses a shallow Transformer architecture to learn word representations from their characters, and a deep inter-word Transformer module that contextualizes each word representation by attending to the entire word sequence. Our model thus directly operates on character sequences with explicit awareness of word boundaries, but without biased sub-word or word-level vocabulary. Experiments on various downstream tasks show that our method outperforms strong baselines. We also demonstrate that our hierarchical model is robust to textual corruption and domain shift.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.200.pdf",
        "keywords": [
            "open vocabulary language",
            "open vocabulary language model",
            "hierarchical model",
            "tokenization"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This fixed vocabulary limits the model’s robustness to spelling errors and its capacity to adapt to new domains.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"This fixed vocabulary limits the model’s robustness to spelling errors and its capacity to adapt to new domains.\""
    },
    {
        "title": "MatSci-NLP: Evaluating Scientific Language Models on Materials Science Language Tasks Using Text-to-Schema Modeling",
        "authors": [
            "Yu Song",
            "Santiago Miret",
            "Bang Liu"
        ],
        "published": "2023",
        "summary": "We present MatSci-NLP, a natural language benchmark for evaluating the performance of natural language processing (NLP) models on materials science text. We construct the benchmark from publicly available materials science text data to encompass seven different NLP tasks, including conventional NLP tasks like named entity recognition and relation classification, as well as NLP tasks specific to materials science, such as synthesis action retrieval which relates to creating synthesis procedures for materials. We study various BERT-based models pretrained on different scientific text corpora on MatSci-NLP to understand the impact of pretraining strategies on understanding materials science text. Given the scarcity of high-quality annotated data in the materials science domain, we perform our fine-tuning experiments with limited training data to encourage the generalize across MatSci-NLP tasks. Our experiments in this low-resource training setting show that language models pretrained on scientific text outperform BERT trained on general text. MatBERT, a model pretrained specifically on materials science journals, generally performs best for most tasks. Moreover, we propose a unified text-to-schema for multitask learning on {pasted macro ‘BENCHMARK’} and compare its performance with traditional fine-tuning methods. In our analysis of different training methods, we find that our proposed text-to-schema methods inspired by question-answering consistently outperform single and multitask NLP fine-tuning methods. The code and datasets are publicly available https://github.com/BangLab-UdeM-Mila/NLP4MatSci-ACL23.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.201.pdf",
        "keywords": [
            "natural language processing",
            "text",
            "materials science text",
            "scientific text corpora",
            "scientific language models",
            "materials science language tasks",
            "named entity recognition",
            "schema",
            "classification"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Given the scarcity of high-quality annotated data in the materials science domain, we perform our fine-tuning experiments with limited training data to encourage the generalize across MatSci-NLP tasks.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs (the scarcity of high-quality annotated data in the materials science domain) but does not explore it in depth and uses",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Given the scarcity of high-quality annotated data in the materials science domain, we perform our fine-tuning experiments with limited training data to encourage the generalize across MatSci-NLP tasks.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs (the scarcity of high-quality annotated data in the materials science domain) but does not explore it in depth and uses"
    },
    {
        "title": "Code4Struct: Code Generation for Few-Shot Event Structure Prediction",
        "authors": [
            "Xingyao Wang",
            "Sha Li",
            "Heng Ji"
        ],
        "published": "2023",
        "summary": "Large Language Model (LLM) trained on a mixture of text and code has demonstrated impressive capability in translating natural language (NL) into structured code. We observe that semantic structures can be conveniently translated into code and propose Code4Struct to leverage such text-to-structure translation capability to tackle structured prediction tasks. As a case study, we formulate Event Argument Extraction (EAE) as converting text into event-argument structures that can be represented as a class object using code. This alignment between structures and code enables us to take advantage of Programming Language (PL) features such as inheritance and type annotation to introduce external knowledge or add constraints. We show that, with sufficient in-context examples, formulating EAE as a code generation problem is advantageous over using variants of text-based prompts. Despite only using 20 training event instances for each event type, Code4Struct is comparable to supervised models trained on 4,202 instances and outperforms current state-of-the-art (SOTA) trained on 20-shot data by 29.5% absolute F1. Code4Struct can use 10-shot training data from a sibling event type to predict arguments for zero-resource event types and outperforms the zero-shot baseline by 12% absolute F1.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.202.pdf",
        "keywords": [
            "event argument extraction",
            "code generation",
            "structure prediction",
            "language model"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs, only mentions the capabilities of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No evidence of discussion of limitations of LLMs, only mentions the capabilities of LLMs."
    },
    {
        "title": "Tree-Based Representation and Generation of Natural and Mathematical Language",
        "authors": [
            "Alexander Scarlatos",
            "Andrew Lan"
        ],
        "published": "2023",
        "summary": "Mathematical language in scientific communications and educational scenarios is important yet relatively understudied compared to natural languages. Recent works on mathematical language focus either on representing stand-alone mathematical expressions, especially in their natural tree format, or mathematical reasoning in pre-trained natural language models. Existing works on jointly modeling and generating natural and mathematical languages simply treat mathematical expressions as text, without accounting for the rigid structural properties of mathematical expressions. In this paper, we propose a series of modifications to existing language models to jointly represent and generate text and math: representing mathematical expressions as sequences of node tokens in their operator tree format, using math symbol and tree position embeddings to preserve the semantic and structural properties of mathematical expressions, and using a constrained decoding method to generate mathematically valid expressions. We ground our modifications in GPT-2, resulting in a model MathGPT, and demonstrate that it outperforms baselines on mathematical expression generation tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.205.pdf",
        "keywords": [
            "mathematical expression generation",
            "mathematical expressions",
            "mathematical language",
            "natural languages",
            "natural tree"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing works on jointly modeling and generating natural and mathematical languages simply treat mathematical expressions as text, without accounting for the rigid structural properties of mathematical expressions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing works on jointly modeling and generating natural and mathematical languages simply treat mathematical expressions as text, without accounting for the rigid structural properties of mathematical expressions.\""
    },
    {
        "title": "ParaLS: Lexical Substitution via Pretrained Paraphraser",
        "authors": [
            "Jipeng Qiang",
            "Kang Liu",
            "Yun Li",
            "Yunhao Yuan",
            "Yi Zhu"
        ],
        "published": "2023",
        "summary": "Lexical substitution (LS) aims at finding appropriate substitutes for a target word in a sentence. Recently, LS methods based on pretrained language models have made remarkable progress, generating potential substitutes for a target word through analysis of its contextual surroundings. However, these methods tend to overlook the preservation of the sentence’s meaning when generating the substitutes. This study explores how to generate the substitute candidates from a paraphraser, as the generated paraphrases from a paraphraser contain variations in word choice and preserve the sentence’s meaning. Since we cannot directly generate the substitutes via commonly used decoding strategies, we propose two simple decoding strategies that focus on the variations of the target word during decoding. Experimental results show that our methods outperform state-of-the-art LS methods based on pre-trained language models on three benchmarks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.206.pdf",
        "keywords": [
            "paraphraser",
            "lexical substitution",
            "parals"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these methods tend to overlook the preservation of the sentence’s meaning when generating the substitutes.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these methods tend to overlook the preservation of the sentence’s meaning when generating the substitutes.\""
    },
    {
        "title": "Free Lunch for Efficient Textual Commonsense Integration in Language Models",
        "authors": [
            "Wanyun Cui",
            "Xingran Chen"
        ],
        "published": "2023",
        "summary": "Recent years have witnessed the emergence of textual commonsense knowledge bases, aimed at providing more nuanced and context-rich knowledge. The integration of external commonsense into language models has been shown to be a key enabler in advancing the state-of-the-art for a wide range of NLP tasks. However, incorporating textual commonsense descriptions is computationally expensive, as compared to encoding conventional symbolic knowledge. In this paper, we propose a method to improve its efficiency without modifying the model. Our idea is to group training samples with similar commonsense descriptions into a single batch, thus reusing the encoded description across multiple samples. We theoretically investigate this problem and demonstrate that its upper bound can be reduced to the classic graph k-cut problem. Consequently, we propose a spectral clustering-based algorithm to solve this problem. Extensive experiments illustrate that the proposed batch partitioning approach effectively reduces the computational cost while preserving performance. The efficiency improvement is more pronounced on larger datasets and on devices with more memory capacity, attesting to its practical utility for large-scale applications.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.208.pdf",
        "keywords": [
            "commonsense",
            "free lunch",
            "integration",
            "language models",
            "textual commonsense integration",
            "textual commonsense",
            "textual commonsense knowledge",
            "clustering"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, incorporating textual commonsense descriptions is computationally expensive, as compared to encoding conventional symbolic knowledge.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, incorporating textual commonsense descriptions is computationally expensive, as compared to encoding conventional symbolic knowledge.\""
    },
    {
        "title": "MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset",
        "authors": [
            "Leonhard Hennig",
            "Philippe Thomas",
            "Sebastian Möller"
        ],
        "published": "2023",
        "summary": "Relation extraction (RE) is a fundamental task in information extraction, whose extension to multilingual settings has been hindered by the lack of supervised resources comparable in size to large English datasets such as TACRED (Zhang et al., 2017). To address this gap, we introduce the MultiTACRED dataset, covering 12 typologically diverse languages from 9 language families, which is created by machine-translating TACRED instances and automatically projecting their entity annotations. We analyze translation and annotation projection quality, identify error categories, and experimentally evaluate fine-tuned pretrained mono- and multilingual language models in common transfer learning scenarios. Our analyses show that machine translation is a viable strategy to transfer RE instances, with native speakers judging more than 83% of the translated instances to be linguistically and semantically acceptable. We find monolingual RE model performance to be comparable to the English original for many of the target languages, and that multilingual models trained on a combination of English and target language data can outperform their monolingual counterparts. However, we also observe a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages, such as pronoun-dropping, compounding and inflection, that degrade dataset quality and RE model performance.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.210.pdf",
        "keywords": [
            "relation extraction",
            "tac relation extraction"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, we also observe a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages, such as pronoun-dropping, compounding and inflection, that degrade dataset quality and RE model performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, we also observe a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages, such as pronoun-dropping, compounding and inflection, that degrade dataset quality and RE model performance.\""
    },
    {
        "title": "Small Pre-trained Language Models Can be Fine-tuned as Large Models via Over-Parameterization",
        "authors": [
            "Ze-Feng Gao",
            "Kun Zhou",
            "Peiyu Liu",
            "Wayne Xin Zhao",
            "Ji-Rong Wen"
        ],
        "published": "2023",
        "summary": "By scaling the model size, large pre-trained language models (PLMs) have shown remarkable performance in various natural language processing tasks, mostly outperforming small PLMs by a large margin. However, due to the high computational cost, the huge number of parameters also restricts the applicability of large PLMs in real-world systems. In this paper, we focus on scaling up the parameters of PLMs only during fine-tuning, to benefit from the over-parameterization, while without increasing the inference latency. Given a relatively small PLM, we over-parameterize it by employing a matrix product operator, an efficient and almost lossless decomposition method to factorize its contained parameter matrices into a set of higher-dimensional tensors.Considering the efficiency, we further propose both static and dynamic strategies to select the most important parameter matrices for over-parameterization.Extensive experiments have demonstrated that our approach can significantly boost the fine-tuning performance of small PLMs and even help small PLMs outperform 3× parameterized larger ones.Our code is publicly available at https://github.com/zfgao66/OPF.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.212.pdf",
        "keywords": [
            "language",
            "pre trained language models",
            "parameterization",
            "tuning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, due to the high computational cost, the huge number of parameters also restricts the applicability of large PLMs in real-world systems.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, due to the high computational cost, the huge number of parameters also restricts the applicability of large PLMs in real-world systems.\""
    },
    {
        "title": "Entity Tracking in Language Models",
        "authors": [
            "Najoung Kim",
            "Sebastian Schuster"
        ],
        "published": "2023",
        "summary": "Keeping track of how states of entities change as a text or dialog unfolds is a key prerequisite to discourse understanding. Yet, there have been few systematic investigations into the ability of large language models (LLMs) to track discourse entities. In this work, we present a task probing to what extent a language model can infer the final state of an entity given an English description of the initial state and a series of state-changing operations. We use this task to first investigate whether Flan-T5, GPT-3 and GPT-3.5 can track the state of entities, and find that only GPT-3.5 models, which have been pretrained on large amounts of code, exhibit this ability. We then investigate whether smaller models pretrained primarily on text can learn to track entities, through finetuning T5 on several training/evaluation splits. While performance degrades for more complex splits, we find that even when evaluated on a different set of entities from training or longer operation sequences, a finetuned model can perform non-trivial entity tracking. Taken together, these results suggest that language models can learn to track entities but pretraining on text corpora alone does not make this capacity surface.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.213.pdf",
        "keywords": [
            "entity tracking",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that only GPT-3.5 models, which have been pretrained on large amounts of code, exhibit this ability.\" and \"pretraining on text corpora alone does not make this capacity surface.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We find that only GPT-3.5 models, which have been pretrained on large amounts of code, exhibit this ability.\" and \"pretraining on text corpora alone does not make this capacity surface.\""
    },
    {
        "title": "Faithful Question Answering with Monte-Carlo Planning",
        "authors": [
            "Ruixin Hong",
            "Hongming Zhang",
            "Hong Zhao",
            "Dong Yu",
            "Changshui Zhang"
        ],
        "published": "2023",
        "summary": "Although large language models demonstrate remarkable question-answering performances, revealing the intermediate reasoning steps that the models faithfully follow remains challenging. In this paper, we propose FAME (FAithful question answering with MontE-carlo planning) to answer questions based on faithful reasoning steps. The reasoning steps are organized as a structured entailment tree, which shows how premises are used to produce intermediate conclusions that can prove the correctness of the answer. We formulate the task as a discrete decision-making problem and solve it through the interaction of a reasoning environment and a controller. The environment is modular and contains several basic task-oriented modules, while the controller proposes actions to assemble the modules. Since the search space could be large, we introduce a Monte-Carlo planning algorithm to do a look-ahead search and select actions that will eventually lead to high-quality steps. FAME achieves advanced performance on the standard benchmark. It can produce valid and faithful reasoning steps compared with large language models with a much smaller model size.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.218.pdf",
        "keywords": [
            "reasoning",
            "question answering",
            "faithful question answering",
            "faithful reasoning",
            "monte carlo planning",
            "structured entailment tree",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although large language models demonstrate remarkable question-answering performances, revealing the intermediate reasoning steps that the models faithfully follow remains challenging.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although large language models demonstrate remarkable question-answering performances, revealing the intermediate reasoning steps that the models faithfully follow remains challenging.\""
    },
    {
        "title": "Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer with Fine-tuning Slow and Fast",
        "authors": [
            "Yiduo Guo",
            "Yaobo Liang",
            "Dongyan Zhao",
            "Bing Liu",
            "Nan Duan"
        ],
        "published": "2023",
        "summary": "Existing research has shown that a multilingual pre-trained language model fine-tuned with one (source) language also performs well on downstream tasks for non-source languages, even though no fine-tuning is done on these languages. However, there is a clear gap between the performance of the source language and that of the non-source languages. This paper analyzes the fine-tuning process, discovers when the performance gap changes and identifies which network weights affect the overall performance most. Additionally, the paper seeks to answer to what extent the gap can be reduced by reducing forgetting. Based on the analysis results, a method named Fine-tuning slow and fast with four training policies is proposed to address these issues. Experimental results show the proposed method outperforms baselines by a clear margin.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.221.pdf",
        "keywords": [
            "tuning",
            "fine tuned"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, there is a clear gap between the performance of the source language and that of the non-source languages.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, there is a clear gap between the performance of the source language and that of the non-source languages.\""
    },
    {
        "title": "MM-SHAP: A Performance-agnostic Metric for Measuring Multimodal Contributions in Vision and Language Models & Tasks",
        "authors": [
            "Letitia Parcalabescu",
            "Anette Frank"
        ],
        "published": "2023",
        "summary": "Vision and language models (VL) are known to exploit unrobust indicators in individual modalities (e.g., introduced by distributional biases) instead of focusing on relevant information in each modality. That a unimodal model achieves similar accuracy on a VL task to a multimodal one, indicates that so-called unimodal collapse occurred. However, accuracy-based tests fail to detect e.g., when the model prediction is wrong, while the model used relevant information from a modality. Instead, we propose MM-SHAP, a performance-agnostic multimodality score based on Shapley values that reliably quantifies in which proportions a multimodal model uses individual modalities. We apply MM-SHAP in two ways: (1) to compare models for their average degree of multimodality, and (2) to measure for individual models the contribution of individual modalities for different tasks and datasets. Experiments with six VL models – LXMERT, CLIP and four ALBEF variants – on four VL tasks highlight that unimodal collapse can occur to different degrees and in different directions, contradicting the wide-spread assumption that unimodal collapse is one-sided. Based on our results, we recommend MM-SHAP for analysing multimodal tasks, to diagnose and guide progress towards multimodal integration. Code available at https://github.com/Heidelberg-NLP/MM-SHAP.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.223.pdf",
        "keywords": [
            "shap",
            "mm shap",
            "multimodal integration",
            "performance agnostic",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Vision and language models (VL) are known to exploit unrobust indicators in individual modalities (e.g., introduced by distributional biases) instead of focusing on relevant information in each modality.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Vision and language models (VL) are known to exploit unrobust indicators in individual modalities (e.g., introduced by distributional biases) instead of focusing on relevant information in each modality.\""
    },
    {
        "title": "Towards Boosting the Open-Domain Chatbot with Human Feedback",
        "authors": [
            "Hua Lu",
            "Siqi Bao",
            "Huang He",
            "Fan Wang",
            "Hua Wu",
            "Haifeng Wang"
        ],
        "published": "2023",
        "summary": "Many open-domain dialogue models pre-trained with social media comments can generate coherent replies but have difficulties producing engaging responses. This phenomenon might mainly result from the deficiency of annotated human-human conversations and the misalignment with human preference. In this paper, we propose a novel and efficient framework Diamante to boost the open-domain chatbot, where two kinds of human feedback (including explicit demonstration and implicit preference) are collected and leveraged. By asking annotators to select or amend the model-generated candidate responses, Diamante efficiently collects the human demonstrated responses and constructs a Chinese chit-chat dataset. To enhance the alignment with human preference, Diamante leverages the implicit preference in the data collection process and introduces the generation-evaluation joint training. Comprehensive experiments indicate that the Diamante dataset and joint training paradigm can significantly boost the performance of pre-trained dialogue models. The overall engagingness of the previous state-of-the-art model has been improved remarkably by 50% in Chinese open-domain conversations.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.224.pdf",
        "keywords": [
            "chatbot",
            "boosting",
            "feedback"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Many open-domain dialogue models pre-trained with social media comments can generate coherent replies but have difficulties producing engaging responses.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Many open-domain dialogue models pre-trained with social media comments can generate coherent replies but have difficulties producing engaging responses.\""
    },
    {
        "title": "Social-Group-Agnostic Bias Mitigation via the Stereotype Content Model",
        "authors": [
            "Ali Omrani",
            "Alireza Salkhordeh Ziabari",
            "Charles Yu",
            "Preni Golazizian",
            "Brendan Kennedy",
            "Mohammad Atari",
            "Heng Ji",
            "Morteza Dehghani"
        ],
        "published": "2023",
        "summary": "Existing bias mitigation methods require social-group-specific word pairs (e.g., “man” – “woman”) for each social attribute (e.g., gender), restricting the bias mitigation to only one specified social attribute. Further, this constraint renders such methods impractical and costly for mitigating bias in understudied and/or unmarked social groups. We propose that the Stereotype Content Model (SCM) — a theoretical framework developed in social psychology for understanding the content of stereotyping — can help debiasing efforts to become social-group-agnostic by capturing the underlying connection between bias and stereotypes. SCM proposes that the content of stereotypes map to two psychological dimensions of warmth and competence. Using only pairs of terms for these two dimensions (e.g., warmth: “genuine” – “fake”; competence: “smart” – “stupid”), we perform debiasing with established methods on both pre-trained word embeddings and large language models. We demonstrate that our social-group-agnostic, SCM-based debiasing technique performs comparably to group-specific debiasing on multiple bias benchmarks, but has theoretical and practical advantages over existing approaches.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.227.pdf",
        "keywords": [
            "debiasing",
            "bias",
            "social psychology",
            "stereotype content model",
            "social group",
            "stereotypes",
            "group",
            "social group agnostic",
            "language models",
            "warmth",
            "gender"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations of LLMs are mentioned in the abstract, but it discusses the limitations of existing bias mitigation methods for LLMs, specifically that they are impractical and costly for mitigating bias in understudied and/or unmarked social groups.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: No explicit limitations of LLMs are mentioned in the abstract, but it discusses the limitations of existing bias mitigation methods for LLMs, specifically that they are impractical and costly for mitigating bias in understudied and/or unmarked social groups."
    },
    {
        "title": "Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation",
        "authors": [
            "Yixin Liu",
            "Alex Fabbri",
            "Pengfei Liu",
            "Yilun Zhao",
            "Linyong Nan",
            "Ruilin Han",
            "Simeng Han",
            "Shafiq Joty",
            "Chien-Sheng Wu",
            "Caiming Xiong",
            "Dragomir Radev"
        ],
        "published": "2023",
        "summary": "Human evaluation is the foundation upon which the evaluation of both summarization systems and automatic metrics rests. However, existing human evaluation studies for summarization either exhibit a low inter-annotator agreement or have insufficient scale, and an in-depth analysis of human evaluation is lacking. Therefore, we address the shortcomings of existing summarization evaluation along the following axes: (1) We propose a modified summarization salience protocol, Atomic Content Units (ACUs), which is based on fine-grained semantic units and allows for a high inter-annotator agreement. (2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large human evaluation dataset consisting of 22,000 summary-level annotations over 28 top-performing systems on three datasets. (3) We conduct a comparative study of four human evaluation protocols, underscoring potential confounding factors in evaluation setups. (4) We evaluate 50 automatic metrics and their variants using the collected human annotations across evaluation protocols and demonstrate how our benchmark leads to more statistically stable and significant results. The metrics we benchmarked include recent methods based on large language models (LLMs), GPTScore and G-Eval. Furthermore, our findings have important implications for evaluating LLMs, as we show that LLMs adjusted by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation, which is affected by the annotators’ prior, input-agnostic preferences, calling for more robust, targeted evaluation methods.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.228.pdf",
        "keywords": [
            "summarization",
            "summarization evaluation"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Furthermore, our findings have important implications for evaluating LLMs, as we show that LLMs adjusted by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation, which is affected by the annotators’ prior, input-agnostic preferences, calling for more robust, targeted evaluation methods.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Furthermore, our findings have important implications for evaluating LLMs, as we show that LLMs adjusted by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation, which is affected by the annotators’ prior, input-agnostic preferences, calling for more robust, targeted evaluation methods.\""
    },
    {
        "title": "FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information",
        "authors": [
            "Andrew Zhu",
            "Karmanya Aggarwal",
            "Alexander Feng",
            "Lara J. Martin",
            "Chris Callison-Burch"
        ],
        "published": "2023",
        "summary": "Dungeons & Dragons (D&D) is a tabletop roleplaying game with complex natural language interactions between players and hidden state information. Recent work has shown that large language models (LLMs) that have access to state information can generate higher quality game turns than LLMs that use dialog history alone. However, previous work used game state information that was heuristically created and was not a true gold standard game state. We present FIREBALL, a large dataset containing nearly 25,000 unique sessions from real D&D gameplay on Discord with true game state info. We recorded game play sessions of players who used the Avrae bot, which was developed to aid people in playing D&D online, capturing language, game commands and underlying game state information. We demonstrate that FIREBALL can improve natural language generation (NLG) by using Avrae state information, improving both automated metrics and human judgments of quality. Additionally, we show that LLMs can generate executable Avrae commands, particularly after finetuning.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.229.pdf",
        "keywords": [
            "natural language generation",
            "tabletop roleplaying",
            "structured game",
            "fireball"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, previous work used game state information that was heuristically created and was not a true gold standard game state.\"\n\nThis evidence suggests that the paper mentions a limitation of previous work with LLMs, but it is not a limitation of LLMs themselves.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, previous work used game state information that was heuristically created and was not a true gold standard game state.\"\n\nThis evidence suggests that the paper mentions a limitation of previous work with LLMs, but it is not a limitation of LLMs themselves."
    },
    {
        "title": "A fine-grained comparison of pragmatic language understanding in humans and language models",
        "authors": [
            "Jennifer Hu",
            "Sammy Floyd",
            "Olessia Jouravlev",
            "Evelina Fedorenko",
            "Edward Gibson"
        ],
        "published": "2023",
        "summary": "Pragmatics and non-literal language understanding are essential to human communication, and present a long-standing challenge for artificial language models. We perform a fine-grained comparison of language models and humans on seven pragmatic phenomena, using zero-shot prompting on an expert-curated set of English materials. We ask whether models (1) select pragmatic interpretations of speaker utterances, (2) make similar error patterns as humans, and (3) use similar linguistic cues as humans to solve the tasks. We find that the largest models achieve high accuracy and match human error patterns: within incorrect responses, models favor literal interpretations over heuristic-based distractors. We also find preliminary evidence that models and humans are sensitive to similar linguistic cues. Our results suggest that pragmatic behaviors can emerge in models without explicitly constructed representations of mental states. However, models tend to struggle with phenomena relying on social expectation violations.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.230.pdf",
        "keywords": [
            "language",
            "language models",
            "artificial language models",
            "non literal language",
            "perform",
            "fine grained"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, models tend to struggle with phenomena relying on social expectation violations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, models tend to struggle with phenomena relying on social expectation violations.\""
    },
    {
        "title": "Causal-Debias: Unifying Debiasing in Pretrained Language Models and Fine-tuning via Causal Invariant Learning",
        "authors": [
            "Fan Zhou",
            "Yuzhou Mao",
            "Liu Yu",
            "Yi Yang",
            "Ting Zhong"
        ],
        "published": "2023",
        "summary": "Demographic biases and social stereotypes are common in pretrained language models (PLMs), and a burgeoning body of literature focuses on removing the unwanted stereotypical associations from PLMs. However, when fine-tuning these bias-mitigated PLMs in downstream natural language processing (NLP) applications, such as sentiment classification, the unwanted stereotypical associations resurface or even get amplified. Since pretrain&fine-tune is a major paradigm in NLP applications, separating the debiasing procedure of PLMs from fine-tuning would eventually harm the actual downstream utility. In this paper, we propose a unified debiasing framework Causal-Debias to remove unwanted stereotypical associations in PLMs during fine-tuning. Specifically, CausalDebias mitigates bias from a causal invariant perspective by leveraging the specific downstream task to identify bias-relevant and labelrelevant factors. We propose that bias-relevant factors are non-causal as they should have little impact on downstream tasks, while labelrelevant factors are causal. We perform interventions on non-causal factors in different demographic groups and design an invariant risk minimization loss to mitigate bias while maintaining task performance. Experimental results on three downstream tasks show that our proposed method can remarkably reduce unwanted stereotypical associations after PLMs are finetuned, while simultaneously minimizing the impact on PLMs and downstream applications.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.232.pdf",
        "keywords": [
            "debiasing",
            "language models",
            "tuning",
            "fine tuning",
            "pretrained language models",
            "natural language processing",
            "invariant"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Demographic biases and social stereotypes are common in pretrained language models (PLMs), and a burgeoning body of literature focuses on removing the unwanted stereotypical associations from PLMs. However, when fine-tuning these bias-mitigated PLMs in downstream natural language processing (NLP) applications, such as sentiment classification, the unwanted stereotypical associations resurface or even get amplified.\"\n\nNote",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Demographic biases and social stereotypes are common in pretrained language models (PLMs), and a burgeoning body of literature focuses on removing the unwanted stereotypical associations from PLMs. However, when fine-tuning these bias-mitigated PLMs in downstream natural language processing (NLP) applications, such as sentiment classification, the unwanted stereotypical associations resurface or even get amplified.\"\n\nNote"
    },
    {
        "title": "Parameter-Efficient Fine-Tuning without Introducing New Latency",
        "authors": [
            "Baohao Liao",
            "Yan Meng",
            "Christof Monz"
        ],
        "published": "2023",
        "summary": "Parameter-efficient fine-tuning (PEFT) of pre-trained language models has recently demonstrated remarkable achievements, effectively matching the performance of full fine-tuning while utilizing significantly fewer trainable parameters, and consequently addressing the storage and communication constraints. Nonetheless, various PEFT methods are limited by their inherent characteristics. In the case of sparse fine-tuning, which involves modifying only a small subset of the existing parameters, the selection of fine-tuned parameters is task- and domain-specific, making it unsuitable for federated learning. On the other hand, PEFT methods with adding new parameters typically introduce additional inference latency. In this paper, we demonstrate the feasibility of generating a sparse mask in a task-agnostic manner, wherein all downstream tasks share a common mask. Our approach, which relies solely on the magnitude information of pre-trained parameters, surpasses existing methodologies by a significant margin when evaluated on the GLUE benchmark. Additionally, we introduce a novel adapter technique that directly applies the adapter to pre-trained parameters instead of the hidden representation, thereby achieving identical inference speed to that of full fine-tuning. Through extensive experiments, our proposed method attains a new state-of-the-art outcome in terms of both performance and storage efficiency, storing only 0.03% parameters of full fine-tuning.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.233.pdf",
        "keywords": [
            "tuning",
            "fine tuning",
            "adapter",
            "inference latency",
            "parameter efficient fine tuning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"various PEFT methods are limited by their inherent characteristics... the selection of fine-tuned parameters is task- and domain-specific, making it unsuitable for federated learning... PEFT methods with adding new parameters typically introduce additional inference latency.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"various PEFT methods are limited by their inherent characteristics... the selection of fine-tuned parameters is task- and domain-specific, making it unsuitable for federated learning... PEFT methods with adding new parameters typically introduce additional inference latency.\""
    },
    {
        "title": "Distilling Script Knowledge from Large Language Models for Constrained Language Planning",
        "authors": [
            "Siyu Yuan",
            "Jiangjie Chen",
            "Ziquan Fu",
            "Xuyang Ge",
            "Soham Shah",
            "Charles Jankowski",
            "Yanghua Xiao",
            "Deqing Yang"
        ],
        "published": "2023",
        "summary": "In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., “make a cake”), but leaves more specific goals with multi-facet constraints understudied (e.g., “make a cake for diabetics”). In this paper, we define the task of constrained language planning for the first time. We propose an over-generate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, Coscript, which consists of 55,000 scripts. Empirical results demonstrate that our method significantly improves the constrained language planning ability of LLMs, especially on constraint faithfulness. Furthermore, Coscript is demonstrated to be quite effective in endowing smaller LMs with constrained language planning ability.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.236.pdf",
        "keywords": [
            "language models",
            "constrained language planning",
            "constraint faithfulness",
            "script knowledge"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., “make a cake”), but leaves more specific goals with multi-facet constraints understudied (e.g., “make a cake for diabetics”).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., “make a cake”), but leaves more specific goals with multi-facet constraints understudied (e.g., “make a cake for diabetics”).\""
    },
    {
        "title": "CELDA: Leveraging Black-box Language Model as Enhanced Classifier without Labels",
        "authors": [
            "Hyunsoo Cho",
            "Youna Kim",
            "Sang-goo Lee"
        ],
        "published": "2023",
        "summary": "Utilizing language models (LMs) without internal access is becoming an attractive paradigm in the field of NLP as many cutting-edge LMs are released through APIs and boast a massive scale. The de-facto method in this type of black-box scenario is known as prompting, which has shown progressive performance enhancements in situations where data labels are scarce or unavailable. Despite their efficacy, they still fall short in comparison to fully supervised counterparts and are generally brittle to slight modifications. In this paper, we propose Clustering-enhanced Linear Discriminative Analysis (CELDA), a novel approach that improves the text classification accuracy with a very weak-supervision signal (i.e., name of the labels).Our framework draws a precise decision boundary without accessing weights or gradients of the LM model or data labels. The core ideas of CELDA are twofold:(1) extracting a refined pseudo-labeled dataset from an unlabeled dataset, and (2) training a lightweight and robust model on the top of LM, which learns an accurate decision boundary from an extracted noisy dataset. Throughout in-depth investigations on various datasets, we demonstrated that CELDA reaches new state-of-the-art in weakly-supervised text classification and narrows the gap with a fully-supervised model. Additionally, our proposed methodology can be applied universally to any LM and has the potential to scale to larger models, making it a more viable option for utilizing large LMs.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.239.pdf",
        "keywords": [
            "classifier without labels",
            "language models",
            "black box",
            "linear discriminative analysis",
            "classification",
            "clustering"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite their efficacy, they still fall short in comparison to fully supervised counterparts and are generally brittle to slight modifications.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite their efficacy, they still fall short in comparison to fully supervised counterparts and are generally brittle to slight modifications.\""
    },
    {
        "title": "MvP: Multi-view Prompting Improves Aspect Sentiment Tuple Prediction",
        "authors": [
            "Zhibin Gou",
            "Qingyan Guo",
            "Yujiu Yang"
        ],
        "published": "2023",
        "summary": "Generative methods greatly promote aspect-based sentiment analysis via generating a sequence of sentiment elements in a specified format. However, existing studies usually predict sentiment elements in a fixed order, which ignores the effect of the interdependence of the elements in a sentiment tuple and the diversity of language expression on the results. In this work, we propose Multi-view Prompting (MVP) that aggregates sentiment elements generated in different orders, leveraging the intuition of human-like problem-solving processes from different views. Specifically, MVP introduces element order prompts to guide the language model to generate multiple sentiment tuples, each with a different element order, and then selects the most reasonable tuples by voting. MVP can naturally model multi-view and multi-task as permutations and combinations of elements, respectively, outperforming previous task-specific designed methods on multiple ABSA tasks with a single model. Extensive experiments show that MVP significantly advances the state-of-the-art performance on 10 datasets of 4 benchmark tasks, and performs quite effectively in low-resource settings. Detailed evaluation verified the effectiveness, flexibility, and cross-task transferability of MVP.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.240.pdf",
        "keywords": [
            "multi view prompting",
            "aspect sentiment tuple prediction",
            "aspect based sentiment analysis",
            "permutations"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions \"existing studies\" and their limitations, which implies that the authors are aware of limitations in the field, but it does not explicitly discuss limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions \"existing studies\" and their limitations, which implies that the authors are aware of limitations in the field, but it does not explicitly discuss limitations of LLMs."
    },
    {
        "title": "ACCENT: An Automatic Event Commonsense Evaluation Metric for Open-Domain Dialogue Systems",
        "authors": [
            "Sarik Ghazarian",
            "Yijia Shao",
            "Rujun Han",
            "Aram Galstyan",
            "Nanyun Peng"
        ],
        "published": "2023",
        "summary": "Commonsense reasoning is omnipresent in human communications and thus is an important feature for open-domain dialogue systems. However, evaluating commonsense in dialogue systems is still an open challenge. We take the first step by focusing on event commonsense that considers events and their relations, and is crucial in both dialogues and general commonsense reasoning. We propose ACCENT, an event commonsense evaluation metric empowered by commonsense knowledge bases (CSKBs). ACCENT first extracts event-relation tuples from a dialogue, and then evaluates the response by scoring the tuples in terms of their compatibility with the CSKB. To evaluate ACCENT, we construct the first public event commonsense evaluation dataset for open-domain dialogues.Our experiments show that ACCENT is an efficient metric for event commonsense evaluation, which achieves higher correlations with human judgments than existing baselines.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.241.pdf",
        "keywords": [
            "commonsense",
            "dialogue",
            "event commonsense",
            "dialogue systems",
            "metric",
            "commonsense reasoning",
            "accent"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Explanation-based Finetuning Makes Models More Robust to Spurious Cues",
        "authors": [
            "Josh Magnus Ludan",
            "Yixuan Meng",
            "Tai Nguyen",
            "Saurabh Shah",
            "Qing Lyu",
            "Marianna Apidianaki",
            "Chris Callison-Burch"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task, leading to poor generalization on out-of-distribution data. We propose explanation-based finetuning as a general approach to mitigate LLMs’ reliance on spurious correlations. Unlike standard finetuning where the model only predicts the answer given the input, we finetune the model to additionally generate a free-text explanation supporting its answer. To evaluate our method, we finetune the model on artificially constructed training sets containing different types of spurious cues, and test it on a test set without these cues. Compared to standard finetuning, our method makes GPT-3 (davinci) remarkably more robust against spurious cues in terms of accuracy drop across four classification tasks: ComVE (+1.2), CREAK (+9.1), e-SNLI (+15.4), and SBIC (+6.5). The efficacy generalizes across multiple model families and scales, with greater gains for larger models. Finally, our method also works well with explanations generated by the model, implying its applicability to more datasets without human-written explanations.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.242.pdf",
        "keywords": [
            "finetuning",
            "spurious correlations"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large Language Models (LLMs) are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task, leading to poor generalization on out-of-distribution data.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Large Language Models (LLMs) are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task, leading to poor generalization on out-of-distribution data.\""
    },
    {
        "title": "CAME: Confidence-guided Adaptive Memory Efficient Optimization",
        "authors": [
            "Yang Luo",
            "Xiaozhe Ren",
            "Zangwei Zheng",
            "Zhuo Jiang",
            "Xin Jiang",
            "Yang You"
        ],
        "published": "2023",
        "summary": "Adaptive gradient methods, such as Adam and LAMB, have demonstrated excellent performance in the training of large language models. Nevertheless, the need for adaptivity requires maintaining second-moment estimates of the per-parameter gradients, which entails a high cost of extra memory overheads. To solve this problem, several memory-efficient optimizers (e.g., Adafactor) have been proposed to obtain a drastic reduction in auxiliary memory usage, but with a performance penalty. In this paper, we first study a confidence-guided strategy to reduce the instability of existing memory efficient optimizers. Based on this strategy, we propose CAME to simultaneously achieve two goals: fast convergence as in traditional adaptive methods, and low memory usage as in memory-efficient methods. Extensive experiments demonstrate the training stability and superior performance of CAME across various NLP tasks such as BERT and GPT-2 training. Notably, for BERT pre-training on the large batch size of 32,768, our proposed optimizer attains faster convergence and higher accuracy compared with the Adam optimizer. The implementation of CAME is publicly available.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.243.pdf",
        "keywords": [
            "confidence",
            "optimizers",
            "adaptive",
            "adaptive memory efficient optimization",
            "adam optimizer",
            "confidence guided"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the need for adaptivity requires maintaining second-moment estimates of the per-parameter gradients, which entails a high cost of extra memory overheads.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the need for adaptivity requires maintaining second-moment estimates of the per-parameter gradients, which entails a high cost of extra memory overheads.\""
    },
    {
        "title": "On Second Thought, Let’s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning",
        "authors": [
            "Omar Shaikh",
            "Hongxin Zhang",
            "William Held",
            "Michael Bernstein",
            "Diyi Yang"
        ],
        "published": "2023",
        "summary": "Generating a Chain of Thought (CoT) has been shown to consistently improve large language model (LLM) performance on a wide range of NLP tasks. However, prior work has mainly focused on logical reasoning tasks (e.g. arithmetic, commonsense QA); it remains unclear whether improvements hold for more diverse types of reasoning, especially in socially situated contexts. Concretely, we perform a controlled evaluation of zero-shot CoT across two socially sensitive domains: harmful questions and stereotype benchmarks. We find that zero-shot CoT reasoning in sensitive domains significantly increases a model’s likelihood to produce harmful or undesirable output, with trends holding across different prompt formats and model variants. Furthermore, we show that harmful CoTs increase with model size, but decrease with improved instruction following. Our work suggests that zero-shot CoT should be used with caution on socially important tasks, especially when marginalized groups or sensitive topics are involved.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.244.pdf",
        "keywords": [
            "chain of thought",
            "zero shot reasoning"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that zero-shot CoT reasoning in sensitive domains significantly increases a model’s likelihood to produce harmful or undesirable output, with trends holding across different prompt formats and model variants.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We find that zero-shot CoT reasoning in sensitive domains significantly increases a model’s likelihood to produce harmful or undesirable output, with trends holding across different prompt formats and model variants.\""
    },
    {
        "title": "Solving Math Word Problems via Cooperative Reasoning induced Language Models",
        "authors": [
            "Xinyu Zhu",
            "Junjie Wang",
            "Lin Zhang",
            "Yuxiang Zhang",
            "Yongfeng Huang",
            "Ruyi Gan",
            "Jiaxing Zhang",
            "Yujiu Yang"
        ],
        "published": "2023",
        "summary": "Large-scale pre-trained language models (PLMs) bring new opportunities to challenging problems, especially those that need high-level intelligence, such as the math word problem (MWPs). However, directly applying existing PLMs to MWPs can fail as the generation process lacks sufficient supervision and thus lacks fast adaptivity as humans. We notice that human reasoning has a dual reasoning framework that consists of an immediate reaction system (system 1) and a delicate reasoning system (system 2), where the entire reasoning is determined by their interaction. This inspires us to develop a cooperative reasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe), resulting in a human-like reasoning architecture with system 1 as the generator and system 2 as the verifier. In our approach, the generator is responsible for generating reasoning paths, and the verifiers are used to supervise the evaluation in order to obtain reliable feedback for the generator. We evaluate our CoRe framework on several mathematical reasoning datasets and achieve decent improvement over state-of-the-art methods, up to 9.6% increase over best baselines.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.245.pdf",
        "keywords": [
            "cooperative reasoning",
            "pre trained language models",
            "math word problems"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, directly applying existing PLMs to MWPs can fail as the generation process lacks sufficient supervision and thus lacks fast adaptivity as humans.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, directly applying existing PLMs to MWPs can fail as the generation process lacks sufficient supervision and thus lacks fast adaptivity as humans.\""
    },
    {
        "title": "DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models",
        "authors": [
            "Zhengfu He",
            "Tianxiang Sun",
            "Qiong Tang",
            "Kuanning Wang",
            "Xuanjing Huang",
            "Xipeng Qiu"
        ],
        "published": "2023",
        "summary": "We present DiffusionBERT, a new generative masked language model based on discrete dif- fusion models. Diffusion models and many pre- trained language models have a shared training objective, i.e., denoising, making it possible to combine the two powerful models and enjoy the best of both worlds. On the one hand, dif- fusion models offer a promising training strat- egy that helps improve the generation quality. On the other hand, pre-trained denoising lan- guage models (e.g., BERT) can be used as a good initialization that accelerates convergence. We explore training BERT to learn the reverse process of a discrete diffusion process with an absorbing state and elucidate several designs to improve it. First, we propose a new noise schedule for the forward diffusion process that controls the degree of noise added at each step based on the information of each token. Sec- ond, we investigate several designs of incorpo- rating the time step into BERT. Experiments on unconditional text generation demonstrate that DiffusionBERT achieves significant improve- ment over existing diffusion models for text (e.g., D3PM and Diffusion-LM) and previous generative masked language models in terms of perplexity and BLEU score. Promising re- sults in conditional generation tasks show that DiffusionBERT can generate texts of compa- rable quality and more diverse than a series of established baselines.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.248.pdf",
        "keywords": [
            "masked",
            "denoising",
            "masked language model",
            "language models",
            "generative masked language models",
            "fusion"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations of LLMs are mentioned, but the abstract implies that existing generative masked language models and diffusion models have limitations in terms of generation quality, which DiffusionBERT aims to improve.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations of LLMs are mentioned, but the abstract implies that existing generative masked language models and diffusion models have limitations in terms of generation quality, which DiffusionBERT aims to improve."
    },
    {
        "title": "Lifting the Curse of Capacity Gap in Distilling Language Models",
        "authors": [
            "Chen Zhang",
            "Yang Yang",
            "Jiahao Liu",
            "Jingang Wang",
            "Yunsen Xian",
            "Benyou Wang",
            "Dawei Song"
        ],
        "published": "2023",
        "summary": "Pretrained language models (LMs) have shown compelling performance on various downstream tasks, but unfortunately they require a tremendous amount of inference compute. Knowledge distillation finds a path to compress LMs to small ones with a teacher-student paradigm. However, when the capacity gap between the teacher and the student is large, a curse of capacity gap appears, invoking a deficiency in distilling LMs. While a few studies have been carried out to fill the gap, the curse is not yet well tackled. In this paper, we aim at lifting the curse of capacity gap via enlarging the capacity of the student without notably increasing the inference compute. Largely motivated by sparse activation regime of mixture of experts (MoE), we propose a mixture of minimal experts (MiniMoE), which imposes extra parameters to the student but introduces almost no additional inference compute. Experimental results on GLUE and CoNLL demonstrate the curse of capacity gap is lifted by the magic of MiniMoE to a large extent. MiniMoE also achieves the state-of-the-art performance at small FLOPs compared with a range of competitive baselines. With a compression rate as much as ~50×, MiniMoE preserves ~95% GLUE score of the teacher.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.249.pdf",
        "keywords": [
            "capacity",
            "knowledge distillation",
            "capacity gap",
            "of capacity gap",
            "curse of capacity gap",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"a curse of capacity gap appears, invoking a deficiency in distilling LMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"a curse of capacity gap appears, invoking a deficiency in distilling LMs.\""
    },
    {
        "title": "Enhancing Dialogue Generation via Dynamic Graph Knowledge Aggregation",
        "authors": [
            "Chen Tang",
            "Hongbo Zhang",
            "Tyler Loakman",
            "Chenghua Lin",
            "Frank Guerin"
        ],
        "published": "2023",
        "summary": "Incorporating external graph knowledge into neural chatbot models has been proven effective for enhancing dialogue generation. However, in conventional graph neural networks (GNNs), message passing on a graph is independent from text, resulting in the graph representation hidden space differing from that of the text. This training regime of existing models therefore leads to a semantic gap between graph knowledge and text. In this study, we propose a novel framework for knowledge graph enhanced dialogue generation. We dynamically construct a multi-hop knowledge graph with pseudo nodes to involve the language model in feature aggregation within the graph at all steps. To avoid the semantic biases caused by learning on vanilla subgraphs, the proposed framework applies hierarchical graph attention to aggregate graph features on pseudo nodes and then attains a global feature. Therefore, the framework can better utilise the heterogeneous features from both the post and external graph knowledge. Extensive experiments demonstrate that our framework outperforms state-of-the-art (SOTA) baselines on dialogue generation. Further analysis also shows that our representation learning framework can fill the semantic gap by coagulating representations of both text and graph knowledge. Moreover, the language model also learns how to better select knowledge triples for a more informative response via exploiting subgraph patterns within our feature aggregation process. Our code and resources are available at https://github.com/tangg555/SaBART.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.253.pdf",
        "keywords": [
            "dialogue generation",
            "feature aggregation",
            "knowledge aggregation",
            "knowledge graph"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, in conventional graph neural networks (GNNs), message passing on a graph is independent from text, resulting in the graph representation hidden space differing from that of the text.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, in conventional graph neural networks (GNNs), message passing on a graph is independent from text, resulting in the graph representation hidden space differing from that of the text.\""
    },
    {
        "title": "Multi-modal Action Chain Abductive Reasoning",
        "authors": [
            "Mengze Li",
            "Tianbao Wang",
            "Jiahe Xu",
            "Kairong Han",
            "Shengyu Zhang",
            "Zhou Zhao",
            "Jiaxu Miao",
            "Wenqiao Zhang",
            "Shiliang Pu",
            "Fei Wu"
        ],
        "published": "2023",
        "summary": "Abductive Reasoning, has long been considered to be at the core ability of humans, which enables us to infer the most plausible explanation of incomplete known phenomena in daily life. However, such critical reasoning capability is rarely investigated for contemporary AI systems under such limited observations. To facilitate this research community, this paper sheds new light on Abductive Reasoning by studying a new vision-language task, Multi-modal Action chain abductive Reasoning (MAR), together with a large-scale Abductive Reasoning dataset: Given an incomplete set of language described events, MAR aims to imagine the most plausible event by spatio-temporal grounding in past video and then infer the hypothesis of subsequent action chain that can best explain the language premise. To solve this task, we propose a strong baseline model that realizes MAR from two perspectives: (i) we first introduce the transformer, which learns to encode the observation to imagine the plausible event with explicitly interpretable event grounding in the video based on the commonsense knowledge recognition ability. (ii) To complete the assumption of a follow-up action chain, we design a novel symbolic module that can complete strict derivation of the progressive action chain layer by layer. We conducted extensive experiments on the proposed dataset, and the experimental study shows that the proposed model significantly outperforms existing video-language models in terms of effectiveness on our newly created MAR dataset.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.254.pdf",
        "keywords": [
            "abductive",
            "chain abductive reasoning",
            "action chain abductive reasoning"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Exploring the Capacity of Pretrained Language Models for Reasoning about Actions and Change",
        "authors": [
            "Weinan He",
            "Canming Huang",
            "Zhanhao Xiao",
            "Yongmei Liu"
        ],
        "published": "2023",
        "summary": "Reasoning about actions and change (RAC) is essential to understand and interact with the ever-changing environment. Previous AI research has shown the importance of fundamental and indispensable knowledge of actions, i.e., preconditions and effects. However, traditional methods rely on logical formalization which hinders practical applications. With recent transformer-based language models (LMs), reasoning over text is desirable and seemingly feasible, leading to the question of whether LMs can effectively and efficiently learn to solve RAC problems. We propose four essential RAC tasks as a comprehensive textual benchmark and generate problems in a way that minimizes the influence of other linguistic requirements (e.g., grounding) to focus on RAC. The resulting benchmark, TRAC, encompassing problems of various complexities, facilitates a more granular evaluation of LMs, precisely targeting the structural generalization ability much needed for RAC. Experiments with three high-performing transformers indicate that additional efforts are needed to tackle challenges raised by TRAC.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.255.pdf",
        "keywords": [
            "language models",
            "actions",
            "actions and change",
            "generalization ability"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Experiments with three high-performing transformers indicate that additional efforts are needed to tackle challenges raised by TRAC.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Experiments with three high-performing transformers indicate that additional efforts are needed to tackle challenges raised by TRAC.\""
    },
    {
        "title": "Unified Demonstration Retriever for In-Context Learning",
        "authors": [
            "Xiaonan Li",
            "Kai Lv",
            "Hang Yan",
            "Tianyang Lin",
            "Wei Zhu",
            "Yuan Ni",
            "Guotong Xie",
            "Xiaoling Wang",
            "Xipeng Qiu"
        ],
        "published": "2023",
        "summary": "In-context learning is a new learning paradigm where a language model conditions on a few input-output pairs (demonstrations) and a test input, and directly outputs the prediction. It has been shown sensitive to the provided demonstrations and thus promotes the research of demonstration retrieval: given a test input, relevant examples are retrieved from the training set to serve as informative demonstrations for in-context learning. While previous works train task-specific retrievers for several tasks separately, these methods are hard to transfer and scale on various tasks, and separately trained retrievers will cause a lot of parameter storage and deployment cost. In this paper, we propose Unified Demonstration Retriever (UDR), a single model to retrieve demonstrations for a wide range of tasks. To train UDR, we cast various tasks’ training signals into a unified list-wise ranking formulation by language model’s feedback. Then we propose a multi-task list-wise ranking training framework with an iterative mining strategy to find high-quality candidates, which can help UDR fully incorporate various tasks’ signals. Experiments on 30+ tasks across 13 task families and multiple data domains show that UDR significantly outperforms baselines. Further analyses show the effectiveness of each proposed component and UDR’s strong ability in various scenarios including different LMs (1.3B 175B), unseen datasets, varying demonstration quantities, etc. We will release the code and model checkpoint after review.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.256.pdf",
        "keywords": [
            "demonstration retriever",
            "s strong ability",
            "context learning",
            "model"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"including different LMs (1.3B, 175B)\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"including different LMs (1.3B, 175B)\""
    },
    {
        "title": "Enhancing Language Representation with Constructional Information for Natural Language Understanding",
        "authors": [
            "Lvxiaowei Xu",
            "Jianwang Wu",
            "Jiawei Peng",
            "Zhilin Gong",
            "Ming Cai",
            "Tianxiang Wang"
        ],
        "published": "2023",
        "summary": "Natural language understanding (NLU) is an essential branch of natural language processing, which relies on representations generated by pre-trained language models (PLMs). However, PLMs primarily focus on acquiring lexico-semantic information, while they may be unable to adequately handle the meaning of constructions. To address this issue, we introduce construction grammar (CxG), which highlights the pairings of form and meaning, to enrich language representation. We adopt usage-based construction grammar as the basis of our work, which is highly compatible with statistical models such as PLMs. Then a HyCxG framework is proposed to enhance language representation through a three-stage solution. First, all constructions are extracted from sentences via a slot-constraints approach. As constructions can overlap with each other, bringing redundancy and imbalance, we formulate the conditional max coverage problem for selecting the discriminative constructions. Finally, we propose a relational hypergraph attention network to acquire representation from constructional information by capturing high-order word interactions among constructions. Extensive experiments demonstrate the superiority of the proposed model on a variety of NLU tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.258.pdf",
        "keywords": [
            "language representation",
            "construction grammar",
            "natural language",
            "conditional max coverage problem"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, PLMs primarily focus on acquiring lexico-semantic information, while they may be unable to adequately handle the meaning of constructions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, PLMs primarily focus on acquiring lexico-semantic information, while they may be unable to adequately handle the meaning of constructions.\""
    },
    {
        "title": "Query Structure Modeling for Inductive Logical Reasoning Over Knowledge Graphs",
        "authors": [
            "Siyuan Wang",
            "Zhongyu Wei",
            "Meng Han",
            "Zhihao Fan",
            "Haijun Shan",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2023",
        "summary": "Logical reasoning over incomplete knowledge graphs to answer complex logical queries is a challenging task. With the emergence of new entities and relations in constantly evolving KGs, inductive logical reasoning over KGs has become a crucial problem. However, previous PLMs-based methods struggle to model the logical structures of complex queries, which limits their ability to generalize within the same structure. In this paper, we propose a structure-modeled textual encoding framework for inductive logical reasoning over KGs. It encodes linearized query structures and entities using pre-trained language models to find answers. For structure modeling of complex queries, we design stepwise instructions that implicitly prompt PLMs on the execution order of geometric operations in each query. We further separately model different geometric operations (i.e., projection, intersection, and union) on the representation space using a pre-trained encoder with additional attention and maxout layers to enhance structured modeling. We conduct experiments on two inductive logical reasoning datasets and three transductive datasets. The results demonstrate the effectiveness of our method on logical reasoning over KGs in both inductive and transductive settings.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.259.pdf",
        "keywords": [
            "logical reasoning",
            "inductive logical reasoning",
            "transductive datasets",
            "query structure modeling",
            "inductive and transductive",
            "knowledge graphs"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, previous PLMs-based methods struggle to model the logical structures of complex queries, which limits their ability to generalize within the same structure.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, previous PLMs-based methods struggle to model the logical structures of complex queries, which limits their ability to generalize within the same structure.\""
    },
    {
        "title": "DimonGen: Diversified Generative Commonsense Reasoning for Explaining Concept Relationships",
        "authors": [
            "Chenzhengyi Liu",
            "Jie Huang",
            "Kerui Zhu",
            "Kevin Chen-Chuan Chang"
        ],
        "published": "2023",
        "summary": "In this paper, we propose DimonGen, which aims to generate diverse sentences describing concept relationships in various everyday scenarios. To support this, we first create a benchmark dataset for this task by adapting the existing CommonGen dataset. We then propose a two-stage model called MoREE to generate the target sentences. MoREE consists of a mixture of retrievers model that retrieves diverse context sentences related to the given concepts, and a mixture of generators model that generates diverse sentences based on the retrieved contexts. We conduct experiments on the DimonGen task and show that MoREE outperforms strong baselines in terms of both the quality and diversity of the generated sentences. Our results demonstrate that MoREE is able to generate diverse sentences that reflect different relationships between concepts, leading to a comprehensive understanding of concept relationships.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.260.pdf",
        "keywords": [
            "concept relationships",
            "explaining concept relationships",
            "generative commonsense reasoning"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Hidden Schema Networks",
        "authors": [
            "Ramses Sanchez",
            "Lukas Conrads",
            "Pascal Welke",
            "Kostadin Cvejoski",
            "Cesar Ojeda Marin"
        ],
        "published": "2023",
        "summary": "Large, pretrained language models infer powerful representations that encode rich semantic and syntactic content, albeit implicitly. In this work we introduce a novel neural language model that enforces, via inductive biases, explicit relational structures which allow for compositionality onto the output representations of pretrained language models. Specifically, the model encodes sentences into sequences of symbols (composed representations), which correspond to the nodes visited by biased random walkers on a global latent graph, and infers the posterior distribution of the latter. We first demonstrate that the model is able to uncover ground-truth graphs from artificially generated datasets of random token sequences. Next, we leverage pretrained BERT and GPT-2 language models as encoder and decoder, respectively, to infer networks of symbols (schemata) from natural language datasets. Our experiments show that (i) the inferred symbols can be interpreted as encoding different aspects of language, as e.g. topics or sentiments, and that (ii) GPT-2-like models can effectively be conditioned on symbolic representations. Finally, we explore training autoregressive, random walk “reasoning” models on schema networks inferred from commonsense knowledge databases, and using the sampled paths to enhance the performance of pretrained language models on commonsense If-Then reasoning tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.263.pdf",
        "keywords": [
            "schema networks",
            "hidden schema networks",
            "commonsense",
            "latent graph",
            "random walk",
            "commonsense knowledge databases",
            "language models",
            "compositionality"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"albeit implicitly\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"albeit implicitly\""
    },
    {
        "title": "Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations",
        "authors": [
            "Linlin Liu",
            "Xingxuan Li",
            "Megh Thakkar",
            "Xin Li",
            "Shafiq Joty",
            "Luo Si",
            "Lidong Bing"
        ],
        "published": "2023",
        "summary": "Due to the huge amount of parameters, finetuning of pretrained language models (PLMs) is prone to overfitting in the low resource scenarios. In this work, we present a novel method that operates on the hidden representations of a PLM to reduce overfitting. During fine-tuning, our method inserts random autoencoders between the hidden layers of a PLM, which transform activations from the previous layers into multi-view compressed representations before feeding them into the upper layers. The autoencoders are plugged out after fine-tuning, so our method does not add extra parameters or increase computation cost during inference. Our method demonstrates promising performance improvement across a wide range of sequence- and token-level lowresource NLP tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.264.pdf",
        "keywords": [
            "fine tuning",
            "finetuning",
            "multi view compressed representations",
            "overfitting"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Due to the huge amount of parameters, finetuning of pretrained language models (PLMs) is prone to overfitting in the low resource scenarios.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Due to the huge amount of parameters, finetuning of pretrained language models (PLMs) is prone to overfitting in the low resource scenarios.\""
    },
    {
        "title": "Pre-Training to Learn in Context",
        "authors": [
            "Yuxian Gu",
            "Li Dong",
            "Furu Wei",
            "Minlie Huang"
        ],
        "published": "2023",
        "summary": "In-context learning, where pre-trained language models learn to perform tasks from task examples and instructions in their contexts, has attracted much attention in the NLP community. However, the ability of in-context learning is not fully exploited because language models are not explicitly trained to learn in context. To this end, we propose PICL (Pre-training for In-Context Learning), a framework to enhance the language models’ in-context learning ability by pre-training the model on a large collection of “intrinsic tasks” in the general plain-text corpus using the simple language modeling objective. PICL encourages the model to infer and perform tasks by conditioning on the contexts while maintaining task generalization of pre-trained models. We evaluate the in-context learning performance of the model trained with PICL on seven widely-used text classification datasets and the Super-NaturalInstrctions benchmark, which contains 100+ NLP tasks formulated to text generation. Our experiments show that PICL is more effective and task-generalizable than a range of baselines, outperforming larger language models with nearly 4x parameters. The code is publicly available at https://github.com/thu-coai/PICL.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.267.pdf",
        "keywords": [
            "language",
            "learn",
            "language models learn",
            "language models",
            "context learning",
            "plain text corpus",
            "pre training",
            "pre trained models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the ability of in-context learning is not fully exploited because language models are not explicitly trained to learn in context.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of language models (not being explicitly trained to learn in context), but it is a minor detail and not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the ability of in-context learning is not fully exploited because language models are not explicitly trained to learn in context.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of language models (not being explicitly trained to learn in context), but it is a minor detail and not the primary focus of the paper."
    },
    {
        "title": "Revisiting non-English Text Simplification: A Unified Multilingual Benchmark",
        "authors": [
            "Michael Ryan",
            "Tarek Naous",
            "Wei Xu"
        ],
        "published": "2023",
        "summary": "Recent advancements in high-quality, large-scale English resources have pushed the frontier of English Automatic Text Simplification (ATS) research. However, less work has been done on multilingual text simplification due to the lack of a diverse evaluation benchmark that covers complex-simple sentence pairs in many languages. This paper introduces the MultiSim benchmark, a collection of 27 resources in 12 distinct languages containing over 1.7 million complex-simple sentence pairs. This benchmark will encourage research in developing more effective multilingual text simplification models and evaluation metrics. Our experiments using MultiSim with pre-trained multilingual language models reveal exciting performance improvements from multilingual training in non-English settings. We observe strong performance from Russian in zero-shot cross-lingual transfer to low-resource languages. We further show that few-shot prompting with BLOOM-176b achieves comparable quality to reference simplifications outperforming fine-tuned models in most languages. We validate these findings through human evaluation.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.269.pdf",
        "keywords": [
            "multilingual text simplification"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Don’t Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments",
        "authors": [
            "Yu Gu",
            "Xiang Deng",
            "Yu Su"
        ],
        "published": "2023",
        "summary": "A key missing capacity of current language models (LMs) is grounding to real-world environments. Most existing work for grounded language understanding uses LMs to directly generate plans that can be executed in the environment to achieve the desired effects. It thereby casts the burden of ensuring grammaticality, faithfulness, and controllability all on the LMs. We propose Pangu, a generic framework for grounded language understanding that capitalizes on the discriminative ability of LMs instead of their generative ability. Pangu consists of a symbolic agent and a neural LM working in a concerted fashion: The agent explores the environment to incrementally construct valid plans, and the LM evaluates the plausibility of the candidate plans to guide the search process. A case study on the challenging problem of knowledge base question answering (KBQA), which features a massive environment, demonstrates the remarkable effectiveness and flexibility of Pangu: A BERT-base LM is sufficient for setting a new record on standard KBQA datasets, and larger LMs further bring substantial gains.Pangu also enables, for the first time, effective few-shot in-context learning for KBQA with large LMs such as Codex.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.270.pdf",
        "keywords": [
            "language models",
            "grounded language",
            "grounding language models",
            "discriminative ability",
            "agent",
            "knowledge base question answering",
            "lm"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"A key missing capacity of current language models (LMs) is grounding to real-world environments.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"A key missing capacity of current language models (LMs) is grounding to real-world environments.\""
    },
    {
        "title": "ConvGQR: Generative Query Reformulation for Conversational Search",
        "authors": [
            "Fengran Mo",
            "Kelong Mao",
            "Yutao Zhu",
            "Yihong Wu",
            "Kaiyu Huang",
            "Jian-Yun Nie"
        ],
        "published": "2023",
        "summary": "In conversational search, the user’s real search intent for the current conversation turn is dependent on the previous conversation history. It is challenging to determine a good search query from the whole conversation context. To avoid the expensive re-training of the query encoder, most existing methods try to learn a rewriting model to de-contextualize the current query by mimicking the manual query rewriting. However, manually rewritten queries are not always the best search queries. Thus, training a rewriting model on them would lead to sub-optimal queries. Another useful information to enhance the search query is the potential answer to the question. In this paper, we propose ConvGQR, a new framework to reformulate conversational queries based on generative pre-trained language models (PLMs), one for query rewriting and another for generating potential answers. By combining both, ConvGQR can produce better search queries. In addition, to relate query reformulation to the retrieval task, we propose a knowledge infusion mechanism to optimize both query reformulation and retrieval. Extensive experiments on four conversational search datasets demonstrate the effectiveness of ConvGQR.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.274.pdf",
        "keywords": [
            "query reformulation",
            "query rewriting",
            "conversational",
            "conversational search"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, manually rewritten queries are not always the best search queries. Thus, training a rewriting model on them would lead to sub-optimal queries.\"\n\nThis evidence is brief and mentions a limitation of the LLMs, but the primary focus of the paper is on the proposed solution, ConvGQR.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, manually rewritten queries are not always the best search queries. Thus, training a rewriting model on them would lead to sub-optimal queries.\"\n\nThis evidence is brief and mentions a limitation of the LLMs, but the primary focus of the paper is on the proposed solution, ConvGQR."
    },
    {
        "title": "KILM: Knowledge Injection into Encoder-Decoder Language Models",
        "authors": [
            "Yan Xu",
            "Mahdi Namazifar",
            "Devamanyu Hazarika",
            "Aishwarya Padmakumar",
            "Yang Liu",
            "Dilek Hakkani-Tur"
        ],
        "published": "2023",
        "summary": "Large pre-trained language models (PLMs) have been shown to retain implicit knowledge within their parameters. To enhance this implicit knowledge, we propose Knowledge Injection into Language Models (KILM), a novel approach that injects entity-related knowledge into encoder-decoder PLMs, via a generative knowledge infilling objective through continued pre-training. This is done without architectural modifications to the PLMs or adding additional parameters. Experimental results over a suite of knowledge-intensive tasks spanning numerous datasets show that KILM enables models to retain more knowledge and hallucinate less while preserving their original performance on general NLU and NLG tasks. KILM also demonstrates improved zero-shot performances on tasks such as entity disambiguation, outperforming state-of-the-art models having 30x more parameters.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.275.pdf",
        "keywords": [
            "encoder",
            "knowledge injection",
            "pre trained language models",
            "encoder decoder",
            "generative"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large pre-trained language models (PLMs) have been shown to retain implicit knowledge within their parameters.\"\n\nNote that the paper mentions the limitation of PLMs (a type of LLMs) in retaining implicit knowledge, but it is not the primary focus of the paper, and the limitation is not explored in depth.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large pre-trained language models (PLMs) have been shown to retain implicit knowledge within their parameters.\"\n\nNote that the paper mentions the limitation of PLMs (a type of LLMs) in retaining implicit knowledge, but it is not the primary focus of the paper, and the limitation is not explored in depth."
    },
    {
        "title": "Z-Code++: A Pre-trained Language Model Optimized for Abstractive Summarization",
        "authors": [
            "Pengcheng He",
            "Baolin Peng",
            "Song Wang",
            "Yang Liu",
            "Ruochen Xu",
            "Hany Hassan",
            "Yu Shi",
            "Chenguang Zhu",
            "Wayne Xiong",
            "Michael Zeng",
            "Jianfeng Gao",
            "Xuedong Huang"
        ],
        "published": "2023",
        "summary": "This paper presents Z-Code++, a new pre-trained language model optimized for abstractive text summarization. The model extends the state-of-the-art encoder-decoder model using three techniques. First, we use a two-phase pre-training to improve the model’s performance on low-resource summarization tasks. The model is first pre-trained using text corpora for language understanding, then is continually pre-trained on summarization corpora for grounded text generation. Second, we replace self-attention layers in the encoder with disentangled attention layers, where each word is represented using two vectors that encode its content and position, respectively. Third, we use fusion-in-encoder, a simple yet effective method of encoding long sequences in a hierarchical manner. Z-Code++ createsa new state-of-the-art on 9 of 13 text summarization tasks across 5 languages. Our model is parameter-efficient in that it outperforms the 600x larger PaLM540B on XSum, and the finetuned 200x larger GPT3175B on SAMSum. In zero-shot and few-shot settings, our model substantially outperforms the competing models.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.279.pdf",
        "keywords": [
            "summarization",
            "text summarization",
            "abstractive text summarization",
            "fusion",
            "pre trained language model",
            "encoder",
            "grounded text generation"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations of LLMs, but the paper presents a new model that outperforms existing large models like PaLM and GPT, implying that those models have limitations in terms of performance and parameter efficiency.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations of LLMs, but the paper presents a new model that outperforms existing large models like PaLM and GPT, implying that those models have limitations in terms of performance and parameter efficiency."
    },
    {
        "title": "Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models’ Memories",
        "authors": [
            "Shizhe Diao",
            "Tianyang Xu",
            "Ruijia Xu",
            "Jiawei Wang",
            "Tong Zhang"
        ],
        "published": "2023",
        "summary": "Pre-trained language models (PLMs) demonstrate excellent abilities to understand texts in the generic domain while struggling in a specific domain. Although continued pre-training on a large domain-specific corpus is effective, it is costly to tune all the parameters on the domain. In this paper, we investigate whether we can adapt PLMs both effectively and efficiently by only tuning a few parameters. Specifically, we decouple the feed-forward networks (FFNs) of the Transformer architecture into two parts: the original pre-trained FFNs to maintain the old-domain knowledge and our novel domain-specific adapters to inject domain-specific knowledge in parallel. Then we adopt a mixture-of-adapters gate to fuse the knowledge from different domain adapters dynamically. Our proposed Mixture-of-Domain-Adapters (MixDA) employs a two-stage adapter-tuning strategy that leverages both unlabeled data and labeled data to help the domain adaptation: i) domain-specific adapter on unlabeled data; followed by ii) the task-specific adapter on labeled data. MixDA can be seamlessly plugged into the pretraining-finetuning paradigm and our experiments demonstrate that MixDA achieves superior performance on in-domain tasks (GLUE), out-of-domain tasks (ChemProt, RCT, IMDB, Amazon), and knowledge-intensive tasks (KILT).Further analyses demonstrate the reliability, scalability, and efficiency of our method.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.280.pdf",
        "keywords": [
            "adapter",
            "domain adapters",
            "domain specific adapters",
            "language models",
            "mixture of domain adapters",
            "mixture of adapters",
            "domain specific knowledge",
            "pre trained language models",
            "knowledge",
            "feed forward networks",
            "memories"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Pre-trained language models (PLMs) demonstrate excellent abilities to understand texts in the generic domain while struggling in a specific domain.\"\n\nThis abstract mentions a limitation of pre-trained language models (PLMs), which are a type of LLM, in a specific domain, but it does not elaborate on this limitation and focuses on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Pre-trained language models (PLMs) demonstrate excellent abilities to understand texts in the generic domain while struggling in a specific domain.\"\n\nThis abstract mentions a limitation of pre-trained language models (PLMs), which are a type of LLM, in a specific domain, but it does not elaborate on this limitation and focuses on the proposed solution."
    },
    {
        "title": "Unsupervised Graph-Text Mutual Conversion with a Unified Pretrained Language Model",
        "authors": [
            "Yi Xu",
            "Shuqian Sheng",
            "Jiexing Qi",
            "Luoyi Fu",
            "Zhouhan Lin",
            "Xinbing Wang",
            "Chenghu Zhou"
        ],
        "published": "2023",
        "summary": "Graph-to-text (G2T) generation and text-to-graph (T2G) triple extraction are two essential tasks for knowledge graphs. Existing unsupervised approaches become suitable candidates for jointly learning the two tasks due to their avoidance of using graph-text parallel data. However, they adopt multiple complex modules and still require entity information or relation type for training. To this end, we propose INFINITY, a simple yet effective unsupervised method with a unified pretrained language model that does not introduce external annotation tools or additional parallel information. It achieves fully unsupervised graph-text mutual conversion for the first time. Specifically, INFINITY treats both G2T and T2G as a bidirectional sequence generation task by fine-tuning only one pretrained seq2seq model. A novel back-translation-based framework is then designed to generate synthetic parallel data automatically. Besides, we investigate the impact of graph linearization and introduce the structure-aware fine-tuning strategy to alleviate possible performance deterioration via retaining structural information in graph sequences. As a fully unsupervised framework, INFINITY is empirically verified to outperform state-of-the-art baselines for G2T and T2G tasks. Additionally, we also devise a new training setting called cross learning for low-resource unsupervised information extraction.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.281.pdf",
        "keywords": [
            "mutual conversion",
            "text to graph",
            "cross learning",
            "knowledge graphs",
            "language model"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"alleviate possible performance deterioration via retaining structural information in graph sequences.\"\n\nThis evidence is weak and only mentions a potential limitation of LLMs in passing, without elaborating on it or making it a major focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"alleviate possible performance deterioration via retaining structural information in graph sequences.\"\n\nThis evidence is weak and only mentions a potential limitation of LLMs in passing, without elaborating on it or making it a major focus of the paper."
    },
    {
        "title": "Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications",
        "authors": [
            "Han Cheol Moon",
            "Shafiq Joty",
            "Ruochen Zhao",
            "Megh Thakkar",
            "Chi Xu"
        ],
        "published": "2023",
        "summary": "Large-scale pre-trained language models have shown outstanding performance in a variety of NLP tasks. However, they are also known to be significantly brittle against specifically crafted adversarial examples, leading to increasing interest in probing the adversarial robustness of NLP systems. We introduce RSMI, a novel two-stage framework that combines randomized smoothing (RS) with masked inference (MI) to improve the adversarial robustness of NLP systems. RS transforms a classifier into a smoothed classifier to obtain robust representations, whereas MI forces a model to exploit the surrounding context of a masked token in an input sequence. RSMI improves adversarial robustness by 2 to 3 times over existing state-of-the-art methods on benchmark datasets. We also perform in-depth qualitative analysis to validate the effectiveness of the different stages of RSMI and probe the impact of its components through extensive ablations. By empirically proving the stability of RSMI, we put it forward as a practical method to robustly train large-scale NLP models. Our code and datasets are available at https://github.com/Han8931/rsmi_nlp",
        "pdf_link": "https://aclanthology.org/2023.acl-long.282.pdf",
        "keywords": [
            "masked",
            "masked inference",
            "text classifications",
            "randomized smoothing",
            "randomized"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, they are also known to be significantly brittle against specifically crafted adversarial examples\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, they are also known to be significantly brittle against specifically crafted adversarial examples\""
    },
    {
        "title": "Contextual Distortion Reveals Constituency: Masked Language Models are Implicit Parsers",
        "authors": [
            "Jiaxi Li",
            "Wei Lu"
        ],
        "published": "2023",
        "summary": "Recent advancements in pre-trained language models (PLMs) have demonstrated that these models possess some degree of syntactic awareness. To leverage this knowledge, we propose a novel chart-based method for extracting parse trees from masked language models (LMs) without the need to train separate parsers. Our method computes a score for each span based on the distortion of contextual representations resulting from linguistic perturbations. We design a set of perturbations motivated by the linguistic concept of constituency tests, and use these to score each span by aggregating the distortion scores. To produce a parse tree, we use chart parsing to find the tree with the minimum score. Our method consistently outperforms previous state-of-the-art methods on English with masked LMs, and also demonstrates superior performance in a multilingual setting, outperforming the state-of-the-art in 6 out of 8 languages. Notably, although our method does not involve parameter updates or extensive hyperparameter search, its performance can even surpass some unsupervised parsing methods that require fine-tuning. Our analysis highlights that the distortion of contextual representation resulting from syntactic perturbation can serve as an effective indicator of constituency across languages.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.285.pdf",
        "keywords": [
            "constituency",
            "parsers",
            "implicit parsers",
            "contextual distortion",
            "hyperparameter search",
            "masked language",
            "masked language models",
            "masked lms",
            "parse"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "MetaAdapt: Domain Adaptive Few-Shot Misinformation Detection via Meta Learning",
        "authors": [
            "Zhenrui Yue",
            "Huimin Zeng",
            "Yang Zhang",
            "Lanyu Shang",
            "Dong Wang"
        ],
        "published": "2023",
        "summary": "With emerging topics (e.g., COVID-19) on social media as a source for the spreading misinformation, overcoming the distributional shifts between the original training domain (i.e., source domain) and such target domains remains a non-trivial task for misinformation detection. This presents an elusive challenge for early-stage misinformation detection, where a good amount of data and annotations from the target domain is not available for training. To address the data scarcity issue, we propose MetaAdapt, a meta learning based approach for domain adaptive few-shot misinformation detection. MetaAdapt leverages limited target examples to provide feedback and guide the knowledge transfer from the source to the target domain (i.e., learn to adapt). In particular, we train the initial model with multiple source tasks and compute their similarity scores to the meta task. Based on the similarity scores, we rescale the meta gradients to adaptively learn from the source tasks. As such, MetaAdapt can learn how to adapt the misinformation detection model and exploit the source data for improved performance in the target domain. To demonstrate the efficiency and effectiveness of our method, we perform extensive experiments to compare MetaAdapt with state-of-the-art baselines and large language models (LLMs) such as LLaMA, where MetaAdapt achieves better performance in domain adaptive few-shot misinformation detection with substantially reduced parameters on real-world datasets.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.286.pdf",
        "keywords": [
            "misinformation",
            "misinformation detection",
            "meta learning",
            "social media",
            "few shot misinformation detection",
            "domain"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"compare MetaAdapt with state-of-the-art baselines and large language models (LLMs) such as LLaMA, where MetaAdapt achieves better performance in domain adaptive few-shot misinformation detection with substantially reduced parameters on real-world datasets.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs (large number of parameters) but only in passing and",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"compare MetaAdapt with state-of-the-art baselines and large language models (LLMs) such as LLaMA, where MetaAdapt achieves better performance in domain adaptive few-shot misinformation detection with substantially reduced parameters on real-world datasets.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs (large number of parameters) but only in passing and"
    },
    {
        "title": "Making Language Models Better Reasoners with Step-Aware Verifier",
        "authors": [
            "Yifei Li",
            "Zeqi Lin",
            "Shizhuo Zhang",
            "Qiang Fu",
            "Bei Chen",
            "Jian-Guang Lou",
            "Weizhu Chen"
        ],
        "published": "2023",
        "summary": "Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in problem-solving rate. In this paper, we present DiVeRSe (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models. DiVeRSe has three main components: first, it generates diverse prompts to explore different reasoning paths for the same question; second, it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step individually instead of the whole chain. We evaluate DiVeRSe on the latest language model code-davinci-002 and show that it achieves new state-of-the-art results on six of eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%).",
        "pdf_link": "https://aclanthology.org/2023.acl-long.291.pdf",
        "keywords": [
            "language models",
            "reasoners",
            "step aware verifier"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems.\""
    },
    {
        "title": "MISGENDERED: Limits of Large Language Models in Understanding Pronouns",
        "authors": [
            "Tamanna Hossain",
            "Sunipa Dev",
            "Sameer Singh"
        ],
        "published": "2023",
        "summary": "Content Warning: This paper contains examples of misgendering and erasure that could be offensive and potentially triggering. Gender bias in language technologies has been widely studied, but research has mostly been restricted to a binary paradigm of gender. It is essential also to consider non-binary gender identities, as excluding them can cause further harm to an already marginalized group. In this paper, we comprehensively evaluate popular language models for their ability to correctly use English gender-neutral pronouns (e.g., singular they, them) and neo-pronouns (e.g., ze, xe, thon) that are used by individuals whose gender identity is not represented by binary pronouns. We introduce Misgendered, a framework for evaluating large language models’ ability to correctly use preferred pronouns, consisting of (i) instances declaring an individual’s pronoun, followed by a sentence with a missing pronoun, and (ii) an experimental setup for evaluating masked and auto-regressive language models using a unified method. When prompted out-of-the-box, language models perform poorly at correctly predicting neo-pronouns (averaging 7.6% accuracy) and gender-neutral pronouns (averaging 31.0% accuracy). This inability to generalize results from a lack of representation of non-binary pronouns in training data and memorized associations. Few-shot adaptation with explicit examples in the prompt improves the performance but plateaus at only 45.4% for neo-pronouns. We release the full dataset, code, and demo at https://tamannahossainkay.github.io/misgendered/.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.293.pdf",
        "keywords": [
            "language models",
            "pronouns",
            "erasure"
        ],
        "Rate_Llama-3.1-70b": "5",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"When prompted out-of-the-box, language models perform poorly at correctly predicting neo-pronouns (averaging 7.6% accuracy) and gender-neutral pronouns (averaging 31.0% accuracy). This inability to generalize results from a lack of representation of non-binary pronouns in training data and memorized associations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 5\nEvidence: \"When prompted out-of-the-box, language models perform poorly at correctly predicting neo-pronouns (averaging 7.6% accuracy) and gender-neutral pronouns (averaging 31.0% accuracy). This inability to generalize results from a lack of representation of non-binary pronouns in training data and memorized associations.\""
    },
    {
        "title": "Reasoning with Language Model Prompting: A Survey",
        "authors": [
            "Shuofei Qiao",
            "Yixin Ou",
            "Ningyu Zhang",
            "Xiang Chen",
            "Yunzhi Yao",
            "Shumin Deng",
            "Chuanqi Tan",
            "Fei Huang",
            "Huajun Chen"
        ],
        "published": "2023",
        "summary": "Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).",
        "pdf_link": "https://aclanthology.org/2023.acl-long.294.pdf",
        "keywords": [
            "language model prompting"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations mentioned, but the paper mentions \"potential reasons for emerging such reasoning abilities\" which could imply some limitations in the current state of LLMs, however, it is not clear and not the focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations mentioned, but the paper mentions \"potential reasons for emerging such reasoning abilities\" which could imply some limitations in the current state of LLMs, however, it is not clear and not the focus of the paper."
    },
    {
        "title": "Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment",
        "authors": [
            "Rohan Pandey",
            "Rulin Shao",
            "Paul Pu Liang",
            "Ruslan Salakhutdinov",
            "Louis-Philippe Morency"
        ],
        "published": "2023",
        "summary": "Despite recent progress towards scaling up multimodal vision-language models, these models are still known to struggle on compositional generalization benchmarks such as Winoground. We find that a critical component lacking from current vision-language models is relation-level alignment: the ability to match directional semantic relations in text (e.g., ‘mug in grass’) with spatial relationships in the image (e.g., the position of the mug relative to the grass). To tackle this problem, we show that relation alignment can be enforced by encouraging the language attention from ‘mug’ to ‘grass’ (capturing the semantic relation ‘in’) to match the visual attention from the mug to the grass (capturing the corresponding physical relation). Tokens and their corresponding objects are softly identified using a weighted mean of cross-modal attention. We prove that this notion of soft cross-modal equivalence is equivalent to enforcing congruence between vision and language attention matrices under a ‘change of basis’ provided by the cross-modal attention matrix. Intuitively, our approach projects visual attention into the language attention space to calculate its divergence from the actual language attention, and vice versa. We apply our Cross-modal Attention Congruence Regularization (CACR) loss to fine-tune UNITER and improve its Winoground Group score by 5.75 points.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.298.pdf",
        "keywords": [
            "relation alignment",
            "soft cross modal equivalence",
            "relation level alignment",
            "vision language relation alignment"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite recent progress towards scaling up multimodal vision-language models, these models are still known to struggle on compositional generalization benchmarks such as Winoground.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite recent progress towards scaling up multimodal vision-language models, these models are still known to struggle on compositional generalization benchmarks such as Winoground.\""
    },
    {
        "title": "Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge",
        "authors": [
            "Yasumasa Onoe",
            "Michael Zhang",
            "Shankar Padmanabhan",
            "Greg Durrett",
            "Eunsol Choi"
        ],
        "published": "2023",
        "summary": "Pre-trained language models (LMs) are used for knowledge intensive tasks like question answering, but their knowledge gets continuously outdated as the world changes. Prior work has studied targeted updates to LMs, injecting individual facts and evaluating whether the model learns these facts while not changing predictions on other contexts. We take a step forward and study LMs’ abilities to make inferences based on injected facts (or propagate those facts): for example, after learning that something is a TV show, does an LM predict that you can watch it? We study this with two cloze-style tasks: an existing dataset of real-world sentences about novel entities (ECBD) as well as a new controlled benchmark with manually designed templates requiring varying levels of inference about injected knowledge. Surprisingly, we find that existing methods for updating knowledge (gradient-based fine-tuning and modifications of this approach) show little propagation of injected knowledge. These methods improve performance on cloze instances only when there is lexical overlap between injected facts and target inferences. Yet, prepending entity definitions in an LM’s context improves performance across all settings, suggesting that there is substantial headroom for parameter-updating approaches for knowledge injection.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.300.pdf",
        "keywords": [
            "injected knowledge",
            "trained language models"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Surprisingly, we find that existing methods for updating knowledge (gradient-based fine-tuning and modifications of this approach) show little propagation of injected knowledge. These methods improve performance on cloze instances only when there is lexical overlap between injected facts and target inferences.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Surprisingly, we find that existing methods for updating knowledge (gradient-based fine-tuning and modifications of this approach) show little propagation of injected knowledge. These methods improve performance on cloze instances only when there is lexical overlap between injected facts and target inferences.\""
    },
    {
        "title": "DISCO: Distilling Counterfactuals with Large Language Models",
        "authors": [
            "Zeming Chen",
            "Qiyue Gao",
            "Antoine Bosselut",
            "Ashish Sabharwal",
            "Kyle Richardson"
        ],
        "published": "2023",
        "summary": "Models trained with counterfactually augmented data learn representations of the causal structure of tasks, enabling robust generalization. However, high-quality counterfactual data is scarce for most tasks and not easily generated at scale. When crowdsourced, such data is typically limited in scale and diversity; when generated using supervised methods, it is computationally expensive to extend to new counterfactual dimensions. In this work, we introduce DISCO (DIStilled COunterfactual Data), a new method for automatically generating high-quality counterfactual data at scale. DISCO engineers prompts to generate phrasal perturbations with a large general language model. Then, a task-specific teacher model filters these generations to distill high-quality counterfactual data. While task-agnostic, we apply our pipeline to the task of natural language inference (NLI) and find that on challenging evaluations such as the NLI stress test, comparatively smaller student models trained with DISCO generated counterfactuals are more robust (6% absolute) and generalize better across distributions (2%) compared to models trained without data augmentation. Furthermore, DISCO augmented models are 10% more consistent between counterfactual pairs on three evaluation sets, demonstrating that DISCO augmentation enables models to more reliably learn causal representations. Our repository are available at: https://github.com/eric11eca/disco",
        "pdf_link": "https://aclanthology.org/2023.acl-long.302.pdf",
        "keywords": [
            "counterfactuals",
            "counterfactual data",
            "language models",
            "natural language inference",
            "disco augmentation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, high-quality counterfactual data is scarce for most tasks and not easily generated at scale.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, high-quality counterfactual data is scarce for most tasks and not easily generated at scale.\""
    },
    {
        "title": "SCOTT: Self-Consistent Chain-of-Thought Distillation",
        "authors": [
            "Peifeng Wang",
            "Zhengyang Wang",
            "Zheng Li",
            "Yifan Gao",
            "Bing Yin",
            "Xiang Ren"
        ],
        "published": "2023",
        "summary": "Large language models (LMs) beyond a certain scale, demonstrate the emergent capability of generating free-text rationales for their predictions via chain-of-thought (CoT) prompting. While CoT can yield dramatically improved performance, such gains are only observed for sufficiently large LMs. Even more concerning, there is little guarantee that the generated rationales are consistent with LM’s predictions or faithfully justify the decisions. In this work, we propose SCOTT, a faithful knowledge distillation method to learn a small, self-consistent CoT model from a teacher model that is orders of magnitude larger. To form better supervision, we elicit rationales supporting the gold answers from a large LM (teacher) by contrastive decoding, which encourages the teacher to generate tokens that become more plausible only when the answer is considered. To ensure faithful distillation, we use the teacher-generated rationales to learn a student LM with a counterfactual reasoning objective, which prevents the student from ignoring the rationales to make inconsistent predictions. Experiments show that while yielding comparable performance, our method leads to a more faithful model than baselines. Further analysis shows that such a model respects the rationales more when making decisions; thus, we can improve its performance more by refining its rationales.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.304.pdf",
        "keywords": [
            "thought distillation",
            "knowledge distillation",
            "large language models",
            "self consistent"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While CoT can yield dramatically improved performance, such gains are only observed for sufficiently large LMs. Even more concerning, there is little guarantee that the generated rationales are consistent with LM’s predictions or faithfully justify the decisions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"While CoT can yield dramatically improved performance, such gains are only observed for sufficiently large LMs. Even more concerning, there is little guarantee that the generated rationales are consistent with LM’s predictions or faithfully justify the decisions.\""
    },
    {
        "title": "Evaluating Open-Domain Question Answering in the Era of Large Language Models",
        "authors": [
            "Ehsan Kamalloo",
            "Nouha Dziri",
            "Charles Clarke",
            "Davood Rafiei"
        ],
        "published": "2023",
        "summary": "Lexical matching remains the de facto evaluation method for open-domain question answering (QA). Unfortunately, lexical matching fails completely when a plausible candidate answer does not appear in the list of gold answers, which is increasingly the case as we shift from extractive to generative models. The recent success of large language models (LLMs) for QA aggravates lexical matching failures since candidate answers become longer, thereby making matching with the gold answers even more challenging. Without accurate evaluation, the true progress in open-domain QA remains unknown. In this paper, we conduct a thorough analysis of various open-domain QA models, including LLMs, by manually evaluating their answers on a subset of NQ-open, a popular benchmark. Our assessments reveal that while the true performance of all models is significantly underestimated, the performance of the InstructGPT (zero-shot) LLM increases by nearly +60%, making it on par with existing top models, and the InstructGPT (few-shot) model actually achieves a new state-of-the-art on NQ-open. We also find that more than 50% of lexical matching failures are attributed to semantically equivalent answers. We further demonstrate that regex matching ranks QA models consistent with human judgments, although still suffering from unnecessary strictness. Finally, we demonstrate that automated evaluation models are a reasonable surrogate for lexical matching in some circumstances, but not for long-form answers generated by LLMs. The automated models struggle in detecting hallucinations in LLM answers and are thus unable to evaluate LLMs. At this time, there appears to be no substitute for human evaluation.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.307.pdf",
        "keywords": [
            "lexical matching",
            "large language models",
            "open domain question answering",
            "evaluation method"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Unfortunately, lexical matching fails completely when a plausible candidate answer does not appear in the list of gold answers, which is increasingly the case as we shift from extractive to generative models. The recent success of large language models (LLMs) for QA aggravates lexical matching failures since candidate answers become longer, thereby making matching with the gold answers even more challenging... We further demonstrate that",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Unfortunately, lexical matching fails completely when a plausible candidate answer does not appear in the list of gold answers, which is increasingly the case as we shift from extractive to generative models. The recent success of large language models (LLMs) for QA aggravates lexical matching failures since candidate answers become longer, thereby making matching with the gold answers even more challenging... We further demonstrate that"
    },
    {
        "title": "No clues good clues: out of context Lexical Relation Classification",
        "authors": [
            "Lucia Pitarch",
            "Jordi Bernad",
            "Lacramioara Dranca",
            "Carlos Bobed Lisbona",
            "Jorge Gracia"
        ],
        "published": "2023",
        "summary": "The accurate prediction of lexical relations between words is a challenging task in Natural Language Processing (NLP). The most recent advances in this direction come with the use of pre-trained language models (PTLMs). A PTLM typically needs “well-formed” verbalized text to interact with it, either to fine-tune it or to exploit it. However, there are indications that commonly used PTLMs already encode enough linguistic knowledge to allow the use of minimal (or none) textual context for some linguistically motivated tasks, thus notably reducing human effort, the need for data pre-processing, and favoring techniques that are language neutral since do not rely on syntactic structures. In this work, we explore this idea for the tasks of lexical relation classification (LRC) and graded Lexical Entailment (LE). After fine-tuning PTLMs for LRC with different verbalizations, our evaluation results show that very simple prompts are competitive for LRC and significantly outperform graded LE SoTA. In order to gain a better insight into this phenomenon, we perform a number of quantitative statistical analyses on the results, as well as a qualitative visual exploration based on embedding projections.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.308.pdf",
        "keywords": [
            "lexical relations",
            "lexical relation classification",
            "graded lexical entailment",
            "embedding projections",
            "clues"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicit, but implies limitations in the need for \"well-formed\" verbalized text and the potential for reduced performance in certain tasks like graded Lexical Entailment (LE).",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None explicit, but implies limitations in the need for \"well-formed\" verbalized text and the potential for reduced performance in certain tasks like graded Lexical Entailment (LE)."
    },
    {
        "title": "Won’t Get Fooled Again: Answering Questions with False Premises",
        "authors": [
            "Shengding Hu",
            "Yifan Luo",
            "Huadong Wang",
            "Xingyi Cheng",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published": "2023",
        "summary": "Pre-trained language models (PLMs) have shown unprecedented potential in various fields, especially as the backbones for question-answering (QA) systems. However, they tend to be easily deceived by tricky questions such as “How many eyes does the sun have?”. Such frailties of PLMs often allude to the lack of knowledge within them. In this paper, we find that the PLMs already possess the knowledge required to rebut such questions, and the key is how to activate the knowledge. To systematize this observation, we investigate the PLMs’ responses to one kind of tricky questions, i.e., the false premises questions (FPQs). We annotate a FalseQA dataset containing 2365 human-written FPQs, with the corresponding explanations for the false premises and the revised true premise questions. Using FalseQA, we discover that PLMs are capable of discriminating FPQs by fine-tuning on moderate numbers (e.g., 256) of examples. PLMs also generate reasonable explanations for the false premise, which serve as rebuttals. Further replaying a few general questions during training allows PLMs to excel on FPQs and general questions simultaneously. Our work suggests that once the rebuttal ability is stimulated, knowledge inside the PLMs can be effectively utilized to handle FPQs, which incentivizes the research on PLM-based QA systems. The FalseQA dataset and code are available at https://github.com/thunlp/FalseQA .",
        "pdf_link": "https://aclanthology.org/2023.acl-long.309.pdf",
        "keywords": [
            "false premise",
            "false premises questions",
            "pre trained language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, they tend to be easily deceived by tricky questions such as “How many eyes does the sun have?”. Such frailties of PLMs often allude to the lack of knowledge within them.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, they tend to be easily deceived by tricky questions such as “How many eyes does the sun have?”. Such frailties of PLMs often allude to the lack of knowledge within them.\""
    },
    {
        "title": "Zero-shot Faithful Factual Error Correction",
        "authors": [
            "Kung-Hsiang Huang",
            "Hou Pong Chan",
            "Heng Ji"
        ],
        "published": "2023",
        "summary": "Faithfully correcting factual errors is critical for maintaining the integrity of textual knowledge bases and preventing hallucinations in sequence-to-sequence models. Drawing on humans’ ability to identify and correct factual errors, we present a zero-shot framework that formulates questions about input claims, looks for correct answers in the given evidence, and assesses the faithfulness of each correction based on its consistency with the evidence. Our zero-shot framework outperforms fully-supervised approaches, as demonstrated by experiments on the FEVER and SciFact datasets, where our outputs are shown to be more faithful. More importantly, the decomposability nature of our framework inherently provides interpretability. Additionally, to reveal the most suitable metrics for evaluating factual error corrections, we analyze the correlation between commonly used metrics with human judgments in terms of three different dimensions regarding intelligibility and faithfulness.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.311.pdf",
        "keywords": [
            "factual error correction",
            "factual errors",
            "factual",
            "intelligibility",
            "faithfulness",
            "reveal"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Faithfully correcting factual errors is critical for maintaining the integrity of textual knowledge bases and preventing hallucinations in sequence-to-sequence models.\"\n\nThis evidence suggests that the paper discusses limitations of LLMs in terms of factual errors and hallucinations, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Faithfully correcting factual errors is critical for maintaining the integrity of textual knowledge bases and preventing hallucinations in sequence-to-sequence models.\"\n\nThis evidence suggests that the paper discusses limitations of LLMs in terms of factual errors and hallucinations, but it is not the primary focus of the paper."
    },
    {
        "title": "Open-Domain Hierarchical Event Schema Induction by Incremental Prompting and Verification",
        "authors": [
            "Sha Li",
            "Ruining Zhao",
            "Manling Li",
            "Heng Ji",
            "Chris Callison-Burch",
            "Jiawei Han"
        ],
        "published": "2023",
        "summary": "Event schemas are a form of world knowledge about the typical progression of events. Recent methods for event schema induction use information extraction systems to construct a large number of event graph instances from documents, and then learn to generalize the schema from such instances. In contrast, we propose to treat event schemas as a form of commonsense knowledge that can be derived from large language models (LLMs). This new paradigm greatly simplifies the schema induction process and allows us to handle both hierarchical relations and temporal relations between events in a straightforward way. Since event schemas have complex graph structures, we design an incremental prompting and verification method IncPrompt to break down the construction of a complex event graph into three stages: event skeleton construction, event expansion, and event-event relation verification. Compared to directly using LLMs to generate a linearized graph, IncSchema can generate large and complex schemas with 7.2% F1 improvement in temporal relations and 31.0% F1 improvement in hierarchical relations. In addition, compared to the previous state-of-the-art closed-domain schema induction model, human assessors were able to cover ~10% more events when translating the schemas into coherent stories and rated our schemas 1.3 points higher (on a 5-point scale) in terms of readability.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.312.pdf",
        "keywords": [
            "event",
            "event schemas",
            "event schema induction",
            "schema induction",
            "prompting",
            "verification",
            "incremental prompting",
            "event graph"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Compared to directly using LLMs to generate a linearized graph, IncSchema can generate large and complex schemas with 7.2% F1 improvement in temporal relations and 31.0% F1 improvement in hierarchical relations.\"\n\nThis abstract mentions a limitation of LLMs in generating complex event schemas, but only briefly and as a minor detail to justify the proposed Inc",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Compared to directly using LLMs to generate a linearized graph, IncSchema can generate large and complex schemas with 7.2% F1 improvement in temporal relations and 31.0% F1 improvement in hierarchical relations.\"\n\nThis abstract mentions a limitation of LLMs in generating complex event schemas, but only briefly and as a minor detail to justify the proposed Inc"
    },
    {
        "title": "Zero-shot Approach to Overcome Perturbation Sensitivity of Prompts",
        "authors": [
            "Mohna Chakraborty",
            "Adithya Kulkarni",
            "Qi Li"
        ],
        "published": "2023",
        "summary": "Recent studies have demonstrated that natural-language prompts can help to leverage the knowledge learned by pre-trained language models for the binary sentence-level sentiment classification task. Specifically, these methods utilize few-shot learning settings to fine-tune the sentiment classification model using manual or automatically generated prompts. However, the performance of these methods is sensitive to the perturbations of the utilized prompts. Furthermore, these methods depend on a few labeled instances for automatic prompt generation and prompt ranking. This study aims to find high-quality prompts for the given task in a zero-shot setting. Given a base prompt, our proposed approach automatically generates multiple prompts similar to the base prompt employing positional, reasoning, and paraphrasing techniques and then ranks the prompts using a novel metric. We empirically demonstrate that the top-ranked prompts are high-quality and significantly outperform the base prompt and the prompts generated using few-shot learning for the binary sentence-level sentiment classification task.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.313.pdf",
        "keywords": [
            "sentiment classification",
            "perturbation sensitivity",
            "language",
            "sentiment classification model",
            "zero shot approach"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the performance of these methods is sensitive to the perturbations of the utilized prompts.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the performance of these methods is sensitive to the perturbations of the utilized prompts.\""
    },
    {
        "title": "Free Lunch: Robust Cross-Lingual Transfer via Model Checkpoint Averaging",
        "authors": [
            "Fabian David Schmidt",
            "Ivan Vulić",
            "Goran Glavaš"
        ],
        "published": "2023",
        "summary": "Massively multilingual language models have displayed strong performance in zero-shot (ZS-XLT) and few-shot (FS-XLT) cross-lingual transfer setups, where models fine-tuned on task data in a source language are transferred without any or with only a few annotated instances to the target language(s). However, current work typically overestimates model performance as fine-tuned models are frequently evaluated at model checkpoints that generalize best to validation instances in the target languages. This effectively violates the main assumptions of ‘true’ ZS-XLT and FS-XLT. Such XLT setups require robust methods that do not depend on labeled target language data for validation and model selection. In this work, aiming to improve the robustness of ‘true’ ZS-XLT and FS-XLT, we propose a simple and effective method that averages different checkpoints (i.e., model snapshots) during task fine-tuning. We conduct exhaustive ZS-XLT and FS-XLT experiments across higher-level semantic tasks (NLI, extractive QA) and lower-level token classification tasks (NER, POS). The results indicate that averaging model checkpoints yields systematic and consistent performance gains across diverse target languages in all tasks. Importantly, it simultaneously substantially desensitizes XLT to varying hyperparameter choices in the absence of target language validation. We also show that checkpoint averaging benefits performance when further combined with run averaging (i.e., averaging the parameters of models fine-tuned over independent runs).",
        "pdf_link": "https://aclanthology.org/2023.acl-long.314.pdf",
        "keywords": [
            "checkpoint averaging",
            "free lunch"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, current work typically overestimates model performance as fine-tuned models are frequently evaluated at model checkpoints that generalize best to validation instances in the target languages.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, current work typically overestimates model performance as fine-tuned models are frequently evaluated at model checkpoints that generalize best to validation instances in the target languages.\""
    },
    {
        "title": "Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training",
        "authors": [
            "Yan Zeng",
            "Wangchunshu Zhou",
            "Ao Luo",
            "Ziming Cheng",
            "Xinsong Zhang"
        ],
        "published": "2023",
        "summary": "In this paper, we introduce Cross-View Language Modeling, a simple and effective pre-training framework that unifies cross-lingual and cross-modal pre-training with shared architectures and objectives. Our approach is motivated by a key observation that cross-lingual and cross-modal pre-training share the same goal of aligning two different views of the same object into a common semantic space. To this end, the cross-view language modeling framework considers both multi-modal data (i.e., image-caption pairs) and multi-lingual data (i.e., parallel sentence pairs) as two different views of the same object, and trains the model to align the two views by maximizing the mutual information between them with conditional masked language modeling and contrastive learning. We pre-train CCLM, a Cross-lingual Cross-modal Language Model, with the cross-view language modeling framework. Empirical results on IGLUE, a multi-lingual multi-modal benchmark, and two multi-lingual image-text retrieval datasets show that while conceptually simpler, CCLM significantly outperforms the prior state-of-the-art with an average absolute improvement of over 10%. Moreover, CCLM is the first multi-lingual multi-modal pre-trained model that surpasses the translate-test performance of representative English vision-language models by zero-shot cross-lingual transfer.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.315.pdf",
        "keywords": [
            "cross view language modeling",
            "cross modal language model"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs, only a presentation of a new pre-training framework for LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No evidence of discussion of limitations of LLMs, only a presentation of a new pre-training framework for LLMs."
    },
    {
        "title": "Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework",
        "authors": [
            "Ruochen Zhao",
            "Xingxuan Li",
            "Shafiq Joty",
            "Chengwei Qin",
            "Lidong Bing"
        ],
        "published": "2023",
        "summary": "As large language models (LLMs) have become the norm in NLP, demonstrating good performance in generation and reasoning tasks, one of its most fatal disadvantages is the lack of factual correctness. Generating unfactual texts not only leads to lower performances but also degrades the trust and validity of their applications. Chain-of-Thought (CoT) prompting improves trust and model performance on complex reasoning tasks by generating interpretable reasoning chains, but still suffers from factuality concerns in knowledge-intensive tasks. In this paper, we propose the Verify-and-Edit framework for CoT prompting, which seeks to increase prediction factuality by post-editing reasoning chains according to external knowledge. Building on top of GPT-3, our framework lead to accuracy improvements in multiple open-domain question-answering tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.320.pdf",
        "keywords": [
            "prediction factuality"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"one of its most fatal disadvantages is the lack of factual correctness. Generating unfactual texts not only leads to lower performances but also degrades the trust and validity of their applications.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"one of its most fatal disadvantages is the lack of factual correctness. Generating unfactual texts not only leads to lower performances but also degrades the trust and validity of their applications.\""
    },
    {
        "title": "Towards a Common Understanding of Contributing Factors for Cross-Lingual Transfer in Multilingual Language Models: A Review",
        "authors": [
            "Fred Philippy",
            "Siwen Guo",
            "Shohreh Haddadan"
        ],
        "published": "2023",
        "summary": "In recent years, pre-trained Multilingual Language Models (MLLMs) have shown a strong ability to transfer knowledge across different languages. However, given that the aspiration for such an ability has not been explicitly incorporated in the design of the majority of MLLMs, it is challenging to obtain a unique and straightforward explanation for its emergence. In this review paper, we survey literature that investigates different factors contributing to the capacity of MLLMs to perform zero-shot cross-lingual transfer and subsequently outline and discuss these factors in detail. To enhance the structure of this review and to facilitate consolidation with future studies, we identify five categories of such factors. In addition to providing a summary of empirical evidence from past studies, we identify consensuses among studies with consistent findings and resolve conflicts among contradictory ones. Our work contextualizes and unifies existing research streams which aim at explaining the cross-lingual potential of MLLMs. This review provides, first, an aligned reference point for future research and, second, guidance for a better-informed and more efficient way of leveraging the cross-lingual capacity of MLLMs.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.323.pdf",
        "keywords": [
            "multilingual language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, given that the aspiration for such an ability has not been explicitly incorporated in the design of the majority of MLLMs, it is challenging to obtain a unique and straightforward explanation for its emergence.\"\n\nThis rating is given because the abstract mentions a limitation of MLLMs (the lack of a clear explanation for their ability to transfer knowledge across languages) but does not explore it in",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, given that the aspiration for such an ability has not been explicitly incorporated in the design of the majority of MLLMs, it is challenging to obtain a unique and straightforward explanation for its emergence.\"\n\nThis rating is given because the abstract mentions a limitation of MLLMs (the lack of a clear explanation for their ability to transfer knowledge across languages) but does not explore it in"
    },
    {
        "title": "Toward Human-Like Evaluation for Natural Language Generation with Error Analysis",
        "authors": [
            "Qingyu Lu",
            "Liang Ding",
            "Liping Xie",
            "Kanjian Zhang",
            "Derek F. Wong",
            "Dacheng Tao"
        ],
        "published": "2023",
        "summary": "The pretrained language model (PLM) based metrics have been successfully used in evaluating language generation tasks. Recent studies of the human evaluation community show that considering both major errors (e.g. mistranslated tokens) and minor errors (e.g. imperfections in fluency) can produce high-quality judgments. This inspires us to approach the final goal of the automatic metrics (human-like evaluations) by fine-grained error analysis. In this paper, we argue that the ability to estimate sentence confidence is the tip of the iceberg for PLM-based metrics. And it can be used to refine the generated sentence toward higher confidence and more reference-grounded, where the costs of refining and approaching reference are used to determine the major and minor errors, respectively. To this end, we take BARTScore as the testbed and present an innovative solution to marry the unexploited sentence refining capacity of BARTScore and human-like error analysis, where the final score consists of both the evaluations of major and minor errors. Experiments show that our solution consistently and significantly improves BARTScore, and outperforms top-scoring metrics in 19/25 test settings. Analyses demonstrate our method robustly and efficiently approaches human-like evaluations, enjoying better interpretability. Our code and scripts will be publicly released in https://github.com/Coldmist-Lu/ErrorAnalysis_NLGEvaluation.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.324.pdf",
        "keywords": [
            "language model",
            "error analysis",
            "natural language generation",
            "generated",
            "human like evaluations",
            "human evaluation",
            "automatic metrics",
            "bartscore"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None directly, but the paper aims to improve the evaluation of language generation tasks, which might be related to limitations in the generated text by LLMs, however, the paper does not explicitly discuss LLMs or their limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None directly, but the paper aims to improve the evaluation of language generation tasks, which might be related to limitations in the generated text by LLMs, however, the paper does not explicitly discuss LLMs or their limitations."
    },
    {
        "title": "Connective Prediction for Implicit Discourse Relation Recognition via Knowledge Distillation",
        "authors": [
            "Hongyi Wu",
            "Hao Zhou",
            "Man Lan",
            "Yuanbin Wu",
            "Yadong Zhang"
        ],
        "published": "2023",
        "summary": "Implicit discourse relation recognition (IDRR) remains a challenging task in discourse analysis due to the absence of connectives. Most existing methods utilize one-hot labels as the sole optimization target, ignoring the internal association among connectives. Besides, these approaches spend lots of effort on template construction, negatively affecting the generalization capability. To address these problems,we propose a novel Connective Prediction via Knowledge Distillation (CP-KD) approach to instruct large-scale pre-trained language models (PLMs) mining the latent correlations between connectives and discourse relations, which is meaningful for IDRR. Experimental results on the PDTB 2.0/3.0 and CoNLL2016 datasets show that our method significantly outperforms the state-of-the-art models on coarse-grained and fine-grained discourse relations. Moreover, our approach can be transferred to explicit discourse relation recognition(EDRR) and achieve acceptable performance.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.325.pdf",
        "keywords": [
            "knowledge distillation",
            "implicit discourse relation recognition",
            "connective prediction",
            "explicit discourse relation recognition"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but \"large-scale pre-trained language models (PLMs)\" is mentioned, implying that the paper discusses LLMs, but does not explicitly mention any limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but \"large-scale pre-trained language models (PLMs)\" is mentioned, implying that the paper discusses LLMs, but does not explicitly mention any limitations."
    },
    {
        "title": "Scene Graph as Pivoting: Inference-time Image-free Unsupervised Multimodal Machine Translation with Visual Scene Hallucination",
        "authors": [
            "Hao Fei",
            "Qian Liu",
            "Meishan Zhang",
            "Min Zhang",
            "Tat-Seng Chua"
        ],
        "published": "2023",
        "summary": "In this work, we investigate a more realistic unsupervised multimodal machine translation (UMMT) setup, inference-time image-free UMMT, where the model is trained with source-text image pairs, and tested with only source-text inputs. First, we represent the input images and texts with the visual and language scene graphs (SG), where such fine-grained vision-language features ensure a holistic understanding of the semantics. To enable pure-text input during inference, we devise a visual scene hallucination mechanism that dynamically generates pseudo visual SG from the given textual SG. Several SG-pivoting based learning objectives are introduced for unsupervised translation training. On the benchmark Multi30K data, our SG-based method outperforms the best-performing baseline by significant BLEU scores on the task and setup, helping yield translations with better completeness, relevance and fluency without relying on paired images. Further in-depth analyses reveal how our model advances in the task setting.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.329.pdf",
        "keywords": [
            "multimodal machine translation",
            "inference time",
            "scene graph",
            "visual scene hallucination",
            "sg pivoting",
            "unsupervised multimodal machine translation",
            "ummt"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Language model acceptability judgements are not always robust to context",
        "authors": [
            "Koustuv Sinha",
            "Jon Gauthier",
            "Aaron Mueller",
            "Kanishka Misra",
            "Keren Fuentes",
            "Roger Levy",
            "Adina Williams"
        ],
        "published": "2023",
        "summary": "Targeted syntactic evaluations of language models ask whether models show stable preferences for syntactically acceptable content over minimal-pair unacceptable inputs. Our best syntactic evaluation datasets, however, provide substantially less linguistic context than models receive during pretraining. This mismatch raises an important question: how robust are models’ syntactic judgements across different contexts? In this paper, we vary the input contexts based on: length, the types of syntactic phenomena it contains, and whether or not there are grammatical violations. We find that model judgements are generally robust when placed in randomly sampled linguistic contexts, but are unstable when contexts match the test stimuli in syntactic structure. Among all tested models (GPT-2 and five variants of OPT), we find that model performance is affected when we provided contexts with matching syntactic structure: performance significantly improves when contexts are acceptable, and it significantly declines when they are unacceptable. This effect is amplified by the length of the context, except for unrelated inputs. We show that these changes in model performance are not explainable by acceptability-preserving syntactic perturbations. This sensitivity to highly specific syntactic features of the context can only be explained by the models’ implicit in-context learning abilities.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.333.pdf",
        "keywords": [
            "contexts",
            "syntactic evaluation",
            "linguistic context",
            "linguistic",
            "syntactic judgements",
            "language model acceptability judgements",
            "language models",
            "grammatical violations",
            "robust"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that model judgements are generally robust when placed in randomly sampled linguistic contexts, but are unstable when contexts match the test stimuli in syntactic structure.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We find that model judgements are generally robust when placed in randomly sampled linguistic contexts, but are unstable when contexts match the test stimuli in syntactic structure.\""
    },
    {
        "title": "RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations",
        "authors": [
            "Yilun Zhao",
            "Chen Zhao",
            "Linyong Nan",
            "Zhenting Qi",
            "Wenlin Zhang",
            "Xiangru Tang",
            "Boyu Mi",
            "Dragomir Radev"
        ],
        "published": "2023",
        "summary": "Despite significant progress having been made in question answering on tabular data (Table QA), it’s unclear whether, and to what extent existing Table QA models are robust to task-specific perturbations, e.g., replacing key question entities or shuffling table columns. To systematically study the robustness of Table QA models, we propose a benchmark called RobuT, which builds upon existing Table QA datasets (WTQ, WikiSQL-Weak, and SQA) and includes human-annotated adversarial perturbations in terms of table header, table content, and question. Our results indicate that both state-of-the-art Table QA models and large language models (e.g., GPT-3) with few-shot learning falter in these adversarial sets. We propose to address this problem by using large language models to generate adversarial examples to enhance training, which significantly improves the robustness of Table QA models.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.334.pdf",
        "keywords": [
            "adversarial perturbations",
            "robustness",
            "adversarial examples",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our results indicate that both state-of-the-art Table QA models and large language models (e.g., GPT-3) with few-shot learning falter in these adversarial sets.\"\n\nThis rating is chosen because the abstract mentions a limitation of LLMs (faltering in adversarial sets) but does not explore it in depth, instead focusing on a proposed solution to address",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Our results indicate that both state-of-the-art Table QA models and large language models (e.g., GPT-3) with few-shot learning falter in these adversarial sets.\"\n\nThis rating is chosen because the abstract mentions a limitation of LLMs (faltering in adversarial sets) but does not explore it in depth, instead focusing on a proposed solution to address"
    },
    {
        "title": "Similarity-weighted Construction of Contextualized Commonsense Knowledge Graphs for Knowledge-intense Argumentation Tasks",
        "authors": [
            "Moritz Plenz",
            "Juri Opitz",
            "Philipp Heinisch",
            "Philipp Cimiano",
            "Anette Frank"
        ],
        "published": "2023",
        "summary": "Arguments often do not make explicit how a conclusion follows from its premises. To compensate for this lack, we enrich arguments with structured background knowledge to support knowledge-intense argumentation tasks. We present a new unsupervised method for constructing Contextualized Commonsense Knowledge Graphs (CCKGs) that selects contextually relevant knowledge from large knowledge graphs (KGs) efficiently and at high quality. Our work goes beyond context-insensitive knowledge extraction heuristics by computing semantic similarity between KG triplets and textual arguments. Using these triplet similarities as weights, we extract contextualized knowledge paths that connect a conclusion to its premise, while maximizing similarity to the argument. We combine multiple paths into a CCKG that we optionally prune to reduce noise and raise precision. Intrinsic evaluation of the quality of our graphs shows that our method is effective for (re)constructing human explanation graphs. Manual evaluations in a large-scale knowledge selection setup verify high recall and precision of implicit CSK in the CCKGs. Finally, we demonstrate the effectiveness of CCKGs in a knowledge-insensitive argument quality rating task, outperforming strong baselines and rivaling a GPT-3 based system.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.338.pdf",
        "keywords": [
            "commonsense knowledge graphs",
            "contextualized commonsense knowledge graphs",
            "knowledge extraction",
            "similarity weighted construction",
            "argumentation"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"outperforming strong baselines and rivaling a GPT-3 based system.\"\n\nThis paper talks about LLMs, but only mentions a comparison with a GPT-3 based system without discussing any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"outperforming strong baselines and rivaling a GPT-3 based system.\"\n\nThis paper talks about LLMs, but only mentions a comparison with a GPT-3 based system without discussing any limitations of LLMs."
    },
    {
        "title": "Learning Non-linguistic Skills without Sacrificing Linguistic Proficiency",
        "authors": [
            "Mandar Sharma",
            "Nikhil Muralidhar",
            "Naren Ramakrishnan"
        ],
        "published": "2023",
        "summary": "The field of Math-NLP has witnessed significant growth in recent years, motivated by the desire to expand LLM performance to the leaning of non-linguistic notions (numerals, and subsequently, arithmetic reasoning). However, non-linguistic skill injection typically comes at a cost for LLMs: it leads to catastrophic forgetting of core linguistic skills, a consequence that often remains unaddressed in the literature. As Math-NLP has been able to create LLMs that can closely approximate the mathematical skills of a grade schooler or the arithmetic reasoning skills of a calculator, the practicality of these models fail if they concomitantly shed their linguistic capabilities. In this work, we take a closer look into the phenomena of catastrophic forgetting as it pertains to LLMs and subsequently offer a novel framework for non-linguistic skill injection for LLMs based on information-theoretic interventions and skill-specific losses that enable the learning of strict arithmetic reasoning. Our model outperforms the state-of-the-art both on injected non-linguistic skills and on linguistic knowledge retention, and does so with a fraction of the non-linguistic training data (1/4) and zero additional synthetic linguistic training data.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.340.pdf",
        "keywords": [
            "linguistic skills",
            "arithmetic reasoning skills",
            "arithmetic reasoning",
            "non linguistic skill",
            "mathematical skills",
            "learning non linguistic skills",
            "sacrificing linguistic proficiency",
            "linguistic knowledge"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"non-linguistic skill injection typically comes at a cost for LLMs: it leads to catastrophic forgetting of core linguistic skills, a consequence that often remains unaddressed in the literature.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"non-linguistic skill injection typically comes at a cost for LLMs: it leads to catastrophic forgetting of core linguistic skills, a consequence that often remains unaddressed in the literature.\""
    },
    {
        "title": "Measuring the Instability of Fine-Tuning",
        "authors": [
            "Yupei Du",
            "Dong Nguyen"
        ],
        "published": "2023",
        "summary": "Fine-tuning pre-trained language models on downstream tasks with varying random seeds has been shown to be unstable, especially on small datasets. Many previous studies have investigated this instability and proposed methods to mitigate it. However, most of these studies only used the standard deviation of performance scores (SD) as their measure, which is a narrow characterization of instability. In this paper, we analyze SD and six other measures quantifying instability of different granularity levels. Moreover, we propose a systematic evaluation framework of these measures’ validity. Finally, we analyze the consistency and difference between different measures by reassessing existing instability mitigation methods. We hope our results will inform better measurements of the fine-tuning instability.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.342.pdf",
        "keywords": [
            "instability",
            "fine tuning instability",
            "fine tuning",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Fine-tuning pre-trained language models on downstream tasks with varying random seeds has been shown to be unstable, especially on small datasets.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Fine-tuning pre-trained language models on downstream tasks with varying random seeds has been shown to be unstable, especially on small datasets.\""
    },
    {
        "title": "Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment",
        "authors": [
            "Eshaan Tanwar",
            "Subhabrata Dutta",
            "Manish Borthakur",
            "Tanmoy Chakraborty"
        ],
        "published": "2023",
        "summary": "In-context learning (ICL) unfolds as large language models become capable of inferring test labels conditioned on a few labeled samples without any gradient update. ICL-enabled large language models provide a promising step forward toward bypassing recurrent annotation costs in a low-resource setting. Yet, only a handful of past studies have explored ICL in a cross-lingual setting, in which the need for transferring label-knowledge from a high-resource language to a low-resource one is immensely crucial. To bridge the gap, we provide the first in-depth analysis of ICL for cross-lingual text classification. We find that the prevalent mode of selecting random input-label pairs to construct the prompt-context is severely limited in the case of cross-lingual ICL, primarily due to the lack of alignment in the input as well as the output spaces. To mitigate this, we propose a novel prompt construction strategy — Cross-lingual In-context Source Target Alignment (X-InSTA). With an injected coherence in the semantics of the input examples and a task-based alignment across the source and target languages, X-InSTA is able to outperform random prompt selection by a large margin across three different tasks using 44 different cross-lingual pairs.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.346.pdf",
        "keywords": [
            "large language models",
            "classification"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"we find that the prevalent mode of selecting random input-label pairs to construct the prompt-context is severely limited in the case of cross-lingual ICL, primarily due to the lack of alignment in the input as well as the output spaces.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"we find that the prevalent mode of selecting random input-label pairs to construct the prompt-context is severely limited in the case of cross-lingual ICL, primarily due to the lack of alignment in the input as well as the output spaces.\""
    },
    {
        "title": "APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning",
        "authors": [
            "Soumya Sanyal",
            "Yichong Xu",
            "Shuohang Wang",
            "Ziyi Yang",
            "Reid Pryzant",
            "Wenhao Yu",
            "Chenguang Zhu",
            "Xiang Ren"
        ],
        "published": "2023",
        "summary": "Logical reasoning over text is an important ability that requires understanding the semantics of the text and reasoning through them to arrive at correct inferences. Prior works on pretraining language models to improve the logical reasoning ability require complex processing of training data (e.g., aligning symbolic knowledge to text), yielding task-specific data augmentation that is not easy to adapt to any general text corpus. In this work, we propose APOLLO, a simple adaptive pretraining approach to improve the logical reasoning skills of language models. We select a subset of Wikipedia for adaptive pretraining using a set of logical inference keywords as filter words. Further, we propose two self-supervised loss functions for training. First, we modify the masked language modeling loss only to mask specific parts-of-speech words that likely require higher-order reasoning to predict them. Second, we propose a sentence-level classification loss that teaches the model to distinguish between entailment and contradiction types of sentences. The proposed pretraining paradigm is both simple and independent of task formats. We demonstrate the effectiveness of APOLLO by comparing it with prior baselines on two logical reasoning datasets. APOLLO performs comparably on ReClor and outperforms baselines on LogiQA.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.347.pdf",
        "keywords": [
            "logical reasoning",
            "pretraining",
            "logical inference",
            "pretraining language models",
            "language models",
            "apollo",
            "adaptive pretraining"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Prior works on pretraining language models to improve the logical reasoning ability require complex processing of training data (e.g., aligning symbolic knowledge to text), yielding task-specific data augmentation that is not easy to adapt to any general text corpus.\"\n\nThis evidence suggests that the paper mentions a limitation of prior works on pretraining language models, but it does not extensively discuss the limitations of LLM",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Prior works on pretraining language models to improve the logical reasoning ability require complex processing of training data (e.g., aligning symbolic knowledge to text), yielding task-specific data augmentation that is not easy to adapt to any general text corpus.\"\n\nThis evidence suggests that the paper mentions a limitation of prior works on pretraining language models, but it does not extensively discuss the limitations of LLM"
    },
    {
        "title": "MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering",
        "authors": [
            "Vaishali Pal",
            "Andrew Yates",
            "Evangelos Kanoulas",
            "Maarten de Rijke"
        ],
        "published": "2023",
        "summary": "Recent advances in tabular question answering (QA) with large language models are constrained in their coverage and only answer questions over a single table. However, real-world queries are complex in nature, often over multiple tables in a relational database or web page. Single table questions do not involve common table operations such as set operations, Cartesian products (joins), or nested queries. Furthermore, multi-table operations often result in a tabular output, which necessitates table generation capabilities of tabular QA models. To fill this gap, we propose a new task of answering questions over multiple tables. Our model, MultiTabQA, not only answers questions over multiple tables, but also generalizes to generate tabular answers. To enable effective training, we build a pre-training dataset comprising of 132,645 SQL queries and tabular answers. Further, we evaluate the generated tables by introducing table-specific metrics of varying strictness assessing various levels of granularity of the table structure. MultiTabQA outperforms state-of-the-art single table QA models adapted to a multi-table QA setting by finetuning on three datasets: Spider, Atis and GeoQuery.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.348.pdf",
        "keywords": [
            "tabular question answering",
            "tabular answers",
            "multi table question answering",
            "multi table",
            "single table questions"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent advances in tabular question answering (QA) with large language models are constrained in their coverage and only answer questions over a single table.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Recent advances in tabular question answering (QA) with large language models are constrained in their coverage and only answer questions over a single table.\""
    },
    {
        "title": "Long-Tailed Question Answering in an Open World",
        "authors": [
            "Yi Dai",
            "Hao Lang",
            "Yinhe Zheng",
            "Fei Huang",
            "Yongbin Li"
        ],
        "published": "2023",
        "summary": "Real-world data often have an open long-tailed distribution, and building a unified QA model supporting various tasks is vital for practical QA applications. However, it is non-trivial to extend previous QA approaches since they either require access to seen tasks of adequate samples or do not explicitly model samples from unseen tasks. In this paper, we define Open Long-Tailed QA (OLTQA) as learning from long-tailed distributed data and optimizing performance over seen and unseen QA tasks. We propose an OLTQA model that encourages knowledge sharing between head, tail and unseen tasks, and explicitly mines knowledge from a large pre-trained language model (LM).Specifically, we organize our model through a pool of fine-grained components and dynamically combine these components for an input to facilitate knowledge sharing.A retrieve-then-rerank frame is further introduced to select in-context examples, which guild the LM to generate text that express knowledge for QA tasks. Moreover, a two-stage training approach is introduced to pre-train the framework by knowledge distillation (KD) from the LM and then jointly train the frame and a QA model through an adaptive mutual KD method. On a large-scale OLTQA dataset we curate from 43 existing QA datasets, our model consistently outperforms the state-of-the-art.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.351.pdf",
        "keywords": [
            "knowledge distillation",
            "mutual kd method",
            "question answering",
            "language model"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it is non-trivial to extend previous QA approaches since they either require access to seen tasks of adequate samples or do not explicitly model samples from unseen tasks.\"\n\n(Note: Although the abstract does not explicitly mention limitations of LLMs, it mentions challenges in extending previous QA approaches, which might be related to LLMs. However, the focus is not on LLM limitations",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, it is non-trivial to extend previous QA approaches since they either require access to seen tasks of adequate samples or do not explicitly model samples from unseen tasks.\"\n\n(Note: Although the abstract does not explicitly mention limitations of LLMs, it mentions challenges in extending previous QA approaches, which might be related to LLMs. However, the focus is not on LLM limitations"
    },
    {
        "title": "Parallel Context Windows for Large Language Models",
        "authors": [
            "Nir Ratner",
            "Yoav Levine",
            "Yonatan Belinkov",
            "Ori Ram",
            "Inbal Magar",
            "Omri Abend",
            "Ehud Karpas",
            "Amnon Shashua",
            "Kevin Leyton-Brown",
            "Yoav Shoham"
        ],
        "published": "2023",
        "summary": "When applied to processing long text, Large Language Models (LLMs) are limited by their context window. Existing efforts to address this limitation involve training specialized architectures, and cannot be easily applied to off- the-shelf LLMs. We present Parallel Context Windows (PCW), a method that alleviates the context window restriction for any off-the-shelf LLM without further training. The key to the approach is to carve a long context into chunks (“windows”), restrict the attention mechanism to apply only within each window, and re-use the positional embeddings across the windows. Our main results test the PCW approach on in-context learning with models that range in size between 750 million and 178 billion parameters, and show substantial improvements for tasks with diverse input and output spaces. We show additional benefits in other settings where long context windows may be beneficial: multi-hop questions and retrieval-augmented question answering with multiple retrieved documents. Our results highlight Parallel Context Windows as a promising method for applying off-the-shelf LLMs in a range of settings that require long text sequences. We make our code publicly available at https://github.com/ai21labs/parallel-context-windows.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.352.pdf",
        "keywords": [
            "context window",
            "parallel context windows",
            "long context windows",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"When applied to processing long text, Large Language Models (LLMs) are limited by their context window.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"When applied to processing long text, Large Language Models (LLMs) are limited by their context window.\""
    },
    {
        "title": "Efficient Transformers with Dynamic Token Pooling",
        "authors": [
            "Piotr Nawrot",
            "Jan Chorowski",
            "Adrian Lancucki",
            "Edoardo Maria Ponti"
        ],
        "published": "2023",
        "summary": "Transformers achieve unrivalled performance in modelling language, but remain inefficient in terms of memory and time complexity. A possible remedy is to reduce the sequence length in the intermediate layers by pooling fixed-length segments of tokens. Nevertheless, natural units of meaning, such as words or phrases, display varying sizes. To address this mismatch, we equip language models with a dynamic-pooling mechanism, which predicts segment boundaries in an autoregressive fashion. We compare several methods to infer boundaries, including end-to-end learning through stochastic re-parameterisation, supervised learning (based on segmentations from subword tokenizers or spikes in conditional entropy), as well as linguistically motivated boundaries. We perform character-level evaluation on texts from multiple datasets and morphologically diverse languages. The results demonstrate that dynamic pooling, which jointly segments and models language, is both faster and more accurate than vanilla Transformers and fixed-length pooling within the same computational budget.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.353.pdf",
        "keywords": [
            "pooling",
            "dynamic pooling",
            "dynamic token pooling",
            "transformers",
            "segments",
            "efficient transformers",
            "fixed"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Transformers achieve unrivalled performance in modelling language, but remain inefficient in terms of memory and time complexity.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Transformers achieve unrivalled performance in modelling language, but remain inefficient in terms of memory and time complexity.\""
    },
    {
        "title": "ContraCLM: Contrastive Learning For Causal Language Model",
        "authors": [
            "Nihal Jain",
            "Dejiao Zhang",
            "Wasi Uddin Ahmad",
            "Zijian Wang",
            "Feng Nan",
            "Xiaopeng Li",
            "Ming Tan",
            "Ramesh Nallapati",
            "Baishakhi Ray",
            "Parminder Bhatia",
            "Xiaofei Ma",
            "Bing Xiang"
        ],
        "published": "2023",
        "summary": "Despite exciting progress in causal language models, the expressiveness of their representations is largely limited due to poor discrimination ability. To remedy this issue, we present CONTRACLM, a novel contrastive learning framework at both the token-level and the sequence-level. We assess CONTRACLM on a variety of downstream tasks. We show that CONTRACLM enhances the discrimination of representations and bridges the gap with encoder-only models, which makes causal language models better suited for tasks beyond language generation. Specifically, we attain 44% relative improvement on the Semantic Textual Similarity tasks and 34% on Code-to-Code Search tasks. Furthermore, by improving the expressiveness of representations, CONTRACLM also boosts the source code generation capability with 9% relative improvement on execution accuracy on the HumanEval benchmark.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.355.pdf",
        "keywords": [
            "causal language model",
            "contraclm",
            "contrastive learning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite exciting progress in causal language models, the expressiveness of their representations is largely limited due to poor discrimination ability.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite exciting progress in causal language models, the expressiveness of their representations is largely limited due to poor discrimination ability.\""
    },
    {
        "title": "Contrastive Learning with Adversarial Examples for Alleviating Pathology of Language Model",
        "authors": [
            "Pengwei Zhan",
            "Jing Yang",
            "Xiao Huang",
            "Chunlei Jing",
            "Jingying Li",
            "Liming Wang"
        ],
        "published": "2023",
        "summary": "Neural language models have achieved superior performance. However, these models also suffer from the pathology of overconfidence in the out-of-distribution examples, potentially making the model difficult to interpret and making the interpretation methods fail to provide faithful attributions. In this paper, we explain the model pathology from the view of sentence representation and argue that the counter-intuitive bias degree and direction of the out-of-distribution examples’ representation cause the pathology. We propose a Contrastive learning regularization method using Adversarial examples for Alleviating the Pathology (ConAAP), which calibrates the sentence representation of out-of-distribution examples. ConAAP generates positive and negative examples following the attribution results and utilizes adversarial examples to introduce direction information in regularization. Experiments show that ConAAP effectively alleviates the model pathology while slightly impacting the generalization ability on in-distribution examples and thus helps interpretation methods obtain more faithful results.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.358.pdf",
        "keywords": [
            "adversarial examples",
            "regularization",
            "bias degree",
            "neural language models",
            "language model",
            "distribution examples",
            "alleviating the pathology",
            "conaap"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these models also suffer from the pathology of overconfidence in the out-of-distribution examples, potentially making the model difficult to interpret and making the interpretation methods fail to provide faithful attributions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, these models also suffer from the pathology of overconfidence in the out-of-distribution examples, potentially making the model difficult to interpret and making the interpretation methods fail to provide faithful attributions.\""
    },
    {
        "title": "FutureTOD: Teaching Future Knowledge to Pre-trained Language Model for Task-Oriented Dialogue",
        "authors": [
            "Weihao Zeng",
            "Keqing He",
            "Yejie Wang",
            "Chen Zeng",
            "Jingang Wang",
            "Yunsen Xian",
            "Weiran Xu"
        ],
        "published": "2023",
        "summary": "Pre-trained language models based on general text enable huge success in the NLP scenario. But the intrinsical difference of linguistic patterns between general text and task-oriented dialogues makes existing pre-trained language models less useful in practice. Current dialogue pre-training methods rely on a contrastive framework and face the challenges of both selecting true positives and hard negatives. In this paper, we propose a novel dialogue pre-training model, FutureTOD, which distills future knowledge to the representation of the previous dialogue context using a self-training framework. Our intuition is that a good dialogue representation both learns local context information and predicts future information. Extensive experiments on diverse downstream dialogue tasks demonstrate the effectiveness of our model, especially the generalization, robustness, and learning discriminative dialogue representations capabilities.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.360.pdf",
        "keywords": [
            "dialogue",
            "language",
            "language models",
            "trained language models",
            "training",
            "general text"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"But the intrinsical difference of linguistic patterns between general text and task-oriented dialogues makes existing pre-trained language models less useful in practice.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"But the intrinsical difference of linguistic patterns between general text and task-oriented dialogues makes existing pre-trained language models less useful in practice.\""
    },
    {
        "title": "LAMBADA: Backward Chaining for Automated Reasoning in Natural Language",
        "authors": [
            "Mehran Kazemi",
            "Najoung Kim",
            "Deepti Bhatia",
            "Xin Xu",
            "Deepak Ramachandran"
        ],
        "published": "2023",
        "summary": "Remarkable progress has been made on automated reasoning with natural text, by using Large Language Models (LLMs) and methods such as Chain-of-Thought prompting and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning. The classical automated reasoning literature has shown that reasoning in the backward direction (i.e. from intended conclusion to supporting axioms) is significantly more efficient at proof-finding. Importing this intuition into the LM setting, we develop a Backward Chaining algorithm, called LAMBADA, that decomposes reasoning into four sub-modules, that are simply implemented by few-shot prompted LLM inference. We show that LAMBADA achieves sizable accuracy boosts over state-of-the-art forward reasoning methods on two challenging logical reasoning datasets, particularly when deep and accurate proof chains are required.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.361.pdf",
        "keywords": [
            "automated reasoning",
            "logical reasoning",
            "forward reasoning",
            "natural language",
            "backward chaining algorithm"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning.\""
    },
    {
        "title": "PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives",
        "authors": [
            "Silin Gao",
            "Beatriz Borges",
            "Soyoung Oh",
            "Deniz Bayazit",
            "Saya Kanno",
            "Hiromi Wakaki",
            "Yuki Mitsufuji",
            "Antoine Bosselut"
        ],
        "published": "2023",
        "summary": "Sustaining coherent and engaging narratives requires dialogue or storytelling agents to understandhow the personas of speakers or listeners ground the narrative. Specifically, these agents must infer personas of their listeners to produce statements that cater to their interests. They must also learn to maintain consistent speaker personas for themselves throughout the narrative, so that their counterparts feel involved in a realistic conversation or story. However, personas are diverse and complex: they entail large quantities of rich interconnected world knowledge that is challenging to robustly represent in general narrative systems (e.g., a singer is good at singing, and may have attended conservatoire). In this work, we construct a new large-scale persona commonsense knowledge graph, PeaCoK, containing ~100K human-validated persona facts. Our knowledge graph schematizes five dimensions of persona knowledge identified in previous studies of human interactive behaviours, and distils facts in this schema from both existing commonsense knowledge graphs and large-scale pretrained language models. Our analysis indicates that PeaCoK contains rich and precise world persona inferences that help downstream systems generate more consistent and engaging narratives.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.362.pdf",
        "keywords": [
            "narratives",
            "engaging narratives",
            "storytelling",
            "commonsense knowledge",
            "commonsense knowledge graph",
            "consistent and engaging narratives"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, personas are diverse and complex: they entail large quantities of rich interconnected world knowledge that is challenging to robustly represent in general narrative systems\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, personas are diverse and complex: they entail large quantities of rich interconnected world knowledge that is challenging to robustly represent in general narrative systems\""
    },
    {
        "title": "Prompting Language Models for Linguistic Structure",
        "authors": [
            "Terra Blevins",
            "Hila Gonen",
            "Luke Zettlemoyer"
        ],
        "published": "2023",
        "summary": "Although pretrained language models (PLMs) can be prompted to perform a wide range of language tasks, it remains an open question how much this ability comes from generalizable linguistic understanding versus surface-level lexical patterns. To test this, we present a structured prompting approach for linguistic structured prediction tasks, allowing us to perform zero- and few-shot sequence tagging with autoregressive PLMs. We evaluate this approach on part-of-speech tagging, named entity recognition, and sentence chunking, demonstrating strong few-shot performance in all cases. We also find that while PLMs contain significant prior knowledge of task labels due to task leakage into the pretraining corpus, structured prompting can also retrieve linguistic structure with arbitrary labels. These findings indicate that the in-context learning ability and linguistic knowledge of PLMs generalizes beyond memorization of their training data.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.367.pdf",
        "keywords": [
            "language models",
            "prompting language models",
            "prompting",
            "structured prompting",
            "linguistic structure",
            "entity recognition",
            "speech tagging",
            "chunking"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "Trillion Dollar Words: A New Financial Dataset, Task & Market Analysis",
        "authors": [
            "Agam Shah",
            "Suvan Paturi",
            "Sudheer Chava"
        ],
        "published": "2023",
        "summary": "Monetary policy pronouncements by Federal Open Market Committee (FOMC) are a major driver of financial market returns. We construct the largest tokenized and annotated dataset of FOMC speeches, meeting minutes, and press conference transcripts in order to understand how monetary policy influences financial markets. In this study, we develop a novel task of hawkish-dovish classification and benchmark various pre-trained language models on the proposed dataset. Using the best-performing model (RoBERTa-large), we construct a measure of monetary policy stance for the FOMC document release days. To evaluate the constructed measure, we study its impact on the treasury market, stock market, and macroeconomic indicators. Our dataset, models, and code are publicly available on Huggingface and GitHub under CC BY-NC 4.0 license.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.368.pdf",
        "keywords": [
            "trillion dollar words",
            "financial dataset",
            "market analysis"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"benchmark various pre-trained language models on the proposed dataset.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"benchmark various pre-trained language models on the proposed dataset.\""
    },
    {
        "title": "SQuARe: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created through Human-Machine Collaboration",
        "authors": [
            "Hwaran Lee",
            "Seokhee Hong",
            "Joonsuk Park",
            "Takyoung Kim",
            "Meeyoung Cha",
            "Yejin Choi",
            "Byoungpil Kim",
            "Gunhee Kim",
            "Eun-Ju Lee",
            "Yong Lim",
            "Alice Oh",
            "Sangchul Park",
            "Jung-Woo Ha"
        ],
        "published": "2023",
        "summary": "The potential social harms that large language models pose, such as generating offensive content and reinforcing biases, are steeply rising. Existing works focus on coping with this concern while interacting with ill-intentioned users, such as those who explicitly make hate speech or elicit harmful responses. However, discussions on sensitive issues can become toxic even if the users are well-intentioned. For safer models in such scenarios, we present the Sensitive Questions and Acceptable Response (SQuARe) dataset, a large-scale Korean dataset of 49k sensitive questions with 42k acceptable and 46k non-acceptable responses. The dataset was constructed leveraging HyperCLOVA in a human-in-the-loop manner based on real news headlines. Experiments show that acceptable response generation significantly improves for HyperCLOVA and GPT-3, demonstrating the efficacy of this dataset.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.370.pdf",
        "keywords": [
            "human machine collaboration"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The potential social harms that large language models pose, such as generating offensive content and reinforcing biases, are steeply rising.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The potential social harms that large language models pose, such as generating offensive content and reinforcing biases, are steeply rising.\""
    },
    {
        "title": "FLamE: Few-shot Learning from Natural Language Explanations",
        "authors": [
            "Yangqiaoyu Zhou",
            "Yiming Zhang",
            "Chenhao Tan"
        ],
        "published": "2023",
        "summary": "Natural language explanations have the potential to provide rich information that in principle guides model reasoning. Yet, recent work by Lampinen et al. has shown limited utility of natural language explanations in improving classification. To effectively learn from explanations, we present FLamE, a two-stage few-shot learning framework that first generates explanations using GPT-3, and then fine-tunes a smaller model (e.g., RoBERTa) with generated explanations. Our experiments on natural language inference demonstrate effectiveness over strong baselines, increasing accuracy by 17.6% over GPT-3 Babbage and 5.7% over GPT-3 Davinci in e-SNLI.Despite improving classification performance, human evaluation surprisingly reveals that the majority of generated explanations does not adequately justify classification decisions. Additional analyses point to the important role of label-specific cues (e.g., “not know” for the neutral label) in generated explanations.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.372.pdf",
        "keywords": [
            "explanations",
            "natural language explanations",
            "natural language inference",
            "flame"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"human evaluation surprisingly reveals that the majority of generated explanations does not adequately justify classification decisions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"human evaluation surprisingly reveals that the majority of generated explanations does not adequately justify classification decisions.\""
    },
    {
        "title": "Learning Symbolic Rules over Abstract Meaning Representations for Textual Reinforcement Learning",
        "authors": [
            "Subhajit Chaudhury",
            "Sarathkrishna Swaminathan",
            "Daiki Kimura",
            "Prithviraj Sen",
            "Keerthiram Murugesan",
            "Rosario Uceda-Sosa",
            "Michiaki Tatsubori",
            "Achille Fokoue",
            "Pavan Kapanipathi",
            "Asim Munawar",
            "Alexander Gray"
        ],
        "published": "2023",
        "summary": "Text-based reinforcement learning agents have predominantly been neural network-based models with embeddings-based representation, learning uninterpretable policies that often do not generalize well to unseen games. On the other hand, neuro-symbolic methods, specifically those that leverage an intermediate formal representation, are gaining significant attention in language understanding tasks. This is because of their advantages ranging from inherent interpretability, the lesser requirement of training data, and being generalizable in scenarios with unseen data. Therefore, in this paper, we propose a modular, NEuro-Symbolic Textual Agent (NESTA) that combines a generic semantic parser with a rule induction system to learn abstract interpretable rules as policies. Our experiments on established text-based game benchmarks show that the proposed NESTA method outperforms deep reinforcement learning-based techniques by achieving better generalization to unseen test games and learning from fewer training interactions.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.373.pdf",
        "keywords": [
            "reinforcement learning",
            "reinforcement learning agents",
            "abstract meaning representations",
            "textual agent",
            "neural network",
            "text based game",
            "embeddings",
            "neuro symbolic",
            "rule"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Counterfactual Debiasing for Fact Verification",
        "authors": [
            "Weizhi Xu",
            "Qiang Liu",
            "Shu Wu",
            "Liang Wang"
        ],
        "published": "2023",
        "summary": "Fact verification aims to automatically judge the veracity of a claim according to several pieces of evidence. Due to the manual construction of datasets, spurious correlations between claim patterns and its veracity (i.e., biases) inevitably exist. Recent studies show that models usually learn such biases instead of understanding the semantic relationship between the claim and evidence. Existing debiasing works can be roughly divided into data-augmentation-based and weight-regularization-based pipeline, where the former is inflexible and the latter relies on the uncertain output on the training stage. Unlike previous works, we propose a novel method from a counterfactual view, namely CLEVER, which is augmentation-free and mitigates biases on the inference stage. Specifically, we train a claim-evidence fusion model and a claim-only model independently. Then, we obtain the final prediction via subtracting output of the claim-only model from output of the claim-evidence fusion model, which counteracts biases in two outputs so that the unbiased part is highlighted. Comprehensive experiments on several datasets have demonstrated the effectiveness of CLEVER.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.374.pdf",
        "keywords": [
            "fact verification",
            "debiasing",
            "counterfactual debiasing",
            "weight regularization",
            "data augmentation"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "What social attitudes about gender does BERT encode? Leveraging insights from psycholinguistics",
        "authors": [
            "Julia Watson",
            "Barend Beekhuizen",
            "Suzanne Stevenson"
        ],
        "published": "2023",
        "summary": "Much research has sought to evaluate the degree to which large language models reflect social biases. We complement such work with an approach to elucidating the connections between language model predictions and people’s social attitudes. We show how word preferences in a large language model reflect social attitudes about gender, using two datasets from human experiments that found differences in gendered or gender neutral word choices by participants with differing views on gender (progressive, moderate, or conservative). We find that the language model BERT takes into account factors that shape human lexical choice of such language, but may not weigh those factors in the same way people do. Moreover, we show that BERT’s predictions most resemble responses from participants with moderate to conservative views on gender. Such findings illuminate how a language model: (1) may differ from people in how it deploys words that signal gender, and (2) may prioritize some social attitudes over others.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.375.pdf",
        "keywords": [
            "psycholinguistics",
            "gender",
            "social attitudes"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that the language model BERT takes into account factors that shape human lexical choice of such language, but may not weigh those factors in the same way people do. Moreover, we show that BERT’s predictions most resemble responses from participants with moderate to conservative views on gender. Such findings illuminate how a language model: (1) may differ from people in how it deploys",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We find that the language model BERT takes into account factors that shape human lexical choice of such language, but may not weigh those factors in the same way people do. Moreover, we show that BERT’s predictions most resemble responses from participants with moderate to conservative views on gender. Such findings illuminate how a language model: (1) may differ from people in how it deploys"
    },
    {
        "title": "Improving the Robustness of Summarization Systems with Dual Augmentation",
        "authors": [
            "Xiuying Chen",
            "Guodong Long",
            "Chongyang Tao",
            "Mingzhe Li",
            "Xin Gao",
            "Chengqi Zhang",
            "Xiangliang Zhang"
        ],
        "published": "2023",
        "summary": "A robust summarization system should be able to capture the gist of the document, regardless of the specific word choices or noise in the input. In this work, we first explore the summarization models’ robustness against perturbations including word-level synonym substitution and noise. To create semantic-consistent substitutes, we propose a SummAttacker, which is an efficient approach to generating adversarial samples based on pre-trained language models. Experimental results show that state-of-the-art summarization models have a significant decrease in performance on adversarial and noisy test sets. Next, we analyze the vulnerability of the summarization systems and explore improving the robustness by data augmentation. Specifically, the first vulnerability factor we found is the low diversity of the training inputs. Correspondingly, we expose the encoder to more diverse cases created by SummAttacker in the input space. The second factor is the vulnerability of the decoder, and we propose an augmentation in the latent space of the decoder to improve its robustness. Concretely, we create virtual cases by manifold softmixing two decoder hidden states of similar semantic meanings. Experimental results on Gigaword and CNN/DM datasets demonstrate that our approach achieves significant improvements over strong baselines and exhibits higher robustness on noisy, attacked, and clean datasets",
        "pdf_link": "https://aclanthology.org/2023.acl-long.378.pdf",
        "keywords": [
            "robustness",
            "robust summarization",
            "summarization",
            "synonym substitution",
            "semantic meanings",
            "summarization systems",
            "summattacker"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Experimental results show that state-of-the-art summarization models have a significant decrease in performance on adversarial and noisy test sets.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Experimental results show that state-of-the-art summarization models have a significant decrease in performance on adversarial and noisy test sets.\""
    },
    {
        "title": "Interpretable Math Word Problem Solution Generation via Step-by-step Planning",
        "authors": [
            "Mengxue Zhang",
            "Zichao Wang",
            "Zhichao Yang",
            "Weiqi Feng",
            "Andrew Lan"
        ],
        "published": "2023",
        "summary": "Solutions to math word problems (MWPs) with step-by-step explanations are valuable, especially in education, to help students better comprehend problem-solving strategies. Most existing approaches only focus on obtaining the final correct answer. A few recent approaches leverage intermediate solution steps to improve final answer correctness but often cannot generate coherent steps with a clear solution strategy. Contrary to existing work, we focus on improving the correctness and coherence of the intermediate solutions steps. We propose a step-by-step planning approach for intermediate solution generation, which strategically plans the generation of the next solution step based on the MWP and the previous solution steps. Our approach first plans the next step by predicting the necessary math operation needed to proceed, given history steps, then generates the next step, token-by-token, by prompting a language model with the predicted math operation. Experiments on the GSM8K dataset demonstrate that our approach improves the accuracy and interpretability of the solution on both automatic metrics and human evaluation.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.379.pdf",
        "keywords": [
            "plans"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"prompting a language model with the predicted math operation.\"\n\nThis paper mentions the use of a language model but does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"prompting a language model with the predicted math operation.\"\n\nThis paper mentions the use of a language model but does not discuss any limitations of LLMs."
    },
    {
        "title": "Are Experts Needed? On Human Evaluation of Counselling Reflection Generation",
        "authors": [
            "Zixiu Wu",
            "Simone Balloccu",
            "Ehud Reiter",
            "Rim Helaoui",
            "Diego Reforgiato Recupero",
            "Daniele Riboni"
        ],
        "published": "2023",
        "summary": "Reflection is a crucial counselling skill where the therapist conveys to the client their interpretation of what the client said. Language models have recently been used to generate reflections automatically, but human evaluation is challenging, particularly due to the cost of hiring experts. Laypeople-based evaluation is less expensive and easier to scale, but its quality is unknown for reflections. Therefore, we explore whether laypeople can be an alternative to experts in evaluating a fundamental quality aspect: coherence and context-consistency. We do so by asking a group of laypeople and a group of experts to annotate both synthetic reflections and human reflections from actual therapists. We find that both laypeople and experts are reliable annotators and that they have moderate-to-strong inter-group correlation, which shows that laypeople can be trusted for such evaluations. We also discover that GPT-3 mostly produces coherent and consistent reflections, and we explore changes in evaluation results when the source of synthetic reflections changes to GPT-3 from the less powerful GPT-2.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.382.pdf",
        "keywords": [
            "reflection",
            "counselling reflection generation",
            "coherence",
            "human evaluation",
            "context consistency"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicitly mentioned, but \"we also discover that GPT-3 mostly produces coherent and consistent reflections\" implies some limitations in the model's performance.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None explicitly mentioned, but \"we also discover that GPT-3 mostly produces coherent and consistent reflections\" implies some limitations in the model's performance."
    },
    {
        "title": "PairSpanBERT: An Enhanced Language Model for Bridging Resolution",
        "authors": [
            "Hideo Kobayashi",
            "Yufang Hou",
            "Vincent Ng"
        ],
        "published": "2023",
        "summary": "We present PairSpanBERT, a SpanBERT-based pre-trained model specialized for bridging resolution. To this end, we design a novel pre-training objective that aims to learn the contexts in which two mentions are implicitly linked to each other from a large amount of data automatically generated either heuristically or via distance supervision with a knowledge graph. Despite the noise inherent in the automatically generated data, we achieve the best results reported to date on three evaluation datasets for bridging resolution when replacing SpanBERT with PairSpanBERT in a state-of-the-art resolver that jointly performs entity coreference resolution and bridging resolution.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.383.pdf",
        "keywords": [
            "resolution",
            "bridging resolution",
            "pairspanbert",
            "resolver",
            "entity coreference resolution"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the noise inherent in the automatically generated data\"\n\n(Note: The paper mentions a limitation of the data used to train the model, but it's not a limitation of the LLM itself. However, it's still a minor mention of a challenge related to the LLM, so it's rated as 1)",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Despite the noise inherent in the automatically generated data\"\n\n(Note: The paper mentions a limitation of the data used to train the model, but it's not a limitation of the LLM itself. However, it's still a minor mention of a challenge related to the LLM, so it's rated as 1)"
    },
    {
        "title": "Few-shot In-context Learning on Knowledge Base Question Answering",
        "authors": [
            "Tianle Li",
            "Xueguang Ma",
            "Alex Zhuang",
            "Yu Gu",
            "Yu Su",
            "Wenhu Chen"
        ],
        "published": "2023",
        "summary": "Question answering over knowledge bases is considered a difficult problem due to the challenge of generalizing to a wide variety of possible natural language questions. Additionally, the heterogeneity of knowledge base schema items between different knowledge bases often necessitates specialized training for different knowledge base question-answering (KBQA) datasets. To handle questions over diverse KBQA datasets with a unified training-free framework, we propose KB-BINDER, which for the first time enables few-shot in-context learning over KBQA tasks. Firstly, KB-BINDER leverages large language models like Codex to generate logical forms as the draft for a specific question by imitating a few demonstrations. Secondly, KB-BINDER grounds on the knowledge base to bind the generated draft to an executable one with BM25 score matching. The experimental results on four public heterogeneous KBQA datasets show that KB-BINDER can achieve a strong performance with only a few in-context demonstrations. Especially on GraphQA and 3-hop MetaQA, KB-BINDER can even outperform the state-of-the-art trained models. On GrailQA and WebQSP, our model is also on par with other fully-trained models. We believe KB-BINDER can serve as an important baseline for future research. We plan to release all the code and data. Our code is available at https://github.com/ltl3A87/KB-BINDER.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.385.pdf",
        "keywords": [
            "knowledge bases",
            "knowledge base question answering",
            "question answering"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Additionally, the heterogeneity of knowledge base schema items between different knowledge bases often necessitates specialized training for different knowledge base question-answering (KBQA) datasets.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Additionally, the heterogeneity of knowledge base schema items between different knowledge bases often necessitates specialized training for different knowledge base question-answering (KBQA) datasets.\""
    },
    {
        "title": "Fact-Checking Complex Claims with Program-Guided Reasoning",
        "authors": [
            "Liangming Pan",
            "Xiaobao Wu",
            "Xinyuan Lu",
            "Anh Tuan Luu",
            "William Yang Wang",
            "Min-Yen Kan",
            "Preslav Nakov"
        ],
        "published": "2023",
        "summary": "Fact-checking real-world claims often requires collecting multiple pieces of evidence and applying complex multi-step reasoning. In this paper, we present Program-Guided Fact-Checking (ProgramFC), a novel fact-checking model that decomposes complex claims into simpler sub-tasks that can be solved using a shared library of specialized functions. We first leverage the in-context learning ability of large language models to generate reasoning programs to guide the verification process. Afterward, we execute the program by delegating each sub-task to the corresponding sub-task handler. This process makes our model both explanatory and data-efficient, providing clear explanations of its reasoning process and requiring minimal training data. We evaluate ProgramFC on two challenging fact-checking datasets and show that it outperforms seven fact-checking baselines across different settings of evidence availability, with explicit output programs that benefit human debugging. Our codes and data are publicly available at https://github.com/mbzuai-nlp/ProgramFC.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.386.pdf",
        "keywords": [
            "fact checking",
            "program guided fact checking",
            "program guided reasoning",
            "reasoning",
            "complex multi step reasoning",
            "verification",
            "context learning"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"leverage the in-context learning ability of large language models\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"leverage the in-context learning ability of large language models\""
    },
    {
        "title": "Patton: Language Model Pretraining on Text-Rich Networks",
        "authors": [
            "Bowen Jin",
            "Wentao Zhang",
            "Yu Zhang",
            "Yu Meng",
            "Xinyang Zhang",
            "Qi Zhu",
            "Jiawei Han"
        ],
        "published": "2023",
        "summary": "A real-world text corpus sometimes comprises not only text documents, but also semantic links between them (e.g., academic papers in a bibliographic network are linked by citations and co-authorships).Text documents and semantic connections form a text-rich network, which empowers a wide range of downstream tasks such as classification and retrieval. However, pretraining methods for such structures are still lacking, making it difficult to build one generic model that can be adapted to various tasks on text-rich networks. Current pretraining objectives, such as masked language modeling, purely model texts and do not take inter-document structure information into consideration. To this end, we propose our PretrAining on TexT-Rich NetwOrk framework Patton.Patton includes two pretraining strategies: network-contextualized masked language modeling and masked node prediction, to capture the inherent dependency between textual attributes and network structure. We conduct experiments on four downstream tasks in five datasets from both academic and e-commerce domains, where Patton outperforms baselines significantly and consistently.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.387.pdf",
        "keywords": [
            "pretraining",
            "text rich networks",
            "language model pretraining",
            "classification",
            "text corpus",
            "masked node prediction"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, pretraining methods for such structures are still lacking, making it difficult to build one generic model that can be adapted to various tasks on text-rich networks. Current pretraining objectives, such as masked language modeling, purely model texts and do not take inter-document structure information into consideration.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, pretraining methods for such structures are still lacking, making it difficult to build one generic model that can be adapted to various tasks on text-rich networks. Current pretraining objectives, such as masked language modeling, purely model texts and do not take inter-document structure information into consideration.\""
    },
    {
        "title": "Soft Language Clustering for Multilingual Model Pre-training",
        "authors": [
            "Jiali Zeng",
            "Yufan Jiang",
            "Yongjing Yin",
            "Yi Jing",
            "Fandong Meng",
            "Binghuai Lin",
            "Yunbo Cao",
            "Jie Zhou"
        ],
        "published": "2023",
        "summary": "Multilingual pre-trained language models have demonstrated impressive (zero-shot) cross-lingual transfer abilities, however, their performance is hindered when the target language has distant typologyfrom the source language or when pre-training data is limited in size. In this paper, we propose XLM-P, a method that contextually retrieves prompts as flexible guidance for encoding instances conditionally. Our space-efficient and model-agnostic XLM-P approach enables (1) lightweight modeling of language-invariant and language-specific knowledge across languages, and (2) easy integration with other multilingual pre-training methods. On the tasks of XTREME, which include text classification, sequence labeling, question answering, and sentence retrieval, both base- and large-size language models pre-trained with our proposed method exhibit consistent performance improvement. Furthermore, it provides substantial advantages for low-resource languages in unsupervised sentence retrieval and for target languages that differ greatly from the source language in cross-lingual transfer.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.388.pdf",
        "keywords": [
            "soft language clustering",
            "models",
            "pre trained language models",
            "text classification",
            "multilingual model pre training",
            "multilingual"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"however, their performance is hindered when the target language has distant typology from the source language or when pre-training data is limited in size.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"however, their performance is hindered when the target language has distant typology from the source language or when pre-training data is limited in size.\""
    },
    {
        "title": "When and how to paraphrase for named entity recognition?",
        "authors": [
            "Saket Sharma",
            "Aviral Joshi",
            "Yiyun Zhao",
            "Namrata Mukhija",
            "Hanoz Bhathena",
            "Prateek Singh",
            "Sashank Santhanam"
        ],
        "published": "2023",
        "summary": "While paraphrasing is a promising approach for data augmentation in classification tasks, its effect on named entity recognition (NER) is not investigated systematically due to the difficulty of span-level label preservation. In this paper, we utilize simple strategies to annotate entity spans in generations and compare established and novel methods of paraphrasing in NLP such as back translation, specialized encoder-decoder models such as Pegasus, and GPT-3 variants for their effectiveness in improving downstream performance for NER across different levels of gold annotations and paraphrasing strength on 5 datasets. We thoroughly explore the influence of paraphrasers, and dynamics between paraphrasing strength and gold dataset size on the NER performance with visualizations and statistical testing. We find that the choice of the paraphraser greatly impacts NER performance, with one of the larger GPT-3 variants exceedingly capable of generating high quality paraphrases, yielding statistically significant improvements in NER performance with increasing paraphrasing strength, while other paraphrasers show more mixed results. Additionally, inline auto annotations generated by larger GPT-3 are strictly better than heuristic based annotations. We also find diminishing benefits of paraphrasing as gold annotations increase for most datasets. Furthermore, while most paraphrasers promote entity memorization in NER, the proposed GPT-3 configuration performs most favorably among the compared paraphrasers when tested on unseen entities, with memorization reducing further with paraphrasing strength. Finally, we explore mention replacement using GPT-3, which provides additional benefits over base paraphrasing for specific datasets.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.390.pdf",
        "keywords": [
            "visualizations",
            "named entity recognition",
            "entity memorization",
            "paraphrases",
            "annotations"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"while other paraphrasers show more mixed results.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"while other paraphrasers show more mixed results.\""
    },
    {
        "title": "Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-text Rationales",
        "authors": [
            "Brihi Joshi",
            "Ziyi Liu",
            "Sahana Ramnath",
            "Aaron Chan",
            "Zhewei Tong",
            "Shaoliang Nie",
            "Qifan Wang",
            "Yejin Choi",
            "Xiang Ren"
        ],
        "published": "2023",
        "summary": "Among the remarkable emergent capabilities of large language models (LMs) is free-text rationalization; beyond certain scale, large LMs are capable of generating seemingly useful rationalizations, which in turn, can dramatically enhance their performances on leaderboards. This phenomenon raises a question: can machine generated rationales also be useful for humans, especially when lay humans try to answer questions based on those machine rationales? We observe that human utility of existing rationales is far from satisfactory and expensive to estimate with human studies. Existing metrics like task performance of the LM generating the rationales or similarity between generated and gold rationales are not good indicators of their human utility. While we observe that certain properties of rationales like conciseness and novelty are correlated with their human utility, estimating them without human involvement is challenging. We show that, by estimating a rationale’s helpfulness in answering similar unseen instances, we can measure its human utility to a better extent. We also translate this finding into an automated score, Gen-U, that we propose, which can help improve LMs’ ability to generate rationales with better human utility, while maintaining most of its task performance. Lastly, we release all code and collected data with this project.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.392.pdf",
        "keywords": [
            "helpfulness",
            "rationale",
            "machine rationales",
            "utility",
            "human utility",
            "machine generated rationales",
            "language models",
            "free text rationalization"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We observe that human utility of existing rationales is far from satisfactory\"; \"Existing metrics like task performance of the LM generating the rationales or similarity between generated and gold rationales are not good indicators of their human utility.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We observe that human utility of existing rationales is far from satisfactory\"; \"Existing metrics like task performance of the LM generating the rationales or similarity between generated and gold rationales are not good indicators of their human utility.\""
    },
    {
        "title": "Dynamic Transformers Provide a False Sense of Efficiency",
        "authors": [
            "Yiming Chen",
            "Simin Chen",
            "Zexin Li",
            "Wei Yang",
            "Cong Liu",
            "Robby Tan",
            "Haizhou Li"
        ],
        "published": "2023",
        "summary": "Despite much success in natural language processing (NLP), pre-trained language models typically lead to a high computational cost during inference. Multi-exit is a mainstream approach to address this issue by making a trade-off between efficiency and accuracy, where the saving of computation comes from an early exit. However, whether such saving from early-exiting is robust remains unknown. Motivated by this, we first show that directly adapting existing adversarial attack approaches targeting model accuracy cannot significantly reduce inference efficiency. To this end, we propose a simple yet effective attacking framework, SAME, a novel slowdown attack framework on multi-exit models, which is specially tailored to reduce the efficiency of the multi-exit models. By leveraging the multi-exit models’ design characteristics, we utilize all internal predictions to guide the adversarial sample generation instead of merely considering the final prediction. Experiments on the GLUE benchmark show that SAME can effectively diminish the efficiency gain of various multi-exit models by 80% on average, convincingly validating its effectiveness and generalization ability.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.395.pdf",
        "keywords": [
            "natural language processing",
            "dynamic transformers",
            "inference efficiency",
            "efficiency"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite much success in natural language processing (NLP), pre-trained language models typically lead to a high computational cost during inference.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Despite much success in natural language processing (NLP), pre-trained language models typically lead to a high computational cost during inference.\""
    },
    {
        "title": "Empowering Cross-lingual Behavioral Testing of NLP Models with Typological Features",
        "authors": [
            "Ester Hlavnova",
            "Sebastian Ruder"
        ],
        "published": "2023",
        "summary": "A challenge towards developing NLP systems for the world’s languages is understanding how they generalize to typological differences relevant for real-world applications. To this end, we propose M2C, a morphologically-aware framework for behavioral testing of NLP models. We use M2C to generate tests that probe models’ behavior in light of specific linguistic features in 12 typologically diverse languages. We evaluate state-of-the-art language models on the generated tests. While models excel at most tests in English, we highlight generalization failures to specific typological characteristics such as temporal expressions in Swahili and compounding possessives in Finish. Our findings motivate the development of models that address these blind spots.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.396.pdf",
        "keywords": [
            "behavioral testing of nlp",
            "models excel"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While models excel at most tests in English, we highlight generalization failures to specific typological characteristics such as temporal expressions in Swahili and compounding possessives in Finish.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"While models excel at most tests in English, we highlight generalization failures to specific typological characteristics such as temporal expressions in Swahili and compounding possessives in Finish.\""
    },
    {
        "title": "Learning Better Masking for Better Language Model Pre-training",
        "authors": [
            "Dongjie Yang",
            "Zhuosheng Zhang",
            "Hai Zhao"
        ],
        "published": "2023",
        "summary": "Masked Language Modeling (MLM) has been widely used as the denoising objective in pre-training language models (PrLMs). Existing PrLMs commonly adopt a Random-Token Masking strategy where a fixed masking ratio is applied and different contents are masked by an equal probability throughout the entire training. However, the model may receive complicated impact from pre-training status, which changes accordingly as training time goes on. In this paper, we show that such time-invariant MLM settings on masking ratio and masked content are unlikely to deliver an optimal outcome, which motivates us to explore the influence of time-variant MLM settings. We propose two scheduled masking approaches that adaptively tune the masking ratio and masked content in different training stages, which improves the pre-training efficiency and effectiveness verified on the downstream tasks. Our work is a pioneer study on time-variant masking strategy on ratio and content and gives a better understanding of how masking ratio and masked content influence the MLM pre-training.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.400.pdf",
        "keywords": [
            "masking",
            "time variant masking"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the model may receive complicated impact from pre-training status, which changes accordingly as training time goes on.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the model may receive complicated impact from pre-training status, which changes accordingly as training time goes on.\""
    },
    {
        "title": "VisText: A Benchmark for Semantically Rich Chart Captioning",
        "authors": [
            "Benny Tang",
            "Angie Boggust",
            "Arvind Satyanarayan"
        ],
        "published": "2023",
        "summary": "Captions that describe or explain charts help improve recall and comprehension of the depicted data and provide a more accessible medium for people with visual disabilities. However, current approaches for automatically generating such captions struggle to articulate the perceptual or cognitive features that are the hallmark of charts (e.g., complex trends and patterns). In response, we introduce VisText: a dataset of 12,441 pairs of charts and captions that describe the charts’ construction, report key statistics, and identify perceptual and cognitive phenomena. In VisText, a chart is available as three representations: a rasterized image, a backing data table, and a scene graph—a hierarchical representation of a chart’s visual elements akin to a web page’s Document Object Model (DOM). To evaluate the impact of VisText, we fine-tune state-of-the-art language models on our chart captioning task and apply prefix-tuning to produce captions that vary the semantic content they convey. Our models generate coherent, semantically rich captions and perform on par with state-of-the-art chart captioning models across machine translation and text generation metrics. Through qualitative analysis, we identify six broad categories of errors that our models make that can inform future work.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.401.pdf",
        "keywords": [
            "captions",
            "vistext",
            "chart captioning",
            "benchmark"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, current approaches for automatically generating such captions struggle to articulate the perceptual or cognitive features that are the hallmark of charts (e.g., complex trends and patterns)\"; \"Through qualitative analysis, we identify six broad categories of errors that our models make that can inform future work.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, current approaches for automatically generating such captions struggle to articulate the perceptual or cognitive features that are the hallmark of charts (e.g., complex trends and patterns)\"; \"Through qualitative analysis, we identify six broad categories of errors that our models make that can inform future work.\""
    },
    {
        "title": "Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text",
        "authors": [
            "Qianhui Wu",
            "Huiqiang Jiang",
            "Haonan Yin",
            "Börje Karlsson",
            "Chin-Yew Lin"
        ],
        "published": "2023",
        "summary": "Self-supervised representation learning has proved to be a valuable component for out-of-distribution (OoD) detection with only the texts of in-distribution (ID) examples. These approaches either train a language model from scratch or fine-tune a pre-trained language model using ID examples, and then take the perplexity output by the language model as OoD scores. In this paper, we analyze the complementary characteristic of both methods and propose a multi-level knowledge distillation approach that integrates their strengths while mitigating their limitations. Specifically, we use a fine-tuned model as the teacher to teach a randomly initialized student model on the ID examples. Besides the prediction layer distillation, we present a similarity-based intermediate layer distillation method to thoroughly explore the representation space of the teacher model. In this way, the learned student can better represent the ID data manifold while gaining a stronger ability to map OoD examples outside the ID data manifold with the regularization inherited from pre-training. Besides, the student model sees only ID examples during parameter learning, further promoting more distinguishable features for OoD detection. We conduct extensive experiments over multiple benchmark datasets, i.e., CLINC150, SST, ROSTD, 20 NewsGroups, and AG News; showing that the proposed method yields new state-of-the-art performance. We also explore its application as an AIGC detector to distinguish answers generated by ChatGPT and human experts. It is observed that our model exceeds human evaluators in the pair-expert task on the Human ChatGPT Comparison Corpus.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.403.pdf",
        "keywords": [
            "knowledge distillation",
            "out of distribution detection",
            "out of distribution",
            "language model",
            "layer"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"we analyze the complementary characteristic of both methods and propose a multi-level knowledge distillation approach that integrates their strengths while mitigating their limitations.\"\n\nNote that the paper mentions limitations of the methods, but it is not clear if these limitations are specific to LLMs. However, given the context, it can be inferred that the limitations are related to LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"we analyze the complementary characteristic of both methods and propose a multi-level knowledge distillation approach that integrates their strengths while mitigating their limitations.\"\n\nNote that the paper mentions limitations of the methods, but it is not clear if these limitations are specific to LLMs. However, given the context, it can be inferred that the limitations are related to LLMs."
    },
    {
        "title": "MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation",
        "authors": [
            "Jiazhan Feng",
            "Qingfeng Sun",
            "Can Xu",
            "Pu Zhao",
            "Yaming Yang",
            "Chongyang Tao",
            "Dongyan Zhao",
            "Qingwei Lin"
        ],
        "published": "2023",
        "summary": "Responding with multi-modal content has been recognized as an essential capability for an intelligent conversational agent. In this paper, we introduce the MMDialog dataset to facilitate multi-modal conversation better. MMDialog is composed of a curated set of 1.08 million real-world dialogues with 1.53 million unique images across 4,184 topics. MMDialog has two main and unique advantages. First, it is the largest multi-modal conversation dataset by the number of dialogues by 88x. Second, it contains massive topics to generalize the open domain. To build an engaging dialogue system with this dataset, we propose and normalize two response prediction tasks based on retrieval and generative scenarios. In addition, we build two baselines for the above tasks with state-of-the-art techniques and report their experimental performance. We also propose a novel evaluation metric MM-Relevance to measure the multi-modal responses. Our dataset is available in https://github.com/victorsungo/MMDialog.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.405.pdf",
        "keywords": [
            "multi modal conversation",
            "dialogue"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models",
        "authors": [
            "Jonas Belouadi",
            "Steffen Eger"
        ],
        "published": "2023",
        "summary": "State-of-the-art poetry generation systems are often complex. They either consist of task-specific model pipelines, incorporate prior knowledge in the form of manually created constraints, or both. In contrast, end-to-end models would not suffer from the overhead of having to model prior knowledge and could learn the nuances of poetry from data alone, reducing the degree of human supervision required. In this work, we investigate end-to-end poetry generation conditioned on styles such as rhyme, meter, and alliteration. We identify and address lack of training data and mismatching tokenization algorithms as possible limitations of past attempts. In particular, we successfully pre-train ByGPT5, a new token-free decoder-only language model, and fine-tune it on a large custom corpus of English and German quatrains annotated with our styles. We show that ByGPT5 outperforms other models such as mT5, ByT5, GPT-2 and ChatGPT, while also being more parameter efficient and performing favorably compared to humans. In addition, we analyze its runtime performance and demonstrate that it is not prone to memorization. We make our code, models, and datasets publicly available.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.406.pdf",
        "keywords": [
            "bygpt5",
            "prior knowledge",
            "language model",
            "token free language models",
            "token free",
            "tokenization",
            "poetry"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We identify and address lack of training data and mismatching tokenization algorithms as possible limitations of past attempts.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We identify and address lack of training data and mismatching tokenization algorithms as possible limitations of past attempts.\""
    },
    {
        "title": "Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models",
        "authors": [
            "Myles Foley",
            "Ambrish Rawat",
            "Taesung Lee",
            "Yufang Hou",
            "Gabriele Picco",
            "Giulio Zizzo"
        ],
        "published": "2023",
        "summary": "The wide applicability and adaptability of generative large language models (LLMs) has enabled their rapid adoption. While the pre-trained models can perform many tasks, such models are often fine-tuned to improve their performance on various downstream applications. However, this leads to issues over violation of model licenses, model theft, and copyright infringement. Moreover, recent advances show that generative technology is capable of producing harmful content which exacerbates the problems of accountability within model supply chains. Thus, we need a method to investigate how a model was trained or a piece of text was generated and what their pre-trained base model was. In this paper we take the first step to address this open problem by tracing back the origin of a given fine-tuned LLM to its corresponding pre-trained base model. We consider different knowledge levels and attribution strategies, and find that we can correctly trace back 8 out of the 10 fine tuned models with our best method.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.410.pdf",
        "keywords": [
            "fine tuned",
            "large language models",
            "model supply chains",
            "pre trained models",
            "attribution"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, this leads to issues over violation of model licenses, model theft, and copyright infringement. Moreover, recent advances show that generative technology is capable of producing harmful content which exacerbates the problems of accountability within model supply chains.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, this leads to issues over violation of model licenses, model theft, and copyright infringement. Moreover, recent advances show that generative technology is capable of producing harmful content which exacerbates the problems of accountability within model supply chains.\""
    },
    {
        "title": "Large Language Models Meet NL2Code: A Survey",
        "authors": [
            "Daoguang Zan",
            "Bei Chen",
            "Fengji Zhang",
            "Dianjie Lu",
            "Bingchao Wu",
            "Bei Guan",
            "Wang Yongji",
            "Jian-Guang Lou"
        ],
        "published": "2023",
        "summary": "The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models on the HumanEval benchmark. Through in-depth observation and analysis, we provide some insights and conclude that the key factors contributing to the success of large language models for NL2Code are “Large Size, Premium Data, Expert Tuning”. In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website https://nl2code.github.io to track the latest progress through crowd-sourcing. To the best of our knowledge, this is the first survey of large language models for NL2Code, and we believe it will contribute to the ongoing development of the field.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.411.pdf",
        "keywords": [
            "nl2code",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We also discuss challenges and opportunities regarding the gap between models and humans.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We also discuss challenges and opportunities regarding the gap between models and humans.\""
    },
    {
        "title": "DarkBERT: A Language Model for the Dark Side of the Internet",
        "authors": [
            "Youngjin Jin",
            "Eugene Jang",
            "Jian Cui",
            "Jin-Woo Chung",
            "Yongjae Lee",
            "Seungwon Shin"
        ],
        "published": "2023",
        "summary": "Recent research has suggested that there are clear differences in the language used in the Dark Web compared to that of the Surface Web. As studies on the Dark Web commonly require textual analysis of the domain, language models specific to the Dark Web may provide valuable insights to researchers. In this work, we introduce DarkBERT, a language model pretrained on Dark Web data. We describe the steps taken to filter and compile the text data used to train DarkBERT to combat the extreme lexical and structural diversity of the Dark Web that may be detrimental to building a proper representation of the domain. We evaluate DarkBERT and its vanilla counterpart along with other widely used language models to validate the benefits that a Dark Web domain specific model offers in various use cases. Our evaluations show that DarkBERT outperforms current language models and may serve as a valuable resource for future research on the Dark Web.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.415.pdf",
        "keywords": [
            "darkbert",
            "dark web",
            "dark side",
            "language model",
            "surface web"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the extreme lexical and structural diversity of the Dark Web that may be detrimental to building a proper representation of the domain.\"\n\nThis rating is chosen because the abstract mentions a limitation of general language models (difficulty in representing the Dark Web due to its diversity) but does not explore this limitation in depth and primarily focuses on introducing DarkBERT as a solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the extreme lexical and structural diversity of the Dark Web that may be detrimental to building a proper representation of the domain.\"\n\nThis rating is chosen because the abstract mentions a limitation of general language models (difficulty in representing the Dark Web due to its diversity) but does not explore this limitation in depth and primarily focuses on introducing DarkBERT as a solution."
    },
    {
        "title": "Towards Zero-Shot Multilingual Transfer for Code-Switched Responses",
        "authors": [
            "Ting-Wei Wu",
            "Changsheng Zhao",
            "Ernie Chang",
            "Yangyang Shi",
            "Pierce Chuang",
            "Vikas Chandra",
            "Biing Juang"
        ],
        "published": "2023",
        "summary": "Recent task-oriented dialog systems have had great success in building English-based personal assistants, but extending these systems to a global audience is challenging due to the need for annotated data in the target language. An alternative approach is to leverage existing data in a high-resource language to enable cross-lingual transfer in low-resource language models. However, this type of transfer has not been widely explored in natural language response generation. In this research, we investigate the use of state-of-the-art multilingual models such as mBART and T5 to facilitate zero-shot and few-shot transfer of code-switched responses. We propose a new adapter-based framework that allows for efficient transfer by learning task-specific representations and encapsulating source and target language representations. Our framework is able to successfully transfer language knowledge even when the target language corpus is limited. We present both quantitative and qualitative analyses to evaluate the effectiveness of our approach.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.417.pdf",
        "keywords": [
            "code switched",
            "switched",
            "transfer language knowledge"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"extending these systems to a global audience is challenging due to the need for annotated data in the target language.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"extending these systems to a global audience is challenging due to the need for annotated data in the target language.\""
    },
    {
        "title": "One Network, Many Masks: Towards More Parameter-Efficient Transfer Learning",
        "authors": [
            "Guangtao Zeng",
            "Peiyuan Zhang",
            "Wei Lu"
        ],
        "published": "2023",
        "summary": "Fine-tuning pre-trained language models for multiple tasks can be expensive in terms of storage. Parameter-efficient transfer learning (PETL) methods have been proposed to address this issue, but they still require a significant number of parameters when being applied to broader ranges of tasks. To achieve even greater storage reduction, we propose ProPETL, a novel method that enables efficient sharing of a single prototype PETL network (e.g. adapter, LoRA, and prefix-tuning) across layers and tasks. We learn binary masks to select different sub-networks from the prototype network and apply them as PETL modules into different layers. We find that the binary masks can determine crucial structural information from the network, which is often ignored in previous studies. Our work can also be seen as a type of pruning method, where we find that overparameterization also exists in the seemingly small PETL modules. We evaluate ProPETL on various downstream tasks and show that it can outperform other PETL methods with around 10% parameters required by the latter.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.418.pdf",
        "keywords": [
            "efficient transfer learning",
            "parameter efficient transfer learning",
            "adapter"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Fine-tuning pre-trained language models for multiple tasks can be expensive in terms of storage.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Fine-tuning pre-trained language models for multiple tasks can be expensive in terms of storage.\""
    },
    {
        "title": "Can Language Models Make Fun? A Case Study in Chinese Comical Crosstalk",
        "authors": [
            "Jianquan Li",
            "XiangBo Wu",
            "Xiaokang Liu",
            "Qianqian Xie",
            "Prayag Tiwari",
            "Benyou Wang"
        ],
        "published": "2023",
        "summary": "Language is the principal tool for human communication, in which humor is one of the most attractive parts. Producing natural language like humans using computers, a.k.a, Natural Language Generation (NLG), has been widely used for dialogue systems, chatbots, machine translation, as well as computer-aid creation e.g., idea generations, scriptwriting. However, the humor aspect of natural language is relatively under-investigated, especially in the age of pre-trained language models. In this work, we aim to preliminarily test *whether NLG can generate humor as humans do*. We build a largest dataset consisting of numerous **C**hinese **C**omical **C**rosstalk scripts (called **C**3 in short), which is for a popular Chinese performing art called ‘Xiangsheng’ or ‘相声’ since 1800s.We benchmark various generation approaches including training-from-scratch Seq2seq, fine-tuned middle-scale PLMs, and large-scale PLMs (with and without fine-tuning). Moreover, we also conduct a human assessment, showing that 1) *large-scale pretraining largely improves crosstalk generation quality*; and 2) *even the scripts generated from the best PLM is far from what we expect*. We conclude humor generation could be largely improved using large-scaled PLMs, but it is still in its infancy. The data and benchmarking code are publicly available in [https://github.com/anonNo2/crosstalk-generation](https://github.com/anonNo2/crosstalk-generation).",
        "pdf_link": "https://aclanthology.org/2023.acl-long.419.pdf",
        "keywords": [
            "humor",
            "language",
            "humor generation",
            "natural language",
            "natural language generation",
            "language models",
            "comical crosstalk"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"even the scripts generated from the best PLM is far from what we expect\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"even the scripts generated from the best PLM is far from what we expect\""
    },
    {
        "title": "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark",
        "authors": [
            "Wenjun Peng",
            "Jingwei Yi",
            "Fangzhao Wu",
            "Shangxi Wu",
            "Bin Bin Zhu",
            "Lingjuan Lyu",
            "Binxing Jiao",
            "Tong Xu",
            "Guangzhong Sun",
            "Xing Xie"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have demonstrated powerful capabilities in both text understanding and generation. Companies have begun to offer Embedding as a Service (EaaS) based on these LLMs, which can benefit various natural language processing (NLP) tasks for customers. However, previous studies have shown that EaaS is vulnerable to model extraction attacks, which can cause significant losses for the owners of LLMs, as training these models is extremely expensive. To protect the copyright of LLMs for EaaS, we propose an Embedding Watermark method called {pasted macro ‘METHOD’} that implants backdoors on embeddings. Our method selects a group of moderate-frequency words from a general text corpus to form a trigger set, then selects a target embedding as the watermark, and inserts it into the embeddings of texts containing trigger words as the backdoor. The weight of insertion is proportional to the number of trigger words included in the text. This allows the watermark backdoor to be effectively transferred to EaaS-stealer’s model for copyright verification while minimizing the adverse impact on the original embeddings’ utility. Our extensive experiments on various datasets show that our method can effectively protect the copyright of EaaS models without compromising service quality. Our code is available at https://github.com/yjw1029/EmbMarker.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.423.pdf",
        "keywords": [
            "eaas",
            "copyright",
            "backdoor",
            "backdoor watermark",
            "watermark backdoor",
            "natural language processing",
            "embedding watermark",
            "copyright verification",
            "implants backdoors",
            "large language",
            "copying my model"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, previous studies have shown that EaaS is vulnerable to model extraction attacks, which can cause significant losses for the owners of LLMs, as training these models is extremely expensive.\"\n\nThis abstract mentions a limitation of LLMs (vulnerability to model extraction attacks) in passing, but the primary focus of the paper is on the proposed solution to protect the copyright of L",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, previous studies have shown that EaaS is vulnerable to model extraction attacks, which can cause significant losses for the owners of LLMs, as training these models is extremely expensive.\"\n\nThis abstract mentions a limitation of LLMs (vulnerability to model extraction attacks) in passing, but the primary focus of the paper is on the proposed solution to protect the copyright of L"
    },
    {
        "title": "Answering Ambiguous Questions via Iterative Prompting",
        "authors": [
            "Weiwei Sun",
            "Hengyi Cai",
            "Hongshen Chen",
            "Pengjie Ren",
            "Zhumin Chen",
            "Maarten de Rijke",
            "Zhaochun Ren"
        ],
        "published": "2023",
        "summary": "In open-domain question answering, due to the ambiguity of questions, multiple plausible answers may exist. To provide feasible answers to an ambiguous question,one approach is to directly predict all valid answers, but this can struggle with balancing relevance and diversity. An alternative is to gather candidate answers and aggregate them, but this method can be computationally costly and may neglect dependencies among answers. In this paper, we present AmbigPrompt to address the imperfections of existing approaches to answering ambiguous questions. Specifically, we integrate an answering model with a prompting model in an iterative manner. The prompting model adaptively tracks the reading process and progressively triggers the answering model to compose distinct and relevant answers. Additionally, we develop a task-specific post-pretraining approach for both the answering model and the prompting model, which greatly improves the performance of our framework. Empirical studies on two commonly-used open benchmarks show that AmbigPrompt achieves state-of-the-art or competitive results while using less memory and having a lower inference latency than competing approaches. Additionally, AmbigPrompt also performs well in low-resource settings.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.424.pdf",
        "keywords": [
            "prompting",
            "prompting model"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "A Dataset of Argumentative Dialogues on Scientific Papers",
        "authors": [
            "Federico Ruggeri",
            "Mohsen Mesgar",
            "Iryna Gurevych"
        ],
        "published": "2023",
        "summary": "With recent advances in question-answering models, various datasets have been collected to improve and study the effectiveness of these models on scientific texts. Questions and answers in these datasets explore a scientific paper by seeking factual information from the paper’s content. However, these datasets do not tackle the argumentative content of scientific papers, which is of huge importance in persuasiveness of a scientific discussion. We introduce ArgSciChat, a dataset of 41 argumentative dialogues between scientists on 20 NLP papers. The unique property of our dataset is that it includes both exploratory and argumentative questions and answers in a dialogue discourse on a scientific paper. Moreover, the size of ArgSciChat demonstrates the difficulties in collecting dialogues for specialized domains. Thus, our dataset is a challenging resource to evaluate dialogue agents in low-resource domains, in which collecting training data is costly. We annotate all sentences of dialogues in ArgSciChat and analyze them extensively. The results confirm that dialogues in ArgSciChat include exploratory and argumentative interactions. Furthermore, we use our dataset to fine-tune and evaluate a pre-trained document-grounded dialogue agent. The agent achieves a low performance on our dataset, motivating a need for dialogue agents with a capability to reason and argue about their answers. We publicly release ArgSciChat.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.425.pdf",
        "keywords": [
            "argumentative",
            "argumentative dialogues",
            "dialogues",
            "dialogue agents"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The agent achieves a low performance on our dataset, motivating a need for dialogue agents with a capability to reason and argue about their answers.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"The agent achieves a low performance on our dataset, motivating a need for dialogue agents with a capability to reason and argue about their answers.\""
    },
    {
        "title": "Massively Multilingual Lexical Specialization of Multilingual Transformers",
        "authors": [
            "Tommaso Green",
            "Simone Paolo Ponzetto",
            "Goran Glavaš"
        ],
        "published": "2023",
        "summary": "While pretrained language models (PLMs) primarily serve as general-purpose text encoders that can be fine-tuned for a wide variety of downstream tasks, recent work has shown that they can also be rewired to produce high-quality word representations (i.e., static word embeddings) and yield good performance in type-level lexical tasks. While existing work primarily focused on the lexical specialization of monolingual PLMs with immense quantities of monolingual constraints, in this work we expose massively multilingual transformers (MMTs, e.g., mBERT or XLM-R) to multilingual lexical knowledge at scale, leveraging BabelNet as the readily available rich source of multilingual and cross-lingual type-level lexical knowledge. Concretely, we use BabelNet’s multilingual synsets to create synonym pairs (or synonym-gloss pairs) across 50 languages and then subject the MMTs (mBERT and XLM-R) to a lexical specialization procedure guided by a contrastive objective. We show that such massively multilingual lexical specialization brings substantial gains in two standard cross-lingual lexical tasks, bilingual lexicon induction and cross-lingual word similarity, as well as in cross-lingual sentence retrieval. Crucially, we observe gains for languages unseen in specialization, indicating that multilingual lexical specialization enables generalization to languages with no lexical constraints. In a series of subsequent controlled experiments, we show that the number of specialization constraints plays a much greater role than the set of languages from which they originate.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.426.pdf",
        "keywords": [
            "transformers",
            "language models",
            "synonym pairs",
            "synonym gloss pairs",
            "specialization",
            "lexical specialization",
            "bilingual lexicon induction",
            "multilingual transformers",
            "word similarity"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs, only benefits and improvements are mentioned.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No evidence of discussion of limitations of LLMs, only benefits and improvements are mentioned."
    },
    {
        "title": "RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs",
        "authors": [
            "Afra Feyza Akyurek",
            "Ekin Akyurek",
            "Ashwin Kalyan",
            "Peter Clark",
            "Derry Tanti Wijaya",
            "Niket Tandon"
        ],
        "published": "2023",
        "summary": "Despite their unprecedented success, even the largest language models make mistakes. Similar to how humans learn and improve using feedback, previous work proposed providing language models with natural language feedback to guide them in repairing their outputs. Because human-generated critiques are expensive to obtain, researchers have devised learned critique generators in lieu of human critics while assuming one can train downstream models to utilize generated feedback. However, this approach does not apply to black-box or limited access models such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of large general-purpose language agents, fine-tuning is neither computationally nor spatially efficient as it results in multiple copies of the network. In this work, we introduce RL4F (Reinforcement Learning for Feedback), a multi-agent collaborative framework where the critique generator is trained to maximize end-task performance of GPT-3, a fixed model more than 200 times its size. RL4F produces critiques that help GPT-3 revise its outputs. We study three datasets for action planning, summarization and alphabetization and show relative improvements up to 10% in multiple text similarity metrics over other learned, retrieval-augmented or prompting-based critique generators.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.427.pdf",
        "keywords": [
            "feedback",
            "natural language feedback",
            "reinforcement learning",
            "action planning",
            "repairing model",
            "agent collaborative framework"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite their unprecedented success, even the largest language models make mistakes.\"; \"However, this approach does not apply to black-box or limited access models such as ChatGPT, as they cannot be fine-tuned.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite their unprecedented success, even the largest language models make mistakes.\"; \"However, this approach does not apply to black-box or limited access models such as ChatGPT, as they cannot be fine-tuned.\""
    },
    {
        "title": "DIP: Dead code Insertion based Black-box Attack for Programming Language Model",
        "authors": [
            "CheolWon Na",
            "YunSeok Choi",
            "Jee-Hyong Lee"
        ],
        "published": "2023",
        "summary": "Automatic processing of source code, such as code clone detection and software vulnerability detection, is very helpful to software engineers. Large pre-trained Programming Language (PL) models (such as CodeBERT, GraphCodeBERT, CodeT5, etc.), show very powerful performance on these tasks. However, these PL models are vulnerable to adversarial examples that are generated with slight perturbation. Unlike natural language, an adversarial example of code must be semantic-preserving and compilable. Due to the requirements, it is hard to directly apply the existing attack methods for natural language models. In this paper, we propose DIP (Dead code Insertion based Black-box Attack for Programming Language Model), a high-performance and effective black-box attack method to generate adversarial examples using dead code insertion. We evaluate our proposed method on 9 victim downstream-task large code models. Our method outperforms the state-of-the-art black-box attack in both attack efficiency and attack quality, while generated adversarial examples are compiled preserving semantic functionality.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.430.pdf",
        "keywords": [
            "dead code",
            "black box attack",
            "code clone detection",
            "code models",
            "dead code insertion",
            "software vulnerability detection",
            "natural language models",
            "programming language",
            "trained programming language"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these PL models are vulnerable to adversarial examples that are generated with slight perturbation... it is hard to directly apply the existing attack methods for natural language models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, these PL models are vulnerable to adversarial examples that are generated with slight perturbation... it is hard to directly apply the existing attack methods for natural language models.\""
    },
    {
        "title": "Query Refinement Prompts for Closed-Book Long-Form QA",
        "authors": [
            "Reinald Kim Amplayo",
            "Kellie Webster",
            "Michael Collins",
            "Dipanjan Das",
            "Shashi Narayan"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have been shown to perform well in answering questions and in producing long-form texts, both in few-shot closed-book settings. While the former can be validated using well-known evaluation metrics, the latter is difficult to evaluate. We resolve the difficulties to evaluate long-form output by doing both tasks at once – to do question answering that requires long-form answers. Such questions tend to be multifaceted, i.e., they may have ambiguities and/or require information from multiple sources. To this end, we define query refinement prompts that encourage LLMs to explicitly express the multifacetedness in questions and generate long-form answers covering multiple facets of the question. Our experiments on two long-form question answering datasets, ASQA and AQuAMuSe, show that using our prompts allows us to outperform fully finetuned models in the closed book setting, as well as achieve results comparable to retrieve-then-generate open-book models.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.444.pdf",
        "keywords": [
            "query refinement"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While the former can be validated using well-known evaluation metrics, the latter is difficult to evaluate.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While the former can be validated using well-known evaluation metrics, the latter is difficult to evaluate.\""
    },
    {
        "title": "Data Curation Alone Can Stabilize In-context Learning",
        "authors": [
            "Ting-Yun Chang",
            "Robin Jia"
        ],
        "published": "2023",
        "summary": "In-context learning (ICL) enables large language models (LLMs) to perform new tasks by prompting them with a sequence of training examples. However, it is known that ICL is very sensitive to the choice of training examples: randomly sampling examples from a training set leads to high variance in performance. In this paper, we show that carefully curating a subset of training data greatly stabilizes ICL performance without any other changes to the ICL algorithm (e.g., prompt retrieval or calibration). We introduce two methods to choose training subsets—both score training examples individually, then select the highest-scoring ones. CondAcc scores a training example by its average dev-set ICL accuracy when combined with random training examples, while Datamodels learns linear regressors that estimate how the presence of each training example influences LLM outputs. Across five tasks and two LLMs, sampling from stable subsets selected by CondAcc and Datamodels improves average accuracy over sampling from the entire training set by 7.7% and 6.3%, respectively. Surprisingly, the stable subset examples are not especially diverse in content or low in perplexity, in contrast with other work suggesting that diversity and perplexity are important when prompting LLMs.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.452.pdf",
        "keywords": [
            "data curation",
            "context learning",
            "perplexity"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it is known that ICL is very sensitive to the choice of training examples: randomly sampling examples from a training set leads to high variance in performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, it is known that ICL is very sensitive to the choice of training examples: randomly sampling examples from a training set leads to high variance in performance.\""
    },
    {
        "title": "FiD-ICL: A Fusion-in-Decoder Approach for Efficient In-Context Learning",
        "authors": [
            "Qinyuan Ye",
            "Iz Beltagy",
            "Matthew Peters",
            "Xiang Ren",
            "Hannaneh Hajishirzi"
        ],
        "published": "2023",
        "summary": "Large pre-trained models are capable of few-shot in-context learning (ICL), i.e., performing a new task by prepending a few demonstrations before the test input. However, the concatenated demonstrations are often excessively long and induce additional computation. Inspired by fusion-in-decoder (FiD) models which efficiently aggregate more passages and thus outperforms concatenation-based models in open-domain QA, we hypothesize that similar techniques can be applied to improve the efficiency and end-task performance of ICL. To verify this, we present a comprehensive study on applying three fusion methods—concatenation-based (early fusion), FiD (intermediate), and ensemble-based (late)—to ICL. We adopt a meta-learning setup where a model is first trained to perform ICL on a mixture of tasks using one selected fusion method, then evaluated on held-out tasks for ICL. Results on 11 held-out tasks show that FiD-ICL matches or outperforms the other two fusion methods. Additionally, we show that FiD-ICL (1) is 10x faster at inference time compared to concat-based and ensemble-based ICL, as we can easily pre-compute the representations of in-context examples and reuse them; (2) enables scaling up to meta-training 3B-sized models, which would fail for concat-based ICL.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.454.pdf",
        "keywords": [
            "ensemble",
            "meta learning",
            "fid",
            "fusion",
            "context learning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the concatenated demonstrations are often excessively long and induce additional computation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the concatenated demonstrations are often excessively long and induce additional computation.\""
    },
    {
        "title": "S2ynRE: Two-stage Self-training with Synthetic data for Low-resource Relation Extraction",
        "authors": [
            "Benfeng Xu",
            "Quan Wang",
            "Yajuan Lyu",
            "Dai Dai",
            "Yongdong Zhang",
            "Zhendong Mao"
        ],
        "published": "2023",
        "summary": "Current relation extraction methods suffer from the inadequacy of large-scale annotated data. While distant supervision alleviates the problem of data quantities, there still exists domain disparity in data qualities due to its reliance on domain-restrained knowledge bases. In this work, we propose S2ynRE, a framework of two-stage Self-training with Synthetic data for Relation Extraction.We first leverage the capability of large language models to adapt to the target domain and automatically synthesize large quantities of coherent, realistic training data. We then propose an accompanied two-stage self-training algorithm that iteratively and alternately learns from synthetic and golden data together. We conduct comprehensive experiments and detailed ablations on popular relation extraction datasets to demonstrate the effectiveness of the proposed framework.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.455.pdf",
        "keywords": [
            "relation extraction",
            "synthetic data",
            "self training",
            "low resource relation extraction"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While distant supervision alleviates the problem of data quantities, there still exists domain disparity in data qualities due to its reliance on domain-restrained knowledge bases.\"\n\nThis paper talks about LLMs but does not explicitly mention any limitations of the models themselves. The limitation mentioned is related to the distant supervision method, not the LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"While distant supervision alleviates the problem of data quantities, there still exists domain disparity in data qualities due to its reliance on domain-restrained knowledge bases.\"\n\nThis paper talks about LLMs but does not explicitly mention any limitations of the models themselves. The limitation mentioned is related to the distant supervision method, not the LLMs."
    },
    {
        "title": "DSEE: Dually Sparsity-embedded Efficient Tuning of Pre-trained Language Models",
        "authors": [
            "Xuxi Chen",
            "Tianlong Chen",
            "Weizhu Chen",
            "Ahmed Hassan Awadallah",
            "Zhangyang Wang",
            "Yu Cheng"
        ],
        "published": "2023",
        "summary": "Gigantic pre-trained models have become central to natural language processing (NLP), serving as the starting point for fine-tuning towards a range of downstream tasks. However, two pain points persist for this paradigm: (a) as the pre-trained models grow bigger (e.g., 175B parameters for GPT-3), even the fine-tuning process can be time-consuming and computationally expensive; (b) the fine-tuned model has the same size as its starting point by default, which is neither sensible due to its more specialized functionality, nor practical since many fine-tuned models will be deployed in resource-constrained environments. To address these pain points, we propose a framework for resource- and parameter-efficient fine-tuning by leveraging the sparsity prior in both weight updates and the final model weights. Our proposed framework, dubbed Dually Sparsity-Embedded Efficient Tuning (DSEE), aims to achieve two key objectives: (i) parameter efficient fine-tuning - by enforcing sparsity-aware low-rank updates on top of the pre-trained weights; and (ii) resource-efficient inference - by encouraging a sparse weight structure towards the final fine-tuned model. We leverage sparsity in these two directions by exploiting both unstructured and structured sparse patterns in pre-trained language models viaa unified approach. Extensive experiments and in-depth investigations, with diverse network backbones (i.e., BERT, RoBERTa, and GPT-2) on dozens of datasets, consistently demonstrate impressive parameter-/inference-efficiency, while maintaining competitive downstream performance. For instance, DSEE saves about 25% inference FLOPs while achieving comparable performance, with 0.5% trainable parameters on BERT. Codes are available at https://github.com/VITA-Group/DSEE.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.456.pdf",
        "keywords": [
            "dsee",
            "tuning",
            "fine tuned",
            "sparsity",
            "parameter efficient"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, two pain points persist for this paradigm: (a) as the pre-trained models grow bigger (e.g., 175B parameters for GPT-3), even the fine-tuning process can be time-consuming and computationally expensive; (b) the fine-tuned model has the same size as its starting point by default, which is neither sensible due to its more",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, two pain points persist for this paradigm: (a) as the pre-trained models grow bigger (e.g., 175B parameters for GPT-3), even the fine-tuning process can be time-consuming and computationally expensive; (b) the fine-tuned model has the same size as its starting point by default, which is neither sensible due to its more"
    },
    {
        "title": "READIN: A Chinese Multi-Task Benchmark with Realistic and Diverse Input Noises",
        "authors": [
            "Chenglei Si",
            "Zhengyan Zhang",
            "Yingfa Chen",
            "Xiaozhi Wang",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published": "2023",
        "summary": "For many real-world applications, the user-generated inputs usually contain various noises due to speech recognition errors caused by linguistic variations or typographical errors (typos). Thus, it is crucial to test model performance on data with realistic input noises to ensure robustness and fairness. However, little study has been done to construct such benchmarks for Chinese, where various language-specific input noises happen in the real world. In order to fill this important gap, we construct READIN: a Chinese multi-task benchmark with REalistic And Diverse Input Noises. READIN contains four diverse tasks and requests annotators to re-enter the original test data with two commonly used Chinese input methods: Pinyin input and speech input. We designed our annotation pipeline to maximize diversity, for example by instructing the annotators to use diverse input method editors (IMEs) for keyboard noises and recruiting speakers from diverse dialectical groups for speech noises. We experiment with a series of strong pretrained language models as well as robust training methods, we find that these models often suffer significant performance drops on READIN even with robustness methods like data augmentation. As the first large-scale attempt in creating a benchmark with noises geared towards user-generated inputs, we believe that READIN serves as an important complement to existing Chinese NLP benchmarks. The source code and dataset can be obtained from https://github.com/thunlp/READIN.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.460.pdf",
        "keywords": [
            "speech noises",
            "language models",
            "multi task benchmark"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We experiment with a series of strong pretrained language models as well as robust training methods, we find that these models often suffer significant performance drops on READIN even with robustness methods like data augmentation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We experiment with a series of strong pretrained language models as well as robust training methods, we find that these models often suffer significant performance drops on READIN even with robustness methods like data augmentation.\""
    },
    {
        "title": "A New Dataset and Empirical Study for Sentence Simplification in Chinese",
        "authors": [
            "Shiping Yang",
            "Renliang Sun",
            "Xiaojun Wan"
        ],
        "published": "2023",
        "summary": "Sentence Simplification is a valuable technique that can benefit language learners and children a lot. However, current research focuses more on English sentence simplification. The development of Chinese sentence simplification is relatively slow due to the lack of data. To alleviate this limitation, this paper introduces CSS, a new dataset for assessing sentence simplification in Chinese. We collect manual simplifications from human annotators and perform data analysis to show the difference between English and Chinese sentence simplifications. Furthermore, we test several unsupervised and zero/few-shot learning methods on CSS and analyze the automatic evaluation and human evaluation results. In the end, we explore whether Large Language Models can serve as high-quality Chinese sentence simplification systems by evaluating them on CSS.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.462.pdf",
        "keywords": [
            "sentence simplification",
            "chinese sentence simplification",
            "empirical study"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"whether Large Language Models can serve as high-quality Chinese sentence simplification systems by evaluating them on CSS.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"whether Large Language Models can serve as high-quality Chinese sentence simplification systems by evaluating them on CSS.\""
    },
    {
        "title": "Factual or Contextual? Disentangling Error Types in Entity Description Generation",
        "authors": [
            "Navita Goyal",
            "Ani Nenkova",
            "Hal Daumé III"
        ],
        "published": "2023",
        "summary": "In the task of entity description generation, given a context and a specified entity, a model must describe that entity correctly and in a contextually-relevant way. In this task, as well as broader language generation tasks, the generation of a nonfactual description (factual error) versus an incongruous description (contextual error) is fundamentally different, yet often conflated. We develop an evaluation paradigm that enables us to disentangle these two types of errors in naturally occurring textual contexts. We find that factuality and congruity are often at odds, and that models specifically struggle with accurate descriptions of entities that are less familiar to people. This shortcoming of language models raises concerns around the trustworthiness of such models, since factual errors on less well-known entities are exactly those that a human reader will not recognize.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.463.pdf",
        "keywords": [
            "entity description generation",
            "factuality",
            "congruity",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This shortcoming of language models raises concerns around the trustworthiness of such models, since factual errors on less well-known entities are exactly those that a human reader will not recognize.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"This shortcoming of language models raises concerns around the trustworthiness of such models, since factual errors on less well-known entities are exactly those that a human reader will not recognize.\""
    },
    {
        "title": "Learning Answer Generation using Supervision from Automatic Question Answering Evaluators",
        "authors": [
            "Matteo Gabburo",
            "Siddhant Garg",
            "Rik Koncel-Kedziorski",
            "Alessandro Moschitti"
        ],
        "published": "2023",
        "summary": "Recent studies show that sentence-level extractive QA, i.e., based on Answer Sentence Selection (AS2), is outperformed by Generation-based QA (GenQA) models, which generate answers using the top-k answer sentences ranked by AS2 models (a la retrieval-augmented generation style). In this paper, we propose a novel training paradigm for GenQA using supervision from automatic QA evaluation models (GAVA). Specifically, we propose three strategies to transfer knowledge from these QA evaluation models to a GenQA model: (i) augmenting training data with answers generated by the GenQA model and labelled by GAVA (either statically, before training, or (ii) dynamically, at every training epoch); and (iii) using the GAVA score for weighting the generator loss during the learning of the GenQA model. We evaluate our proposed methods on two academic and one industrial dataset, obtaining a significant improvement in answering accuracy over the previous state of the art.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.467.pdf",
        "keywords": [
            "supervision",
            "sentence selection",
            "question answering"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "AD-KD: Attribution-Driven Knowledge Distillation for Language Model Compression",
        "authors": [
            "Siyue Wu",
            "Hongzhan Chen",
            "Xiaojun Quan",
            "Qifan Wang",
            "Rui Wang"
        ],
        "published": "2023",
        "summary": "Knowledge distillation has attracted a great deal of interest recently to compress large language models. However, existing knowledge distillation methods suffer from two limitations. First, the student model simply imitates the teacher’s behavior while ignoring the reasoning behind it. Second, these methods usually focus on the transfer of sophisticated model-specific knowledge but overlook data-specific knowledge. In this paper, we present a novel attribution-driven knowledge distillation approach, which explores the token-level rationale behind the teacher model based on Integrated Gradients (IG) and transfers attribution knowledge to the student model. To enhance the knowledge transfer of model reasoning and generalization, we further explore multi-view attribution distillation on all potential decisions of the teacher. Comprehensive experiments are conducted with BERT on the GLUE benchmark. The experimental results demonstrate the superior performance of our approach to several state-of-the-art methods.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.471.pdf",
        "keywords": [
            "knowledge distillation",
            "attribution distillation",
            "attribution",
            "transfers attribution",
            "ad kd",
            "language model compression",
            "generalization"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing knowledge distillation methods suffer from two limitations. First, the student model simply imitates the teacher’s behavior while ignoring the reasoning behind it. Second, these methods usually focus on the transfer of sophisticated model-specific knowledge but overlook data-specific knowledge.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, existing knowledge distillation methods suffer from two limitations. First, the student model simply imitates the teacher’s behavior while ignoring the reasoning behind it. Second, these methods usually focus on the transfer of sophisticated model-specific knowledge but overlook data-specific knowledge.\""
    },
    {
        "title": "Targeted Data Generation: Finding and Fixing Model Weaknesses",
        "authors": [
            "Zexue He",
            "Marco Tulio Ribeiro",
            "Fereshte Khani"
        ],
        "published": "2023",
        "summary": "Even when aggregate accuracy is high, state-of-the-art NLP models often fail systematically on specific subgroups of data, resulting in unfair outcomes and eroding user trust. Additional data collection may not help in addressing these weaknesses, as such challenging subgroups may be unknown to users, and underrepresented in the existing and new data. We propose Targeted Data Generation (TDG), a framework that automatically identifies challenging subgroups, and generates new data for those subgroups using large language models (LLMs) with a human in the loop. TDG estimates the expected benefit and potential harm of data augmentation for each subgroup, and selects the ones most likely to improve within-group performance without hurting overall performance. In our experiments, TDG significantly improves the accuracy on challenging subgroups for state-of-the-art sentiment analysis and natural language inference models, while also improving overall test accuracy.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.474.pdf",
        "keywords": [
            "data generation",
            "targeted data generation",
            "sentiment analysis",
            "data augmentation",
            "language",
            "natural language inference"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Even when aggregate accuracy is high, state-of-the-art NLP models often fail systematically on specific subgroups of data, resulting in unfair outcomes and eroding user trust.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Even when aggregate accuracy is high, state-of-the-art NLP models often fail systematically on specific subgroups of data, resulting in unfair outcomes and eroding user trust.\""
    },
    {
        "title": "HiFi: High-Information Attention Heads Hold for Parameter-Efficient Model Adaptation",
        "authors": [
            "Anchun Gui",
            "Han Xiao"
        ],
        "published": "2023",
        "summary": "To fully leverage the advantages of large-scale pre-trained language models (PLMs) on downstream tasks, it has become a ubiquitous adaptation paradigm to fine-tune the entire parameters of PLMs. However, this paradigm poses issues of inefficient updating and resource over-consuming for fine-tuning in data-scarce and resource-limited scenarios, because of the large scale of parameters in PLMs. To alleviate these concerns, in this paper, we propose a parameter-efficient fine-tuning method HiFi, that is, only the highly informative and strongly correlated attention heads for the specific task are fine-tuned. To search for those significant attention heads, we develop a novel framework to analyze the effectiveness of heads. Specifically, we first model the relationship between heads into a graph from two perspectives of information richness and correlation, and then apply PageRank algorithm to determine the relative importance of each head. Extensive experiments on the GLUE benchmark demonstrate the effectiveness of our method, and show that HiFi obtains state-of-the-art performance over the prior baselines.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.475.pdf",
        "keywords": [
            "pre trained language models",
            "parameter efficient model adaptation",
            "information attention",
            "fine tuned",
            "parameter",
            "hifi"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, this paradigm poses issues of inefficient updating and resource over-consuming for fine-tuning in data-scarce and resource-limited scenarios, because of the large scale of parameters in PLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, this paradigm poses issues of inefficient updating and resource over-consuming for fine-tuning in data-scarce and resource-limited scenarios, because of the large scale of parameters in PLMs.\""
    },
    {
        "title": "On “Scientific Debt” in NLP: A Case for More Rigour in Language Model Pre-Training Research",
        "authors": [
            "Made Nindyatama Nityasya",
            "Haryo Wibowo",
            "Alham Fikri Aji",
            "Genta Winata",
            "Radityo Eko Prasojo",
            "Phil Blunsom",
            "Adhiguna Kuncoro"
        ],
        "published": "2023",
        "summary": "This evidence-based position paper critiques current research practices within the language model pre-training literature. Despite rapid recent progress afforded by increasingly better pre-trained language models (PLMs), current PLM research practices often conflate different possible sources of model improvement, without conducting proper ablation studies and principled comparisons between different models under comparable conditions. These practices (i) leave us ill-equipped to understand which pre-training approaches should be used under what circumstances; (ii) impede reproducibility and credit assignment; and (iii) render it difficult to understand: “How exactly does each factor contribute to the progress that we have today?” We provide a case in point by revisiting the success of BERT over its baselines, ELMo and GPT-1, and demonstrate how — under comparable conditions where the baselines are tuned to a similar extent — these baselines (and even-simpler variants thereof) can, in fact, achieve competitive or better performance than BERT. These findings demonstrate how disentangling different factors of model improvements can lead to valuable new insights. We conclude with recommendations for how to encourage and incentivize this line of work, and accelerate progress towards a better and more systematic understanding of what factors drive the progress of our foundation models today.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.477.pdf",
        "keywords": [
            "language model pre training",
            "pre trained language models",
            "reproducibility",
            "credit assignment",
            "plm research"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite rapid recent progress afforded by increasingly better pre-trained language models (PLMs), current PLM research practices often conflate different possible sources of model improvement, without conducting proper ablation studies and principled comparisons between different models under comparable conditions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Despite rapid recent progress afforded by increasingly better pre-trained language models (PLMs), current PLM research practices often conflate different possible sources of model improvement, without conducting proper ablation studies and principled comparisons between different models under comparable conditions.\""
    },
    {
        "title": "Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method",
        "authors": [
            "Yiming Wang",
            "Zhuosheng Zhang",
            "Rui Wang"
        ],
        "published": "2023",
        "summary": "Automatic summarization generates concise summaries that contain key ideas of source documents. As the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be noisy, mainly in terms of factual hallucination and information redundancy. To address this challenge, we first annotate new expert-writing Element-aware test sets following the “Lasswell Communication Model” proposed by Lasswell, allowing reference summaries to focus on more fine-grained news elements objectively and comprehensively. Utilizing the new test sets, we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs’ zero-shot summaries in prior work. Further, we propose a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with the human writing mindset. Experimental results show our method outperforms state-of-the-art fine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two datasets, respectively. Dataset and code are publicly available at https://github.com/Alsace08/SumCoT.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.482.pdf",
        "keywords": [
            "summarization",
            "automatic summarization",
            "summary chain of thought",
            "expert"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs’ zero-shot summaries in prior work.\"\n\nNote that the limitation mentioned is brief and not the primary focus of the paper, hence the rating of 2.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs’ zero-shot summaries in prior work.\"\n\nNote that the limitation mentioned is brief and not the primary focus of the paper, hence the rating of 2."
    },
    {
        "title": "bgGLUE: A Bulgarian General Language Understanding Evaluation Benchmark",
        "authors": [
            "Momchil Hardalov",
            "Pepa Atanasova",
            "Todor Mihaylov",
            "Galia Angelova",
            "Kiril Simov",
            "Petya Osenova",
            "Veselin Stoyanov",
            "Ivan Koychev",
            "Preslav Nakov",
            "Dragomir Radev"
        ],
        "published": "2023",
        "summary": "We present bgGLUE (Bulgarian General Language Understanding Evaluation), a benchmark for evaluating language models on Natural Language Understanding (NLU) tasks in Bulgarian. Our benchmark includes NLU tasks targeting a variety of NLP problems (e.g., natural language inference, fact-checking, named entity recognition, sentiment analysis, question answering, etc.) and machine learning tasks (sequence labeling, document-level classification, and regression). We run the first systematic evaluation of pre-trained language models for Bulgarian, comparing and contrasting results across the nine tasks in the benchmark. The evaluation results show strong performance on sequence labeling tasks, but there is a lot of room for improvement for tasks that require more complex reasoning. We make bgGLUE publicly available together with the fine-tuning and the evaluation code, as well as a public leaderboard at https://bgglue.github.io, and we hope that it will enable further advancements in developing NLU models for Bulgarian.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.487.pdf",
        "keywords": [
            "sequence labeling",
            "natural language",
            "natural language understanding",
            "bulgarian",
            "general language",
            "language models",
            "benchmark"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The evaluation results show strong performance on sequence labeling tasks, but there is a lot of room for improvement for tasks that require more complex reasoning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"The evaluation results show strong performance on sequence labeling tasks, but there is a lot of room for improvement for tasks that require more complex reasoning.\""
    },
    {
        "title": "What does the Failure to Reason with “Respectively” in Zero/Few-Shot Settings Tell Us about Language Models?",
        "authors": [
            "Ruixiang Cui",
            "Seolhwa Lee",
            "Daniel Hershcovich",
            "Anders Søgaard"
        ],
        "published": "2023",
        "summary": "Humans can effortlessly understand the coordinate structure of sentences such as “Niels Bohr and Kurt Cobain were born in Copenhagen and Seattle, *respectively*”. In the context of natural language inference (NLI), we examine how language models (LMs) reason with respective readings (Gawron and Kehler, 2004) from two perspectives: syntactic-semantic and commonsense-world knowledge. We propose a controlled synthetic dataset WikiResNLI and a naturally occurring dataset NatResNLI to encompass various explicit and implicit realizations of “respectively”. We show that fine-tuned NLI models struggle with understanding such readings without explicit supervision. While few-shot learning is easy in the presence of explicit cues, longer training is required when the reading is evoked implicitly, leaving models to rely on common sense inferences. Furthermore, our fine-grained analysis indicates models fail to generalize across different constructions. To conclude, we demonstrate that LMs still lag behind humans in generalizing to the long tail of linguistic constructions.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.489.pdf",
        "keywords": [
            "natural language inference",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "5",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We show that fine-tuned NLI models struggle with understanding such readings without explicit supervision...Furthermore, our fine-grained analysis indicates models fail to generalize across different constructions. To conclude, we demonstrate that LMs still lag behind humans in generalizing to the long tail of linguistic constructions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 5\nEvidence: \"We show that fine-tuned NLI models struggle with understanding such readings without explicit supervision...Furthermore, our fine-grained analysis indicates models fail to generalize across different constructions. To conclude, we demonstrate that LMs still lag behind humans in generalizing to the long tail of linguistic constructions.\""
    },
    {
        "title": "Soft Alignment Objectives for Robust Adaptation of Language Generation",
        "authors": [
            "Michal Štefánik",
            "Marek Kadlcik",
            "Petr Sojka"
        ],
        "published": "2023",
        "summary": "Domain adaptation allows generative language models to address specific flaws caused by the domain shift of their application. However, the traditional adaptation by further training on in-domain data rapidly weakens the model’s ability to generalize to other domains, making the open-ended deployments of the adapted models prone to errors. This work introduces novel training objectives built upon a semantic similarity of the predicted tokens to the reference. Our results show that (1) avoiding the common assumption of a single correct prediction by constructing the training target from tokens’ semantic similarity can largely mitigate catastrophic forgetting of adaptation, while (2) preserving the adaptation in-domain quality, (3) with negligible additions to compute costs. In the broader context, the objectives grounded in a continuous token similarity pioneer the exploration of the middle ground between the efficient but naive exact-match token-level objectives and expressive but computationally- and resource-intensive sequential objectives.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.492.pdf",
        "keywords": [
            "adaptation",
            "domain adaptation",
            "robust adaptation",
            "soft alignment objectives",
            "similarity"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the traditional adaptation by further training on in-domain data rapidly weakens the model’s ability to generalize to other domains, making the open-ended deployments of the adapted models prone to errors.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, the traditional adaptation by further training on in-domain data rapidly weakens the model’s ability to generalize to other domains, making the open-ended deployments of the adapted models prone to errors.\""
    },
    {
        "title": "The CRINGE Loss: Learning what language not to model",
        "authors": [
            "Leonard Adolphs",
            "Tianyu Gao",
            "Jing Xu",
            "Kurt Shuster",
            "Sainbayar Sukhbaatar",
            "Jason Weston"
        ],
        "published": "2023",
        "summary": "Standard language model training employs gold human documents or human-human interaction data, and treats all training data as positive examples. Growing evidence shows that even with very large amounts of positive training data, issues remain that can be alleviated with relatively small amounts of negative data – examples of what the model should not do. In this work, we propose a novel procedure to train with such data called the “CRINGE” loss (ContRastive Iterative Negative GEneration). We show the effectiveness of this approach across three different experiments on the tasks of safe generation, contradiction avoidance, and open-domain dialogue. Our models outperform multiple strong baselines and are conceptually simple, easy to train and implement.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.493.pdf",
        "keywords": [
            "standard language model",
            "contradiction avoidance"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Growing evidence shows that even with very large amounts of positive training data, issues remain that can be alleviated with relatively small amounts of negative data – examples of what the model should not do.\"\n\nThis evidence suggests that the paper mentions limitations of LLMs in passing, but it is not the primary focus of the abstract. The limitation is used to justify the proposed approach, but it",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Growing evidence shows that even with very large amounts of positive training data, issues remain that can be alleviated with relatively small amounts of negative data – examples of what the model should not do.\"\n\nThis evidence suggests that the paper mentions limitations of LLMs in passing, but it is not the primary focus of the abstract. The limitation is used to justify the proposed approach, but it"
    },
    {
        "title": "My side, your side and the evidence: Discovering aligned actor groups and the narratives they weave",
        "authors": [
            "Pavan Holur",
            "David Chong",
            "Timothy Tangherlini",
            "Vwani Roychowdhury"
        ],
        "published": "2023",
        "summary": "News reports about emerging issues often include several conflicting story lines. Individual stories can be conceptualized as samples from an underlying mixture of competing narratives. The automated identification of these distinct narratives from unstructured text is a fundamental yet difficult task in Computational Linguistics since narratives are often intertwined and only implicitly conveyed in text. In this paper, we consider a more feasible proxy task: Identify the distinct sets of aligned story actors responsible for sustaining the issue-specific narratives. Discovering aligned actors, and the groups these alignments create, brings us closer to estimating the narrative that each group represents. With the help of Large Language Models (LLM), we address this task by: (i) Introducing a corpus of text segments rich in narrative content associated with six different current issues; (ii) Introducing a novel two-step graph-based framework that (a) identifies alignments between actors (INCANT) and (b) extracts aligned actor groups using the network structure (TAMPA). Amazon Mechanical Turk evaluations demonstrate the effectiveness of our framework. Across domains, alignment relationships from INCANT are accurate (macro F1 >= 0.75) and actor groups from TAMPA are preferred over 2 non-trivial baseline models (ACC >= 0.75).",
        "pdf_link": "https://aclanthology.org/2023.acl-long.497.pdf",
        "keywords": [
            "narratives"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"With the help of Large Language Models (LLM), we address this task\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"With the help of Large Language Models (LLM), we address this task\""
    },
    {
        "title": "WebCPM: Interactive Web Search for Chinese Long-form Question Answering",
        "authors": [
            "Yujia Qin",
            "Zihan Cai",
            "Dian Jin",
            "Lan Yan",
            "Shihao Liang",
            "Kunlun Zhu",
            "Yankai Lin",
            "Xu Han",
            "Ning Ding",
            "Huadong Wang",
            "Ruobing Xie",
            "Fanchao Qi",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Jie Zhou"
        ],
        "published": "2023",
        "summary": "Long-form question answering (LFQA) aims at answering complex, open-ended questions with detailed, paragraph-length responses. The de facto paradigm of LFQA necessitates two procedures: information retrieval, which searches for relevant supporting facts, and information synthesis, which integrates these facts into a coherent answer. In this paper, we introduce WebCPM, the first Chinese LFQA dataset. One unique feature of WebCPM is that its information retrieval is based on interactive web search, which engages with a search engine in real time. Following WebGPT, we develop a web search interface. We recruit annotators to search for relevant information using our interface and then answer questions. Meanwhile, the web search behaviors of our annotators would be recorded. In total, we collect 5,500 high-quality question-answer pairs, together with 15,372 supporting facts and 125,954 web search actions. We fine-tune pre-trained language models to imitate human behaviors for web search and to generate answers based on the collected facts. Our LFQA pipeline, built on these fine-tuned models, generates answers that are no worse than human-written ones in 32.5% and 47.5% of the cases on our dataset and DuReader, respectively. The interface, dataset, and codes are publicly available at https://github.com/thunlp/WebCPM.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.499.pdf",
        "keywords": [
            "web search",
            "webcpm",
            "question answer",
            "interactive web search",
            "long form question answering"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We fine-tune pre-trained language models to imitate human behaviors for web search and to generate answers based on the collected facts.\"\n\nThis rating is given because the paper discusses LLMs, but does not mention any limitations of the models in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"We fine-tune pre-trained language models to imitate human behaviors for web search and to generate answers based on the collected facts.\"\n\nThis rating is given because the paper discusses LLMs, but does not mention any limitations of the models in the abstract."
    },
    {
        "title": "Synthesize, Prompt and Transfer: Zero-shot Conversational Question Generation with Pre-trained Language Model",
        "authors": [
            "Hongwei Zeng",
            "Bifan Wei",
            "Jun Liu",
            "Weiping Fu"
        ],
        "published": "2023",
        "summary": "Conversational question generation aims to generate questions that depend on both context and conversation history. Conventional works utilizing deep learning have shown promising results, but heavily rely on the availability of large-scale annotated conversations. In this paper, we introduce a more realistic and less explored setting, Zero-shot Conversational Question Generation (ZeroCQG), which requires no human-labeled conversations for training. To solve ZeroCQG, we propose a multi-stage knowledge transfer framework, Synthesize, Prompt, and trAnsfer with pRe-Trained lAnguage model (SPARTA) to effectively leverage knowledge from single-turn question generation instances. To validate the zero-shot performance of SPARTA, we conduct extensive experiments on three conversational datasets: CoQA, QuAC, and DoQA by transferring knowledge from three single-turn datasets: MS MARCO, NewsQA, and SQuAD. The experimental results demonstrate the superior performance of our method. Specifically, SPARTA has achieved 14.81 BLEU-4 (88.2% absolute improvement compared to T5) in CoQA with knowledge transferred from SQuAD.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.500.pdf",
        "keywords": [
            "knowledge transfer",
            "conversational question generation",
            "language model"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Conventional works utilizing deep learning have shown promising results, but heavily rely on the availability of large-scale annotated conversations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Conventional works utilizing deep learning have shown promising results, but heavily rely on the availability of large-scale annotated conversations.\""
    },
    {
        "title": "MixCE: Training Autoregressive Language Models by Mixing Forward and Reverse Cross-Entropies",
        "authors": [
            "Shiyue Zhang",
            "Shijie Wu",
            "Ozan Irsoy",
            "Steven Lu",
            "Mohit Bansal",
            "Mark Dredze",
            "David Rosenberg"
        ],
        "published": "2023",
        "summary": "Autoregressive language models are trained by minimizing the cross-entropy of the model distribution Q relative to the data distribution P – that is, minimizing the forward cross-entropy, which is equivalent to maximum likelihood estimation (MLE). We have observed that models trained in this way may “over-generalize”, in the sense that they produce non-human-like text. Moreover, we believe that reverse cross-entropy, i.e., the cross-entropy of P relative to Q, is a better reflection of how a human would evaluate text generated by a model. Hence, we propose learning with MixCE, an objective that mixes the forward and reverse cross-entropies. We evaluate models trained with this objective on synthetic data settings (where P is known) and real data, and show that the resulting models yield better generated text without complex decoding strategies.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.502.pdf",
        "keywords": [
            "cross entropy",
            "reverse cross entropies",
            "forward cross entropy",
            "autoregressive language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We have observed that models trained in this way may “over-generalize”, in the sense that they produce non-human-like text.\"\n\nThis rating is given because the paper mentions a limitation of autoregressive language models (which can be classified as LLMs) in passing, specifically that they may \"over-generalize\" and produce non-human-like text. However, this limitation is",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We have observed that models trained in this way may “over-generalize”, in the sense that they produce non-human-like text.\"\n\nThis rating is given because the paper mentions a limitation of autoregressive language models (which can be classified as LLMs) in passing, specifically that they may \"over-generalize\" and produce non-human-like text. However, this limitation is"
    },
    {
        "title": "Backpack Language Models",
        "authors": [
            "John Hewitt",
            "John Thickstun",
            "Christopher Manning",
            "Percy Liang"
        ],
        "published": "2023",
        "summary": "We present Backpacks: a new neural architecture that marries strong modeling performancewith an interface for interpretability and control. Backpacks learn multiple non-contextual sense vectors for each word in a vocabulary, and represent a word in a sequence as a context-dependent, non-negative linear combination ofsense vectors in this sequence. We find that, after training, sense vectors specialize, each encoding a different aspect of a word. We can interpret a sense vector by inspecting its (non-contextual, linear) projection onto the output space, and intervene on these interpretable hooks to change the model’s behavior in predictable ways. We train a 170M-parameter Backpack language model on OpenWebText, matching the loss of a GPT-2 small (124Mparameter) Transformer. On lexical similarity evaluations, we find that Backpack sense vectors outperform even a 6B-parameter Transformer LM’s word embeddings. Finally, we present simple algorithms that intervene on sense vectors to perform controllable text generation and debiasing. For example, we can edit the sense vocabulary to tend more towards a topic, or localize a source of gender bias to a sense vector and globally suppress that sense.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.506.pdf",
        "keywords": [
            "control",
            "encoding",
            "behavior",
            "vector",
            "example",
            "language model",
            "algorithms",
            "model",
            "sequence",
            "interface",
            "similarity",
            "generation"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations of LLMs are mentioned in the abstract, but it presents a new architecture that claims to provide interpretability and control, implying that existing LLMs may lack these aspects.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations of LLMs are mentioned in the abstract, but it presents a new architecture that claims to provide interpretability and control, implying that existing LLMs may lack these aspects."
    },
    {
        "title": "WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models",
        "authors": [
            "Virginia Felkner",
            "Ho-Chun Herbert Chang",
            "Eugene Jang",
            "Jonathan May"
        ],
        "published": "2023",
        "summary": "We present WinoQueer: a benchmark specifically designed to measure whether large language models (LLMs) encode biases that are harmful to the LGBTQ+ community. The benchmark is community-sourced, via application of a novel method that generates a bias benchmark from a community survey. We apply our benchmark to several popular LLMs and find that off-the-shelf models generally do exhibit considerable anti-queer bias. Finally, we show that LLM bias against a marginalized community can be somewhat mitigated by finetuning on data written about or by members of that community, and that social media text written by community members is more effective than news text written about the community by non-members. Our method for community-in-the-loop benchmark development provides a blueprint for future researchers to develop community-driven, harms-grounded LLM benchmarks for other marginalized communities.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.507.pdf",
        "keywords": [
            "bias",
            "language models",
            "social media",
            "anti queer bias",
            "bias benchmark",
            "loop benchmark",
            "community",
            "shelf models"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We apply our benchmark to several popular LLMs and find that off-the-shelf models generally do exhibit considerable anti-queer bias.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We apply our benchmark to several popular LLMs and find that off-the-shelf models generally do exhibit considerable anti-queer bias.\""
    },
    {
        "title": "Preserving Commonsense Knowledge from Pre-trained Language Models via Causal Inference",
        "authors": [
            "Junhao Zheng",
            "Qianli Ma",
            "Shengjie Qiu",
            "Yue Wu",
            "Peitian Ma",
            "Junlong Liu",
            "Huawen Feng",
            "Xichen Shang",
            "Haibin Chen"
        ],
        "published": "2023",
        "summary": "Fine-tuning has been proven to be a simple and effective technique to transfer the learned knowledge of Pre-trained Language Models (PLMs) to downstream tasks. However, vanilla fine-tuning easily overfits the target data and degrades the generalization ability. Most existing studies attribute it to catastrophic forgetting, and they retain the pre-trained knowledge indiscriminately without identifying what knowledge is transferable. Motivated by this, we frame fine-tuning into a causal graph and discover that the crux of catastrophic forgetting lies in the missing causal effects from the pre-trained data. Based on the causal view, we propose a unified objective for fine-tuning to retrieve the causality back. Intriguingly, the unified objective can be seen as the sum of the vanilla fine-tuning objective, which learns new knowledge from target data, and the causal objective, which preserves old knowledge from PLMs. Therefore, our method is flexible and can mitigate negative transfer while preserving knowledge. Since endowing models with commonsense is a long-standing challenge, we implement our method on commonsense QA with a proposed heuristic estimation to verify its effectiveness. In the experiments, our method outperforms state-of-the-art fine-tuning methods on all six commonsense QA datasets and can be implemented as a plug-in module to inflate the performance of existing QA models.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.509.pdf",
        "keywords": [
            "commonsense",
            "commonsense knowledge",
            "causal inference",
            "fine tuning",
            "trained language models",
            "pre trained language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, vanilla fine-tuning easily overfits the target data and degrades the generalization ability. Most existing studies attribute it to catastrophic forgetting, and they retain the pre-trained knowledge indiscriminately without identifying what knowledge is transferable.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, vanilla fine-tuning easily overfits the target data and degrades the generalization ability. Most existing studies attribute it to catastrophic forgetting, and they retain the pre-trained knowledge indiscriminately without identifying what knowledge is transferable.\""
    },
    {
        "title": "Benchmarking Large Language Model Capabilities for Conditional Generation",
        "authors": [
            "Joshua Maynez",
            "Priyanka Agrawal",
            "Sebastian Gehrmann"
        ],
        "published": "2023",
        "summary": "Pre-trained large language models (PLMs) underly most new developments in natural language processing. They have shifted the field from application-specific model pipelines to a single model that is adapted to a wide range of tasks. Autoregressive PLMs like GPT-3 or PaLM and associated techniques like fewshot learning, have additionally shifted the output modality to generation instead of classification or regression. Despite their ubiquitous use, the generation quality of language models is rarely evaluated when these models are introduced. Additionally, it is unclear how existing generation tasks–while they can be used to compare systems at a high level–relate to the real world use cases for which people have been adopting them. In this work, we discuss how to adapt existing application-specific generation benchmarks to PLMs and provide an in-depth, empirical study of the limitations and capabilities of PLMs in natural language generation tasks along dimensions such as scale, architecture, input and output language. Our results show that PLMs differ in their applicability to different data regimes and their generalization to multiple languages. They further inform practitioners as to which PLMs to use for a given generation task setup. We share best practices to be taken into consideration when benchmarking generation capabilities during the development of upcoming PLMs.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.511.pdf",
        "keywords": [
            "benchmarking",
            "conditional generation",
            "application specific generation benchmarks",
            "language model capabilities",
            "large language models",
            "empirical study"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite their ubiquitous use, the generation quality of language models is rarely evaluated when these models are introduced. Additionally, it is unclear how existing generation tasks–while they can be used to compare systems at a high level–relate to the real world use cases for which people have been adopting them.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Despite their ubiquitous use, the generation quality of language models is rarely evaluated when these models are introduced. Additionally, it is unclear how existing generation tasks–while they can be used to compare systems at a high level–relate to the real world use cases for which people have been adopting them.\""
    },
    {
        "title": "lilGym: Natural Language Visual Reasoning with Reinforcement Learning",
        "authors": [
            "Anne Wu",
            "Kiante Brantley",
            "Noriyuki Kojima",
            "Yoav Artzi"
        ],
        "published": "2023",
        "summary": "We present lilGym, a new benchmark for language-conditioned reinforcement learning in visual environments. lilGym is based on 2,661 highly-compositional human-written natural language statements grounded in an interactive visual environment. We introduce a new approach for exact reward computation in every possible world state by annotating all statements with executable Python programs. Each statement is paired with multiple start states and reward functions to form thousands of distinct Markov Decision Processes of varying difficulty. We experiment with lilGym with different models and learning regimes. Our results and analysis show that while existing methods are able to achieve non-trivial performance, lilGym forms a challenging open problem. lilGym is available at https://lil.nlp.cornell.edu/lilgym/.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.512.pdf",
        "keywords": [
            "reinforcement learning",
            "natural language visual reasoning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our results and analysis show that while existing methods are able to achieve non-trivial performance, lilGym forms a challenging open problem.\"\n\nThis rating is given because the abstract mentions a limitation of existing methods (including LLMs) in achieving good performance on the lilGym benchmark, but it is a minor detail and not the primary focus of the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Our results and analysis show that while existing methods are able to achieve non-trivial performance, lilGym forms a challenging open problem.\"\n\nThis rating is given because the abstract mentions a limitation of existing methods (including LLMs) in achieving good performance on the lilGym benchmark, but it is a minor detail and not the primary focus of the abstract."
    },
    {
        "title": "Causality-aware Concept Extraction based on Knowledge-guided Prompting",
        "authors": [
            "Siyu Yuan",
            "Deqing Yang",
            "Jinxi Liu",
            "Shuyu Tian",
            "Jiaqing Liang",
            "Yanghua Xiao",
            "Rui Xie"
        ],
        "published": "2023",
        "summary": "Concepts benefit natural language understanding but are far from complete in existing knowledge graphs (KGs). Recently, pre-trained language models (PLMs) have been widely used in text-based concept extraction (CE). However, PLMs tend to mine the co-occurrence associations from massive corpus as pre-trained knowledge rather than the real causal effect between tokens. As a result, the pre-trained knowledge confounds PLMs to extract biased concepts based on spurious co-occurrence correlations, inevitably resulting in low precision. In this paper, through the lens of a Structural Causal Model (SCM), we propose equipping the PLM-based extractor with a knowledge-guided prompt as an intervention to alleviate concept bias. The prompt adopts the topic of the given entity from the existing knowledge in KGs to mitigate the spurious co-occurrence correlations between entities and biased concepts. Our extensive experiments on representative multilingual KG datasets justify that our proposed prompt can effectively alleviate concept bias and improve the performance of PLM-based CE models.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.514.pdf",
        "keywords": [
            "concept extraction",
            "structural causal model",
            "text based concept extraction",
            "knowledge guided prompting",
            "knowledge graphs",
            "trained language models",
            "spurious co occurrence correlations"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, PLMs tend to mine the co-occurrence associations from massive corpus as pre-trained knowledge rather than the real causal effect between tokens. As a result, the pre-trained knowledge confounds PLMs to extract biased concepts based on spurious co-occurrence correlations, inevitably resulting in low precision.\"\n\nThis evidence shows that the paper discusses the limitations of pre-trained language models (",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, PLMs tend to mine the co-occurrence associations from massive corpus as pre-trained knowledge rather than the real causal effect between tokens. As a result, the pre-trained knowledge confounds PLMs to extract biased concepts based on spurious co-occurrence correlations, inevitably resulting in low precision.\"\n\nThis evidence shows that the paper discusses the limitations of pre-trained language models ("
    },
    {
        "title": "Limitations of Language Models in Arithmetic and Symbolic Induction",
        "authors": [
            "Jing Qian",
            "Hong Wang",
            "Zekun Li",
            "Shiyang Li",
            "Xifeng Yan"
        ],
        "published": "2023",
        "summary": "Recent work has shown that large pretrained Language Models (LMs) can not only perform remarkably well on a range of Natural Language Processing (NLP) tasks but also start improving on reasoning tasks such as arithmetic induction, symbolic manipulation, and commonsense reasoning with increasing size of models. However, it is still unclear what the underlying capabilities of these LMs are. Surprisingly, we find that these models have limitations on certain basic symbolic manipulation tasks such as copy, reverse, and addition. When the total number of symbols or repeating symbols increases, the model performance drops quickly. We investigate the potential causes behind this phenomenon and examine a set of possible methods, including explicit positional markers, fine-grained computation steps, and LMs with callable programs. Experimental results show that none of these techniques can solve the simplest addition induction problem completely. In the end, we introduce LMs with tutor, which demonstrates every single step of teaching. LMs with tutor is able to deliver 100% accuracy in situations of OOD and repeating symbols, shedding new insights on the boundary of large LMs in induction.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.516.pdf",
        "keywords": [
            "language models",
            "arithmetic induction",
            "addition induction",
            "symbolic induction",
            "natural language processing"
        ],
        "Rate_Llama-3.1-70b": "5",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it is still unclear what the underlying capabilities of these LMs are. Surprisingly, we find that these models have limitations on certain basic symbolic manipulation tasks such as copy, reverse, and addition. When the total number of symbols or repeating symbols increases, the model performance drops quickly.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 5\nEvidence: \"However, it is still unclear what the underlying capabilities of these LMs are. Surprisingly, we find that these models have limitations on certain basic symbolic manipulation tasks such as copy, reverse, and addition. When the total number of symbols or repeating symbols increases, the model performance drops quickly.\""
    },
    {
        "title": "EEL: Efficiently Encoding Lattices for Reranking",
        "authors": [
            "Prasann Singhal",
            "Jiacheng Xu",
            "Xi Ye",
            "Greg Durrett"
        ],
        "published": "2023",
        "summary": "Standard decoding approaches for conditional text generation tasks typically search for an output hypothesis with high model probability, but this may not yield the best hypothesis according to human judgments of quality. Reranking to optimize for “downstream” metrics can more closely optimize for quality, but many metrics of interest are computed with pre-trained language models, which are slow to apply to large numbers of hypotheses. We explore an approach for reranking hypotheses by using Transformers to efficiently encode lattices of generated outputs, a method we call EEL. With a single Transformer pass over the entire lattice, we can approximately compute a contextualized representation of each token as if it were only part of a single hypothesis in isolation. We combine this approach with a new class of token-factored rerankers (TFRs) that allow for efficient extraction of high reranker-scoring hypotheses from the lattice. Empirically, our approach incurs minimal degradation error compared to the exponentially slower approach of encoding each hypothesis individually. When applying EEL with TFRs across three text generation tasks, our results show both substantial speedup compared to naive reranking and often better performance on downstream metrics than comparable approaches.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.517.pdf",
        "keywords": [
            "reranking",
            "encoding each",
            "efficiently encoding lattices",
            "naive reranking",
            "contextualized"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Learning Dynamic Contextualised Word Embeddings via Template-based Temporal Adaptation",
        "authors": [
            "Xiaohang Tang",
            "Yi Zhou",
            "Danushka Bollegala"
        ],
        "published": "2023",
        "summary": "Dynamic contextualised word embeddings (DCWEs) represent the temporal semantic variations of words. We propose a method for learning DCWEs by time-adapting a pretrained Masked Language Model (MLM) using time-sensitive templates. Given two snapshots C1 and C2 of a corpus taken respectively at two distinct timestamps T1 and T2, we first propose an unsupervised method to select (a) pivot terms related to both C1 and C2, and (b) anchor terms that are associated with a specific pivot term in each individual snapshot.We then generate prompts by filling manually compiled templates using the extracted pivot and anchor terms.Moreover, we propose an automatic method to learn time-sensitive templates from C1 and C2, without requiring any human supervision.Next, we use the generated prompts to adapt a pretrained MLM to T2 by fine-tuning using those prompts.Multiple experiments show that our proposed method significantly reduces the perplexity of test sentences in C2, outperforming the current state-of-the-art.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.520.pdf",
        "keywords": [
            "word embeddings",
            "temporal adaptation",
            "contextualised word embeddings"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations of LLMs, but the paper proposes a method to adapt a pretrained MLM, implying that the model may not perform well without adaptation, and the goal is to \"reduce the perplexity of test sentences in C2\", suggesting that the model may struggle with understanding temporal semantic variations of words.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: No explicit mention of limitations of LLMs, but the paper proposes a method to adapt a pretrained MLM, implying that the model may not perform well without adaptation, and the goal is to \"reduce the perplexity of test sentences in C2\", suggesting that the model may struggle with understanding temporal semantic variations of words."
    },
    {
        "title": "GanLM: Encoder-Decoder Pre-training with an Auxiliary Discriminator",
        "authors": [
            "Jian Yang",
            "Shuming Ma",
            "Li Dong",
            "Shaohan Huang",
            "Haoyang Huang",
            "Yuwei Yin",
            "Dongdong Zhang",
            "Liqun Yang",
            "Furu Wei",
            "Zhoujun Li"
        ],
        "published": "2023",
        "summary": "Pre-trained models have achieved remarkable success in natural language processing (NLP). However, existing pre-training methods underutilize the benefits of language understanding for generation. Inspired by the idea of Generative Adversarial Networks (GANs), we propose a GAN-style model for encoder-decoder pre-training by introducing an auxiliary discriminator, unifying the ability of language understanding and generation in a single model. Our model, named as GanLM, is trained with two pre-training objectives: replaced token detection and replaced token denoising. Specifically, given masked source sentences, the generator outputs the target distribution and the discriminator predicts whether the target sampled tokens from distribution are incorrect. The target sentence is replaced with misclassified tokens to construct noisy previous context, which is used to generate the gold sentence. In general, both tasks improve the ability of language understanding and generation by selectively using the denoising data. Extensive experiments in language generation benchmarks show that GanLM with the powerful language understanding capability outperforms various strong pre-trained language models (PLMs) and achieves state-of-the-art performance.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.522.pdf",
        "keywords": [
            "encoder",
            "discriminator",
            "token denoising",
            "encoder decoder pre training"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing pre-training methods underutilize the benefits of language understanding for generation.\"\n\nThis paper discusses LLMs but does not explicitly mention any limitations of LLMs. The sentence provided as evidence is more of a motivation for the proposed method rather than a discussion of LLM limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, existing pre-training methods underutilize the benefits of language understanding for generation.\"\n\nThis paper discusses LLMs but does not explicitly mention any limitations of LLMs. The sentence provided as evidence is more of a motivation for the proposed method rather than a discussion of LLM limitations."
    },
    {
        "title": "Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM’s Translation Capability",
        "authors": [
            "Eleftheria Briakou",
            "Colin Cherry",
            "George Foster"
        ],
        "published": "2023",
        "summary": "Large, multilingual language models exhibit surprisingly good zero- or few-shot machine translation capabilities, despite having never seen the intentionally-included translation examples provided to typical neural translation systems. We investigate the role of incidental bilingualism—the unintentional consumption of bilingual signals, including translation examples—in explaining the translation capabilities of large language models, taking the Pathways Language Model (PaLM) as a case study. We introduce a mixed-method approach to measure and understand incidental bilingualism at scale. We show that PaLM is exposed to over 30 million translation pairs across at least 44 languages. Furthermore, the amount of incidental bilingual content is highly correlated with the amount of monolingual in-language content for non-English languages. We relate incidental bilingual content to zero-shot prompts and show that it can be used to mine new prompts to improve PaLM’s out-of-English zero-shot translation quality. Finally, in a series of small-scale ablations, we show that its presence has a substantial impact on translation capabilities, although this impact diminishes with model scale.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.524.pdf",
        "keywords": [
            "haystack",
            "bilingual signals",
            "translation",
            "translation capabilities",
            "languages",
            "language models",
            "incidental bilingualism"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"although this impact diminishes with model scale.\"\n\nThis paper mentions a limitation of LLMs, specifically that the impact of incidental bilingual content on translation capabilities diminishes with model scale, but it is not a major focus of the paper and is only briefly mentioned.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"although this impact diminishes with model scale.\"\n\nThis paper mentions a limitation of LLMs, specifically that the impact of incidental bilingual content on translation capabilities diminishes with model scale, but it is not a major focus of the paper and is only briefly mentioned."
    },
    {
        "title": "Learning to Imagine: Visually-Augmented Natural Language Generation",
        "authors": [
            "Tianyi Tang",
            "Yushuo Chen",
            "Yifan Du",
            "Junyi Li",
            "Wayne Xin Zhao",
            "Ji-Rong Wen"
        ],
        "published": "2023",
        "summary": "People often imagine relevant scenes to aid in the writing process. In this work, we aim to utilize visual information for composition in the same manner as humans. We propose a method, LIVE, that makes pre-trained language models (PLMs) Learn to Imagine for Visually-augmented natural language gEneration. First, we imagine the scene based on the text: we use a diffusion model to synthesize high-quality images conditioned on the input texts. Second, we use CLIP to determine whether the text can evoke the imagination in a posterior way. Finally, our imagination is dynamic, and we conduct synthesis for each sentence rather than generate only one image for an entire paragraph. Technically, we propose a novel plug-and-play fusion layer to obtain visually-augmented representations for each text. Our vision-text fusion layer is compatible with Transformer-based architecture. We have conducted extensive experiments on four generation tasks using BART and T5, and the automatic results and human evaluation demonstrate the effectiveness of our proposed method. We will release the code, model, and data at the link: https://github.com/RUCAIBox/LIVE.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.526.pdf",
        "keywords": [
            "augmented natural language generation",
            "pre trained language models"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations of LLMs are mentioned in the abstract, but it does mention \"pre-trained language models (PLMs)\" which is related to LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations of LLMs are mentioned in the abstract, but it does mention \"pre-trained language models (PLMs)\" which is related to LLMs."
    },
    {
        "title": "I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation",
        "authors": [
            "Chandra Bhagavatula",
            "Jena D. Hwang",
            "Doug Downey",
            "Ronan Le Bras",
            "Ximing Lu",
            "Lianhui Qin",
            "Keisuke Sakaguchi",
            "Swabha Swayamdipta",
            "Peter West",
            "Yejin Choi"
        ],
        "published": "2023",
        "summary": "Commonsense capabilities of pre-trained language models dramatically improve with scale, leading many to believe that scale is the only winning recipe. But is it? Here, we investigate an alternative that a priori seems impossible: can smaller language models (e.g., GPT-2) win over models that are orders of magnitude larger and better (e.g., GPT-3), if powered with novel commonsense distillation algorithms?The key intellectual challenge is to design a learning algorithm that achieve a competitive level of commonsense acquisition, without relying on the benefits of scale. In particular, we study generative models of commonsense knowledge, focusing on the task of generating generics, statements of commonsense facts about everyday concepts, e.g., birds can fly. We introduce I2D2, a novel commonsense distillation framework that loosely follows the Symbolic Knowledge Distillation of West et al. but breaks the dependence on the extreme-scale teacher model with two innovations: (1) the novel adaptation of NeuroLogic Decoding to enhance the generation quality of the weak, off-the-shelf language models, and (2) self-imitation learning to iteratively learn from the model’s own enhanced commonsense acquisition capabilities. Empirical results suggest that scale is not the only way, as novel algorithms can be a promising alternative. Moreover, our study leads to a new corpus of generics, Gen-A-tomic, that is the largest and highest quality available to date.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.535.pdf",
        "keywords": [
            "commonsense",
            "commonsense knowledge",
            "self imitation",
            "commonsense distillation",
            "symbolic knowledge distillation",
            "inductive knowledge distillation",
            "scale",
            "a priori",
            "language"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"But is it?\" and \"the key intellectual challenge is to design a learning algorithm that achieve a competitive level of commonsense acquisition, without relying on the benefits of scale.\"\n\n(Note: Although the paper does not directly mention limitations of LLMs, it implies that large language models may not be the only solution and that scale may not be the only winning recipe, which indirectly points to",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"But is it?\" and \"the key intellectual challenge is to design a learning algorithm that achieve a competitive level of commonsense acquisition, without relying on the benefits of scale.\"\n\n(Note: Although the paper does not directly mention limitations of LLMs, it implies that large language models may not be the only solution and that scale may not be the only winning recipe, which indirectly points to"
    },
    {
        "title": "DecompEval: Evaluating Generated Texts as Unsupervised Decomposed Question Answering",
        "authors": [
            "Pei Ke",
            "Fei Huang",
            "Fei Mi",
            "Yasheng Wang",
            "Qun Liu",
            "Xiaoyan Zhu",
            "Minlie Huang"
        ],
        "published": "2023",
        "summary": "Existing evaluation metrics for natural language generation (NLG) tasks face the challenges on generalization ability and interpretability. Specifically, most of the well-performed metrics are required to train on evaluation datasets of specific NLG tasks and evaluation dimensions, which may cause over-fitting to task-specific datasets. Furthermore, existing metrics only provide an evaluation score for each dimension without revealing the evidence to interpret how this score is obtained. To deal with these challenges, we propose a simple yet effective metric called DecompEval. This metric formulates NLG evaluation as an instruction-style question answering task and utilizes instruction-tuned pre-trained language models (PLMs) without training on evaluation datasets, aiming to enhance the generalization ability. To make the evaluation process more interpretable, we decompose our devised instruction-style question about the quality of generated texts into the subquestions that measure the quality of each sentence. The subquestions with their answers generated by PLMs are then recomposed as evidence to obtain the evaluation result. Experimental results show that DecompEval achieves state-of-the-art performance in untrained metrics for evaluating text summarization and dialogue generation, which also exhibits strong dimension-level / task-level generalization ability and interpretability.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.539.pdf",
        "keywords": [
            "decompeval",
            "language models",
            "generalization",
            "question answering",
            "natural language generation",
            "generated texts",
            "generalization ability",
            "summarization"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing evaluation metrics for natural language generation (NLG) tasks face the challenges on generalization ability and interpretability.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing evaluation metrics for natural language generation (NLG) tasks face the challenges on generalization ability and interpretability.\""
    },
    {
        "title": "A Measure-Theoretic Characterization of Tight Language Models",
        "authors": [
            "Li Du",
            "Lucas Torroba Hennigen",
            "Tiago Pimentel",
            "Clara Meister",
            "Jason Eisner",
            "Ryan Cotterell"
        ],
        "published": "2023",
        "summary": "Language modeling, a central task in natural language processing, involves estimating a probability distribution over strings. In most cases, the estimated distribution sums to 1 over all finite strings. However, in some pathological cases, probability mass can “leak” onto the set of infinite sequences. In order to characterize the notion of leakage more precisely, this paper offers a measure-theoretic treatment of language modeling. We prove that many popular language model families are in fact tight, meaning that they will not leak in this sense. We also generalize characterizations of tightness proposed in previous works.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.543.pdf",
        "keywords": [
            "language models",
            "language model families",
            "natural language processing",
            "probability distribution"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, in some pathological cases, probability mass can “leak” onto the set of infinite sequences.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, in some pathological cases, probability mass can “leak” onto the set of infinite sequences.\""
    },
    {
        "title": "PromptRank: Unsupervised Keyphrase Extraction Using Prompt",
        "authors": [
            "Aobo Kong",
            "Shiwan Zhao",
            "Hao Chen",
            "Qicheng Li",
            "Yong Qin",
            "Ruiqi Sun",
            "Xiaoyan Bai"
        ],
        "published": "2023",
        "summary": "The keyphrase extraction task refers to the automatic selection of phrases from a given document to summarize its core content. State-of-the-art (SOTA) performance has recently been achieved by embedding-based algorithms, which rank candidates according to how similar their embeddings are to document embeddings. However, such solutions either struggle with the document and candidate length discrepancies or fail to fully utilize the pre-trained language model (PLM) without further fine-tuning. To this end, in this paper, we propose a simple yet effective unsupervised approach, PromptRank, based on the PLM with an encoder-decoder architecture. Specifically, PromptRank feeds the document into the encoder and calculates the probability of generating the candidate with a designed prompt by the decoder. We extensively evaluate the proposed PromptRank on six widely used benchmarks. PromptRank outperforms the SOTA approach MDERank, improving the F1 score relatively by 34.18%, 24.87%, and 17.57% for 5, 10, and 15 returned results, respectively. This demonstrates the great potential of using prompt for unsupervised keyphrase extraction. We release our code at https://github.com/HLT-NLP/PromptRank.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.545.pdf",
        "keywords": [
            "keyphrase extraction"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, such solutions either struggle with the document and candidate length discrepancies or fail to fully utilize the pre-trained language model (PLM) without further fine-tuning.\"\n\nThis evidence is brief and does not elaborate on the limitations of LLMs, it only mentions them as a motivation for the proposed approach.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, such solutions either struggle with the document and candidate length discrepancies or fail to fully utilize the pre-trained language model (PLM) without further fine-tuning.\"\n\nThis evidence is brief and does not elaborate on the limitations of LLMs, it only mentions them as a motivation for the proposed approach."
    },
    {
        "title": "When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories",
        "authors": [
            "Alex Mallen",
            "Akari Asai",
            "Victor Zhong",
            "Rajarshi Das",
            "Daniel Khashabi",
            "Hannaneh Hajishirzi"
        ],
        "published": "2023",
        "summary": "Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the difficulty of encoding a wealth of world knowledge in their parameters. This paper aims to understand LMs’ strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments on two open-domain entity-centric QA datasets: PopQA, our new dataset with 14k questions about long-tail entities, and EntityQuestions, a widely used open-domain QA dataset. We find that LMs struggle with less popular factual knowledge, and that retrieval augmentation helps significantly in these cases. Scaling, on the other hand, mainly improves memorization of popular knowledge, and fails to appreciably improve memorization of factual knowledge in the tail. Based on those findings, we devise a new method for retrieval-augmentation that improves performance and reduces inference costs by only retrieving non-parametric memories when necessary.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.546.pdf",
        "keywords": [
            "factual knowledge",
            "language models",
            "trust language models"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the difficulty of encoding a wealth of world knowledge in their parameters.\"; \"We find that LMs struggle with less popular factual knowledge, and that retrieval augmentation helps significantly in these cases.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the difficulty of encoding a wealth of world knowledge in their parameters.\"; \"We find that LMs struggle with less popular factual knowledge, and that retrieval augmentation helps significantly in these cases.\""
    },
    {
        "title": "SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models",
        "authors": [
            "Akshita Jha",
            "Aida Mostafazadeh Davani",
            "Chandan K Reddy",
            "Shachi Dave",
            "Vinodkumar Prabhakaran",
            "Sunipa Dev"
        ],
        "published": "2023",
        "summary": "Stereotype benchmark datasets are crucial to detect and mitigate social stereotypes about groups of people in NLP models. However, existing datasets are limited in size and coverage, and are largely restricted to stereotypes prevalent in the Western society. This is especially problematic as language technologies gain hold across the globe. To address this gap, we present SeeGULL, a broad-coverage stereotype dataset, built by utilizing generative capabilities of large language models such as PaLM, and GPT-3, and leveraging a globally diverse rater pool to validate the prevalence of those stereotypes in society. SeeGULL is in English, and contains stereotypes about identity groups spanning 178 countries across 8 different geo-political regions across 6 continents, as well as state-level identities within the US and India. We also include fine-grained offensiveness scores for different stereotypes and demonstrate their global disparities. Furthermore, we include comparative annotations about the same groups by annotators living in the region vs. those that are based in North America, and demonstrate that within-region stereotypes about groups differ from those prevalent in North America.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.548.pdf",
        "keywords": [
            "stereotype benchmark",
            "stereotype benchmark datasets",
            "stereotype dataset",
            "generative models",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing datasets are limited in size and coverage, and are largely restricted to stereotypes prevalent in the Western society.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, existing datasets are limited in size and coverage, and are largely restricted to stereotypes prevalent in the Western society.\""
    },
    {
        "title": "Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge",
        "authors": [
            "Jiangjie Chen",
            "Wei Shi",
            "Ziquan Fu",
            "Sijie Cheng",
            "Lei Li",
            "Yanghua Xiao"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have been widely studied for their ability to store and utilize positive knowledge. However, negative knowledge, such as “lions don’t live in the ocean”, is also ubiquitous in the world but rarely mentioned explicitly in text. What do LLMs know about negative knowledge?This work examines the ability of LLMs on negative commonsense knowledge. We design a constrained keywords-to-sentence generation task (CG) and a Boolean question answering task (QA) to probe LLMs.Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions. We term this phenomenon the belief conflict of LLMs.Our further analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.550.pdf",
        "keywords": [
            "commonsense knowledge",
            "negative commonsense knowledge",
            "negative knowledge",
            "language modeling",
            "negation",
            "positive knowledge",
            "large language models",
            "belief conflict"
        ],
        "Rate_Llama-3.1-70b": "5",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions. We term this phenomenon the belief conflict of LLMs.Our further analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 5\nEvidence: \"Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions. We term this phenomenon the belief conflict of LLMs.Our further analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.\""
    },
    {
        "title": "An Inner Table Retriever for Robust Table Question Answering",
        "authors": [
            "Weizhe Lin",
            "Rexhina Blloshmi",
            "Bill Byrne",
            "Adria de Gispert",
            "Gonzalo Iglesias"
        ],
        "published": "2023",
        "summary": "Recent years have witnessed the thriving of pretrained Transformer-based language models for understanding semi-structured tables, with several applications, such as Table Question Answering (TableQA).These models are typically trained on joint tables and surrounding natural language text, by linearizing table content into sequences comprising special tokens and cell information. This yields very long sequences which increase system inefficiency, and moreover, simply truncating long sequences results in information loss for downstream tasks. We propose Inner Table Retriever (ITR), a general-purpose approach for handling long tables in TableQA that extracts sub-tables to preserve the most relevant information for a question. We show that ITR can be easily integrated into existing systems to improve their accuracy with up to 1.3-4.8% and achieve state-of-the-art results in two benchmarks, i.e., 63.4% in WikiTableQuestions and 92.1% in WikiSQL. Additionally, we show that ITR makes TableQA systems more robust to reduced model capacity and to different ordering of columns and rows. We make our code available at: https://github.com/amazon-science/robust-tableqa.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.551.pdf",
        "keywords": [
            "inner table retriever",
            "table question answering"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This yields very long sequences which increase system inefficiency, and moreover, simply truncating long sequences results in information loss for downstream tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"This yields very long sequences which increase system inefficiency, and moreover, simply truncating long sequences results in information loss for downstream tasks.\""
    },
    {
        "title": "SimOAP: Improve Coherence and Consistency in Persona-based Dialogue Generation via Over-sampling and Post-evaluation",
        "authors": [
            "Junkai Zhou",
            "Liang Pang",
            "Huawei Shen",
            "Xueqi Cheng"
        ],
        "published": "2023",
        "summary": "Language models trained on large-scale corpora can generate remarkably fluent results in open-domain dialogue. However, for the persona-based dialogue generation task, consistency and coherence are also key factors, which are great challenges for language models. Existing works mainly focus on valuable data filtering, model structure modifying, or objective function designing, while their improvements are limited and hard to generalize to all types of pre-trained language models. However, we find that language models can produce consistent and coherent responses if we consider enough generations. Thus, the problems lay in large-scale response generation and target response selection. In this work, a simple but effective two-stage SimOAP strategy is proposed, i.e., over-sampling and post-evaluation. The over-sampling stage takes large-scale responses from existing trained models efficiently via off-the-shelf distilling and compressing methods, and the post-evaluation stage selects a good response based on multiple well-designed evaluation metrics from large-scale candidates. Experimental results show that the proposed plug-in SimOAP strategy improves the backbone models and outperforms the baseline strategies in both automatic and human evaluations.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.553.pdf",
        "keywords": [
            "language",
            "language models",
            "post evaluation",
            "sampling",
            "over sampling",
            "consistency",
            "dialogue",
            "coherence"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, for the persona-based dialogue generation task, consistency and coherence are also key factors, which are great challenges for language models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, for the persona-based dialogue generation task, consistency and coherence are also key factors, which are great challenges for language models.\""
    },
    {
        "title": "Cognitive Reframing of Negative Thoughts through Human-Language Model Interaction",
        "authors": [
            "Ashish Sharma",
            "Kevin Rushton",
            "Inna Lin",
            "David Wadden",
            "Khendra Lucas",
            "Adam Miner",
            "Theresa Nguyen",
            "Tim Althoff"
        ],
        "published": "2023",
        "summary": "A proven therapeutic technique to overcome negative thoughts is to replace them with a more hopeful “reframed thought.” Although therapy can help people practice and learn this Cognitive Reframing of Negative Thoughts, clinician shortages and mental health stigma commonly limit people’s access to therapy. In this paper, we conduct a human-centered study of how language models may assist people in reframing negative thoughts. Based on psychology literature, we define a framework of seven linguistic attributes that can be used to reframe a thought. We develop automated metrics to measure these attributes and validate them with expert judgements from mental health practitioners. We collect a dataset of 600 situations, thoughts and reframes from practitioners and use it to train a retrieval-enhanced in-context learning model that effectively generates reframed thoughts and controls their linguistic attributes. To investigate what constitutes a “high-quality” reframe, we conduct an IRB-approved randomized field study on a large mental health website with over 2,000 participants. Amongst other findings, we show that people prefer highly empathic or specific reframes, as opposed to reframes that are overly positive. Our findings provide key implications for the use of LMs to assist people in overcoming negative thoughts.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.555.pdf",
        "keywords": [
            "negative thoughts",
            "language models",
            "cognitive reframing",
            "human language model interaction",
            "mental health website",
            "approved randomized field study"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but \"Our findings provide key implications for the use of LMs\" implies limitations in the context of the study, however the limitations are not explicitly mentioned in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but \"Our findings provide key implications for the use of LMs\" implies limitations in the context of the study, however the limitations are not explicitly mentioned in the abstract."
    },
    {
        "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions",
        "authors": [
            "Harsh Trivedi",
            "Niranjan Balasubramanian",
            "Tushar Khot",
            "Ashish Sabharwal"
        ],
        "published": "2023",
        "summary": "Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, what to retrieve depends on what has already been derived, which in turn may depend on what was previously retrieved. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-of-distribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.557.pdf",
        "keywords": [
            "multi step questions",
            "multi step question answering",
            "thought reasoning",
            "large language models",
            "chain"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters.\""
    },
    {
        "title": "Direct Fact Retrieval from Knowledge Graphs without Entity Linking",
        "authors": [
            "Jinheon Baek",
            "Alham Fikri Aji",
            "Jens Lehmann",
            "Sung Ju Hwang"
        ],
        "published": "2023",
        "summary": "There has been a surge of interest in utilizing Knowledge Graphs (KGs) for various natural language processing/understanding tasks. The conventional mechanism to retrieve facts in KGs usually involves three steps: entity span detection, entity disambiguation, and relation classification. However, this approach requires additional labels for training each of the three subcomponents in addition to pairs of input texts and facts, and also may accumulate errors propagated from failures in previous steps. To tackle these limitations, we propose a simple knowledge retrieval framework, which directly retrieves facts from the KGs given the input text based on their representational similarities, which we refer to as Direct Fact Retrieval (DiFaR). Specifically, we first embed all facts in KGs onto a dense embedding space by using a language model trained by only pairs of input texts and facts, and then provide the nearest facts in response to the input text. Since the fact, consisting of only two entities and one relation, has little context to encode, we propose to further refine ranks of top-k retrieved facts with a reranker that contextualizes the input text and the fact jointly. We validate our DiFaR framework on multiple fact retrieval tasks, showing that it significantly outperforms relevant baselines that use the three-step approach.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.558.pdf",
        "keywords": [
            "entity disambiguation",
            "knowledge graphs",
            "relation classification",
            "knowledge retrieval",
            "retrieval",
            "fact retrieval"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the fact, consisting of only two entities and one relation, has little context to encode\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"the fact, consisting of only two entities and one relation, has little context to encode\""
    },
    {
        "title": "Improved Instruction Ordering in Recipe-Grounded Conversation",
        "authors": [
            "Duong Le",
            "Ruohao Guo",
            "Wei Xu",
            "Alan Ritter"
        ],
        "published": "2023",
        "summary": "In this paper, we study the task of instructional dialogue and focus on the cooking domain. Analyzing the generated output of the GPT-J model, we reveal that the primary challenge for a recipe-grounded dialog system is how to provide the instructions in the correct order. We hypothesize that this is due to the model’s lack of understanding of user intent and inability to track the instruction state (i.e., which step was last instructed). Therefore, we propose to explore two auxiliary subtasks, namely User Intent Detection and Instruction State Tracking, to support Response Generation with improved instruction grounding. Experimenting with our newly collected dataset, ChattyChef, shows that incorporating user intent and instruction state information helps the response generation model mitigate the incorrect order issue. Furthermore, to investigate whether ChatGPT has completely solved this task, we analyze its outputs and find that it also makes mistakes (10.7% of the responses), about half of which are out-of-order instructions. We will release ChattyChef to facilitate further research in this area at: https://github.com/octaviaguo/ChattyChef.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.561.pdf",
        "keywords": [
            "instruction",
            "improved instruction ordering",
            "correct",
            "recipe grounded dialog",
            "cooking"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Analyzing the generated output of the GPT-J model, we reveal that the primary challenge for a recipe-grounded dialog system is how to provide the instructions in the correct order... Furthermore, to investigate whether ChatGPT has completely solved this task, we analyze its outputs and find that it also makes mistakes (10.7% of the responses), about half of which are out",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Analyzing the generated output of the GPT-J model, we reveal that the primary challenge for a recipe-grounded dialog system is how to provide the instructions in the correct order... Furthermore, to investigate whether ChatGPT has completely solved this task, we analyze its outputs and find that it also makes mistakes (10.7% of the responses), about half of which are out"
    },
    {
        "title": "Token-wise Decomposition of Autoregressive Language Model Hidden States for Analyzing Model Predictions",
        "authors": [
            "Byung-Doh Oh",
            "William Schuler"
        ],
        "published": "2023",
        "summary": "While there is much recent interest in studying why Transformer-based large language models make predictions the way they do, the complex computations performed within each layer have made their behavior somewhat opaque. To mitigate this opacity, this work presents a linear decomposition of final hidden states from autoregressive language models based on each initial input token, which is exact for virtually all contemporary Transformer architectures. This decomposition allows the definition of probability distributions that ablate the contribution of specific input tokens, which can be used to analyze their influence on model probabilities over a sequence of upcoming words with only one forward pass from the model. Using the change in next-word probability as a measure of importance, this work first examines which context words make the biggest contribution to language model predictions. Regression experiments suggest that Transformer-based language models rely primarily on collocational associations, followed by linguistic factors such as syntactic dependencies and coreference relationships in making next-word predictions. Additionally, analyses using these measures to predict syntactic dependencies and coreferent mention spans show that collocational association and repetitions of the same token largely explain the language models’ predictions on these tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.562.pdf",
        "keywords": [
            "linear decomposition",
            "transformer",
            "autoregressive language model",
            "language models",
            "token wise decomposition",
            "transformer architectures"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the complex computations performed within each layer have made their behavior somewhat opaque.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (opacity in their behavior) but only briefly and does not explore it in depth. The primary focus of the paper is on the proposed solution to mitigate this limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the complex computations performed within each layer have made their behavior somewhat opaque.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (opacity in their behavior) but only briefly and does not explore it in depth. The primary focus of the paper is on the proposed solution to mitigate this limitation."
    },
    {
        "title": "Dialog-Post: Multi-Level Self-Supervised Objectives and Hierarchical Model for Dialogue Post-Training",
        "authors": [
            "Zhenyu Zhang",
            "Lei Shen",
            "Yuming Zhao",
            "Meng Chen",
            "Xiaodong He"
        ],
        "published": "2023",
        "summary": "Dialogue representation and understanding aim to convert conversational inputs into embeddings and fulfill discriminative tasks. Compared with free-form text, dialogue has two important characteristics, hierarchical semantic structure and multi-facet attributes. Therefore, directly applying the pretrained language models (PLMs) might result in unsatisfactory performance. Recently, several work focused on the dialogue-adaptive post-training (DialPost) that further trains PLMs to fit dialogues. To model dialogues more comprehensively, we propose a DialPost method, Dialog-Post, with multi-level self-supervised objectives and a hierarchical model. These objectives leverage dialogue-specific attributes and use self-supervised signals to fully facilitate the representation and understanding of dialogues. The novel model is a hierarchical segment-wise self-attention network, which contains inner-segment and inter-segment self-attention sub-layers followed by an aggregation and updating module. To evaluate the effectiveness of our methods, we first apply two public datasets for the verification of representation ability. Then we conduct experiments on a newly-labelled dataset that is annotated with 4 dialogue understanding tasks. Experimental results show that our method outperforms existing SOTA models and achieves a 3.3% improvement on average.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.564.pdf",
        "keywords": [
            "supervised",
            "dialog",
            "dialogue"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Compared with free-form text, dialogue has two important characteristics, hierarchical semantic structure and multi-facet attributes. Therefore, directly applying the pretrained language models (PLMs) might result in unsatisfactory performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Compared with free-form text, dialogue has two important characteristics, hierarchical semantic structure and multi-facet attributes. Therefore, directly applying the pretrained language models (PLMs) might result in unsatisfactory performance.\""
    },
    {
        "title": "Language Detoxification with Attribute-Discriminative Latent Space",
        "authors": [
            "Jin Myung Kwak",
            "Minseon Kim",
            "Sung Ju Hwang"
        ],
        "published": "2023",
        "summary": "Transformer-based Language Models (LMs) have achieved impressive results on natural language understanding tasks, but they can also generate toxic text such as insults, threats, and profanity, limiting their real-world applications. To overcome this issue, a few text generation approaches aim to detoxify toxic texts using additional LMs or perturbations. However, previous methods require excessive memory, computations, and time which are serious bottlenecks in their real-world application. To address such limitations, we propose an effective yet efficient method for language detoxification using an attribute-discriminative latent space. Specifically, we project the latent space of an original Transformer LM onto a discriminative latent space that well-separates texts by their attributes using a projection block and an attribute discriminator. This allows the LM to control the text generation to be non-toxic with minimal memory and computation overhead. We validate our model, Attribute-Discriminative Language Model (ADLM) on detoxified language and dialogue generation tasks, on which our method significantly outperforms baselines both in performance and efficiency.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.565.pdf",
        "keywords": [
            "detoxification",
            "latent space",
            "discriminative latent space",
            "transformer based language models"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Transformer-based Language Models (LMs) have achieved impressive results on natural language understanding tasks, but they can also generate toxic text such as insults, threats, and profanity, limiting their real-world applications.\"\n\nThe limitation (generating toxic text) is mentioned, and its impact on real-world applications is discussed, but the primary focus of the paper is on the proposed solution, Attribute",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Transformer-based Language Models (LMs) have achieved impressive results on natural language understanding tasks, but they can also generate toxic text such as insults, threats, and profanity, limiting their real-world applications.\"\n\nThe limitation (generating toxic text) is mentioned, and its impact on real-world applications is discussed, but the primary focus of the paper is on the proposed solution, Attribute"
    },
    {
        "title": "Just Like a Human Would, Direct Access to Sarcasm Augmented with Potential Result and Reaction",
        "authors": [
            "Changrong Min",
            "Ximing Li",
            "Liang Yang",
            "Zhilin Wang",
            "Bo Xu",
            "Hongfei Lin"
        ],
        "published": "2023",
        "summary": "Sarcasm, as a form of irony conveying mockery and contempt, has been widespread in social media such as Twitter and Weibo, where the sarcastic text is commonly characterized as an incongruity between the surface positive and negative situation. Naturally, it has an urgent demand to automatically identify sarcasm from social media, so as to illustrate people’s real views toward specific targets. In this paper, we develop a novel sarcasm detection method, namely Sarcasm Detector with Augmentation of Potential Result and Reaction (SD-APRR). Inspired by the direct access view, we treat each sarcastic text as an incomplete version without latent content associated with implied negative situations, including the result and human reaction caused by its observable content. To fill the latent content, we estimate the potential result and human reaction for each given training sample by [xEffect] and [xReact] relations inferred by the pre-trained commonsense reasoning tool COMET, and integrate the sample with them as an augmented one. We can then employ those augmented samples to train the sarcasm detector, whose encoder is a graph neural network with a denoising module. We conduct extensive empirical experiments to evaluate the effectiveness of SD-APRR. The results demonstrate that SD-APRR can outperform strong baselines on benchmark datasets.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.566.pdf",
        "keywords": [
            "social media",
            "detector",
            "sarcastic"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Revisiting Token Dropping Strategy in Efficient BERT Pretraining",
        "authors": [
            "Qihuang Zhong",
            "Liang Ding",
            "Juhua Liu",
            "Xuebo Liu",
            "Min Zhang",
            "Bo Du",
            "Dacheng Tao"
        ],
        "published": "2023",
        "summary": "Token dropping is a recently-proposed strategy to speed up the pretraining of masked language models, such as BERT, by skipping the computation of a subset of the input tokens at several middle layers. It can effectively reduce the training time without degrading much performance on downstream tasks. However, we empirically find that token dropping is prone to a semantic loss problem and falls short in handling semantic-intense tasks. Motivated by this, we propose a simple yet effective semantic-consistent learning method (ScTD) to improve the token dropping. ScTD aims to encourage the model to learn how to preserve the semantic information in the representation space. Extensive experiments on 12 tasks show that, with the help of our ScTD, token dropping can achieve consistent and significant performance gains across all task types and model sizes. More encouragingly, ScTD saves up to 57% of pretraining time and brings up to +1.56% average improvement over the vanilla token dropping.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.579.pdf",
        "keywords": [
            "token dropping",
            "pretraining",
            "bert pretraining"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, we empirically find that token dropping is prone to a semantic loss problem and falls short in handling semantic-intense tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, we empirically find that token dropping is prone to a semantic loss problem and falls short in handling semantic-intense tasks.\""
    },
    {
        "title": "The Benefits of Bad Advice: Autocontrastive Decoding across Model Layers",
        "authors": [
            "Ariel Gera",
            "Roni Friedman",
            "Ofir Arviv",
            "Chulaka Gunasekara",
            "Benjamin Sznajder",
            "Noam Slonim",
            "Eyal Shnarch"
        ],
        "published": "2023",
        "summary": "Applying language models to natural language processing tasks typically relies on the representations in the final model layer, as intermediate hidden layer representations are presumed to be less informative. In this work, we argue that due to the gradual improvement across model layers, additional information can be gleaned from the contrast between higher and lower layers during inference. Specifically, in choosing between the probable next token predictions of a generative model, the predictions of lower layers can be used to highlight which candidates are best avoided. We propose a novel approach that utilizes the contrast between layers to improve text generation outputs, and show that it mitigates degenerative behaviors of the model in open-ended generation, significantly improving the quality of generated texts. Furthermore, our results indicate that contrasting between model layers at inference time can yield substantial benefits to certain aspects of general language model capabilities, more effectively extracting knowledge during inference from a given set of model parameters.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.580.pdf",
        "keywords": [
            "language models",
            "generated",
            "natural language processing",
            "inference"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"mitigates degenerative behaviors of the model in open-ended generation\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"mitigates degenerative behaviors of the model in open-ended generation\""
    },
    {
        "title": "FACTIFY-5WQA: 5W Aspect-based Fact Verification through Question Answering",
        "authors": [
            "Anku Rani",
            "S.M Towhidul Islam Tonmoy",
            "Dwip Dalal",
            "Shreya Gautam",
            "Megha Chakraborty",
            "Aman Chadha",
            "Amit Sheth",
            "Amitava Das"
        ],
        "published": "2023",
        "summary": "Automatic fact verification has received significant attention recently. Contemporary automatic fact-checking systems focus on estimating truthfulness using numerical scores which are not human-interpretable. A human fact-checker generally follows several logical steps to verify a verisimilitude claim and conclude whether it’s truthful or a mere masquerade. Popular fact-checking websites follow a common structure for fact categorization such as half true, half false, false, pants on fire, etc. Therefore, it is necessary to have an aspect-based (delineating which part(s) are true and which are false) explainable system that can assist human fact-checkers in asking relevant questions related to a fact, which can then be validated separately to reach a final verdict. In this paper, we propose a 5W framework (who, what, when, where, and why) for question-answer-based fact explainability. To that end, we present a semi-automatically generated dataset called FACTIFY-5WQA, which consists of 391, 041 facts along with relevant 5W QAs – underscoring our major contribution to this paper. A semantic role labeling system has been utilized to locate 5Ws, which generates QA pairs for claims using a masked language model. Finally, we report a baseline QA system to automatically locate those answers from evidence documents, which can serve as a baseline for future research in the field. Lastly, we propose a robust fact verification system that takes paraphrased claims and automatically validates them. The dataset and the baseline model are available at https: //github.com/ankuranii/acl-5W-QA",
        "pdf_link": "https://aclanthology.org/2023.acl-long.581.pdf",
        "keywords": [
            "fact explainability",
            "fact verification",
            "automatic fact verification",
            "masked language",
            "truthfulness",
            "fact checking",
            "paraphrased claims",
            "question answering"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"A semantic role labeling system has been utilized to locate 5Ws, which generates QA pairs for claims using a masked language model.\"\n\nThis paper mentions the use of a masked language model but does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"A semantic role labeling system has been utilized to locate 5Ws, which generates QA pairs for claims using a masked language model.\"\n\nThis paper mentions the use of a masked language model but does not discuss any limitations of LLMs."
    },
    {
        "title": "A Universal Discriminator for Zero-Shot Generalization",
        "authors": [
            "Haike Xu",
            "Zongyu Lin",
            "Jing Zhou",
            "Yanan Zheng",
            "Zhilin Yang"
        ],
        "published": "2023",
        "summary": "Generative modeling has been the dominant approach for large-scale pretraining and zero-shot generalization. In this work, we challenge this convention by showing that discriminative approaches perform substantially better than generative ones on a large number of NLP tasks. Technically, we train a single discriminator to predict whether a text sample comes from the true data distribution, similar to GANs. Since many NLP tasks can be formulated as selecting from a few options, we use this discriminator to predict the concatenation of input and which option has the highest probability of coming from the true data distribution. This simple formulation achieves state-of-the-art zero-shot results on the T0 benchmark, outperforming T0 by 16.0%, 7.8%, and 11.5% respectively on different scales. In the finetuning setting, our approach also achieves new state-of-the-art results on a wide range of NLP tasks, with only 1/4 parameters of previous methods. Meanwhile, our approach requires minimal prompting efforts, which largely improves robustness and is essential for real-world applications. Furthermore, we also jointly train a generalized UD in combination with generative tasks, which maintains its advantage on discriminative tasks and simultaneously works on generative tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.589.pdf",
        "keywords": [
            "universal discriminator",
            "discriminator",
            "single discriminator",
            "generalization"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "A Cognitive Stimulation Dialogue System with Multi-source Knowledge Fusion for Elders with Cognitive Impairment",
        "authors": [
            "Jiyue Jiang",
            "Sheng Wang",
            "Qintong Li",
            "Lingpeng Kong",
            "Chuan Wu"
        ],
        "published": "2023",
        "summary": "When communicating with elders with cognitive impairment, cognitive stimulation (CS) help to maintain the cognitive health of elders. Data sparsity is the main challenge in building CS-based dialogue systems, particularly in the Chinese language. To fill this gap, we construct a Chinese CS conversation (CSConv) dataset, which contains about 2.6K groups of dialogues with therapy principles and emotional support strategy labels. Making chit chat while providing emotional support is overlooked by the majority of existing cognitive dialogue systems. In this paper, we propose a multi-source knowledge fusion method for CS dialogue (CSD), to generate open-ended responses guided by the therapy principle and emotional support strategy. We first use a progressive mask method based on external knowledge to learn encoders as effective classifiers, which is the prerequisite to predict the therapy principle and emotional support strategy of the target response. Then a decoder interacts with the perceived therapy principle and emotional support strategy to generate responses. Extensive experiments conducted on the CSConv dataset demonstrate the effectiveness of the proposed method, while there is still a large space for improvement compared to human performance.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.593.pdf",
        "keywords": [
            "cognitive impairment",
            "dialogue",
            "elders",
            "cognitive dialogue systems",
            "knowledge fusion",
            "cognitive stimulation dialogue system",
            "cognitive stimulation",
            "emotional",
            "with elders",
            "cognitive health",
            "multi source knowledge fusion",
            "emotional support"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs or language models.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs or language models."
    },
    {
        "title": "Plug-and-Play Knowledge Injection for Pre-trained Language Models",
        "authors": [
            "Zhengyan Zhang",
            "Zhiyuan Zeng",
            "Yankai Lin",
            "Huadong Wang",
            "Deming Ye",
            "Chaojun Xiao",
            "Xu Han",
            "Zhiyuan Liu",
            "Peng Li",
            "Maosong Sun",
            "Jie Zhou"
        ],
        "published": "2023",
        "summary": "Injecting external knowledge can improve the performance of pre-trained language models (PLMs) on various downstream NLP tasks. However, massive retraining is required to deploy new knowledge injection methods or knowledge bases for downstream tasks. In this work, we are the first to study how to improve the flexibility and efficiency of knowledge injection by reusing existing downstream models. To this end, we explore a new paradigm plug-and-play knowledge injection, where knowledge bases are injected into frozen existing downstream models by a knowledge plugin. Correspondingly, we propose a plug-and-play injection method map-tuning, which trains a mapping of knowledge embeddings to enrich model inputs with mapped embeddings while keeping model parameters frozen. Experimental results on three knowledge-driven NLP tasks show that existing injection methods are not suitable for the new paradigm, while map-tuning effectively improves the performance of downstream models. Moreover, we show that a frozen downstream model can be well adapted to different domains with different mapping networks of domain knowledge. Our code and models are available at https://github.com/THUNLP/Knowledge-Plugin.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.594.pdf",
        "keywords": [
            "knowledge injection",
            "plug and play injection",
            "knowledge bases",
            "paradigm plug and"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, massive retraining is required to deploy new knowledge injection methods or knowledge bases for downstream tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, massive retraining is required to deploy new knowledge injection methods or knowledge bases for downstream tasks.\""
    },
    {
        "title": "Rethinking Masked Language Modeling for Chinese Spelling Correction",
        "authors": [
            "Hongqiu Wu",
            "Shaohua Zhang",
            "Yuchen Zhang",
            "Hai Zhao"
        ],
        "published": "2023",
        "summary": "In this paper, we study Chinese Spelling Correction (CSC) as a joint decision made by two separate models: a language model and an error model. Through empirical analysis, we find that fine-tuning BERT tends to over-fit the error model while under-fit the language model, resulting in poor generalization to out-of-distribution error patterns. Given that BERT is the backbone of most CSC models, this phenomenon has a significant negative impact. To address this issue, we are releasing a multi-domain benchmark LEMON, with higher quality and diversity than existing benchmarks, to allow a comprehensive assessment of the open domain generalization of CSC models. Then, we demonstrate that a very simple strategy – randomly masking 20% non-error tokens from the input sequence during fine-tuning – is sufficient for learning a much better language model without sacrificing the error model. This technique can be applied to any model architecture and achieves new state-of-the-art results on SIGHAN, ECSpell, and LEMON.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.600.pdf",
        "keywords": [
            "language model",
            "chinese spelling correction",
            "chinese spelling",
            "masked"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Through empirical analysis, we find that fine-tuning BERT tends to over-fit the error model while under-fit the language model, resulting in poor generalization to out-of-distribution error patterns.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Through empirical analysis, we find that fine-tuning BERT tends to over-fit the error model while under-fit the language model, resulting in poor generalization to out-of-distribution error patterns.\""
    },
    {
        "title": "A Multi-Modal Context Reasoning Approach for Conditional Inference on Joint Textual and Visual Clues",
        "authors": [
            "Yunxin Li",
            "Baotian Hu",
            "Chen Xinyu",
            "Yuxin Ding",
            "Lin Ma",
            "Min Zhang"
        ],
        "published": "2023",
        "summary": "Conditional inference on joint textual and visual clues is a multi-modal reasoning task that textual clues provide prior permutation or external knowledge, which are complementary with visual content and pivotal to deducing the correct option. Previous methods utilizing pretrained vision-language models (VLMs) have achieved impressive performances, yet they show a lack of multimodal context reasoning capability, especially for text-modal information. To address this issue, we propose a Multi-modal Context Reasoning approach, named ModCR. Compared to VLMs performing reasoning via cross modal semantic alignment, it regards the given textual abstract semantic and objective image information as the pre-context information and embeds them into the language model to perform context reasoning. Different from recent vision-aided language models used in natural language processing, ModCR incorporates the multi-view semantic alignment information between language and vision by introducing the learnable alignment prefix between image and text in the pretrained language model. This makes the language model well-suitable for such multi-modal reasoning scenario on joint textual and visual clues. We conduct extensive experiments on two corresponding data sets and experimental results show significantly improved performance (exact gain by 4.8% on PMR test set) compared to previous strong baselines.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.601.pdf",
        "keywords": [
            "context reasoning",
            "conditional inference",
            "multi modal context reasoning",
            "visual clues"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Previous methods utilizing pretrained vision-language models (VLMs) have achieved impressive performances, yet they show a lack of multimodal context reasoning capability, especially for text-modal information.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Previous methods utilizing pretrained vision-language models (VLMs) have achieved impressive performances, yet they show a lack of multimodal context reasoning capability, especially for text-modal information.\""
    },
    {
        "title": "f-Divergence Minimization for Sequence-Level Knowledge Distillation",
        "authors": [
            "Yuqiao Wen",
            "Zichao Li",
            "Wenyu Du",
            "Lili Mou"
        ],
        "published": "2023",
        "summary": "Knowledge distillation (KD) is the process of transferring knowledge from a large model to a small one. It has gained increasing attention in the natural language processing community, driven by the demands of compressing ever-growing language models. In this work, we propose an FDISTILL framework, which formulates sequence-level knowledge distillation as minimizing a generalized f-divergence function. We propose four distilling variants under our framework and show that existing SeqKD and ENGINE approaches are approximations of our FDISTILL methods. We further derive step-wise decomposition for our FDISTILL, reducing intractable sequence-level divergence to word-level losses that can be computed in a tractable manner. Experiments across four datasets show that our methods outperform existing KD approaches, and that our symmetric distilling losses can better force the student to learn from the teacher distribution.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.605.pdf",
        "keywords": [
            "knowledge distillation",
            "sequence level knowledge distillation",
            "sequence level divergence",
            "f divergence minimization"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "A Synthetic Data Generation Framework for Grounded Dialogues",
        "authors": [
            "Jianzhu Bao",
            "Rui Wang",
            "Yasheng Wang",
            "Aixin Sun",
            "Yitong Li",
            "Fei Mi",
            "Ruifeng Xu"
        ],
        "published": "2023",
        "summary": "Training grounded response generation models often requires a large collection of grounded dialogues. However, it is costly to build such dialogues. In this paper, we present a synthetic data generation framework (SynDG) for grounded dialogues. The generation process utilizes large pre-trained language models and freely available knowledge data (e.g., Wikipedia pages, persona profiles, etc.). The key idea of designing SynDG is to consider dialogue flow and coherence in the generation process. Specifically, given knowledge data, we first heuristically determine a dialogue flow, which is a series of knowledge pieces. Then, we employ T5 to incrementally turn the dialogue flow into a dialogue. To ensure coherence of both the dialogue flow and the synthetic dialogue, we design a two-level filtering strategy, at the flow-level and the utterance-level respectively. Experiments on two public benchmarks show that the synthetic grounded dialogue data produced by our framework is able to significantly boost model performance in both full training data and low-resource scenarios.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.608.pdf",
        "keywords": [
            "dialogues",
            "grounded dialogues",
            "grounded dialogue data",
            "grounded response generation",
            "synthetic data generation",
            "coherence",
            "dialogue flow",
            "synthetic data generation framework"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"utilizes large pre-trained language models\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"utilizes large pre-trained language models\""
    },
    {
        "title": "MasakhaPOS: Part-of-Speech Tagging for Typologically Diverse African languages",
        "authors": [
            "Cheikh M. Bamba Dione",
            "David Ifeoluwa Adelani",
            "Peter Nabende",
            "Jesujoba Alabi",
            "Thapelo Sindane",
            "Happy Buzaaba",
            "Shamsuddeen Hassan Muhammad",
            "Chris Chinenye Emezue",
            "Perez Ogayo",
            "Anuoluwapo Aremu",
            "Catherine Gitau",
            "Derguene Mbaye",
            "Jonathan Mukiibi",
            "Blessing Sibanda",
            "Bonaventure F. P. Dossou",
            "Andiswa Bukula",
            "Rooweither Mabuya",
            "Allahsera Auguste Tapo",
            "Edwin Munkoh-Buabeng",
            "Victoire Memdjokam Koagne",
            "Fatoumata Ouoba Kabore",
            "Amelia Taylor",
            "Godson Kalipe",
            "Tebogo Macucwa",
            "Vukosi Marivate",
            "Tajuddeen Gwadabe",
            "Mboning Tchiaze Elvis",
            "Ikechukwu Onyenwe",
            "Gratien Atindogbe",
            "Tolulope Adelani",
            "Idris Akinade",
            "Olanrewaju Samuel",
            "Marien Nahimana",
            "Théogène Musabeyezu",
            "Emile Niyomutabazi",
            "Ester Chimhenga",
            "Kudzai Gotosa",
            "Patrick Mizha",
            "Apelete Agbolo",
            "Seydou Traore",
            "Chinedu Uchechukwu",
            "Aliyu Yusuf",
            "Muhammad Abdullahi",
            "Dietrich Klakow"
        ],
        "published": "2023",
        "summary": "In this paper, we present AfricaPOS, the largest part-of-speech (POS) dataset for 20 typologically diverse African languages. We discuss the challenges in annotating POS for these languages using the universal dependencies (UD) guidelines. We conducted extensive POS baseline experiments using both conditional random field and several multilingual pre-trained language models. We applied various cross-lingual transfer models trained with data available in the UD. Evaluating on the AfricaPOS dataset, we show that choosing the best transfer language(s) in both single-source and multi-source setups greatly improves the POS tagging performance of the target languages, in particular when combined with parameter-fine-tuning methods. Crucially, transferring knowledge from a language that matches the language family and morphosyntactic properties seems to be more effective for POS tagging in unseen languages.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.609.pdf",
        "keywords": [
            "speech tagging",
            "african languages"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions using \"several multilingual pre-trained language models\" which implies LLMs, but does not discuss any limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions using \"several multilingual pre-trained language models\" which implies LLMs, but does not discuss any limitations."
    },
    {
        "title": "Semantic Structure Enhanced Event Causality Identification",
        "authors": [
            "Zhilei Hu",
            "Zixuan Li",
            "Xiaolong Jin",
            "Long Bai",
            "Saiping Guan",
            "Jiafeng Guo",
            "Xueqi Cheng"
        ],
        "published": "2023",
        "summary": "Event Causality Identification (ECI) aims to identify causal relations between events in unstructured texts. This is a very challenging task, because causal relations are usually expressed by implicit associations between events. Existing methods usually capture such associations by directly modeling the texts with pre-trained language models, which underestimate two kinds of semantic structures vital to the ECI task, namely, event-centric structure and event-associated structure. The former includes important semantic elements related to the events to describe them more precisely, while the latter contains semantic paths between two events to provide possible supports for ECI. In this paper, we study the implicit associations between events by modeling the above explicit semantic structures, and propose a Semantic Structure Integration model (SemSIn).It utilizes a GNN-based event aggregator to integrate the event-centric structure information, and employs an LSTM-based path aggregator to capture the event-associated structure information between two events. Experimental results on three widely used datasets show that SemSIn achieves significant improvements over baseline methods.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.610.pdf",
        "keywords": [
            "events",
            "event aggregator",
            "path aggregator",
            "event causality identification",
            "semantic structure integration model",
            "gnn"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing methods usually capture such associations by directly modeling the texts with pre-trained language models, which underestimate two kinds of semantic structures vital to the ECI task...\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing methods usually capture such associations by directly modeling the texts with pre-trained language models, which underestimate two kinds of semantic structures vital to the ECI task...\""
    },
    {
        "title": "Your spouse needs professional help: Determining the Contextual Appropriateness of Messages through Modeling Social Relationships",
        "authors": [
            "David Jurgens",
            "Agrima Seth",
            "Jackson Sargent",
            "Athena Aghighi",
            "Michael Geraci"
        ],
        "published": "2023",
        "summary": "Understanding interpersonal communication requires, in part, understanding the social context and norms in which a message is said. However, current methods for identifying offensive content in such communication largely operate independent of context, with only a few approaches considering community norms or prior conversation as context. Here, we introduce a new approach to identifying inappropriate communication by explicitly modeling the social relationship between the individuals. We introduce a new dataset of contextually-situated judgments of appropriateness and show that large language models can readily incorporate relationship information to accurately identify appropriateness in a given context. Using data from online conversations and movie dialogues, we provide insight into how the relationships themselves function as implicit norms and quantify the degree to which context-sensitivity is needed in different conversation settings. Further, we also demonstrate that contextual-appropriateness judgments are predictive of other social factors expressed in language such as condescension and politeness.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.616.pdf",
        "keywords": [
            "appropriateness",
            "contextual appropriateness",
            "contextual appropriateness judgments",
            "community norms",
            "language",
            "context"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, current methods for identifying offensive content in such communication largely operate independent of context, with only a few approaches considering community norms or prior conversation as context.\"\n\n(Note: Although this paper mentions a limitation of current methods, it does not specifically discuss limitations of LLMs. However, since the paper discusses using LLMs to address this limitation, it is rated as 1.)",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, current methods for identifying offensive content in such communication largely operate independent of context, with only a few approaches considering community norms or prior conversation as context.\"\n\n(Note: Although this paper mentions a limitation of current methods, it does not specifically discuss limitations of LLMs. However, since the paper discusses using LLMs to address this limitation, it is rated as 1.)"
    },
    {
        "title": "How Do In-Context Examples Affect Compositional Generalization?",
        "authors": [
            "Shengnan An",
            "Zeqi Lin",
            "Qiang Fu",
            "Bei Chen",
            "Nanning Zheng",
            "Jian-Guang Lou",
            "Dongmei Zhang"
        ],
        "published": "2023",
        "summary": "Compositional generalization–understanding unseen combinations of seen primitives–is an essential reasoning capability in human intelligence. The AI community mainly studies this capability by fine-tuning neural networks on lots of training samples, while it is still unclear whether and how in-context learning–the prevailing few-shot paradigm based on large language models–exhibits compositional generalization. In this paper, we present CoFe, a test suite to investigate in-context compositional generalization. We find that the compositional generalization performance can be easily affected by the selection of in-context examples, thus raising the research question what the key factors are to make good in-context examples for compositional generalization. We study three potential factors: similarity, diversity and complexity. Our systematic experiments indicate that in-context examples should be structurally similar to the test case, diverse from each other, and individually simple. Furthermore, two strong limitations are observed: in-context compositional generalization on fictional words is much weaker than that on commonly used ones; it is still critical that the in-context examples should cover required linguistic structures, even though the backbone model has been pre-trained on large corpus. We hope our analysis would facilitate the understanding and utilization of in-context learning paradigm.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.618.pdf",
        "keywords": [
            "in context examples"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"two strong limitations are observed: in-context compositional generalization on fictional words is much weaker than that on commonly used ones; it is still critical that the in-context examples should cover required linguistic structures, even though the backbone model has been pre-trained on large corpus.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"two strong limitations are observed: in-context compositional generalization on fictional words is much weaker than that on commonly used ones; it is still critical that the in-context examples should cover required linguistic structures, even though the backbone model has been pre-trained on large corpus.\""
    },
    {
        "title": "I Cast Detect Thoughts: Learning to Converse and Guide with Intents and Theory-of-Mind in Dungeons and Dragons",
        "authors": [
            "Pei Zhou",
            "Andrew Zhu",
            "Jennifer Hu",
            "Jay Pujara",
            "Xiang Ren",
            "Chris Callison-Burch",
            "Yejin Choi",
            "Prithviraj Ammanabrolu"
        ],
        "published": "2023",
        "summary": "We propose a novel task, G4C, to study teacher-student natural language interactions in a goal-driven and grounded environment. Dungeons and Dragons (D&D), a role-playing game, provides an ideal setting to investigate such interactions. Here, the Dungeon Master (DM), i.e., the teacher, guides the actions of several players—students, each with their own personas and abilities—to achieve shared goals grounded in a fantasy world. Our approach is to decompose and model these interactions into (1) the DM’s intent to guide players toward a given goal; (2) the DM’s guidance utterance to the players expressing this intent; and (3) a theory-of-mind (ToM) model that anticipates the players’ reaction to the guidance one turn into the future. We develop a novel reinforcement learning (RL) method for training a DM that generates guidance for players by rewarding utterances where the intent matches the ToM-anticipated player actions. Human and automated evaluations show that a DM trained to explicitly model intents and incorporate ToM of the players using RL generates better-quality guidance that is 3x more likely to fulfill the DM’s intent than a vanilla natural language generation (NLG) approach.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.624.pdf",
        "keywords": [
            "reinforcement learning",
            "theory of mind",
            "fantasy world",
            "student natural language interactions",
            "i cast detect thoughts"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Multitask Pre-training of Modular Prompt for Chinese Few-Shot Learning",
        "authors": [
            "Tianxiang Sun",
            "Zhengfu He",
            "Qin Zhu",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "published": "2023",
        "summary": "Prompt tuning is a parameter-efficient approach to adapting pre-trained language models to downstream tasks. Although prompt tuning has been shown to match the performance of full model tuning when training data is sufficient, it tends to struggle in few-shot learning settings. In this paper, we present Multi-task Pre-trained Modular Prompt (MP2) to boost prompt tuning for few-shot learning. MP2 is a set of combinable prompts pre-trained on 38 Chinese tasks. On downstream tasks, the pre-trained prompts are selectively activated and combined, leading to strong compositional generalization to unseen tasks. To bridge the gap between pre-training and fine-tuning, we formulate upstream and downstream tasks into a unified machine reading comprehension task. Extensive experiments under two learning paradigms, i.e., gradient descent and black-box tuning, show that MP2 significantly outperforms prompt tuning, full model tuning, and prior prompt pre-training methods in few-shot settings. In addition, we demonstrate that MP2 can achieve surprisingly fast and strong adaptation to downstream tasks by merely learning 8 parameters to combine the pre-trained modular prompts.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.625.pdf",
        "keywords": [
            "multitask",
            "modular"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although prompt tuning has been shown to match the performance of full model tuning when training data is sufficient, it tends to struggle in few-shot learning settings.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although prompt tuning has been shown to match the performance of full model tuning when training data is sufficient, it tends to struggle in few-shot learning settings.\""
    },
    {
        "title": "Is GPT-3 a Good Data Annotator?",
        "authors": [
            "Bosheng Ding",
            "Chengwei Qin",
            "Linlin Liu",
            "Yew Ken Chia",
            "Boyang Li",
            "Shafiq Joty",
            "Lidong Bing"
        ],
        "published": "2023",
        "summary": "Data annotation is the process of labeling data that could be used to train machine learning models. Having high quality annotation is crucial, as it allows the model to learn the relationship between the input data and the desired output. GPT-3, a large-scale language model developed by OpenAI, has demonstrated im- impressive zero- and few-shot performance on a wide range of NLP tasks. It is therefore natural to wonder whether it can be used to effectively annotate data for NLP tasks. In this paper, we evaluate the performance of GPT-3 as a data annotator by comparing it with traditional data annotation methods and analyzing its output on a range of tasks. Through this analysis, we aim to provide insight into the potential of GPT-3 as a general-purpose data annotator in NLP.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.626.pdf",
        "keywords": [
            "annotation",
            "data annotator",
            "language model"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation is mentioned in the abstract, but the evaluation of GPT-3's performance as a data annotator implies a potential limitation of its capabilities.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation is mentioned in the abstract, but the evaluation of GPT-3's performance as a data annotator implies a potential limitation of its capabilities."
    },
    {
        "title": "How to Plant Trees in Language Models: Data and Architectural Effects on the Emergence of Syntactic Inductive Biases",
        "authors": [
            "Aaron Mueller",
            "Tal Linzen"
        ],
        "published": "2023",
        "summary": "Accurate syntactic representations are essential for robust generalization in natural language. Recent work has found that pre-training can teach language models to rely on hierarchical syntactic features—as opposed to incorrect linear features—when performing tasks after fine-tuning. We test what aspects of pre-training are important for endowing encoder-decoder Transformers with an inductive bias that favors hierarchical syntactic generalizations. We focus on architectural features (depth, width, and number of parameters), as well as the genre and size of the pre-training corpus, diagnosing inductive biases using two syntactic transformation tasks: question formation and passivization, both in English. We find that the number of parameters alone does not explain hierarchical generalization: model depth plays greater role than model width. We also find that pre-training on simpler language, such as child-directed speech, induces a hierarchical bias using an order-of-magnitude less data than pre-training on more typical datasets based on web text or Wikipedia; this suggests that in cognitively plausible language acquisition settings, neural language models may be more data-efficient than previously thought.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.629.pdf",
        "keywords": [
            "language",
            "language models",
            "neural language models",
            "plant trees",
            "architectural effects"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that the number of parameters alone does not explain hierarchical generalization: model depth plays greater role than model width.\"\n\nThis paper mentions a limitation of LLMs in passing, but it is not the primary focus of the paper. The limitation is that the number of parameters alone is not enough to explain hierarchical generalization, and the paper explores other factors that contribute to this limitation",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We find that the number of parameters alone does not explain hierarchical generalization: model depth plays greater role than model width.\"\n\nThis paper mentions a limitation of LLMs in passing, but it is not the primary focus of the paper. The limitation is that the number of parameters alone is not enough to explain hierarchical generalization, and the paper explores other factors that contribute to this limitation"
    },
    {
        "title": "HINT: Hypernetwork Instruction Tuning for Efficient Zero- and Few-Shot Generalisation",
        "authors": [
            "Hamish Ivison",
            "Akshita Bhagia",
            "Yizhong Wang",
            "Hannaneh Hajishirzi",
            "Matthew Peters"
        ],
        "published": "2023",
        "summary": "Recent NLP models have shown the remarkable ability to effectively generalise ‘zero-shot’ to new tasks using only natural language instructions as guidance. However, many of these approaches suffer from high computational costs due to their reliance on concatenating lengthy instructions with every input example, resulting in costly reprocessing of the instruction. To avoid this, we introduce Hypernetworks for INstruction Tuning (HINT), which convert task instructions and examples into parameter-efficient modules inserted into an underlying model using a pretrained text encoder, eliminating the need to include instructions in the model input. The hypernetwork in HINT also produces an encoded instruction, which we concatenate with encoded inputs during decoding to further improve performance. HINT models outperform strong state-of-the-art baselines by over 10% when controlling for compute (measured in FLOPs). By converting instructions into modules, HINT models can effectively disregard the length of instructions and few-shot example inputs in terms of compute usage. As a result, HINT can enhance its performance by up to 25% by incorporating additional few-shot data, while utilizing only up to 5% more compute. This combines the strengths of parameter-efficient fine-tuning and in-context learning.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.631.pdf",
        "keywords": [
            "tuning",
            "hint",
            "hypernetworks",
            "generalisation",
            "fine tuning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"many of these approaches suffer from high computational costs due to their reliance on concatenating lengthy instructions with every input example, resulting in costly reprocessing of the instruction.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"many of these approaches suffer from high computational costs due to their reliance on concatenating lengthy instructions with every input example, resulting in costly reprocessing of the instruction.\""
    },
    {
        "title": "Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations",
        "authors": [
            "Chenglei Si",
            "Dan Friedman",
            "Nitish Joshi",
            "Shi Feng",
            "Danqi Chen",
            "He He"
        ],
        "published": "2023",
        "summary": "In-context learning (ICL) is an important paradigm for adapting large language models (LLMs) to new tasks, but the generalization behavior of ICL remains poorly understood. We investigate the inductive biases of ICL from the perspective of feature bias: which feature ICL is more likely to use given a set of underspecified demonstrations in which two features are equally predictive of the labels. First, we characterize the feature biases of GPT-3 models by constructing underspecified demonstrations from a range of NLP datasets and feature combinations. We find that LLMs exhibit clear feature biases—for example, demonstrating a strong bias to predict labels according to sentiment rather than shallow lexical features, like punctuation. Second, we evaluate the effect of different interventions that are designed to impose an inductive bias in favor of a particular feature, such as adding a natural language instruction or using semantically relevant label words. We find that, while many interventions can influence the learner to prefer a particular feature, it can be difficult to overcome strong prior biases. Overall, our results provide a broader picture of the types of features that ICL may be more likely to exploit and how to impose inductive biases that are better aligned with the intended task.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.632.pdf",
        "keywords": [
            "inductive bias",
            "inductive biases",
            "feature bias",
            "strong bias",
            "context",
            "context learning"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that LLMs exhibit clear feature biases—for example, demonstrating a strong bias to predict labels according to sentiment rather than shallow lexical features, like punctuation.\"; \"it can be difficult to overcome strong prior biases.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We find that LLMs exhibit clear feature biases—for example, demonstrating a strong bias to predict labels according to sentiment rather than shallow lexical features, like punctuation.\"; \"it can be difficult to overcome strong prior biases.\""
    },
    {
        "title": "AlignScore: Evaluating Factual Consistency with A Unified Alignment Function",
        "authors": [
            "Yuheng Zha",
            "Yichi Yang",
            "Ruichen Li",
            "Zhiting Hu"
        ],
        "published": "2023",
        "summary": "Many text generation applications require the generated text to be factually consistent with input information. Automatic evaluation of factual consistency is challenging. Previous work has developed various metrics that often depend on specific functions, such as natural language inference (NLI) or question answering (QA), trained on limited data. Those metrics thus can hardly assess diverse factual inconsistencies (e.g., contradictions, hallucinations) that occur in varying inputs/outputs (e.g., sentences, documents) from different tasks. In this paper, we propose AlignScore, a new holistic metric that applies to a variety of factual inconsistency scenarios as above. AlignScore is based on a general function of information alignment between two arbitrary text pieces. Crucially, we develop a unified training framework of the alignment function by integrating a large diversity of data sources, resulting in 4.7M training examples from 7 well-established tasks (NLI, QA, paraphrasing, fact verification, information retrieval, semantic similarity, and summarization). We conduct extensive experiments on large-scale benchmarks including 22 evaluation datasets, where 19 of the datasets were never seen in the alignment training. AlignScore achieves substantial improvement over a wide range of previous metrics. Moreover, AlignScore (355M parameters) matches or even outperforms metrics based on ChatGPT and GPT-4 that are orders of magnitude larger.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.634.pdf",
        "keywords": [
            "summarization",
            "factual consistency",
            "alignment",
            "information alignment",
            "factual inconsistencies"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Many text generation applications require the generated text to be factually consistent with input information... Those metrics thus can hardly assess diverse factual inconsistencies (e.g., contradictions, hallucinations) that occur in varying inputs/outputs (e.g., sentences, documents) from different tasks.\"\n\nNote that the paper mentions limitations of LLMs in the context of factual inconsistency, but it does not focus",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Many text generation applications require the generated text to be factually consistent with input information... Those metrics thus can hardly assess diverse factual inconsistencies (e.g., contradictions, hallucinations) that occur in varying inputs/outputs (e.g., sentences, documents) from different tasks.\"\n\nNote that the paper mentions limitations of LLMs in the context of factual inconsistency, but it does not focus"
    },
    {
        "title": "Introducing Semantics into Speech Encoders",
        "authors": [
            "Derek Xu",
            "Shuyan Dong",
            "Changhan Wang",
            "Suyoun Kim",
            "Zhaojiang Lin",
            "Bing Liu",
            "Akshat Shrivastava",
            "Shang-Wen Li",
            "Liang-Hsuan Tseng",
            "Guan-Ting Lin",
            "Alexei Baevski",
            "Hung-yi Lee",
            "Yizhou Sun",
            "Wei Wang"
        ],
        "published": "2023",
        "summary": "Recent studies find existing self-supervised speech encoders contain primarily acoustic rather than semantic information. As a result, pipelined supervised automatic speech recognition (ASR) to large language model (LLM) systems achieve state-of-the-art results on semantic spoken language tasks by utilizing rich semantic representations from the LLM. These systems come at the cost of labeled audio transcriptions, which is expensive and time-consuming to obtain. We propose a task-agnostic unsupervised way of incorporating semantic information from LLMs into self-supervised speech encoders without labeled audio transcriptions. By introducing semantics, we improve existing speech encoder spoken language understanding (SLU) performance by over 5% on intent classification (IC), with modest gains in named entity resolution (NER) and slot filling (SF), and spoken question answering (SQA) FF1 score by over 2%. Our approach, which uses no ASR data, achieves similar performance as methods trained on over 100 hours of labeled audio transcripts, demonstrating the feasibility of unsupervised semantic augmentations to existing speech encoders.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.639.pdf",
        "keywords": [
            "speech encoders",
            "speech recognition",
            "named entity resolution",
            "slu",
            "semantics"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations of LLMs are mentioned in the abstract, but it discusses the cost of labeled audio transcriptions for pipelined supervised ASR to LLM systems, which is related to the use of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations of LLMs are mentioned in the abstract, but it discusses the cost of labeled audio transcriptions for pipelined supervised ASR to LLM systems, which is related to the use of LLMs."
    },
    {
        "title": "MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning",
        "authors": [
            "Zhiyang Xu",
            "Ying Shen",
            "Lifu Huang"
        ],
        "published": "2023",
        "summary": "Instruction tuning, a new learning paradigm that fine-tunes pre-trained language models on tasks specified through instructions, has shown promising zero-shot performance on various natural language processing tasks. However, it has yet to be explored for vision and multimodal tasks. In this work, we introduce MultiInstruct, the first multimodal instruction tuning benchmark dataset that consists of 62 diverse multimodal tasks in a unified seq-to-seq format covering 10 broad categories. The tasks are derived from 21 existing open-source datasets and each task is equipped with 5 expert-written instructions. We take OFA as the base pre-trained model for multimodal instruction tuning, and to further improve its zero-shot performance, we explore multiple transfer learning strategies to leverage the large-scale Natural Instructions dataset. Experimental results demonstrate strong zero-shot performance on various unseen multimodal tasks and the benefit of transfer learning from a text-only instruction dataset. We also design a new evaluation metric – Sensitivity, to evaluate how sensitive the model is to the variety of instructions. Our results indicate that fine-tuning the model on a diverse set of tasks and instructions leads to a reduced sensitivity to variations in instructions for each task.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.641.pdf",
        "keywords": [
            "zero shot learning",
            "natural language",
            "transfer learning",
            "instruction tuning"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions \"fine-tunes pre-trained language models\" which implies the existence of limitations, however, it does not explicitly discuss them.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions \"fine-tunes pre-trained language models\" which implies the existence of limitations, however, it does not explicitly discuss them."
    },
    {
        "title": "Contrastive Error Attribution for Finetuned Language Models",
        "authors": [
            "Faisal Ladhak",
            "Esin Durmus",
            "Tatsunori Hashimoto"
        ],
        "published": "2023",
        "summary": "Recent work has identified noisy and misannotated data as a core cause of hallucinations and unfaithful outputs in Natural Language Generation (NLG) tasks. Consequently, identifying and removing these examples is a key open challenge in creating reliable NLG systems. In this work, we introduce a framework to identify and remove low-quality training instances that lead to undesirable outputs, such as faithfulness errors in text summarization. We show that existing approaches for error tracing, such as gradient-based influence measures, do not perform reliably for detecting faithfulness errors in NLG datasets. We overcome the drawbacks of existing error tracing methods through a new, contrast-based estimate that compares undesired generations to human-corrected outputs. Our proposed method can achieve a mean average precision of 0.93 at detecting known data errors across synthetic tasks with known ground truth, substantially outperforming existing approaches. Using this approach and re-training models on cleaned data leads to a 70% reduction in entity hallucinations on the NYT dataset and a 55% reduction in semantic errors on the E2E dataset.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.643.pdf",
        "keywords": [
            "finetuned",
            "natural language generation",
            "error attribution",
            "error tracing"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent work has identified noisy and misannotated data as a core cause of hallucinations and unfaithful outputs in Natural Language Generation (NLG) tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Recent work has identified noisy and misannotated data as a core cause of hallucinations and unfaithful outputs in Natural Language Generation (NLG) tasks.\""
    },
    {
        "title": "Query-Efficient Black-Box Red Teaming via Bayesian Optimization",
        "authors": [
            "Deokjae Lee",
            "JunYeong Lee",
            "Jung-Woo Ha",
            "Jin-Hwa Kim",
            "Sang-Woo Lee",
            "Hwaran Lee",
            "Hyun Oh Song"
        ],
        "published": "2023",
        "summary": "The deployment of large-scale generative models is often restricted by their potential risk of causing harm to users in unpredictable ways. We focus on the problem of black-box red teaming, where a red team generates test cases and interacts with the victim model to discover a diverse set of failures with limited query access. Existing red teaming methods construct test cases based on human supervision or language model (LM) and query all test cases in a brute-force manner without incorporating any information from past evaluations, resulting in a prohibitively large number of queries. To this end, we propose Bayesian red teaming (BRT), novel query-efficient black-box red teaming methods based on Bayesian optimization, which iteratively identify diverse positive test cases leading to model failures by utilizing the pre-defined user input pool and the past evaluations. Experimental results on various user input pools demonstrate that our method consistently finds a significantly larger number of diverse positive test cases under the limited query budget than the baseline methods.The source code is available at https://github.com/snu-mllab/Bayesian-Red-Teaming.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.646.pdf",
        "keywords": [
            "bayesian optimization",
            "black box red teaming"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing red teaming methods... query all test cases in a brute-force manner without incorporating any information from past evaluations, resulting in a prohibitively large number of queries.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing red teaming methods... query all test cases in a brute-force manner without incorporating any information from past evaluations, resulting in a prohibitively large number of queries.\""
    },
    {
        "title": "SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control",
        "authors": [
            "Xiaochuang Han",
            "Sachin Kumar",
            "Yulia Tsvetkov"
        ],
        "published": "2023",
        "summary": "Despite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive language models. In this work, we present SSD-LM—a diffusion-based language model with two key design choices. First, SSD-LM is semi-autoregressive, iteratively generating blocks of text, allowing for flexible output length at decoding time while enabling local bidirectional context updates. Second, it is simplex-based, performing diffusion on the natural vocabulary space rather than a learned latent space, allowing us to incorporate classifier guidance and modular control using off-the-shelf classifiers without any adaptation. We evaluate SSD-LM on unconstrained text generation benchmarks, and show that it matches or outperforms strong autoregressive GPT-2 models across standard quality and diversity metrics, while vastly outperforming diffusion-based baselines. On controlled text generation, SSD-LM also outperforms competitive baselines, with an extra advantage in modularity.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.647.pdf",
        "keywords": [
            "modular control",
            "diffusion",
            "semi autoregressive",
            "semi autoregressive simplex",
            "simplex",
            "diffusion language model",
            "language model",
            "ssd",
            "generation",
            "ssd lm"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive language models.\"\n\n(Note: The paper mentions a limitation of diffusion models in discrete domains, but it is not specifically about LLMs. However, the title mentions \"Language Model\" and the paper compares",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Despite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive language models.\"\n\n(Note: The paper mentions a limitation of diffusion models in discrete domains, but it is not specifically about LLMs. However, the title mentions \"Language Model\" and the paper compares"
    },
    {
        "title": "Recall, Expand, and Multi-Candidate Cross-Encode: Fast and Accurate Ultra-Fine Entity Typing",
        "authors": [
            "Chengyue Jiang",
            "Wenyang Hui",
            "Yong Jiang",
            "Xiaobin Wang",
            "Pengjun Xie",
            "Kewei Tu"
        ],
        "published": "2023",
        "summary": "Ultra-fine entity typing (UFET) predicts extremely free-formed types (e.g., president, politician) of a given entity mention (e.g., Joe Biden) in context. State-of-the-art (SOTA) methods use the cross-encoder (CE) based architecture. CE concatenates a mention (and its context) with each type and feeds the pair into a pretrained language model (PLM) to score their relevance. It brings deeper interaction between the mention and the type to reach better performance but has to perform N (the type set size) forward passes to infer all the types of a single mention. CE is therefore very slow in inference when the type set is large (e.g., N=10k for UFET). % Cross-encoder also ignores the correlation between different types.To this end, we propose to perform entity typing in a recall-expand-filter manner. The recall and expansion stages prune the large type set and generate K (typically much smaller than N) most relevant type candidates for each mention. At the filter stage, we use a novel model called {pasted macro ‘NAME’} to concurrently encode and score all these K candidates in only one forward pass to obtain the final type prediction. We investigate different model options for each stage and conduct extensive experiments to compare each option, experiments show that our method reaches SOTA performance on UFET and is thousands of times faster than the CE-based architecture. We also found our method is very effective in fine-grained (130 types) and coarse-grained (9 types) entity typing. Our code is available at {pasted macro ‘CODE’}.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.648.pdf",
        "keywords": [
            "recall",
            "entity typing",
            "cross encoder",
            "multi candidate cross encode",
            "ultra fine entity typing",
            "language model"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"CE is therefore very slow in inference when the type set is large (e.g., N=10k for UFET).\"\n\nThis abstract mentions a limitation of using cross-encoder based architecture with a pretrained language model, which is a type of LLM, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"CE is therefore very slow in inference when the type set is large (e.g., N=10k for UFET).\"\n\nThis abstract mentions a limitation of using cross-encoder based architecture with a pretrained language model, which is a type of LLM, but it is not the primary focus of the paper."
    },
    {
        "title": "GIFT: Graph-Induced Fine-Tuning for Multi-Party Conversation Understanding",
        "authors": [
            "Jia-Chen Gu",
            "Zhenhua Ling",
            "Quan Liu",
            "Cong Liu",
            "Guoping Hu"
        ],
        "published": "2023",
        "summary": "Addressing the issues of who saying what to whom in multi-party conversations (MPCs) has recently attracted a lot of research attention. However, existing methods on MPC understanding typically embed interlocutors and utterances into sequential information flows, or utilize only the superficial of inherent graph structures in MPCs. To this end, we present a plug-and-play and lightweight method named graph-induced fine-tuning (GIFT) which can adapt various Transformer-based pre-trained language models (PLMs) for universal MPC understanding. In detail, the full and equivalent connections among utterances in regular Transformer ignore the sparse but distinctive dependency of an utterance on another in MPCs. To distinguish different relationships between utterances, four types of edges are designed to integrate graph-induced signals into attention mechanisms to refine PLMs originally designed for processing sequential texts. We evaluate GIFT by implementing it into three PLMs, and test the performance on three downstream tasks including addressee recognition, speaker identification and response selection. Experimental results show that GIFT can significantly improve the performance of three PLMs on three downstream tasks and two benchmarks with only 4 additional parameters per encoding layer, achieving new state-of-the-art performance on MPC understanding.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.651.pdf",
        "keywords": [
            "fine tuning",
            "multi party conversations",
            "speaker identification",
            "gift",
            "conversation understanding",
            "graph induced fine tuning",
            "response selection"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the full and equivalent connections among utterances in regular Transformer ignore the sparse but distinctive dependency of an utterance on another in MPCs.\"\n\nThis rating is chosen because the paper mentions a limitation of regular Transformers (a type of LLM) in processing multi-party conversations, but it is not the primary focus of the paper and is only briefly mentioned to motivate the proposed method.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the full and equivalent connections among utterances in regular Transformer ignore the sparse but distinctive dependency of an utterance on another in MPCs.\"\n\nThis rating is chosen because the paper mentions a limitation of regular Transformers (a type of LLM) in processing multi-party conversations, but it is not the primary focus of the paper and is only briefly mentioned to motivate the proposed method."
    },
    {
        "title": "BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting",
        "authors": [
            "Zheng Xin Yong",
            "Hailey Schoelkopf",
            "Niklas Muennighoff",
            "Alham Fikri Aji",
            "David Ifeoluwa Adelani",
            "Khalid Almubarak",
            "M Saiful Bari",
            "Lintang Sutawika",
            "Jungo Kasai",
            "Ahmed Baruwa",
            "Genta Winata",
            "Stella Biderman",
            "Edward Raff",
            "Dragomir Radev",
            "Vassilina Nikoulina"
        ],
        "published": "2023",
        "summary": "The BLOOM model is a large publicly available multilingual language model, but its pretraining was limited to 46 languages. To extend the benefits of BLOOM to other languages without incurring prohibitively large costs, it is desirable to adapt BLOOM to new languages not seen during pretraining. In this work, we apply existing language adaptation strategies to BLOOM and benchmark its zero-shot prompting performance on eight new languages in a resource-constrained setting. We find language adaptation to be effective at improving zero-shot performance in new languages. Surprisingly, we find that adapter-based finetuning is more effective than continued pretraining for large models. In addition, we discover that prompting performance is not significantly affected by language specifics, such as the writing system. It is primarily determined by the size of the language adaptation data. We also add new languages to BLOOMZ, which is a multitask finetuned version of BLOOM capable of following task instructions zero-shot. We find including a new language in the multitask fine-tuning mixture to be the most effective method to teach BLOOMZ a new language. We conclude that with sufficient training data language adaptation can generalize well to diverse languages. Our code is available at https://github.com/bigscience-workshop/multilingual-modeling.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.653.pdf",
        "keywords": [
            "adapter",
            "language adaptation",
            "language",
            "finetuned",
            "prompting",
            "shot prompting",
            "bloom",
            "language model",
            "bloom model"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"To extend the benefits of BLOOM to other languages without incurring prohibitively large costs, it is desirable to adapt BLOOM to new languages not seen during pretraining.\"\n\nThis paper mentions the limitation of BLOOM in terms of its pretraining being limited to 46 languages, but it does not elaborate on this limitation further and instead focuses on proposing a solution to adapt",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"To extend the benefits of BLOOM to other languages without incurring prohibitively large costs, it is desirable to adapt BLOOM to new languages not seen during pretraining.\"\n\nThis paper mentions the limitation of BLOOM in terms of its pretraining being limited to 46 languages, but it does not elaborate on this limitation further and instead focuses on proposing a solution to adapt"
    },
    {
        "title": "Logic-driven Indirect Supervision: An Application to Crisis Counseling",
        "authors": [
            "Mattia Medina Grespan",
            "Meghan Broadbent",
            "Xinyao Zhang",
            "Katherine Axford",
            "Brent Kious",
            "Zac Imel",
            "Vivek Srikumar"
        ],
        "published": "2023",
        "summary": "Ensuring the effectiveness of text-based crisis counseling requires observing ongoing conversations and providing feedback, both labor-intensive tasks. Automatic analysis of conversations—at the full chat and utterance levels—may help support counselors and provide better care. While some session-level training data (e.g., rating of patient risk) is often available from counselors, labeling utterances requires expensive post hoc annotation. But the latter can not only provide insights about conversation dynamics, but can also serve to support quality assurance efforts for counselors. In this paper, we examine if inexpensive—and potentially noisy—session-level annotation can help improve label utterances. To this end, we propose a logic-based indirect supervision approach that exploits declaratively stated structural dependencies between both levels of annotation to improve utterance modeling. We show that adding these rules gives an improvement of 3.5% f-score over a strong multi-task baseline for utterance-level predictions. We demonstrate via ablation studies how indirect supervision via logic rules also improves the consistency and robustness of the system.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.654.pdf",
        "keywords": [
            "indirect supervision",
            "text based crisis counseling",
            "crisis counseling",
            "modeling"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models",
        "authors": [
            "Shangbin Feng",
            "Chan Young Park",
            "Yuhan Liu",
            "Yulia Tsvetkov"
        ],
        "published": "2023",
        "summary": "Language models (LMs) are pretrained on diverse data sources—news, discussion forums, books, online encyclopedias. A significant portion of this data includes facts and opinions which, on one hand, celebrate democracy and diversity of ideas, and on the other hand are inherently socially biased. Our work develops new methods to (1) measure media biases in LMs trained on such corpora, along social and economic axes, and (2) measure the fairness of downstream NLP models trained on top of politically biased LMs. We focus on hate speech and misinformation detection, aiming to empirically quantify the effects of political (social, economic) biases in pretraining data on the fairness of high-stakes social-oriented tasks. Our findings reveal that pretrained LMs do have political leanings which reinforce the polarization present in pretraining corpora, propagating social biases into hate speech predictions and media biases into misinformation detectors. We discuss the implications of our findings for NLP research and propose future directions to mitigate unfairness.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.656.pdf",
        "keywords": [
            "political biases",
            "language models",
            "misinformation detection",
            "pretraining data",
            "pretraining corpora"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our findings reveal that pretrained LMs do have political leanings which reinforce the polarization present in pretraining corpora, propagating social biases into hate speech predictions and media biases into misinformation detectors.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Our findings reveal that pretrained LMs do have political leanings which reinforce the polarization present in pretraining corpora, propagating social biases into hate speech predictions and media biases into misinformation detectors.\""
    },
    {
        "title": "Contrastive Novelty-Augmented Learning: Anticipating Outliers with Large Language Models",
        "authors": [
            "Albert Xu",
            "Xiang Ren",
            "Robin Jia"
        ],
        "published": "2023",
        "summary": "In many task settings, text classification models are likely to encounter examples from novel classes on which they cannot predict correctly. Selective prediction, in which models abstain on low-confidence examples, provides a possible solution, but existing models are often overly confident on unseen classes. To remedy this overconfidence, we introduce Contrastive Novelty-Augmented Learning (CoNAL), a two-step method that generates OOD examples representative of novel classes, then trains to decrease confidence on them. First, we generate OOD examples by prompting a large language model twice: we prompt it to enumerate relevant novel classes, then generate examples from each novel class matching the task format. Second, we train a classifier with a novel contrastive objective that encourages lower confidence on generated OOD examples than training examples. When trained with CoNAL, classifiers improve in their ability to detect and abstain on novel class examples over prior methods by an average of 2.3% in terms of accuracy under the accuracy-coverage curve (AUAC) and 5.5% AUROC across 4 NLP datasets, with no cost to in-distribution accuracy.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.658.pdf",
        "keywords": [
            "language",
            "language models",
            "outliers",
            "novelty",
            "selective prediction",
            "text classification",
            "train"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"existing models are often overly confident on unseen classes.\"\n\nThis paper mentions a limitation of LLMs (overconfidence on unseen classes) but does not explore it in depth and focuses on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"existing models are often overly confident on unseen classes.\"\n\nThis paper mentions a limitation of LLMs (overconfidence on unseen classes) but does not explore it in depth and focuses on the proposed solution."
    },
    {
        "title": "Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?",
        "authors": [
            "Chengwei Qin",
            "Shafiq Joty",
            "Qian Li",
            "Ruochen Zhao"
        ],
        "published": "2023",
        "summary": "Prompt tuning (PT) which only tunes the embeddings of an additional sequence of tokens per task, keeping the pre-trained language model (PLM) frozen, has shown remarkable performance in few-shot learning. Despite this, PT has been shown to rely heavily on good initialization of the prompt embeddings. In this work, we study meta prompt tuning (MPT) to systematically explore how meta-learning can help improve (if it can) cross-task generalization in PT through learning to initialize the prompt embeddings from other relevant tasks. We empirically analyze a representative set of meta learning algorithms in a wide range of adaptation settings with different source/target task configurations on a large set of few-shot tasks. With extensive experiments and analysis, we demonstrate the effectiveness of MPT. We find the improvement to be significant particularly on classification tasks. For other kinds of tasks such as question answering, we observe that while MPT can outperform PT in most cases, it does not always outperform multi-task learning. We further provide an in-depth analysis from the perspective of task similarity.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.659.pdf",
        "keywords": [
            "meta learning",
            "multi task learning",
            "generalization",
            "cross"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite this, PT has been shown to rely heavily on good initialization of the prompt embeddings.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite this, PT has been shown to rely heavily on good initialization of the prompt embeddings.\""
    },
    {
        "title": "Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale",
        "authors": [
            "Hritik Bansal",
            "Karthik Gopalakrishnan",
            "Saket Dingliwal",
            "Sravan Bodapati",
            "Katrin Kirchhoff",
            "Dan Roth"
        ],
        "published": "2023",
        "summary": "Language models have been shown to perform better with an increase in scale on a wide variety of tasks via the in-context learning paradigm. In this paper, we investigate the hypothesis that the ability of a large language model to in-context learn-perform a task is not uniformly spread across all of its underlying components. Using a 66 billion parameter language model (OPT-66B) across a diverse set of 14 downstream tasks, we find this is indeed the case: ~70% of the attention heads and ~20% of the feed forward networks can be removed with minimal decline in task performance. We find substantial overlap in the set of attention heads (un)important for in-context learning across tasks and number of in-context examples. We also address our hypothesis through a task-agnostic lens, finding that a small set of attention heads in OPT-66B score highly on their ability to perform primitive induction operations associated with in-context learning, namely, prefix matching and copying. These induction heads overlap with task-specific important heads, reinforcing arguments by Olsson et al. (2022) regarding induction head generality to more sophisticated behaviors associated with in-context learning. Overall, our study provides several insights that indicate large language models may be under-trained for in-context learning and opens up questions on how to pre-train language models to more effectively perform in-context learning.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.660.pdf",
        "keywords": [
            "context",
            "language",
            "context learning",
            "in context learning",
            "in context learn perform",
            "language models",
            "in context examples",
            "prefix matching",
            "interpretability based case study",
            "scale"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Overall, our study provides several insights that indicate large language models may be under-trained for in-context learning and opens up questions on how to pre-train language models to more effectively perform in-context learning.\"\n\nThis rating is given because the paper discusses limitations of LLMs in the context of in-context learning, but the primary focus is on the study and its findings rather than the limitations",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Overall, our study provides several insights that indicate large language models may be under-trained for in-context learning and opens up questions on how to pre-train language models to more effectively perform in-context learning.\"\n\nThis rating is given because the paper discusses limitations of LLMs in the context of in-context learning, but the primary focus is on the study and its findings rather than the limitations"
    },
    {
        "title": "ESCOXLM-R: Multilingual Taxonomy-driven Pre-training for the Job Market Domain",
        "authors": [
            "Mike Zhang",
            "Rob van der Goot",
            "Barbara Plank"
        ],
        "published": "2023",
        "summary": "The increasing number of benchmarks for Natural Language Processing (NLP) tasks in the computational job market domain highlights the demand for methods that can handle job-related tasks such as skill extraction, skill classification, job title classification, and de-identification. While some approaches have been developed that are specific to the job market domain, there is a lack of generalized, multilingual models and benchmarks for these tasks. In this study, we introduce a language model called ESCOXLM-R, based on XLM-R-large, which uses domain-adaptive pre-training on the European Skills, Competences, Qualifications and Occupations (ESCO) taxonomy, covering 27 languages. The pre-training objectives for ESCOXLM-R include dynamic masked language modeling and a novel additional objective for inducing multilingual taxonomical ESCO relations. We comprehensively evaluate the performance of ESCOXLM-R on 6 sequence labeling and 3 classification tasks in 4 languages and find that it achieves state-of-the-art results on 6 out of 9 datasets. Our analysis reveals that ESCOXLM-R performs better on short spans and outperforms XLM-R-large on entity-level and surface-level span-F1, likely due to ESCO containing short skill and occupation titles, and encoding information on the entity-level.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.662.pdf",
        "keywords": [
            "esco",
            "taxonomy",
            "escoxlm",
            "job market domain",
            "esco relations",
            "escoxlm r"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation of LLMs is mentioned in the abstract, but it implies that existing models may not perform well on job-related tasks or may not be generalized or multilingual, as it introduces a new model to address these issues.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation of LLMs is mentioned in the abstract, but it implies that existing models may not perform well on job-related tasks or may not be generalized or multilingual, as it introduces a new model to address these issues."
    },
    {
        "title": "Generic Temporal Reasoning with Differential Analysis and Explanation",
        "authors": [
            "Yu Feng",
            "Ben Zhou",
            "Haoyu Wang",
            "Helen Jin",
            "Dan Roth"
        ],
        "published": "2023",
        "summary": "Temporal reasoning is the task of predicting temporal relations of event pairs. While temporal reasoning models can perform reasonably well on in-domain benchmarks, we have little idea of these systems’ generalizability due to existing datasets’ limitations. In this work, we introduce a novel task named TODAY that bridges this gap with temporal differential analysis, which as the name suggests, evaluates whether systems can correctly understand the effect of incremental changes. Specifically, TODAY introduces slight contextual changes for given event pairs, and systems are asked to tell how this subtle contextual change would affect relevant temporal relation distributions. To facilitate learning, TODAY also annotates human explanations. We show that existing models, including GPT-3.5, drop to random guessing on TODAY, suggesting that they heavily rely on spurious information rather than proper reasoning for temporal predictions. On the other hand, we show that TODAY’s supervision style and explanation annotations can be used in joint learning, encouraging models to use more appropriate signals during training and thus outperform across several benchmarks. TODAY can also be used to train models to solicit incidental supervision from noisy sources such as GPT-3.5, thus moving us more toward the goal of generic temporal reasoning systems.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.671.pdf",
        "keywords": [
            "temporal reasoning",
            "temporal relation",
            "temporal differential analysis",
            "differential analysis",
            "explanation annotations"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We show that existing models, including GPT-3.5, drop to random guessing on TODAY, suggesting that they heavily rely on spurious information rather than proper reasoning for temporal predictions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We show that existing models, including GPT-3.5, drop to random guessing on TODAY, suggesting that they heavily rely on spurious information rather than proper reasoning for temporal predictions.\""
    },
    {
        "title": "On the Blind Spots of Model-Based Evaluation Metrics for Text Generation",
        "authors": [
            "Tianxing He",
            "Jingyu Zhang",
            "Tianle Wang",
            "Sachin Kumar",
            "Kyunghyun Cho",
            "James Glass",
            "Yulia Tsvetkov"
        ],
        "published": "2023",
        "summary": "In this work, we explore a useful but often neglected methodology for robustness analysis of text generation evaluation metrics: stress tests with synthetic data. Basically, we design and synthesize a wide range of potential errors and check whether they result in a commensurate drop in the metric scores. We examine a range of recently proposed evaluation metrics based on pretrained language models, for the tasks of open-ended generation, translation, and summarization. Our experiments reveal interesting insensitivities, biases, or even loopholes in existing metrics. For example, we find that BERTScore is confused by truncation errors in summarization, and MAUVE (built on top of GPT-2) is insensitive to errors at the beginning or middle of generations. Further, we investigate the reasons behind these blind spots and suggest practical workarounds for a more reliable evaluation of text generation. We have released our code and data at https://github.com/cloudygoose/blindspot_nlg.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.674.pdf",
        "keywords": [
            "blind spots",
            "text generation",
            "text generation evaluation metrics",
            "evaluation metrics"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our experiments reveal interesting insensitivities, biases, or even loopholes in existing metrics. For example, we find that BERTScore is confused by truncation errors in summarization, and MAUVE (built on top of GPT-2) is insensitive to errors at the beginning or middle of generations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Our experiments reveal interesting insensitivities, biases, or even loopholes in existing metrics. For example, we find that BERTScore is confused by truncation errors in summarization, and MAUVE (built on top of GPT-2) is insensitive to errors at the beginning or middle of generations.\""
    },
    {
        "title": "DSRM: Boost Textual Adversarial Training with Distribution Shift Risk Minimization",
        "authors": [
            "SongYang Gao",
            "Shihan Dou",
            "Yan Liu",
            "Xiao Wang",
            "Qi Zhang",
            "Zhongyu Wei",
            "Jin Ma",
            "Ying Shan"
        ],
        "published": "2023",
        "summary": "Adversarial training is one of the best-performing methods in improving the robustness of deep language models. However, robust models come at the cost of high time consumption, as they require multi-step gradient ascents or word substitutions to obtain adversarial samples. In addition, these generated samples are deficient in grammatical quality and semantic consistency, which impairs the effectiveness of adversarial training. To address these problems, we introduce a novel, effective procedure for instead adversarial training with only clean data. Our procedure, distribution shift risk minimization (DSRM), estimates the adversarial loss by perturbing the input data’s probability distribution rather than their embeddings. This formulation results in a robust model that minimizes the expected global loss under adversarial attacks. Our approach requires zero adversarial samples for training and reduces time consumption by up to 70% compared to current best-performing adversarial training methods. Experiments demonstrate that DSRM considerably improves BERT’s resistance to textual adversarial attacks and achieves state-of-the-art robust accuracy on various benchmarks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.680.pdf",
        "keywords": [
            "adversarial training",
            "adversarial",
            "distribution shift risk minimization",
            "textual adversarial training",
            "robust",
            "textual adversarial attacks",
            "quality"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, robust models come at the cost of high time consumption, as they require multi-step gradient ascents or word substitutions to obtain adversarial samples. In addition, these generated samples are deficient in grammatical quality and semantic consistency, which impairs the effectiveness of adversarial training.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, robust models come at the cost of high time consumption, as they require multi-step gradient ascents or word substitutions to obtain adversarial samples. In addition, these generated samples are deficient in grammatical quality and semantic consistency, which impairs the effectiveness of adversarial training.\""
    },
    {
        "title": "XDailyDialog: A Multilingual Parallel Dialogue Corpus",
        "authors": [
            "Zeming Liu",
            "Ping Nie",
            "Jie Cai",
            "Haifeng Wang",
            "Zheng-Yu Niu",
            "Peng Zhang",
            "Mrinmaya Sachan",
            "Kaiping Peng"
        ],
        "published": "2023",
        "summary": "High-quality datasets are significant to the development of dialogue models. However, most existing datasets for open-domain dialogue modeling are limited to a single language. The absence of multilingual open-domain dialog datasets not only limits the research on multilingual or cross-lingual transfer learning, but also hinders the development of robust open-domain dialog systems that can be deployed in other parts of the world. In this paper, we provide a multilingual parallel open-domain dialog dataset, XDailyDialog, to enable researchers to explore the challenging task of multilingual and cross-lingual open-domain dialog. XDailyDialog includes 13K dialogues aligned across 4 languages (52K dialogues and 410K utterances in total). We then propose a dialog generation model, kNN-Chat, which has a novel kNN-search mechanism to support unified response retrieval for monolingual, multilingual, and cross-lingual dialogue. Experiment results show the effectiveness of this framework. We will make XDailyDialog and kNN-Chat publicly available soon.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.684.pdf",
        "keywords": [
            "dialogues",
            "dialogue modeling",
            "parallel dialogue corpus",
            "open domain dialog",
            "multilingual"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "PAL to Lend a Helping Hand: Towards Building an Emotion Adaptive Polite and Empathetic Counseling Conversational Agent",
        "authors": [
            "Kshitij Mishra",
            "Priyanshu Priya",
            "Asif Ekbal"
        ],
        "published": "2023",
        "summary": "The World Health Organization (WHO) has significantly emphasized the need for mental health care. The social stigma associated with mental illness prevents individuals from addressing their issues and getting assistance. In such a scenario, the relevance of online counseling has increased dramatically. The feelings and attitudes that a client and a counselor express towards each other result in a higher or lower counseling experience. A counselor should be friendly and gain clients’ trust to make them share their problems comfortably. Thus, it is essential for the counselor to adequately comprehend the client’s emotions and ensure client’s welfare, i.e. s/he should adapt and deal with the clients politely and empathetically to provide a pleasant, cordial and personalized experience. Motivated by this, in this work, we attempt to build a novel Polite and empAthetic counseLing conversational agent PAL to lay down the counseling support to substance addict and crime victims. To have client’s emotion-based polite and empathetic responses, two counseling datasets laying down the counseling support to substance addicts and crime victims are annotated. These annotated datasets are used to build PAL in a reinforcement learning framework. A novel reward function is formulated to ensure correct politeness and empathy preferences as per client’s emotions with naturalness and non-repetitiveness in responses. Thorough automatic and human evaluation showcase the usefulness and strength of the designed novel reward function. Our proposed system is scalable and can be easily modified with different modules of preference models as per need.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.685.pdf",
        "keywords": [
            "politeness",
            "reinforcement learning",
            "empathy",
            "counselor",
            "emotion",
            "social stigma",
            "pal",
            "personalized"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Contrastive Decoding: Open-ended Text Generation as Optimization",
        "authors": [
            "Xiang Lisa Li",
            "Ari Holtzman",
            "Daniel Fried",
            "Percy Liang",
            "Jason Eisner",
            "Tatsunori Hashimoto",
            "Luke Zettlemoyer",
            "Mike Lewis"
        ],
        "published": "2023",
        "summary": "Given a language model (LM), maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text. On the other hand, sampling can often produce incoherent text that drifts from the original topics. We propose contrastive decoding (CD), a reliable decoding approach that optimizes a contrastive objective subject to a plausibility constraint. The contrastive objective returns the difference between the likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM (called the amateur, e.g. OPT-125M), and the constraint ensures that the outputs are plausible. CD is inspired by the fact that the failures of larger LMs (e.g., repetition, inco- herence) are even more prevalent in smaller LMs, and that this difference signals which texts should be preferred. CD requires zero additional training, and produces higher quality text than decoding from the larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and significantly outperforms four strong decoding algorithms (e.g., nucleus, top-k) in automatic and human evaluations across wikipedia, news and story domains.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.687.pdf",
        "keywords": [
            "text generation",
            "strong decoding",
            "model"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Given a language model (LM), maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Given a language model (LM), maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text.\""
    },
    {
        "title": "Resolving Indirect Referring Expressions for Entity Selection",
        "authors": [
            "Mohammad Javad Hosseini",
            "Filip Radlinski",
            "Silvia Pareti",
            "Annie Louis"
        ],
        "published": "2023",
        "summary": "Recent advances in language modeling have enabled new conversational systems. In particular, it is often desirable for people to make choices among specified options when using such systems. We address the problem of reference resolution, when people use natural expressions to choose between real world entities. For example, given the choice ‘Should we make a Simnel cake or a Pandan cake¿ a natural response from a non-expert may be indirect: ‘let’s make the green one‘. Reference resolution has been little studied with natural expressions, thus robustly understanding such language has large potential for improving naturalness in dialog, recommendation, and search systems. We create AltEntities (Alternative Entities), a new public dataset of entity pairs and utterances, and develop models for the disambiguation problem. Consisting of 42K indirect referring expressions across three domains, it enables for the first time the study of how large language models can be adapted to this task. We find they achieve 82%-87% accuracy in realistic settings, which while reasonable also invites further advances.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.688.pdf",
        "keywords": [
            "reference resolution",
            "entity selection",
            "referring expressions",
            "language models",
            "natural expressions",
            "indirect referring expressions"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find they achieve 82%-87% accuracy in realistic settings, which while reasonable also invites further advances.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We find they achieve 82%-87% accuracy in realistic settings, which while reasonable also invites further advances.\""
    },
    {
        "title": "Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages",
        "authors": [
            "Sumanth Doddapaneni",
            "Rahul Aralikatte",
            "Gowtham Ramesh",
            "Shreya Goyal",
            "Mitesh M. Khapra",
            "Anoop Kunchukuttan",
            "Pratyush Kumar"
        ],
        "published": "2023",
        "summary": "Building Natural Language Understanding (NLU) capabilities for Indic languages, which have a collective speaker base of more than one billion speakers is absolutely crucial. In this work, we aim to improve the NLU capabilities of Indic languages by making contributions along 3 important axes (i) monolingual corpora (ii) NLU testsets (iii) multilingual LLMs focusing on Indic languages. Specifically, we curate the largest monolingual corpora, IndicCorp, with 20.9B tokens covering 24 languages from 4 language families - a 2.3x increase over prior work, while supporting 12 additional languages. Next, we create a human-supervised benchmark, IndicXTREME, consisting of nine diverse NLU tasks covering 20 languages. Across languages and tasks, IndicXTREME contains a total of 105 evaluation sets, of which 52 are new contributions to the literature. To the best of our knowledge, this is the first effort towards creating a standard benchmark for Indic languages that aims to test the multilingual zero-shot capabilities of pretrained language models. Finally, we train IndicBERT v2, a state-of-the-art model supporting all the languages. Averaged across languages and tasks, the model achieves an absolute improvement of 2 points over a strong baseline. The data and models are available at https://github.com/AI4Bharat/IndicBERT.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.693.pdf",
        "keywords": [
            "monolingual corpora",
            "indic languages",
            "benchmark",
            "natural language"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicitly mentioned, but the creation of a new benchmark and model suggests that existing LLMs may have limitations in handling Indic languages, which is the motivation for the work.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None explicitly mentioned, but the creation of a new benchmark and model suggests that existing LLMs may have limitations in handling Indic languages, which is the motivation for the work."
    },
    {
        "title": "Hybrid Transducer and Attention based Encoder-Decoder Modeling for Speech-to-Text Tasks",
        "authors": [
            "Yun Tang",
            "Anna Sun",
            "Hirofumi Inaguma",
            "Xinyue Chen",
            "Ning Dong",
            "Xutai Ma",
            "Paden Tomasello",
            "Juan Pino"
        ],
        "published": "2023",
        "summary": "Transducer and Attention based Encoder-Decoder (AED) are two widely used frameworks for speech-to-text tasks. They are designed for different purposes and each has its own benefits and drawbacks for speech-to-text tasks. In order to leverage strengths of both modeling methods, we propose a solution by combining Transducer and Attention based Encoder-Decoder (TAED) for speech-to-text tasks. The new method leverages AED’s strength in non-monotonic sequence to sequence learning while retaining Transducer’s streaming property. In the proposed framework, Transducer and AED share the same speech encoder. The predictor in Transducer is replaced by the decoder in the AED model, and the outputs of the decoder are conditioned on the speech inputs instead of outputs from an unconditioned language model. The proposed solution ensures that the model is optimized by covering all possible read/write scenarios and creates a matched environment for streaming applications. We evaluate the proposed approach on the MuST-C dataset and the findings demonstrate that TAED performs significantly better than Transducer for offline automatic speech recognition (ASR) and speech-to-text translation (ST) tasks. In the streaming case, TAED outperforms Transducer in the ASR task and one ST direction while comparable results are achieved in another translation direction.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.695.pdf",
        "keywords": [
            "encoder",
            "speech encoder",
            "speech to text tasks",
            "speech to text translation",
            "transducer",
            "automatic speech recognition",
            "hybrid transducer",
            "attention",
            "transducer and attention",
            "encoder decoder modeling"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "What’s the Meaning of Superhuman Performance in Today’s NLU?",
        "authors": [
            "Simone Tedeschi",
            "Johan Bos",
            "Thierry Declerck",
            "Jan Hajič",
            "Daniel Hershcovich",
            "Eduard Hovy",
            "Alexander Koller",
            "Simon Krek",
            "Steven Schockaert",
            "Rico Sennrich",
            "Ekaterina Shutova",
            "Roberto Navigli"
        ],
        "published": "2023",
        "summary": "In the last five years, there has been a significant focus in Natural Language Processing (NLP) on developing larger Pretrained Language Models (PLMs) and introducing benchmarks such as SuperGLUE and SQuAD to measure their abilities in language understanding, reasoning, and reading comprehension. These PLMs have achieved impressive results on these benchmarks, even surpassing human performance in some cases. This has led to claims of superhuman capabilities and the provocative idea that certain tasks have been solved. In this position paper, we take a critical look at these claims and ask whether PLMs truly have superhuman abilities and what the current benchmarks are really evaluating. We show that these benchmarks have serious limitations affecting the comparison between humans and PLMs and provide recommendations for fairer and more transparent benchmarks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.697.pdf",
        "keywords": [
            "superhuman abilities",
            "plms",
            "benchmarks"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We show that these benchmarks have serious limitations affecting the comparison between humans and PLMs\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We show that these benchmarks have serious limitations affecting the comparison between humans and PLMs\""
    },
    {
        "title": "PromptNER: Prompt Locating and Typing for Named Entity Recognition",
        "authors": [
            "Yongliang Shen",
            "Zeqi Tan",
            "Shuhui Wu",
            "Wenqi Zhang",
            "Rongsheng Zhang",
            "Yadong Xi",
            "Weiming Lu",
            "Yueting Zhuang"
        ],
        "published": "2023",
        "summary": "Prompt learning is a new paradigm for utilizing pre-trained language models and has achieved great success in many tasks. To adopt prompt learning in the NER task, two kinds of methods have been explored from a pair of symmetric perspectives, populating the template by enumerating spans to predict their entity types or constructing type-specific prompts to locate entities. However, these methods not only require a multi-round prompting manner with a high time overhead and computational cost, but also require elaborate prompt templates, that are difficult to apply in practical scenarios. In this paper, we unify entity locating and entity typing into prompt learning, and design a dual-slot multi-prompt template with the position slot and type slot to prompt locating and typing respectively. Multiple prompts can be input to the model simultaneously, and then the model extracts all entities by parallel predictions on the slots. To assign labels for the slots during training, we design a dynamic template filling mechanism that uses the extended bipartite graph matching between prompts and the ground-truth entities. We conduct experiments in various settings, including resource-rich flat and nested NER datasets and low-resource in-domain and cross-domain datasets. Experimental results show that the proposed model achieves a significant performance improvement, especially in the cross-domain few-shot setting, which outperforms the state-of-the-art model by +7.7% on average.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.698.pdf",
        "keywords": [
            "typing",
            "named entity recognition",
            "locating and typing",
            "prompt locating",
            "promptner",
            "learning"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these methods not only require a multi-round prompting manner with a high time overhead and computational cost, but also require elaborate prompt templates, that are difficult to apply in practical scenarios.\"\n\nThis paper discusses LLMs, but only mentions limitations of prompt learning methods in passing, without elaborating on the limitations of LLMs themselves.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, these methods not only require a multi-round prompting manner with a high time overhead and computational cost, but also require elaborate prompt templates, that are difficult to apply in practical scenarios.\"\n\nThis paper discusses LLMs, but only mentions limitations of prompt learning methods in passing, without elaborating on the limitations of LLMs themselves."
    },
    {
        "title": "Hints on the data for language modeling of synthetic languages with transformers",
        "authors": [
            "Rodolfo Zevallos",
            "Nuria Bel"
        ],
        "published": "2023",
        "summary": "Language Models (LM) are becoming more and more useful for providing representations upon which to train Natural Language Processing applications. However, there is now clear evidence that attention-based transformers require a critical amount of language data to produce good enough LMs. The question we have addressed in this paper is to what extent the critical amount of data varies for languages of different morphological typology, in particular those that have a rich inflectional morphology, and whether the tokenization method to preprocess the data can make a difference. These details can be important for low-resourced languages that need to plan the production of datasets. We evaluated intrinsically and extrinsically the differences of five different languages with different pretraining dataset sizes and three different tokenization methods for each. The results confirm that the size of the vocabulary due to morphological characteristics is directly correlated with both the LM perplexity and the performance of two typical downstream tasks such as NER identification and POS labeling. The experiments also provide new evidence that a canonical tokenizer can reduce perplexity by more than a half for a polysynthetic language like Quechua as well as raising F1 from 0.8 to more than 0.9 in both downstream tasks with a LM trained with only 6M tokens.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.699.pdf",
        "keywords": [
            "transformers",
            "language",
            "language modeling",
            "synthetic languages",
            "tokenization",
            "morphological typology"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, there is now clear evidence that attention-based transformers require a critical amount of language data to produce good enough LMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, there is now clear evidence that attention-based transformers require a critical amount of language data to produce good enough LMs.\""
    },
    {
        "title": "Large-scale Lifelong Learning of In-context Instructions and How to Tackle It",
        "authors": [
            "Jisoo Mok",
            "Jaeyoung Do",
            "Sungjin Lee",
            "Tara Taghavi",
            "Seunghak Yu",
            "Sungroh Yoon"
        ],
        "published": "2023",
        "summary": "Jointly fine-tuning a Pre-trained Language Model (PLM) on a pre-defined set of tasks with in-context instructions has been proven to improve its generalization performance, allowing us to build a universal language model that can be deployed across task boundaries. In this work, we explore for the first time whether this attractive property of in-context instruction learning can be extended to a scenario in which tasks are fed to the target PLM in a sequential manner. The primary objective of so-called lifelong in-context instruction learning is to improve the target PLM’s instance- and task-level generalization performance as it observes more tasks. DynaInst, the proposed method to lifelong in-context instruction learning, achieves noticeable improvements in both types of generalization, nearly reaching the upper bound performance obtained through joint training.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.703.pdf",
        "keywords": [
            "lifelong learning"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper discusses a method to improve the generalization performance of a pre-trained Language Model, implying that generalization is a limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper discusses a method to improve the generalization performance of a pre-trained Language Model, implying that generalization is a limitation."
    },
    {
        "title": "DT-Solver: Automated Theorem Proving with Dynamic-Tree Sampling Guided by Proof-level Value Function",
        "authors": [
            "Haiming Wang",
            "Ye Yuan",
            "Zhengying Liu",
            "Jianhao Shen",
            "Yichun Yin",
            "Jing Xiong",
            "Enze Xie",
            "Han Shi",
            "Yujun Li",
            "Lin Li",
            "Jian Yin",
            "Zhenguo Li",
            "Xiaodan Liang"
        ],
        "published": "2023",
        "summary": "Recent advances in neural theorem-proving resort to large language models and tree searches. When proving a theorem, a language model advises single-step actions based on the current proving state and the tree search finds a sequence of correct steps using actions given by the language model. However, prior works often conduct constant computation efforts for each proving state while ignoring that the hard states often need more exploration than easy states. Moreover, they evaluate and guide the proof search solely depending on the current proof state instead of considering the whole proof trajectory as human reasoning does. Here, to accommodate general theorems, we propose a novel Dynamic-Tree Driven Theorem Solver (DT-Solver) by guiding the search procedure with state confidence and proof-level values. Specifically, DT-Solver introduces a dynamic-tree Monte-Carlo search algorithm, which dynamically allocates computing budgets for different state confidences, guided by a new proof-level value function to discover proof states that require substantial exploration. Experiments on two popular theorem-proving datasets, PISA and Mathlib, show significant performance gains by our DT-Solver over the state-of-the-art approaches, with a 6.65% improvement on average in terms of success rate. And especially under low computing resource settings (11.03% improvement on average).",
        "pdf_link": "https://aclanthology.org/2023.acl-long.706.pdf",
        "keywords": [
            "theorem proving",
            "proof level value function",
            "automated theorem proving",
            "dt solver",
            "solver",
            "dynamic tree sampling guided",
            "tree driven"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, prior works often conduct constant computation efforts for each proving state while ignoring that the hard states often need more exploration than easy states. Moreover, they evaluate and guide the proof search solely depending on the current proof state instead of considering the whole proof trajectory as human reasoning does.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, prior works often conduct constant computation efforts for each proving state while ignoring that the hard states often need more exploration than easy states. Moreover, they evaluate and guide the proof search solely depending on the current proof state instead of considering the whole proof trajectory as human reasoning does.\""
    },
    {
        "title": "Understanding In-Context Learning via Supportive Pretraining Data",
        "authors": [
            "Xiaochuang Han",
            "Daniel Simig",
            "Todor Mihaylov",
            "Yulia Tsvetkov",
            "Asli Celikyilmaz",
            "Tianlu Wang"
        ],
        "published": "2023",
        "summary": "In-context learning (ICL) improves language models’ performance on a variety of NLP tasks by simply demonstrating a handful of examples at inference time. It is not well understood why ICL ability emerges, as the model has never been specifically trained on such demonstrations. Unlike prior work that explores implicit mechanisms behind ICL, we study ICL via investigating the pretraining data. Specifically, we first adapt an iterative, gradient-based approach to find a small subset of pretraining data that supports ICL. We observe that a continued pretraining on this small subset significantly improves the model’s ICL ability, by up to 18%. We then compare the supportive subset constrastively with random subsets of pretraining data and discover: (1) The supportive pretraining data to ICL do not have a higher domain relevance to downstream tasks. (2) The supportive pretraining data have a higher mass of rarely occurring, long-tail tokens. (3) The supportive pretraining data are challenging examples where the information gain from long-range context is below average, indicating learning to incorporate difficult long-range context encourages ICL. Our work takes a first step towards understanding ICL via analyzing instance-level pretraining data. Our insights have a potential to enhance the ICL ability of language models by actively guiding the construction of pretraining data in the future.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.708.pdf",
        "keywords": [
            "pretraining data",
            "supportive pretraining data",
            "context learning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The supportive pretraining data are challenging examples where the information gain from long-range context is below average, indicating learning to incorporate difficult long-range context encourages ICL.\"\n\nThis rating is based on the fact that the paper mentions a limitation of language models in the context of in-context learning (ICL), specifically that they struggle to incorporate difficult long-range context. However, this limitation is not",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The supportive pretraining data are challenging examples where the information gain from long-range context is below average, indicating learning to incorporate difficult long-range context encourages ICL.\"\n\nThis rating is based on the fact that the paper mentions a limitation of language models in the context of in-context learning (ICL), specifically that they struggle to incorporate difficult long-range context. However, this limitation is not"
    },
    {
        "title": "ETHICIST: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation",
        "authors": [
            "Zhexin Zhang",
            "Jiaxin Wen",
            "Minlie Huang"
        ],
        "published": "2023",
        "summary": "Large pre-trained language models achieve impressive results across many tasks. However, recent works point out that pre-trained language models may memorize a considerable fraction of their training data, leading to the privacy risk of information leakage. In this paper, we propose a method named Ethicist for targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation, investigating how to recover the suffix in the training data when given a prefix. To elicit memorization in the attacked model, we tune soft prompt embeddings while keeping the model fixed. We further propose a smoothing loss that smooths the loss distribution of the suffix tokens to make it easier to sample the correct suffix. In order to select the most probable suffix from a collection of sampled suffixes and estimate the prediction confidence, we propose a calibrated confidence estimation method, which normalizes the confidence of the generated suffixes with a local estimation. We show that Ethicist significantly improves the extraction performance on a recently proposed public benchmark. We also investigate several factors influencing the data extraction performance, including decoding strategy, model scale, prefix length, and suffix length. Our code is availabel at https://github.com/thu-coai/Targeted-Data-Extraction.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.709.pdf",
        "keywords": [
            "data extraction",
            "loss smoothed soft prompting",
            "targeted data extraction",
            "targeted training data extraction",
            "calibrated confidence estimation",
            "trained language"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, recent works point out that pre-trained language models may memorize a considerable fraction of their training data, leading to the privacy risk of information leakage.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, recent works point out that pre-trained language models may memorize a considerable fraction of their training data, leading to the privacy risk of information leakage.\""
    },
    {
        "title": "Effective Contrastive Weighting for Dense Query Expansion",
        "authors": [
            "Xiao Wang",
            "Sean MacAvaney",
            "Craig Macdonald",
            "Iadh Ounis"
        ],
        "published": "2023",
        "summary": "Verbatim queries submitted to search engines often do not sufficiently describe the user’s search intent. Pseudo-relevance feedback (PRF) techniques, which modify a query’srepresentation using the top-ranked documents, have been shown to overcome such inadequacies and improve retrieval effectiveness for both lexical methods (e.g., BM25) and dense methods (e.g., ANCE, ColBERT). For instance, the recent ColBERT-PRF approach heuristically chooses new embeddings to add to the query representation using the inverse document frequency (IDF) of the underlying tokens. However, this heuristic potentially ignores the valuable context encoded by the embeddings. In this work, we present a contrastive solution that learns to select the most useful embeddings for expansion. More specifically, a deep language model-based contrastive weighting model, called CWPRF, is trained to learn to discriminate between relevant and non-relevant documents for semantic search. Our experimental results show that our contrastive weighting model can aid to select useful expansion embeddings and outperform various baselines. In particular, CWPRF can improve nDCG@10 by upto to 4.1% compared to an existing PRF approach for ColBERT while maintaining its efficiency.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.710.pdf",
        "keywords": [
            "ndcg",
            "query expansion",
            "ranked",
            "contrastive weighting",
            "pseudo relevance feedback"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"a deep language model-based contrastive weighting model\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"a deep language model-based contrastive weighting model\""
    },
    {
        "title": "Reanalyzing L2 Preposition Learning with Bayesian Mixed Effects and a Pretrained Language Model",
        "authors": [
            "Jakob Prange",
            "Man Ho Ivy Wong"
        ],
        "published": "2023",
        "summary": "We use both Bayesian and neural models to dissect a data set of Chinese learners’ pre- and post-interventional responses to two tests measuring their understanding of English prepositions. The results mostly replicate previous findings from frequentist analyses and newly reveal crucial interactions between student ability, task type, and stimulus sentence. Given the sparsity of the data as well as high diversity among learners, the Bayesian method proves most useful; but we also see potential in using language model probabilities as predictors of grammaticality and learnability.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.712.pdf",
        "keywords": [
            "prepositions",
            "bayesian mixed effects",
            "grammaticality",
            "preposition learning",
            "language model"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Given the sparsity of the data as well as high diversity among learners, the Bayesian method proves most useful; but we also see potential in using language model probabilities as predictors of grammaticality and learnability.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Given the sparsity of the data as well as high diversity among learners, the Bayesian method proves most useful; but we also see potential in using language model probabilities as predictors of grammaticality and learnability.\""
    },
    {
        "title": "MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering",
        "authors": [
            "Fangyu Liu",
            "Francesco Piccinno",
            "Syrine Krichene",
            "Chenxi Pang",
            "Kenton Lee",
            "Mandar Joshi",
            "Yasemin Altun",
            "Nigel Collier",
            "Julian Eisenschlos"
        ],
        "published": "2023",
        "summary": "Visual language data such as plots, charts, and infographics are ubiquitous in the human world. However, state-of-the-art vision-language models do not perform well on these data. We propose MatCha (Math reasoning and Chart derendering pretraining) to enhance visual language models’ capabilities in jointly modeling charts/plots and language data. Specifically, we propose several pretraining tasks that cover plot deconstruction and numerical reasoning which are the key capabilities in visual language modeling. We perform the MatCha pretraining starting from Pix2Struct, a recently proposed image-to-text visual language model. On standard benchmarks such as PlotQA and ChartQA, the MatCha model outperforms state-of-the-art methods by as much as nearly 20%. We also examine how well MatCha pretraining transfers to domains such as screenshots, textbook diagrams, and document figures and observe overall improvement, verifying the usefulness of MatCha pretraining on broader visual language tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.714.pdf",
        "keywords": [
            "pretraining",
            "visual language pretraining",
            "plots",
            "chart derendering pretraining",
            "visual language modeling",
            "math reasoning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, state-of-the-art vision-language models do not perform well on these data.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, state-of-the-art vision-language models do not perform well on these data.\""
    },
    {
        "title": "Is Fine-tuning Needed? Pre-trained Language Models Are Near Perfect for Out-of-Domain Detection",
        "authors": [
            "Rheeya Uppaal",
            "Junjie Hu",
            "Yixuan Li"
        ],
        "published": "2023",
        "summary": "Out-of-distribution (OOD) detection is a critical task for reliable predictions over text. Fine-tuning with pre-trained language models has been a de facto procedure to derive OOD detectors with respect to in-distribution (ID) data. Despite its common use, the understanding of the role of fine-tuning and its necessity for OOD detection is largely unexplored. In this paper, we raise the question: is fine-tuning necessary for OOD detection? We present a study investigating the efficacy of directly leveraging pre-trained language models for OOD detection, without any model fine-tuning on the ID data. We compare the approach with several competitive fine-tuning objectives, and offer new insights under various types of distributional shifts. Extensive experiments demonstrate near-perfect OOD detection performance (with 0% FPR95 in many cases), strongly outperforming the fine-tuned counterpart.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.717.pdf",
        "keywords": [
            "fine tuning",
            "model fine tuning",
            "language models",
            "trained language models",
            "out of domain detection",
            "out",
            "distribution"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite its common use, the understanding of the role of fine-tuning and its necessity for OOD detection is largely unexplored.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite its common use, the understanding of the role of fine-tuning and its necessity for OOD detection is largely unexplored.\""
    },
    {
        "title": "UniSumm and SummZoo: Unified Model and Diverse Benchmark for Few-Shot Summarization",
        "authors": [
            "Yulong Chen",
            "Yang Liu",
            "Ruochen Xu",
            "Ziyi Yang",
            "Chenguang Zhu",
            "Michael Zeng",
            "Yue Zhang"
        ],
        "published": "2023",
        "summary": "The high annotation costs and diverse demands of various summarization tasks motivate the development of few-shot summarization. However, despite the emergence of many summarization tasks and datasets, the current training paradigm for few-shot summarization systems ignores potentially shareable knowledge in heterogeneous datasets. To this end, we propose UniSumm, a unified few-shot summarization model pre-trained with multiple summarization tasks and can be prefix-tuned to excel at any few-shot summarization task. Meanwhile, to better evaluate few-shot summarizers, under the principles of diversity and robustness, we assemble and release a new benchmark SummZoo. It consists of 8 summarization tasks with multiple sets of few-shot samples for each task, covering diverse domains. Experimental results and analysis show that UniSumm outperforms strong baselines by a large margin across all sub-tasks in SummZoo under both automatic and human evaluations and achieves comparable results in human evaluation compared with a GPT-3.5 model.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.718.pdf",
        "keywords": [
            "summarization",
            "shot summarization",
            "diverse benchmark",
            "unisumm",
            "few shot summarization"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"achieves comparable results in human evaluation compared with a GPT-3.5 model.\"\n\nThis paper mentions a limitation of LLMs in passing, as it compares the performance of the proposed model with a GPT-3.5 model, implying that the GPT-3.5 model may have some limitations that the proposed model can match or surpass.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"achieves comparable results in human evaluation compared with a GPT-3.5 model.\"\n\nThis paper mentions a limitation of LLMs in passing, as it compares the performance of the proposed model with a GPT-3.5 model, implying that the GPT-3.5 model may have some limitations that the proposed model can match or surpass."
    },
    {
        "title": "PuMer: Pruning and Merging Tokens for Efficient Vision Language Models",
        "authors": [
            "Qingqing Cao",
            "Bhargavi Paranjape",
            "Hannaneh Hajishirzi"
        ],
        "published": "2023",
        "summary": "Large-scale vision language (VL) models use Transformers to perform cross-modal interactions between the input text and image. These cross-modal interactions are computationally expensive and memory-intensive due to the quadratic complexity of processing the input image and text. We present PuMer: a token reduction framework that uses text-informed Pruning and modality-aware Merging strategies to progressively reduce the tokens of input image and text, improving model inference speed and reducing memory footprint. PuMer learns to keep salient image tokens related to the input text and merges similar textual and visual tokens by adding lightweight token reducer modules at several cross-modal layers in the VL model. Training PuMer is mostly the same as finetuning the original VL model but faster. Our evaluation for two vision language models on four downstream VL tasks shows PuMer increases inference throughput by up to 2x and reduces memory footprint by over 50% while incurring less than a 1% accuracy drop.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.721.pdf",
        "keywords": [
            "merging tokens",
            "vision language",
            "vision language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"These cross-modal interactions are computationally expensive and memory-intensive due to the quadratic complexity of processing the input image and text.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"These cross-modal interactions are computationally expensive and memory-intensive due to the quadratic complexity of processing the input image and text.\""
    },
    {
        "title": "TAGPRIME: A Unified Framework for Relational Structure Extraction",
        "authors": [
            "I-Hung Hsu",
            "Kuan-Hao Huang",
            "Shuning Zhang",
            "Wenxin Cheng",
            "Prem Natarajan",
            "Kai-Wei Chang",
            "Nanyun Peng"
        ],
        "published": "2023",
        "summary": "Many tasks in natural language processing require the extraction of relationship information for a given condition, such as event argument extraction, relation extraction, and task-oriented semantic parsing. Recent works usually propose sophisticated models for each task independently and pay less attention to the commonality of these tasks and to have a unified framework for all the tasks. In this work, we propose to take a unified view of all these tasks and introduce TAGPRIME to address relational structure extraction problems. TAGPRIME is a sequence tagging model that appends priming words about the information of the given condition (such as an event trigger) to the input text. With the self-attention mechanism in pre-trained language models, the priming words make the output contextualized representations contain more information about the given condition, and hence become more suitable for extracting specific relationships for the condition. Extensive experiments and analyses on three different tasks that cover ten datasets across five different languages demonstrate the generality and effectiveness of TAGPRIME.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.723.pdf",
        "keywords": [
            "structure extraction",
            "relational structure extraction",
            "sequence tagging",
            "language models",
            "models"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"With the self-attention mechanism in pre-trained language models...\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"With the self-attention mechanism in pre-trained language models...\""
    },
    {
        "title": "Exploring and Verbalizing Academic Ideas by Concept Co-occurrence",
        "authors": [
            "Yi Xu",
            "Shuqian Sheng",
            "Bo Xue",
            "Luoyi Fu",
            "Xinbing Wang",
            "Chenghu Zhou"
        ],
        "published": "2023",
        "summary": "Researchers usually come up with new ideas only after thoroughly comprehending vast quantities of literature. The difficulty of this procedure is exacerbated by the fact that the number of academic publications is growing exponentially. In this study, we devise a framework based on concept co-occurrence for academic idea inspiration, which has been integrated into a research assistant system. From our perspective, the emergence of a new idea can be regarded as the fusion of two concepts that co-occur in an academic paper. We construct evolving concept graphs according to the co-occurrence relationship of concepts from 20 disciplines or topics. Then we design a temporal link prediction method based on masked language model to explore potential connections between different concepts. To verbalize the newly discovered connections, we also utilize the pretrained language model to generate a description of an idea based on a new data structure called co-occurrence citation quintuple. We evaluate our proposed system using both automatic metrics and human assessment. The results demonstrate that our system has broad prospects and can assist researchers in expediting the process of discovering new ideas.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.727.pdf",
        "keywords": [
            "co occurrence",
            "concept co occurrence",
            "co occur",
            "verbalizing academic",
            "academic idea inspiration",
            "concept graphs",
            "masked language model",
            "prediction",
            "temporal"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicitly mentioned, but the paper utilizes a \"masked language model\" and a \"pretrained language model\", implying the existence of limitations that the authors are working around.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None explicitly mentioned, but the paper utilizes a \"masked language model\" and a \"pretrained language model\", implying the existence of limitations that the authors are working around."
    },
    {
        "title": "Distantly Supervised Course Concept Extraction in MOOCs with Academic Discipline",
        "authors": [
            "Mengying Lu",
            "Yuquan Wang",
            "Jifan Yu",
            "Yexing Du",
            "Lei Hou",
            "Juanzi Li"
        ],
        "published": "2023",
        "summary": "With the rapid growth of Massive Open Online Courses (MOOCs), it is expensive and time-consuming to extract high-quality knowledgeable concepts taught in the course by human effort to help learners grasp the essence of the course. In this paper, we propose to automatically extract course concepts using distant supervision to eliminate the heavy work of human annotations, which generates labels by matching them with an easily accessed dictionary. However, this matching process suffers from severe noisy and incomplete annotations because of the limited dictionary and diverse MOOCs. To tackle these challenges, we present a novel three-stage framework DS-MOCE, which leverages the power of pre-trained language models explicitly and implicitly and employs discipline-embedding models with a self-train strategy based on label generation refinement across different domains. We also provide an expert-labeled dataset spanning 20 academic disciplines. Experimental results demonstrate the superiority of DS-MOCE over the state-of-the-art distantly supervised methods (with 7% absolute F1 score improvement). Code and data are now available at https://github.com/THU-KEG/MOOC-NER.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.729.pdf",
        "keywords": [],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, this matching process suffers from severe noisy and incomplete annotations because of the limited dictionary and diverse MOOCs.\"\n\nThis abstract mentions a limitation of pre-trained language models in the context of distant supervision, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, this matching process suffers from severe noisy and incomplete annotations because of the limited dictionary and diverse MOOCs.\"\n\nThis abstract mentions a limitation of pre-trained language models in the context of distant supervision, but it is not the primary focus of the paper."
    },
    {
        "title": "CAT: A Contextualized Conceptualization and Instantiation Framework for Commonsense Reasoning",
        "authors": [
            "Weiqi Wang",
            "Tianqing Fang",
            "Baixuan Xu",
            "Chun Yi Louis Bo",
            "Yangqiu Song",
            "Lei Chen"
        ],
        "published": "2023",
        "summary": "Commonsense reasoning, aiming at endowing machines with a human-like ability to make situational presumptions, is extremely challenging to generalize. For someone who barely knows about “meditation,” while is knowledgeable about “singing,” he can still infer that “meditation makes people relaxed” from the existing knowledge that “singing makes people relaxed” by first conceptualizing “singing” as a “relaxing event” and then instantiating that event to “meditation.”This process, known as conceptual induction and deduction, is fundamental to commonsense reasoning while lacking both labeled data and methodologies to enhance commonsense modeling. To fill such a research gap, we propose CAT (Contextualized ConceptuAlization and InsTantiation),a semi-supervised learning framework that integrates event conceptualization and instantiation to conceptualize commonsense knowledge bases at scale. Extensive experiments show that our framework achieves state-of-the-art performances on two conceptualization tasks, and the acquired abstract commonsense knowledge can significantly improve commonsense inference modeling. Our code, data, and fine-tuned models are publicly available at [https://github.com/HKUST-KnowComp/CAT](https://github.com/HKUST-KnowComp/CAT).",
        "pdf_link": "https://aclanthology.org/2023.acl-long.733.pdf",
        "keywords": [
            "commonsense",
            "instantiation",
            "conceptualization",
            "conceptual induction",
            "commonsense reasoning",
            "instantiation framework",
            "commonsense inference",
            "contextualized conceptualization",
            "event conceptualization",
            "commonsense knowledge"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "VLN-Trans: Translator for the Vision and Language Navigation Agent",
        "authors": [
            "Yue Zhang",
            "Parisa Kordjamshidi"
        ],
        "published": "2023",
        "summary": "Language understanding is essential for the navigation agent to follow instructions. We observe two kinds of issues in the instructions that can make the navigation task challenging: 1. The mentioned landmarks are not recognizable by the navigation agent due to the different vision abilities of the instructor and the modeled agent. 2. The mentioned landmarks are applicable to multiple targets, thus not distinctive for selecting the target among the candidate viewpoints. To deal with these issues, we design a translator module for the navigation agent to convert the original instructions into easy-to-follow sub-instruction representations at each step. The translator needs to focus on the recognizable and distinctive landmarks based on the agent’s visual abilities and the observed visual environment. To achieve this goal, we create a new synthetic sub-instruction dataset and design specific tasks to train the translator and the navigation agent. We evaluate our approach on Room2Room (R2R), Room4room (R4R), and Room2Room Last (R2R-Last) datasets and achieve state-of-the-art results on multiple benchmarks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.737.pdf",
        "keywords": [
            "translator",
            "navigation agent",
            "language navigation agent",
            "landmarks",
            "room2room"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs or language models.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs or language models."
    },
    {
        "title": "Bridging the Gap between Decision and Logits in Decision-based Knowledge Distillation for Pre-trained Language Models",
        "authors": [
            "Qinhong Zhou",
            "Zonghan Yang",
            "Peng Li",
            "Yang Liu"
        ],
        "published": "2023",
        "summary": "Conventional knowledge distillation (KD) methods require access to the internal information of teachers, e.g., logits. However, such information may not always be accessible for large pre-trained language models (PLMs). In this work, we focus on decision-based KD for PLMs, where only teacher decisions (i.e., top-1 labels) are accessible. Considering the information gap between logits and decisions, we propose a novel method to estimate logits from the decision distributions. Specifically, decision distributions can be both derived as a function of logits theoretically and estimated with test-time data augmentation empirically. By combining the theoretical and empirical estimations of the decision distributions together, the estimation of logits can be successfully reduced to a simple root-finding problem. Extensive experiments show that our method significantly outperforms strong baselines on both natural language understanding and machine reading comprehension datasets.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.738.pdf",
        "keywords": [
            "knowledge distillation",
            "decision distributions",
            "decision based knowledge distillation",
            "decision based kd",
            "pre trained language models",
            "logits"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, such information may not always be accessible for large pre-trained language models (PLMs).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, such information may not always be accessible for large pre-trained language models (PLMs).\""
    },
    {
        "title": "UniCoRN: Unified Cognitive Signal ReconstructioN bridging cognitive signals and human language",
        "authors": [
            "Nuwa Xi",
            "Sendong Zhao",
            "Haochun Wang",
            "Chi Liu",
            "Bing Qin",
            "Ting Liu"
        ],
        "published": "2023",
        "summary": "Decoding text stimuli from cognitive signals (e.g. fMRI) enhances our understanding of the human language system, paving the way for building versatile Brain-Computer Interface. However, existing studies largely focus on decoding individual word-level fMRI volumes from a restricted vocabulary, which is far too idealized for real-world application. In this paper, we propose fMRI2text, the first open-vocabulary task aiming to bridge fMRI time series and human language. Furthermore, to explore the potential of this new task, we present a baseline solution, UniCoRN: the Unified Cognitive Signal ReconstructioN for Brain Decoding. By reconstructing both individual time points and time series, UniCoRN establishes a robust encoder for cognitive signals (fMRI & EEG). Leveraging a pre-trained language model as decoder, UniCoRN proves its efficacy in decoding coherent text from fMRI series across various split settings. Our model achieves a 34.77% BLEU score on fMRI2text, and a 37.04% BLEU when generalized to EEG-to-text decoding, thereby surpassing the former baseline. Experimental results indicate the feasibility of decoding consecutive fMRI volumes, and the effectiveness of decoding different cognitive signals using a unified structure.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.741.pdf",
        "keywords": [
            "bleu",
            "unicorn",
            "cognitive signals",
            "fmri",
            "human language",
            "eeg",
            "brain decoding"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Leveraging a pre-trained language model as decoder\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Leveraging a pre-trained language model as decoder\""
    },
    {
        "title": "MVP-Tuning: Multi-View Knowledge Retrieval with Prompt Tuning for Commonsense Reasoning",
        "authors": [
            "Yongfeng Huang",
            "Yanyang Li",
            "Yichong Xu",
            "Lin Zhang",
            "Ruyi Gan",
            "Jiaxing Zhang",
            "Liwei Wang"
        ],
        "published": "2023",
        "summary": "Recent advances in pre-trained language models (PLMs) have facilitated the development ofcommonsense reasoning tasks. However, existing methods rely on multi-hop knowledgeretrieval and thus suffer low accuracy due toembedded noise in the acquired knowledge. In addition, these methods often attain highcomputational costs and nontrivial knowledgeloss because they encode the knowledge independently of the PLM, making it less relevant to the task and thus resulting in a poorlocal optimum. In this work, we propose MultiView Knowledge Retrieval with Prompt Tuning (MVP-Tuning). MVP-Tuning leveragessimilar question-answer pairs in the training setto improve knowledge retrieval and employsa single prompt-tuned PLM to model knowledge and input text jointly. We conduct our experiments on five commonsense reasoning QAbenchmarks to show that MVP-Tuning outperforms all other baselines in 4 out of 5 datasetswith less than 2% trainable parameters. MVPTuning even gets a new state-of-the-art resulton OpenBookQA and is number one on theleaderboard.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.750.pdf",
        "keywords": [
            "knowledge retrieval",
            "commonsense reasoning",
            "tuning",
            "mvp tuning",
            "mvptuning",
            "prompt tuning",
            "trained language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing methods... suffer low accuracy due to embedded noise in the acquired knowledge... and often attain high computational costs and non-trivial knowledge loss because they encode the knowledge independently of the PLM, making it less relevant to the task and thus resulting in a poor local optimum.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, existing methods... suffer low accuracy due to embedded noise in the acquired knowledge... and often attain high computational costs and non-trivial knowledge loss because they encode the knowledge independently of the PLM, making it less relevant to the task and thus resulting in a poor local optimum.\""
    },
    {
        "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
        "authors": [
            "Yizhong Wang",
            "Yeganeh Kordi",
            "Swaroop Mishra",
            "Alisa Liu",
            "Noah A. Smith",
            "Daniel Khashabi",
            "Hannaneh Hajishirzi"
        ],
        "published": "2023",
        "summary": "Large “instruction-tuned” language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.754.pdf",
        "keywords": [
            "aligning",
            "aligning language models",
            "language models",
            "finetuned",
            "self",
            "self instruct",
            "self generated instructions",
            "pipeline"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model.\""
    },
    {
        "title": "Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis",
        "authors": [
            "Ta-Chung Chi",
            "Ting-Han Fan",
            "Alexander Rudnicky",
            "Peter Ramadge"
        ],
        "published": "2023",
        "summary": "Length extrapolation permits training a transformer language model on short sequences that preserves perplexities when tested on substantially longer sequences.A relative positional embedding design, ALiBi, has had the widest usage to date. We dissect ALiBi via the lens of receptive field analysis empowered by a novel cumulative normalized gradient tool. The concept of receptive field further allows us to modify the vanilla Sinusoidal positional embedding to create Sandwich, the first parameter-free relative positional embedding design that truly length information uses longer than the training sequence. Sandwich shares with KERPLE and T5 the same logarithmic decaying temporal bias pattern with learnable relative positional embeddings; these elucidate future extrapolatable positional embedding design.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.756.pdf",
        "keywords": [
            "receptive field",
            "receptive field analysis",
            "transformer length extrapolation",
            "length extrapolation",
            "transformer language model",
            "sandwich",
            "cumulative normalized gradient"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Length extrapolation permits training a transformer language model on short sequences that preserves perplexities when tested on substantially longer sequences.\"\n\nThis paper mentions a limitation of transformer language models in handling longer sequences than they were trained on, but it does not elaborate on this limitation in detail.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Length extrapolation permits training a transformer language model on short sequences that preserves perplexities when tested on substantially longer sequences.\"\n\nThis paper mentions a limitation of transformer language models in handling longer sequences than they were trained on, but it does not elaborate on this limitation in detail."
    },
    {
        "title": "CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models",
        "authors": [
            "Jiaxu Zhao",
            "Meng Fang",
            "Zijing Shi",
            "Yitong Li",
            "Ling Chen",
            "Mykola Pechenizkiy"
        ],
        "published": "2023",
        "summary": "redWarning: This paper contains content that may be offensive or upsetting.Pretrained conversational agents have been exposed to safety issues, exhibiting a range of stereotypical human biases such as gender bias. However, there are still limited bias categories in current research, and most of them only focus on English. In this paper, we introduce a new Chinese dataset, CHBias, for bias evaluation and mitigation of Chinese conversational language models.Apart from those previous well-explored bias categories, CHBias includes under-explored bias categories, such as ageism and appearance biases, which received less attention. We evaluate two popular pretrained Chinese conversational models, CDial-GPT and EVA2.0, using CHBias. Furthermore, to mitigate different biases, we apply several debiasing methods to the Chinese pretrained models. Experimental results show that these Chinese pretrained models are potentially risky for generating texts that contain social biases, and debiasing methods using the proposed dataset can make response generation less biased while preserving the models’ conversational capabilities.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.757.pdf",
        "keywords": [
            "bias evaluation",
            "mitigation",
            "conversational language models",
            "debiasing",
            "biased"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, there are still limited bias categories in current research, and most of them only focus on English.\"; \"Experimental results show that these Chinese pretrained models are potentially risky for generating texts that contain social biases\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, there are still limited bias categories in current research, and most of them only focus on English.\"; \"Experimental results show that these Chinese pretrained models are potentially risky for generating texts that contain social biases\""
    },
    {
        "title": "Uncovering and Categorizing Social Biases in Text-to-SQL",
        "authors": [
            "Yan Liu",
            "Yan Gao",
            "Zhe Su",
            "Xiaokang Chen",
            "Elliott Ash",
            "Jian-Guang Lou"
        ],
        "published": "2023",
        "summary": "Large pre-trained language models are acknowledged to carry social bias towards different demographics, which can further amplify existing stereotypes in our society and cause even more harm. Text-to-SQL is an important task, models of which are mainly adopted by administrative industries, where unfair decisions may lead to catastrophic consequences. However, existing Text-to-SQL models are trained on clean, neutral datasets, such as Spider and WikiSQL. This, to some extent, cover up social bias in models under ideal conditions, which nevertheless may emerge in real application scenarios. In this work, we aim to uncover and mitigate social bias in Text-to-SQL models. We summarize the categories of social bias that may occur in structural data for Text-to-SQL models. We build test benchmarks and reveal that models with similar task accuracy can contain social bias at very different rates. We show how to take advantage of our methodology to assess and mitigate social bias in the downstream Text-to-SQL task.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.759.pdf",
        "keywords": [
            "social bias",
            "language models",
            "text to sql",
            "test benchmarks",
            "accuracy"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large pre-trained language models are acknowledged to carry social bias towards different demographics, which can further amplify existing stereotypes in our society and cause even more harm.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Large pre-trained language models are acknowledged to carry social bias towards different demographics, which can further amplify existing stereotypes in our society and cause even more harm.\""
    },
    {
        "title": "On the Compositional Generalization in Versatile Open-domain Dialogue",
        "authors": [
            "Tingchen Fu",
            "Xueliang Zhao",
            "Lemao Liu",
            "Rui Yan"
        ],
        "published": "2023",
        "summary": "Previous research has demonstrated the potential of multi-task learning to foster a conversational agent’s ability to acquire a variety of skills. However, these approaches either suffer from interference among different datasets (also known as negative transfer), or fail to effectively reuse knowledge and skills learned from other datasets. In contrast to previous works, we develop a sparsely activated modular network: (1) We propose a well-rounded set of operators and instantiate each operator with an independent module; (2) We formulate dialogue generation as the execution of a generated programme which recursively composes and assembles modules. Extensive experiments on 9 datasets verify the efficacy of our methods through automatic evaluation and human evaluation. Notably, our model outperforms state-of-the-art supervised approaches on 4 datasets with only 10% training data thanks to the modular architecture and multi-task learning.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.760.pdf",
        "keywords": [
            "dialogue generation"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Training Models to Generate, Recognize, and Reframe Unhelpful Thoughts",
        "authors": [
            "Mounica Maddela",
            "Megan Ung",
            "Jing Xu",
            "Andrea Madotto",
            "Heather Foran",
            "Y-Lan Boureau"
        ],
        "published": "2023",
        "summary": "Many cognitive approaches to well-being, such as recognizing and reframing unhelpful thoughts, have received considerable empirical support over the past decades, yet still lack truly widespread adoption in self-help format. A barrier to that adoption is a lack of adequately specific and diverse dedicated practice material. This work examines whether current language models can be leveraged to both produce a virtually unlimited quantity of practice material illustrating standard unhelpful thought patterns matching specific given contexts, and generate suitable positive reframing proposals. We propose PATTERNREFRAME, a novel dataset of about 10k examples of thoughts containing unhelpful thought patterns conditioned on a given persona, accompanied by about 27k positive reframes. By using this dataset to train and/or evaluate current models, we show that existing models can already be powerful tools to help generate an abundance of tailored practice material and hypotheses, with no or minimal additional model training required.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.763.pdf",
        "keywords": [
            "train",
            "practice",
            "self help",
            "model training",
            "language models",
            "unhelpful thoughts",
            "positive reframing proposals",
            "unhelpful thought patterns",
            "dataset",
            "empirical support",
            "examples"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation of LLMs is mentioned in the abstract, but it implies that current LLMs can be leveraged to generate practice material with \"no or minimal additional model training required\", suggesting that there might be limitations in the model's training or capabilities that need to be addressed.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation of LLMs is mentioned in the abstract, but it implies that current LLMs can be leveraged to generate practice material with \"no or minimal additional model training required\", suggesting that there might be limitations in the model's training or capabilities that need to be addressed."
    },
    {
        "title": "Learning In-context Learning for Named Entity Recognition",
        "authors": [
            "Jiawei Chen",
            "Yaojie Lu",
            "Hongyu Lin",
            "Jie Lou",
            "Wei Jia",
            "Dai Dai",
            "Hua Wu",
            "Boxi Cao",
            "Xianpei Han",
            "Le Sun"
        ],
        "published": "2023",
        "summary": "Named entity recognition in real-world applications suffers from the diversity of entity types, the emergence of new entity types, and the lack of high-quality annotations. To address the above problems, this paper proposes an in-context learning-based NER approach, which can effectively inject in-context NER ability into PLMs and recognize entities of novel types on-the-fly using only a few demonstrative instances. Specifically, we model PLMs as a meta-function Lambda_instruction, demonstrations, text.M, and a new entity extractor can be implicitly constructed by applying new instruction and demonstrations to PLMs, i.e., (Lambda . M) (instruction, demonstrations) ->F where F will be a new entity extractor F: text -> entities. To inject the above in-context NER ability into PLMs, we propose a meta-function pre-training algorithm, which pre-trains PLMs by comparing the (instruction, demonstration)-initialized extractor with a surrogate golden extractor. Experimental results on 4 few-shot NER datasets show that our method can effectively inject in-context NER ability into PLMs and significantly outperforms the PLMs+fine-tuning counterparts.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.764.pdf",
        "keywords": [
            "named entity recognition",
            "context learning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Named entity recognition in real-world applications suffers from the diversity of entity types, the emergence of new entity types, and the lack of high-quality annotations.\"\n\nThis abstract mentions limitations of named entity recognition in real-world applications, which can be related to LLMs, but does not explicitly discuss LLM limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Named entity recognition in real-world applications suffers from the diversity of entity types, the emergence of new entity types, and the lack of high-quality annotations.\"\n\nThis abstract mentions limitations of named entity recognition in real-world applications, which can be related to LLMs, but does not explicitly discuss LLM limitations."
    },
    {
        "title": "Training Trajectories of Language Models Across Scales",
        "authors": [
            "Mengzhou Xia",
            "Mikel Artetxe",
            "Chunting Zhou",
            "Xi Victoria Lin",
            "Ramakanth Pasunuru",
            "Danqi Chen",
            "Luke Zettlemoyer",
            "Veselin Stoyanov"
        ],
        "published": "2023",
        "summary": "Scaling up language models has led to unprecedented performance gains, but little is understood about how the training dynamics change as models get larger. How do language models of different sizes learn during pre-training? Why do larger language models demonstrate more desirable behaviors? In this paper, we analyze the intermediate training checkpoints of differently sized OPT models (Zhang et al., 2022)—from 125M to 175B parameters—on next-token prediction, sequence-level generation and downstream tasks. We find that 1) at a given perplexity and independent of model sizes, a similar subset of training tokens see the most significant reduction in loss, with the rest stagnating or showing double-descent behavior (Nakkiran et al., 2020); 2) early in training, all models learn to reduce the perplexity of grammatical sequences that contain hallucinations, with small models halting at this suboptimal distribution and larger ones eventually learning to assign these sequences lower probabilities; and 3) perplexity is a strong predictor of in-context learning performance on 74 multiple-choice tasks from BIG-Bench, and this holds independent of the model size. Together, these results show that perplexity is more predictive of model behaviors than model size or training computation.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.767.pdf",
        "keywords": [
            "training",
            "language",
            "training dynamics",
            "perplexity",
            "language models",
            "scales",
            "sequence level generation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"small models halting at this suboptimal distribution\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"small models halting at this suboptimal distribution\""
    },
    {
        "title": "A Diverse Set of Freely Available Linguistic Resources for Turkish",
        "authors": [
            "Duygu Altinok"
        ],
        "published": "2023",
        "summary": "This study presents a diverse set of freely available linguistic resources for Turkish natural language processing, including corpora, pretrained models and education material. Although Turkish is spoken by a sizeable population of over 80 million people, Turkish linguistic resources for natural language processing remain scarce. In this study, we provide corpora to allow practitioners to build their own applications and pretrained models that would assist industry researchers in creating quick prototypes. The provided corpora include named entity recognition datasets of diverse genres, including Wikipedia articles and supplement products customer reviews. In addition, crawling e-commerce and movie reviews websites, we compiled several sentiment analysis datasets of different genres. Our linguistic resources for Turkish also include pretrained spaCy language models. To the best of our knowledge, our models are the first spaCy models trained for the Turkish language. Finally, we provide various types of education material, such as video tutorials and code examples, that can support the interested audience on practicing Turkish NLP. The advantages of our linguistic resources are three-fold: they are freely available, they are first of their kind, and they are easy to use in a broad range of implementations. Along with a thorough description of the resource creation process, we also explain the position of our resources in the Turkish NLP world.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.768.pdf",
        "keywords": [
            "natural language processing",
            "sentiment analysis",
            "linguistic resources",
            "diverse",
            "available linguistic resources",
            "entity recognition",
            "diverse set"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Optimal Transport for Unsupervised Hallucination Detection in Neural Machine Translation",
        "authors": [
            "Nuno M. Guerreiro",
            "Pierre Colombo",
            "Pablo Piantanida",
            "André Martins"
        ],
        "published": "2023",
        "summary": "Neural machine translation (NMT) has become the de-facto standard in real-world machine translation applications. However, NMT models can unpredictably produce severely pathological translations, known as hallucinations, that seriously undermine user trust. It becomes thus crucial to implement effective preventive strategies to guarantee their proper functioning. In this paper, we address the problem of hallucination detection in NMT by following a simple intuition: as hallucinations are detached from the source content, they exhibit encoder-decoder attention patterns that are statistically different from those of good quality translations. We frame this problem with an optimal transport formulation and propose a fully unsupervised, plug-in detector that can be used with any attention-based NMT model. Experimental results show that our detector not only outperforms all previous model-based detectors, but is also competitive with detectors that employ external models trained on millions of samples for related tasks such as quality estimation and cross-lingual sentence similarity.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.770.pdf",
        "keywords": [
            "neural machine translation",
            "hallucination detection",
            "optimal transport"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Entailment as Robust Self-Learner",
        "authors": [
            "Jiaxin Ge",
            "Hongyin Luo",
            "Yoon Kim",
            "James Glass"
        ],
        "published": "2023",
        "summary": "Entailment has been recognized as an important metric for evaluating natural language understanding (NLU) models, and recent studies have found that entailment pretraining benefits weakly supervised fine-tuning. In this work, we design a prompting strategy that formulates a number of different NLU tasks as contextual entailment. This approach improves the zero-shot adaptation of pretrained entailment models. Secondly, we notice that self-training entailment-based models with unlabeled data can significantly improve the adaptation performance on downstream tasks. To achieve more stable improvement, we propose the Simple Pseudo-Label Editing (SimPLE) algorithm for better pseudo-labeling quality in self-training. We also found that both pretrained entailment-based models and the self-trained models are robust against adversarial evaluation data. Experiments on binary and multi-class classification tasks show that SimPLE leads to more robust self-training results, indicating that the self-trained entailment models are more efficient and trustworthy than large language models on language understanding tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.772.pdf",
        "keywords": [
            "entailment",
            "classification"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"indicating that the self-trained entailment models are more efficient and trustworthy than large language models on language understanding tasks.\"\n\nThis abstract mentions a limitation of LLMs in passing, stating that they are less efficient and trustworthy than self-trained entailment models on language understanding tasks. However, this limitation is not the primary focus of the abstract, and the discussion is brief.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"indicating that the self-trained entailment models are more efficient and trustworthy than large language models on language understanding tasks.\"\n\nThis abstract mentions a limitation of LLMs in passing, stating that they are less efficient and trustworthy than self-trained entailment models on language understanding tasks. However, this limitation is not the primary focus of the abstract, and the discussion is brief."
    },
    {
        "title": "ReCode: Robustness Evaluation of Code Generation Models",
        "authors": [
            "Shiqi Wang",
            "Zheng Li",
            "Haifeng Qian",
            "Chenghao Yang",
            "Zijian Wang",
            "Mingyue Shang",
            "Varun Kumar",
            "Samson Tan",
            "Baishakhi Ray",
            "Parminder Bhatia",
            "Ramesh Nallapati",
            "Murali Krishna Ramanathan",
            "Dan Roth",
            "Bing Xiang"
        ],
        "published": "2023",
        "summary": "Code generation models have achieved impressive performance. However, they tend to be brittle as slight edits to a prompt could lead to very different generations; these robustness properties, critical for user experience when deployed in real-life applications, are not well understood. Most existing works on robustness in text or code tasks have focused on classification, while robustness in generation tasks is an uncharted area and to date there is no comprehensive benchmark for robustness in code generation. In this paper, we propose ReCode, a comprehensive robustness evaluation benchmark for code generation models. We customize over 30 transformations specifically for code on docstrings, function and variable names, code syntax, and code format. They are carefully designed to be natural in real-life coding practice, preserve the original semantic meaning, and thus provide multifaceted assessments of a model’s robustness performance. With human annotators, we verified that over 90% of the perturbed prompts do not alter the semantic meaning of the original prompt. In addition, we define robustness metrics for code generation models considering the worst-case behavior under each type of perturbation, taking advantage of the fact that executing the generated code can serve as objective evaluation. We demonstrate ReCode on SOTA models using HumanEval, MBPP, as well as function completion tasks derived from them. Interesting observations include: better robustness for CodeGen over InCoder and GPT-J; models are most sensitive to syntax perturbations; more challenging robustness evaluation on MBPP over HumanEval.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.773.pdf",
        "keywords": [
            "code generation",
            "robustness",
            "recode",
            "robustness evaluation",
            "s robustness"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, they tend to be brittle as slight edits to a prompt could lead to very different generations; these robustness properties, critical for user experience when deployed in real-life applications, are not well understood.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, they tend to be brittle as slight edits to a prompt could lead to very different generations; these robustness properties, critical for user experience when deployed in real-life applications, are not well understood.\""
    },
    {
        "title": "Minding Language Models’ (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker",
        "authors": [
            "Melanie Sclar",
            "Sachin Kumar",
            "Peter West",
            "Alane Suhr",
            "Yejin Choi",
            "Yulia Tsvetkov"
        ],
        "published": "2023",
        "summary": "Theory of Mind (ToM)—the ability to reason about the mental states of other people—is a key element of our social intelligence. Yet, despite their ever more impressive performance, large-scale neural language models still lack basic theory of mind capabilities out-of-the-box. We posit that simply scaling up models will not imbue them with theory of mind due to the inherently symbolic and implicit nature of the phenomenon, and instead investigate an alternative: can we design a decoding-time algorithm that enhances theory of mind of off-the-shelf neural language models without explicit supervision? We present SymbolicToM, a plug-and-play approach to reason about the belief states of multiple characters in reading comprehension tasks via explicit symbolic representation. More concretely, our approach tracks each entity’s beliefs, their estimation of other entities’ beliefs, and higher-order levels of reasoning, all through graphical representations, allowing for more precise and interpretable reasoning than previous approaches. Empirical results on the well-known ToMi benchmark (Le et al., 2019) demonstrate that SymbolicToM dramatically enhances off-the-shelf neural networks’ theory of mind in a zero-shot setting while showing robust out-of-distribution performance compared to supervised baselines. Our work also reveals spurious patterns in existing theory of mind benchmarks, emphasizing the importance of out-of-distribution evaluation and methods that do not overfit a particular dataset.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.780.pdf",
        "keywords": [
            "plug and play",
            "theory of mind",
            "social intelligence",
            "neural language models",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Yet, despite their ever more impressive performance, large-scale neural language models still lack basic theory of mind capabilities out-of-the-box. We posit that simply scaling up models will not imbue them with theory of mind due to the inherently symbolic and implicit nature of the phenomenon.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Yet, despite their ever more impressive performance, large-scale neural language models still lack basic theory of mind capabilities out-of-the-box. We posit that simply scaling up models will not imbue them with theory of mind due to the inherently symbolic and implicit nature of the phenomenon.\""
    },
    {
        "title": "Don’t Retrain, Just Rewrite: Countering Adversarial Perturbations by Rewriting Text",
        "authors": [
            "Ashim Gupta",
            "Carter Blum",
            "Temma Choji",
            "Yingjie Fei",
            "Shalin Shah",
            "Alakananda Vempala",
            "Vivek Srikumar"
        ],
        "published": "2023",
        "summary": "Can language models transform inputs to protect text classifiers against adversarial attacks? In this work, we present ATINTER, a model that intercepts and learns to rewrite adversarial inputs to make them non-adversarial for a downstream text classifier. Our experiments on four datasets and five attack mechanisms reveal that ATINTER is effective at providing better adversarial robustness than existing defense approaches, without compromising task accuracy. For example, on sentiment classification using the SST-2 dataset, our method improves the adversarial accuracy over the best existing defense approach by more than 4% with a smaller decrease in task accuracy (0.5 % vs 2.5%). Moreover, we show that ATINTER generalizes across multiple downstream tasks and classifiers without having to explicitly retrain it for those settings. For example, we find that when ATINTER is trained to remove adversarial perturbations for the sentiment classification task on the SST-2 dataset, it even transfers to a semantically different task of news classification (on AGNews) and improves the adversarial robustness by more than 10%.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.781.pdf",
        "keywords": [
            "retrain",
            "sentiment classification",
            "rewrite",
            "adversarial robustness"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper discusses the vulnerability of text classifiers (which are often powered by LLMs) to adversarial attacks, implying a limitation in their robustness.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None, but the paper discusses the vulnerability of text classifiers (which are often powered by LLMs) to adversarial attacks, implying a limitation in their robustness."
    },
    {
        "title": "Mitigating Label Biases for In-context Learning",
        "authors": [
            "Yu Fei",
            "Yifan Hou",
            "Zeming Chen",
            "Antoine Bosselut"
        ],
        "published": "2023",
        "summary": "Various design settings for in-context learning (ICL), such as the choice and order of the in-context examples, can bias the model’s predictions. While many studies discuss these design choices, there have been few systematic investigations into categorizing them and mitigating their impact. In this work, we define a typology for three types of label biases in ICL for text classification: vanilla-label bias, context-label bias, and domain-label bias (which we conceptualize and detect for the first time). Our analysis demonstrates that prior label bias calibration methods fall short of addressing all three types of biases. Specifically, domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples. To mitigate the effect of these biases, we propose a simple bias calibration method that estimates a language model’s label bias using random in-domain words from the task corpus. After controlling for this estimated bias when making predictions, our novel domain-context calibration significantly improves the ICL performance of GPT-J and GPT-3 on a wide range of tasks. The gain is substantial on tasks with large domain-label bias (up to 37% in Macro-F1). Furthermore, our results generalize to models with different scales, pretraining methods, and manually-designed task instructions, showing the prevalence of label biases in ICL.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.783.pdf",
        "keywords": [
            "label bias",
            "bias",
            "context label bias",
            "label biases",
            "s label bias",
            "text classification",
            "biases",
            "context",
            "domain label bias",
            "domain context calibration",
            "context learning",
            "context examples",
            "typology",
            "language"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Specifically, domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Specifically, domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples.\""
    },
    {
        "title": "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense Question Answering",
        "authors": [
            "Yujie Wang",
            "Hu Zhang",
            "Jiye Liang",
            "Ru Li"
        ],
        "published": "2023",
        "summary": "Recently, knowledge graphs (KGs) have won noteworthy success in commonsense question answering. Existing methods retrieve relevant subgraphs in the KGs through key entities and reason about the answer with language models (LMs) and graph neural networks. However, they ignore (i) optimizing the knowledge representation and structure of subgraphs and (ii) deeply fusing heterogeneous QA context with subgraphs. In this paper, we propose a dynamic heterogeneous-graph reasoning method with LMs and knowledge representation learning (DHLK), which constructs a heterogeneous knowledge graph (HKG) based on multiple knowledge sources and optimizes the structure and knowledge representation of the HKG using a two-stage pruning strategy and knowledge representation learning (KRL). It then performs joint reasoning by LMs and Relation Mask Self-Attention (RMSA). Specifically, DHLK filters key entities based on the dictionary vocabulary to achieve the first-stage pruning while incorporating the paraphrases in the dictionary into the subgraph to construct the HKG. Then, DHLK encodes and fuses the QA context and HKG using LM, and dynamically removes irrelevant KG entities based on the attention weights of LM for the second-stage pruning. Finally, DHLK introduces KRL to optimize the knowledge representation and perform answer reasoning on the HKG by RMSA.We evaluate DHLK at CommonsenseQA and OpenBookQA, and show its improvement on existing LM and LM+KG methods.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.785.pdf",
        "keywords": [
            "commonsense question answering",
            "heterogeneous graph reasoning",
            "heterogeneous knowledge graph",
            "knowledge graphs",
            "knowledge representation",
            "dynamic heterogeneous graph reasoning",
            "knowledge representation learning",
            "relation mask self attention"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing methods retrieve relevant subgraphs in the KGs through key entities and reason about the answer with language models (LMs) and graph neural networks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing methods retrieve relevant subgraphs in the KGs through key entities and reason about the answer with language models (LMs) and graph neural networks.\""
    },
    {
        "title": "Ambiguous Learning from Retrieval: Towards Zero-shot Semantic Parsing",
        "authors": [
            "Shan Wu",
            "Chunlei Xin",
            "Hongyu Lin",
            "Xianpei Han",
            "Cao Liu",
            "Jiansong Chen",
            "Fan Yang",
            "Guanglu Wan",
            "Le Sun"
        ],
        "published": "2023",
        "summary": "Current neural semantic parsers take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain. Thus, minimizing the supervision effort is one of the key challenges in semantic parsing. In this paper, we propose the Retrieval as Ambiguous Supervision framework, in which we construct a retrieval system based on pretrained language models to collect high-coverage candidates. Assuming candidates always contain the correct ones, we convert zero-shot task into ambiguously supervised task. To improve the precision and coverage of such ambiguous supervision, we propose a confidence-driven self-training algorithm, in which a semantic parser is learned and exploited to disambiguate the candidates iteratively. Experimental results show that our approach significantly outperforms the state-of-the-art zero-shot semantic parsing methods.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.787.pdf",
        "keywords": [
            "semantic parsing",
            "semantic parsers",
            "shot semantic parsing"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Current neural semantic parsers take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain.\"\n\n(Note: Although the abstract does not explicitly mention LLMs, it mentions \"pretrained language models\" which can be considered as a type of LLM. The limitation mentioned is related to the requirement of a considerable amount of training data, which is a general limitation",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Current neural semantic parsers take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain.\"\n\n(Note: Although the abstract does not explicitly mention LLMs, it mentions \"pretrained language models\" which can be considered as a type of LLM. The limitation mentioned is related to the requirement of a considerable amount of training data, which is a general limitation"
    },
    {
        "title": "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion",
        "authors": [
            "Dongfu Jiang",
            "Xiang Ren",
            "Bill Yuchen Lin"
        ],
        "published": "2023",
        "summary": "We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. Our results demonstrate that PairRanker exhibits the highest correlation with ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large-scale evaluation, we introduce a benchmark dataset, MixInstruct, which is a mixture of multiple instruction datasets featuring oracle pairwise comparisons. Our LLM-Blender significantly outperform individual LLMs and baseline methods across various metrics, establishing a substantial performance gap.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.792.pdf",
        "keywords": [
            "generative fusion",
            "pairwise ranking",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"addressing the observation that optimal LLMs for different examples can significantly vary.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"addressing the observation that optimal LLMs for different examples can significantly vary.\""
    },
    {
        "title": "Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases",
        "authors": [
            "Yingji Li",
            "Mengnan Du",
            "Xin Wang",
            "Ying Wang"
        ],
        "published": "2023",
        "summary": "As the representation capability of Pre-trained Language Models (PLMs) improve, there is growing concern that they will inherit social biases from unprocessed corpora. Most previous debiasing techniques used Counterfactual Data Augmentation (CDA) to balance the training corpus. However, CDA slightly modifies the original corpus, limiting the representation distance between different demographic groups to a narrow range. As a result, the debiasing model easily fits the differences between counterfactual pairs, which affects its debiasing performance with limited text resources. In this paper, we propose an adversarial training-inspired two-stage debiasing model using Contrastive learning with Continuous Prompt Augmentation (named CCPA) to mitigate social biases in PLMs’ encoding. In the first stage, we propose a data augmentation method based on continuous prompt tuning to push farther the representation distance between sample pairs along different demographic groups. In the second stage, we utilize contrastive learning to pull closer the representation distance between the augmented sample pairs and then fine-tune PLMs’ parameters to get debiased encoding. Our approach guides the model to achieve stronger debiasing performance by adding difficulty to the training process. Extensive experiments show that CCPA outperforms baselines in terms of debiasing performance. Meanwhile, experimental results on the GLUE benchmark show that CCPA retains the language modeling capability of PLMs.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.797.pdf",
        "keywords": [
            "social biases",
            "tuning",
            "data augmentation",
            "representation distance",
            "debiasing",
            "pre trained language models",
            "counterfactual data augmentation",
            "fine tune"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"As the representation capability of Pre-trained Language Models (PLMs) improve, there is growing concern that they will inherit social biases from unprocessed corpora.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"As the representation capability of Pre-trained Language Models (PLMs) improve, there is growing concern that they will inherit social biases from unprocessed corpora.\""
    },
    {
        "title": "Python Code Generation by Asking Clarification Questions",
        "authors": [
            "Haau-Sing (Xiaocheng) Li",
            "Mohsen Mesgar",
            "André Martins",
            "Iryna Gurevych"
        ],
        "published": "2023",
        "summary": "Code generation from text requires understanding the user’s intent from a natural languagedescription and generating an executable code snippet that satisfies this intent. While recent pretrained language models demonstrate remarkable performance for this task, these models fail when the given natural language description is under-specified. In this work, we introduce a novel and more realistic setup for this task. We hypothesize that the under-specification of a natural language description can be resolved by asking clarification questions. Therefore, we collect and introduce a new dataset named CodeClarQA containing pairs of natural language descriptions and code with created synthetic clarification questions and answers. The empirical results of our evaluation of pretrained language model performance on code generation show that clarifications result in more precisely generated code, as shown by the substantial improvement of model performance in all evaluation metrics. Alongside this, our task and dataset introduce new challenges to the community, including when and what clarification questions should be asked. Our code and dataset are available on GitHub.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.799.pdf",
        "keywords": [
            "clarifications",
            "clarification questions",
            "code generation",
            "language",
            "natural language description"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While recent pretrained language models demonstrate remarkable performance for this task, these models fail when the given natural language description is under-specified.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While recent pretrained language models demonstrate remarkable performance for this task, these models fail when the given natural language description is under-specified.\""
    },
    {
        "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models",
        "authors": [
            "Joel Jang",
            "Dongkeun Yoon",
            "Sohee Yang",
            "Sungmin Cha",
            "Moontae Lee",
            "Lajanugen Logeswaran",
            "Minjoon Seo"
        ],
        "published": "2023",
        "summary": "Pretrained Language Models (LMs) memorize a vast amount of knowledge during initial pretraining, including information that may violate the privacy of personal lives and identities. Previous work addressing privacy issues for LMs has mostly focused on data preprocessing and differential privacy methods, both requiring re-training the underlying LM. We propose knowledge unlearning as an alternative method to reduce privacy risks for LMs post hoc. We show that simply performing gradient ascent on target token sequences is effective at forgetting them with little to no degradation of general language modeling performances for larger-sized LMs. We also find that sequential unlearning is better than trying to unlearn all the data at once and that unlearning is highly dependent on which kind of data (domain) is forgotten. By showing comparisons with previous methods known to mitigate privacy risks for LMs, we show that our approach can give a stronger empirical privacy guarantee in scenarios where the data vulnerable to extraction attacks are known a priori while being much more efficient and robust.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.805.pdf",
        "keywords": [
            "unlearning",
            "privacy",
            "privacy risks",
            "language models",
            "knowledge unlearning",
            "training"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Pretrained Language Models (LMs) memorize a vast amount of knowledge during initial pretraining, including information that may violate the privacy of personal lives and identities.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Pretrained Language Models (LMs) memorize a vast amount of knowledge during initial pretraining, including information that may violate the privacy of personal lives and identities.\""
    },
    {
        "title": "Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor",
        "authors": [
            "Or Honovich",
            "Thomas Scialom",
            "Omer Levy",
            "Timo Schick"
        ],
        "published": "2023",
        "summary": "Instruction tuning enables pretrained language models to perform new tasks from inference-time natural language descriptions. These approaches rely on vast amounts of human supervision in the form of crowdsourced datasets or user interactions. In this work, we introduce Unnatural Instructions: a large dataset of creative and diverse instructions, collected with virtually no human labor. We collect 64,000 examples by prompting a language model with three seed examples of instructions and eliciting a fourth. This set is then expanded by prompting the model to rephrase each instruction, creating a total of approximately 240,000 examples of instructions, inputs, and outputs. Experiments show that despite containing a fair amount of noise, training on Unnatural Instructions rivals the effectiveness of training on open-source manually-curated datasets, surpassing the performance of models such as T0++ and Tk-Instruct across various benchmarks. These results demonstrate the potential of model-generated data as a cost-effective alternative to crowdsourcing for dataset expansion and diversification.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.806.pdf",
        "keywords": [
            "labor",
            "instruction tuning",
            "human labor",
            "tuning language",
            "unnatural",
            "models"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "A Survey of Deep Learning for Mathematical Reasoning",
        "authors": [
            "Pan Lu",
            "Liang Qiu",
            "Wenhao Yu",
            "Sean Welleck",
            "Kai-Wei Chang"
        ],
        "published": "2023",
        "summary": "Mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems in language has garnered significant interest in the fields of machine learning and natural language processing. For example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep learning models, driving new algorithmic and modeling advances. On the other hand, recent advances in large-scale neural language models have opened up new benchmarks and opportunities to use deep learning for mathematical reasoning. In this survey paper, we review the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade. We also evaluate existing benchmarks and methods, and discuss future research directions in this domain.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.817.pdf",
        "keywords": [
            "deep learning",
            "mathematical reasoning",
            "artificial intelligence",
            "survey"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training",
        "authors": [
            "Nitay Calderon",
            "Subhabrata Mukherjee",
            "Roi Reichart",
            "Amir Kantor"
        ],
        "published": "2023",
        "summary": "Modern Natural Language Generation (NLG) models come with massive computational and storage requirements. In this work, we study the potential of compressing them, which is crucial for real-world applications serving millions of users. We focus on Knowledge Distillation (KD) techniques, in which a small student model learns to imitate a large teacher model, allowing to transfer knowledge from the teacher to the student. In contrast to much of the previous work, our goal is to optimize the model for a specific NLG task and a specific dataset. Typically in real-world applications, in addition to labeled data there is abundant unlabeled task-specific data, which is crucial for attaining high compression rates via KD. In this work, we conduct a systematic study of task-specific KD techniques for various NLG tasks under realistic assumptions. We discuss the special characteristics of NLG distillation and particularly the exposure bias problem. Following, we derive a family of Pseudo-Target (PT) augmentation methods, substantially extending prior work on sequence-level KD. We propose the Joint-Teaching method, which applies word-level KD to multiple PTs generated by both the teacher and the student. Finally, we validate our findings in an extreme setup with no labeled examples using GPT-4 as the teacher. Our study provides practical model design observations and demonstrates the effectiveness of PT training for task-specific KD in NLG.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.818.pdf",
        "keywords": [
            "knowledge distillation",
            "natural language generation",
            "word level kd",
            "exposure bias",
            "nlg distillation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Modern Natural Language Generation (NLG) models come with massive computational and storage requirements.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Modern Natural Language Generation (NLG) models come with massive computational and storage requirements.\""
    },
    {
        "title": "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models",
        "authors": [
            "Qingyu Tan",
            "Hwee Tou Ng",
            "Lidong Bing"
        ],
        "published": "2023",
        "summary": "Reasoning about time is of fundamental importance. Many facts are time-dependent. For example, athletes change teams from time to time, and different government officials are elected periodically. Previous time-dependent question answering (QA) datasets tend to be biased in either their coverage of time spans or question types. In this paper, we introduce a comprehensive probing dataset TempReason to evaluate the temporal reasoning capability of large language models. Our dataset includes questions of three temporal reasoning levels. In addition, we also propose a novel learning framework to improve the temporal reasoning capability of large language models, based on temporal span extraction and time-sensitive reinforcement learning. We conducted experiments in closed book QA, open book QA, and reasoning QA settings and demonstrated the effectiveness of our approach.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.828.pdf",
        "keywords": [
            "temporal reasoning",
            "temporal span",
            "reinforcement learning",
            "benchmarking",
            "learning framework",
            "time dependent question answering"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Previous time-dependent question answering (QA) datasets tend to be biased in either their coverage of time spans or question types.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Previous time-dependent question answering (QA) datasets tend to be biased in either their coverage of time spans or question types.\""
    },
    {
        "title": "Large Language Models Are Reasoning Teachers",
        "authors": [
            "Namgyu Ho",
            "Laura Schmid",
            "Se-Young Yun"
        ],
        "published": "2023",
        "summary": "Recent works have shown that chain-of-thought (CoT) prompting can elicit language models to solve complex reasoning tasks, step-by-step. However, prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale. In this paper, we use these large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude. We propose Fine-tune-CoT, a method that generates reasoning samples from very large teacher models to fine-tune smaller models. We evaluate our method on a wide range of public models and complex tasks. We find that Fine-tune-CoT enables substantial reasoning capability in small models, far outperforming prompt-based baselines and even the teacher model in many tasks. Additionally, we extend our method by leveraging the teacher model’s ability to generate multiple distinct rationales for each original sample. Enriching the fine-tuning data with such diverse reasoning results in a substantial performance boost across datasets, even for very small models. We conduct ablations and sample studies to understand the emergence of reasoning capabilities of student models. Our code implementation and data are available at https://github.com/itsnamgyu/reasoning-teacher.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.830.pdf",
        "keywords": [
            "large language models",
            "reasoning",
            "reasoning teacher",
            "teachers"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale.\""
    },
    {
        "title": "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations",
        "authors": [
            "Wenting Zhao",
            "Justin Chiu",
            "Claire Cardie",
            "Alexander Rush"
        ],
        "published": "2023",
        "summary": "Abductive reasoning aims to find plausible explanations for an event. This style of reasoning is critical for commonsense tasks where there are often multiple plausible explanations. Existing approaches for abductive reasoning in natural language processing (NLP) often rely on manually generated annotations for supervision; however, such annotations can be subjective and biased. Instead of using direct supervision, this work proposes an approach for abductive commonsense reasoning that exploits the fact that only a subset of explanations is correct for a given context. The method uses posterior regularization to enforce a mutual exclusion constraint, encouraging the model to learn the distinction between fluent explanations and plausible ones. We evaluate our approach on a diverse set of abductive reasoning datasets; experimental results show that our approach outperforms or is comparable to directly applying pretrained language models in a zero-shot manner and other knowledge-augmented zero-shot methods.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.831.pdf",
        "keywords": [
            "explanations",
            "commonsense",
            "abductive",
            "abductive commonsense reasoning"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Visually-augmented pretrained language models for NLP tasks without images",
        "authors": [
            "Hangyu Guo",
            "Kun Zhou",
            "Wayne Xin Zhao",
            "Qinyu Zhang",
            "Ji-Rong Wen"
        ],
        "published": "2023",
        "summary": "Although pre-trained language models (PLMs) have shown impressive performance by text-only self-supervised training, they are found lack of visual semantics or commonsense. Existing solutions often rely on explicit images for visual knowledge augmentation (requiring time-consuming retrieval or generation), and they also conduct the augmentation for the whole input text, without considering whether it is actually needed in specific inputs or tasks. To address these issues, we propose a novel **V**isually-**A**ugmented fine-tuning approach that can be generally applied to various PLMs or NLP tasks, **W**ithout using any retrieved or generated **I**mages, namely **VAWI**. Experimental results show that our approach can consistently improve the performance of BERT, RoBERTa, BART, and T5 at different scales, and outperform several competitive baselines on ten tasks. Our codes and data are publicly available at https://github.com/RUCAIBox/VAWI.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.833.pdf",
        "keywords": [
            "language models",
            "trained language models",
            "nlp",
            "visual knowledge augmentation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although pre-trained language models (PLMs) have shown impressive performance by text-only self-supervised training, they are found lack of visual semantics or commonsense.\"\n\nThis rating is given because the paper mentions a limitation of pre-trained language models (PLMs), which is a type of LLM, but does not explore it in depth and focuses on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although pre-trained language models (PLMs) have shown impressive performance by text-only self-supervised training, they are found lack of visual semantics or commonsense.\"\n\nThis rating is given because the paper mentions a limitation of pre-trained language models (PLMs), which is a type of LLM, but does not explore it in depth and focuses on the proposed solution."
    },
    {
        "title": "MeetingQA: Extractive Question-Answering on Meeting Transcripts",
        "authors": [
            "Archiki Prasad",
            "Trung Bui",
            "Seunghyun Yoon",
            "Hanieh Deilamsalehy",
            "Franck Dernoncourt",
            "Mohit Bansal"
        ],
        "published": "2023",
        "summary": "With the ubiquitous use of online meeting platforms and robust automatic speech recognition systems, meeting transcripts have emerged as a promising domain for natural language tasks. Most recent works on meeting transcripts primarily focus on summarization and extraction of action items. However, meeting discussions also have a useful question-answering (QA) component, crucial to understanding the discourse or meeting content, and can be used to build interactive interfaces on top of long transcripts. Hence, in this work, we leverage this inherent QA component of meeting discussions and introduce MeetingQA, an extractive QA dataset comprising of questions asked by meeting participants and corresponding responses. As a result, questions can be open-ended and actively seek discussions, while the answers can be multi-span and distributed across multiple speakers. Our comprehensive empirical study of several robust baselines including long-context language models and recent instruction-tuned models reveals that models perform poorly on this task (F1 = 57.3) and severely lag behind human performance (F1 = 84.6), thus presenting a challenging new task for the community to improve upon.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.837.pdf",
        "keywords": [
            "extractive question answering",
            "extractive",
            "transcripts"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our comprehensive empirical study of several robust baselines including long-context language models and recent instruction-tuned models reveals that models perform poorly on this task (F1 = 57.3) and severely lag behind human performance (F1 = 84.6), thus presenting a challenging new task for the community to improve upon.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Our comprehensive empirical study of several robust baselines including long-context language models and recent instruction-tuned models reveals that models perform poorly on this task (F1 = 57.3) and severely lag behind human performance (F1 = 84.6), thus presenting a challenging new task for the community to improve upon.\""
    },
    {
        "title": "FERMAT: An Alternative to Accuracy for Numerical Reasoning",
        "authors": [
            "Jasivan Sivakumar",
            "Nafise Sadat Moosavi"
        ],
        "published": "2023",
        "summary": "While pre-trained language models achieve impressive performance on various NLP benchmarks, they still struggle with tasks that require numerical reasoning. Recent advances in improving numerical reasoning are mostly achieved using very large language models that contain billions of parameters and are not accessible to everyone. In addition, numerical reasoning is measured using a single score on existing datasets. As a result, we do not have a clear understanding of the strengths and shortcomings of existing models on different numerical reasoning aspects and therefore, potential ways to improve them apart from scaling them up. Inspired by CheckList (Ribeiro et al., 2020), we introduce a multi-view evaluation set for numerical reasoning in English, called FERMAT. Instead of reporting a single score on a whole dataset, FERMAT evaluates models on various key numerical reasoning aspects such as number understanding, mathematical operations, and training dependency. Apart from providing a comprehensive evaluation of models on different numerical reasoning aspects, FERMAT enables a systematic and automated generation of an arbitrarily large training or evaluation set for each aspect. The datasets and codes are publicly available to generate further multi-view data for ulterior tasks and languages.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.838.pdf",
        "keywords": [
            "numerical reasoning",
            "accuracy"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While pre-trained language models achieve impressive performance on various NLP benchmarks, they still struggle with tasks that require numerical reasoning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"While pre-trained language models achieve impressive performance on various NLP benchmarks, they still struggle with tasks that require numerical reasoning.\""
    },
    {
        "title": "Don’t Forget Your ABC’s: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems",
        "authors": [
            "Sarah E. Finch",
            "James D. Finch",
            "Jinho D. Choi"
        ],
        "published": "2023",
        "summary": "Despite tremendous advancements in dialogue systems, stable evaluation still requires human judgments producing notoriously high-variance metrics due to their inherent subjectivity. Moreover, methods and labels in dialogue evaluation are not fully standardized, especially for open-domain chats, with a lack of work to compare and assess the validity of those approaches. The use of inconsistent evaluation can misinform the performance of a dialogue system, which becomes a major hurdle to enhance it. Thus, a dimensional evaluation of chat-oriented open-domain dialogue systems that reliably measures several aspects of dialogue capabilities is desired. This paper presents a novel human evaluation method to estimate the rates of many{pasted macro ‘LN’} dialogue system behaviors. Our method is used to evaluate four state-of-the-art open-domain dialogue systems and compared with existing approaches. The analysis demonstrates that our behavior method is more suitable than alternative Likert-style or comparative approaches for dimensional evaluation of these systems.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.839.pdf",
        "keywords": [
            "dialogue",
            "dialogue evaluation",
            "dialogue systems",
            "chats",
            "forget",
            "chat oriented dialogue systems",
            "standardized"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Decoder Tuning: Efficient Language Understanding as Decoding",
        "authors": [
            "Ganqu Cui",
            "Wentao Li",
            "Ning Ding",
            "Longtao Huang",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published": "2023",
        "summary": "With the evergrowing sizes of pre-trained models (PTMs), it has been an emerging practice to only provide the inference APIs for users, namely model-as-a-service (MaaS) setting. To adapt PTMs with model parameters frozen, most current approaches focus on the input side, seeking powerful prompts to stimulate models for correct answers. However, we argue that input-side adaptation could be arduous due to the lack of gradient signals and they usually require thousands of API queries, resulting in high computation and time costs. Specifically, DecT first extracts prompt-stimulated output scores for initial predictions. On top of that, we train an additional decoder network on the output representations to incorporate posterior data knowledge. By gradient-based optimization, DecT can be trained within several seconds and requires only one PTM query per sample. Empirically, we conduct extensive natural language understanding experiments and show that DecT significantly outperforms state-of-the-art algorithms with a 200x speed-up. Our code is available at https://github.com/thunlp/DecT.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.840.pdf",
        "keywords": [
            "input side adaptation",
            "pre trained models",
            "decoder tuning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, we argue that input-side adaptation could be arduous due to the lack of gradient signals and they usually require thousands of API queries, resulting in high computation and time costs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, we argue that input-side adaptation could be arduous due to the lack of gradient signals and they usually require thousands of API queries, resulting in high computation and time costs.\""
    },
    {
        "title": "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources",
        "authors": [
            "Akshatha Arodi",
            "Martin Pömsl",
            "Kaheer Suleman",
            "Adam Trischler",
            "Alexandra Olteanu",
            "Jackie Chi Kit Cheung"
        ],
        "published": "2023",
        "summary": "Many state-of-the-art natural language understanding (NLU) models are based on pretrained neural language models. These models often make inferences using information from multiple sources. An important class of such inferences are those that require both background knowledge, presumably contained in a model’s pretrained parameters, and instance-specific information that is supplied at inference time. However, the integration and reasoning abilities of NLU models in the presence of multiple knowledge sources have been largely understudied. In this work, we propose a test suite of coreference resolution subtasks that require reasoning over multiple facts. These subtasks differ in terms of which knowledge sources contain the relevant facts. We also introduce subtasks where knowledge is present only at inference time using fictional knowledge. We evaluate state-of-the-art coreference resolution models on our dataset. Our results indicate that several models struggle to reason on-the-fly over knowledge observed both at pretrain time and at inference time. However, with task-specific training, a subset of models demonstrates the ability to integrate certain knowledge types from multiple sources. Still, even the best performing models seem to have difficulties with reliably integrating knowledge presented only at inference time.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.841.pdf",
        "keywords": [
            "knowledge",
            "knowledge integration",
            "kitmus test",
            "coreference resolution",
            "natural language"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the integration and reasoning abilities of NLU models in the presence of multiple knowledge sources have been largely understudied.\"; \"Our results indicate that several models struggle to reason on-the-fly over knowledge observed both at pretrain time and at inference time.\"; \"Still, even the best performing models seem to have difficulties with reliably integrating knowledge presented only at inference time.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, the integration and reasoning abilities of NLU models in the presence of multiple knowledge sources have been largely understudied.\"; \"Our results indicate that several models struggle to reason on-the-fly over knowledge observed both at pretrain time and at inference time.\"; \"Still, even the best performing models seem to have difficulties with reliably integrating knowledge presented only at inference time.\""
    },
    {
        "title": "On Improving Summarization Factual Consistency from Natural Language Feedback",
        "authors": [
            "Yixin Liu",
            "Budhaditya Deb",
            "Milagro Teruel",
            "Aaron Halfaker",
            "Dragomir Radev",
            "Ahmed Hassan Awadallah"
        ],
        "published": "2023",
        "summary": "Despite the recent progress in language generation models, their outputs may not always meet user expectations. In this work, we study whether informational feedback in natural language can be leveraged to improve generation quality and user preference alignment. To this end, we consider factual consistency in summarization, the quality that the summary should only contain information supported by the input documents, as the user-expected preference. We collect a high-quality dataset, DeFacto, containing human demonstrations and informational natural language feedback consisting of corrective instructions, edited summaries, and explanations with respect to the factual consistency of the summary. Using our dataset, we study three natural language generation tasks: (1) editing a summary by following the human feedback, (2) generating human feedback for editing the original summary, and (3) revising the initial summary to correct factual errors by generating both the human feedback and edited summary. We show that DeFacto can provide factually consistent human-edited summaries and further insights into summarization factual consistency thanks to its informational natural language feedback. We further demonstrate that fine-tuned language models can leverage our dataset to improve the summary factual consistency, while large language models lack the zero-shot learning ability in our proposed tasks that require controllable text generation.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.844.pdf",
        "keywords": [
            "factual consistency",
            "feedback",
            "factual errors",
            "natural language",
            "informational natural language feedback",
            "informational feedback",
            "edited",
            "language models",
            "natural language generation",
            "summarization factual consistency",
            "summarization",
            "summary factual consistency",
            "quality"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"while large language models lack the zero-shot learning ability in our proposed tasks that require controllable text generation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"while large language models lack the zero-shot learning ability in our proposed tasks that require controllable text generation.\""
    },
    {
        "title": "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models",
        "authors": [
            "Julia Mendelsohn",
            "Ronan Le Bras",
            "Yejin Choi",
            "Maarten Sap"
        ],
        "published": "2023",
        "summary": "Dogwhistles are coded expressions that simultaneously convey one meaning to a broad audience and a second, often hateful or provocative, meaning to a narrow in-group; they are deployed to evade both political repercussions and algorithmic content moderation. For example, the word “cosmopolitan” in a sentence such as “we need to end the cosmopolitan experiment” can mean “worldly” to many but also secretly mean “Jewish” to a select few. We present the first large-scale computational investigation of dogwhistles. We develop a typology of dogwhistles, curate the largest-to-date glossary of over 300 dogwhistles with rich contextual information and examples, and analyze their usage in historical U.S. politicians’ speeches. We then assess whether a large language model (GPT-3) can identify dogwhistles and their meanings, and find that GPT-3’s performance varies widely across types of dogwhistles and targeted groups. Finally, we show that harmful content containing dogwhistles avoids toxicity detection, highlighting online risks presented by such coded language. This work sheds light on the theoretical and applied importance of dogwhistles in both NLP and computational social science, and provides resources to facilitate future research in modeling dogwhistles and mitigating their online harms.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.845.pdf",
        "keywords": [
            "dogwhistles",
            "coded language",
            "language models",
            "coded rhetoric",
            "bullhorns"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We then assess whether a large language model (GPT-3) can identify dogwhistles and their meanings, and find that GPT-3’s performance varies widely across types of dogwhistles and targeted groups. Finally, we show that harmful content containing dogwhistles avoids toxicity detection, highlighting online risks presented by such coded language.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We then assess whether a large language model (GPT-3) can identify dogwhistles and their meanings, and find that GPT-3’s performance varies widely across types of dogwhistles and targeted groups. Finally, we show that harmful content containing dogwhistles avoids toxicity detection, highlighting online risks presented by such coded language.\""
    },
    {
        "title": "Exploring Large Language Models for Classical Philology",
        "authors": [
            "Frederick Riemenschneider",
            "Anette Frank"
        ],
        "published": "2023",
        "summary": "Recent advances in NLP have led to the creation of powerful language models for many languages including Ancient Greek and Latin. While prior work on Classical languages unanimously uses BERT, in this work we create four language models for Ancient Greek that vary along two dimensions to study their versatility for tasks of interest for Classical languages: we explore (i) encoder-only and encoder-decoder architectures using RoBERTa and T5 as strong model types, and create for each of them (ii) a monolingual Ancient Greek and a multilingual instance that includes Latin and English. We evaluate all models on morphological and syntactic tasks, including lemmatization, which demonstrates the added value of T5’s decoding abilities. We further define two probing tasks to investigate the knowledge acquired by models pre-trained on Classical texts. Our experiments provide the first benchmarking analysis of existing models of Ancient Greek. Results show that our models provide significant improvements over the SoTA. The systematic analysis of model types can inform future research in designing language models for Classical languages, including the development of novel generative tasks. We make all our models available as community resources, along with a large curated pre-training corpus for Ancient Greek, to support the creation of a larger, comparable model zoo for Classical Philology.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.846.pdf",
        "keywords": [
            "philology",
            "language models",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicitly mentioned, but the paper aims to \"inform future research in designing language models for Classical languages\", implying that current models may have limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None explicitly mentioned, but the paper aims to \"inform future research in designing language models for Classical languages\", implying that current models may have limitations."
    },
    {
        "title": "DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media",
        "authors": [
            "Mario Aragon",
            "Adrian Pastor Lopez Monroy",
            "Luis Gonzalez",
            "David E. Losada",
            "Manuel Montes"
        ],
        "published": "2023",
        "summary": "Mental disorders affect millions of people worldwide and cause interference with their thinking and behavior. Through the past years, awareness created by health campaigns and other sources motivated the study of these disorders using information extracted from social media platforms. In this work, we aim to contribute to the study of these disorders and to the understanding of how mental problems reflect on social media. To achieve this goal, we propose a double-domain adaptation of a language model. First, we adapted the model to social media language, and then, we adapted it to the mental health domain. In both steps, we incorporated a lexical resource to guide the masking process of the language model and, therefore, to help it in paying more attention to words related to mental disorders. We have evaluated our model in the detection of signs of three major mental disorders: Anorexia, Self-harm, and Depression. Results are encouraging as they show that the proposed adaptation enhances the classification performance and yields competitive results against state-of-the-art methods.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.853.pdf",
        "keywords": [
            "mental disorders",
            "signs",
            "double domain adaptation",
            "social media",
            "language model",
            "signs of mental disorders",
            "mental health"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"First, we adapted the model to social media language, and then, we adapted it to the mental health domain.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"First, we adapted the model to social media language, and then, we adapted it to the mental health domain.\""
    },
    {
        "title": "Toward Interactive Dictation",
        "authors": [
            "Belinda Z. Li",
            "Jason Eisner",
            "Adam Pauls",
            "Sam Thomson"
        ],
        "published": "2023",
        "summary": "Voice dictation is an increasingly important text input modality. Existing systems that allow both dictation and editing-by-voice restrict their command language to flat templates invoked by trigger words. In this work, we study the feasibility of allowing users to interrupt their dictation with spoken editing commands in open-ended natural language. We introduce a new task and dataset, TERTiUS, to experiment with such systems. To support this flexibility in real-time, a system must incrementally segment and classify spans of speech as either dictation or command, and interpret the spans that are commands. We experiment with using large pre-trained language models to predict the edited text, or alternatively, to predict a small text-editing program. Experiments show a natural trade-off between model accuracy and latency: a smaller model achieves 30% end-state accuracy with 1.3 seconds of latency, while a larger model achieves 55% end-state accuracy with 7 seconds of latency.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.854.pdf",
        "keywords": [
            "segment",
            "dictation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Experiments show a natural trade-off between model accuracy and latency: a smaller model achieves 30% end-state accuracy with 1.3 seconds of latency, while a larger model achieves 55% end-state accuracy with 7 seconds of latency.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Experiments show a natural trade-off between model accuracy and latency: a smaller model achieves 30% end-state accuracy with 1.3 seconds of latency, while a larger model achieves 55% end-state accuracy with 7 seconds of latency.\""
    },
    {
        "title": "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors",
        "authors": [
            "Peng Li",
            "Tianxiang Sun",
            "Qiong Tang",
            "Hang Yan",
            "Yuanbin Wu",
            "Xuanjing Huang",
            "Xipeng Qiu"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) pre-trained on massive corpora have demonstrated impressive few-shot learning ability on many NLP tasks. A common practice is to recast the task into a text-to-text format such that generative LLMs of natural language (NL-LLMs) like GPT-3 can be prompted to solve it. However, it is nontrivial to perform information extraction (IE) tasks with NL-LLMs since the output of the IE task is usually structured and therefore is hard to be converted into plain text. In this paper, we propose to recast the structured output in the form of code instead of natural language and utilize generative LLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular, named entity recognition and relation extraction. In contrast to NL-LLMs, we show that Code-LLMs can be well-aligned with these IE tasks by designing code-style prompts and formulating these IE tasks as code generation tasks. Experiment results on seven benchmarks show that our method consistently outperforms fine-tuning moderate-size pre-trained models specially designed for IE tasks (e.g., UIE) and prompting NL-LLMs under few-shot settings. We further conduct a series of in-depth analyses to demonstrate the merits of leveraging Code-LLMs for IE tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.855.pdf",
        "keywords": [
            "information extraction",
            "code generation models",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it is nontrivial to perform information extraction (IE) tasks with NL-LLMs since the output of the IE task is usually structured and therefore is hard to be converted into plain text.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, it is nontrivial to perform information extraction (IE) tasks with NL-LLMs since the output of the IE task is usually structured and therefore is hard to be converted into plain text.\""
    },
    {
        "title": "Prompting PaLM for Translation: Assessing Strategies and Performance",
        "authors": [
            "David Vilar",
            "Markus Freitag",
            "Colin Cherry",
            "Jiaming Luo",
            "Viresh Ratnakar",
            "George Foster"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) that have been trained on multilingual but not parallel text exhibit a remarkable ability to translate between languages. We probe this ability in an in-depth study of the pathways language model (PaLM), which has demonstrated the strongest machine translation (MT) performance among similarly-trained LLMs to date. We investigate various strategies for choosing translation examples for few-shot prompting, concluding that example quality is the most important factor. Using optimized prompts, we revisit previous assessments of PaLM’s MT capabilities with more recent test sets, modern MT metrics, and human evaluation, and find that its performance, while impressive, still lags that of state-of-the-art supervised systems. We conclude by providing an analysis of PaLM’s MT output which reveals some interesting properties and prospects for future work.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.859.pdf",
        "keywords": [
            "translation",
            "prompting",
            "strategies",
            "language models",
            "pathways language model"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We... find that its performance, while impressive, still lags that of state-of-the-art supervised systems.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We... find that its performance, while impressive, still lags that of state-of-the-art supervised systems.\""
    },
    {
        "title": "Exploring Lottery Prompts for Pre-trained Language Models",
        "authors": [
            "Yulin Chen",
            "Ning Ding",
            "Xiaobin Wang",
            "Shengding Hu",
            "Haitao Zheng",
            "Zhiyuan Liu",
            "Pengjun Xie"
        ],
        "published": "2023",
        "summary": "Consistently scaling pre-trained language models (PLMs) imposes substantial burdens on model adaptation, necessitating more efficient alternatives to conventional fine-tuning. Given the advantage of prompting in the zero-shot setting and the observed performance fluctuation among different prompts, we explore the instance-level prompt and their generalizability.By searching through the prompt space, we first validate the assumption that for every instance, there is almost always a lottery prompt that induces the correct prediction from the PLM, and such prompt can be obtained at a low cost thanks to the inherent ability of PLMs.Meanwhile, it is shown that some strong lottery prompts have high performance over the whole training set, and they are equipped with distinguishable linguistic features. Lastly, we attempt to generalize the searched strong lottery prompts to unseen data with prompt ensembling method. Experiments are conducted on various types of NLP classification tasks and demonstrate that the proposed method can achieve comparable results with other gradient-free and optimization-free baselines.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.860.pdf",
        "keywords": [
            "pre trained language models",
            "prompts",
            "lottery prompts",
            "lottery",
            "classification"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Consistently scaling pre-trained language models (PLMs) imposes substantial burdens on model adaptation, necessitating more efficient alternatives to conventional fine-tuning.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of pre-trained language models (burden of scaling and adaptation), but it is not the primary focus of the paper and is only briefly mentioned to motivate the proposed method",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Consistently scaling pre-trained language models (PLMs) imposes substantial burdens on model adaptation, necessitating more efficient alternatives to conventional fine-tuning.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of pre-trained language models (burden of scaling and adaptation), but it is not the primary focus of the paper and is only briefly mentioned to motivate the proposed method"
    },
    {
        "title": "LeXFiles and LegalLAMA: Facilitating English Multinational Legal Language Model Development",
        "authors": [
            "Ilias Chalkidis",
            "Nicolas Garneau",
            "Catalina Goanta",
            "Daniel Katz",
            "Anders Søgaard"
        ],
        "published": "2023",
        "summary": "In this work, we conduct a detailed analysis on the performance of legal-oriented pre-trained language models (PLMs). We examine the interplay between their original objective, acquired knowledge, and legal language understanding capacities which we define as the upstream, probing, and downstream performance, respectively. We consider not only the models’ size but also the pre-training corpora used as important dimensions in our study. To this end, we release a multinational English legal corpus (LeXFiles) and a legal knowledge probing benchmark (LegalLAMA) to facilitate training and detailed analysis of legal-oriented PLMs. We release two new legal PLMs trained on LeXFiles and evaluate them alongside others on LegalLAMA and LexGLUE. We find that probing performance strongly correlates with upstream performance in related legal topics. On the other hand, downstream performance is mainly driven by the model’s size and prior legal knowledge which can be estimated by upstream and probing performance. Based on these findings, we can conclude that both dimensions are important for those seeking the development of domain-specific PLMs.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.865.pdf",
        "keywords": [
            "lexfiles",
            "legal language",
            "plms",
            "multinational legal language model",
            "corpus"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"On the other hand, downstream performance is mainly driven by the model’s size and prior legal knowledge which can be estimated by upstream and probing performance.\"\n\nThis paper mentions limitations of LLMs in the context of legal language understanding and the importance of model size and prior knowledge, but does not elaborate on these limitations in detail.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"On the other hand, downstream performance is mainly driven by the model’s size and prior legal knowledge which can be estimated by upstream and probing performance.\"\n\nThis paper mentions limitations of LLMs in the context of legal language understanding and the importance of model size and prior knowledge, but does not elaborate on these limitations in detail."
    },
    {
        "title": "Revisiting Commonsense Reasoning in Machine Translation: Training, Evaluation and Challenge",
        "authors": [
            "Xuebo Liu",
            "Yutong Wang",
            "Derek F. Wong",
            "Runzhe Zhan",
            "Liangxuan Yu",
            "Min Zhang"
        ],
        "published": "2023",
        "summary": "The ability of commonsense reasoning (CR) decides whether a neural machine translation (NMT) model can move beyond pattern recognition. Despite the rapid advancement of NMT and the use of pretraining to enhance NMT models, research on CR in NMT is still in its infancy, leaving much to be explored in terms of effectively training NMT models with high CR abilities and devising accurate automatic evaluation metrics. This paper presents a comprehensive study aimed at expanding the understanding of CR in NMT.For the training, we confirm the effectiveness of incorporating pretrained knowledge into NMT models and subsequently utilizing these models as robust testbeds for investigating CR in NMT. For the evaluation, we propose a novel entity-aware evaluation method that takes into account both the NMT candidate and important entities in the candidate, which is more aligned with human judgement. Based on the strong testbed and evaluation methods, we identify challenges in training NMT models with high CR abilities and suggest directions for further unlabeled data utilization and model design. We hope that our methods and findings will contribute to advancing the research of CR in NMT. Source data, code and scripts are freely available at https://github.com/YutongWang1216/CR-NMT.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.866.pdf",
        "keywords": [
            "machine translation",
            "entity aware evaluation",
            "neural machine translation",
            "commonsense reasoning"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.\n\n(Note: Although the paper discusses limitations of NMT models, it does not specifically discuss LLMs or their limitations.)",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs.\n\n(Note: Although the paper discusses limitations of NMT models, it does not specifically discuss LLMs or their limitations.)"
    },
    {
        "title": "NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models",
        "authors": [
            "Kai Mei",
            "Zheng Li",
            "Zhenting Wang",
            "Yang Zhang",
            "Shiqing Ma"
        ],
        "published": "2023",
        "summary": "Prompt-based learning is vulnerable to backdoor attacks. Existing backdoor attacks against prompt-based models consider injecting backdoors into the entire embedding layers or word embedding vectors. Such attacks can be easily affected by retraining on downstream tasks and with different prompting strategies, limiting the transferability of backdoor attacks. In this work, we propose transferable backdoor attacks against prompt-based models, called NOTABLE, which is independent of downstream tasks and prompting strategies. Specifically, NOTABLE injects backdoors into the encoders of PLMs by utilizing an adaptive verbalizer to bind triggers to specific words (i.e., anchors). It activates the backdoor by pasting input with triggers to reach adversary-desired anchors, achieving independence from downstream tasks and prompting strategies. We conduct experiments on six NLP tasks, three popular models, and three prompting strategies. Empirical results show that NOTABLE achieves superior attack performance (i.e., attack success rate over 90% on all the datasets), and outperforms two state-of-the-art baselines. Evaluations on three defenses show the robustness of NOTABLE. Our code can be found at https://github.com/RU-System-Software-and-Security/Notable.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.867.pdf",
        "keywords": [
            "backdoor",
            "nlp",
            "prompting strategies",
            "backdoor attacks",
            "transferable backdoor attacks",
            "prompt based nlp models",
            "plms"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing backdoor attacks against prompt-based models consider injecting backdoors into the entire embedding layers or word embedding vectors. Such attacks can be easily affected by retraining on downstream tasks and with different prompting strategies, limiting the transferability of backdoor attacks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Existing backdoor attacks against prompt-based models consider injecting backdoors into the entire embedding layers or word embedding vectors. Such attacks can be easily affected by retraining on downstream tasks and with different prompting strategies, limiting the transferability of backdoor attacks.\""
    },
    {
        "title": "Revisiting Relation Extraction in the era of Large Language Models",
        "authors": [
            "Somin Wadhwa",
            "Silvio Amir",
            "Byron Wallace"
        ],
        "published": "2023",
        "summary": "Relation extraction (RE) is the core NLP task of inferring semantic relationships between entities from text. Standard supervised RE techniques entail training modules to tag tokens comprising entity spans and then predict the relationship between them. Recent work has instead treated the problem as a sequence-to-sequence task, linearizing relations between entities as target strings to be generated conditioned on the input. Here we push the limits of this approach, using larger language models (GPT-3 and Flan-T5 large) than considered in prior work and evaluating their performance on standard RE tasks under varying levels of supervision. We address issues inherent to evaluating generative approaches to RE by doing human evaluations, in lieu of relying on exact matching. Under this refined evaluation, we find that: (1) Few-shot prompting with GPT-3 achieves near SOTA performance, i.e., roughly equivalent to existing fully supervised models; (2) Flan-T5 is not as capable in the few-shot setting, but supervising and fine-tuning it with Chain-of-Thought (CoT) style explanations (generated via GPT-3) yields SOTA results. We release this model as a new baseline for RE tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.868.pdf",
        "keywords": [
            "relation extraction",
            "large language models",
            "larger language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Flan-T5 is not as capable in the few-shot setting\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Flan-T5 is not as capable in the few-shot setting\""
    },
    {
        "title": "Pre-trained Language Models Can be Fully Zero-Shot Learners",
        "authors": [
            "Xuandong Zhao",
            "Siqi Ouyang",
            "Zhiguo Yu",
            "Ming Wu",
            "Lei Li"
        ],
        "published": "2023",
        "summary": "How can we extend a pre-trained model to many language understanding tasks, without labeled or additional unlabeled data? Pre-trained language models (PLMs) have been effective for a wide range of NLP tasks. However, existing approaches either require fine-tuning on downstream labeled datasets or manually constructing proper prompts. In this paper, we propose nonparametric prompting PLM (NPPrompt) for fully zero-shot language understanding. Unlike previous methods, NPPrompt uses only pre-trained language models and does not require any labeled data or additional raw corpus for further fine-tuning, nor does it rely on humans to construct a comprehensive set of prompt label words. We evaluate NPPrompt against previous major few-shot and zero-shot learning methods on diverse NLP tasks: including text classification, text entailment, similar text retrieval, paraphrasing, and multiple-choice question answering. Experimental results demonstrate that our NPPrompt outperforms the previous best fully zero-shot method by big margins, with absolute gains of 12.8% in accuracy on text classification and 15.6% on the GLUE benchmark. Our source code is available at https://anonymous.4open.science/r/NPPrompt.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.869.pdf",
        "keywords": [
            "trained language models",
            "pre trained language models",
            "nonparametric prompting plm",
            "zero shot"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "Can Large Language Models Be an Alternative to Human Evaluations?",
        "authors": [
            "Cheng-Han Chiang",
            "Hung-yi Lee"
        ],
        "published": "2023",
        "summary": "Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms. Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided. In this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation. We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation. We use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: open-ended story generation and adversarial attacks. We show that the result of LLM evaluation is consistent with the results obtained by expert human evaluation: the texts rated higher by human experts are also rated higher by the LLMs.We also find that the results of LLM evaluation are stable over different formatting of the task instructions and the sampling algorithm used to generate the answer. We are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.870.pdf",
        "keywords": [
            "evaluations",
            "human evaluation",
            "expert human evaluation",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation.\""
    },
    {
        "title": "An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models",
        "authors": [
            "Zhongbin Xie",
            "Thomas Lukasiewicz"
        ],
        "published": "2023",
        "summary": "The increasingly large size of modern pre-trained language models not only makes them inherit more human-like biases from the training corpora, but also makes it computationally expensive to mitigate such biases. In this paper, we investigate recent parameter-efficient methods in combination with counterfactual data augmentation (CDA) for bias mitigation. We conduct extensive experiments with prefix tuning, prompt tuning, and adapter tuning on different language models and bias types to evaluate their debiasing performance and abilities to preserve the internal knowledge of a pre-trained model. We find that the parameter-efficient methods (i) are effective in mitigating gender bias, where adapter tuning is consistently the most effective one and prompt tuning is more suitable for GPT-2 than BERT, (ii) areless effective when it comes to racial and religious bias, which may be attributed to the limitations of CDA, and (iii) can perform similarly to or sometimes better than full fine-tuning with improved time and memory efficiency, as well as maintain the internal knowledge in BERT and GPT-2, evaluated via fact retrieval and downstream fine-tuning.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.876.pdf",
        "keywords": [
            "debiasing",
            "pre trained language models",
            "parameter efficient",
            "counterfactual data augmentation",
            "empirical analysis",
            "bias mitigation"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The increasingly large size of modern pre-trained language models not only makes them inherit more human-like biases from the training corpora, but also makes it computationally expensive to mitigate such biases\"; \"are less effective when it comes to racial and religious bias, which may be attributed to the limitations of CDA\".",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"The increasingly large size of modern pre-trained language models not only makes them inherit more human-like biases from the training corpora, but also makes it computationally expensive to mitigate such biases\"; \"are less effective when it comes to racial and religious bias, which may be attributed to the limitations of CDA\"."
    },
    {
        "title": "Two-Stage Fine-Tuning for Improved Bias and Variance for Large Pretrained Language Models",
        "authors": [
            "Lijing Wang",
            "Yingya Li",
            "Timothy Miller",
            "Steven Bethard",
            "Guergana Savova"
        ],
        "published": "2023",
        "summary": "The bias-variance tradeoff is the idea that learning methods need to balance model complexity with data size to minimize both under-fitting and over-fitting. Recent empirical work and theoretical analysis with over-parameterized neural networks challenges the classic bias-variance trade-off notion suggesting that no such trade-off holds: as the width of the network grows, bias monotonically decreases while variance initially increases followed by a decrease. In this work, we first provide a variance decomposition-based justification criteria to examine whether large pretrained neural models in a fine-tuning setting are generalizable enough to have low bias and variance. We then perform theoretical and empirical analysis using ensemble methods explicitly designed to decrease variance due to optimization. This results in essentially a two-stage fine-tuning algorithm that first ratchets down bias and variance iteratively, and then uses a selected fixed-bias model to further reduce variance due to optimization by ensembling. We also analyze the nature of variance change with the ensemble size in low- and high-resource classes. Empirical results show that this two-stage method obtains strong results on SuperGLUE tasks and clinical information extraction tasks. Code and settings are available: https://github.com/christa60/bias-var-fine-tuning-plms.git",
        "pdf_link": "https://aclanthology.org/2023.acl-long.877.pdf",
        "keywords": [
            "variance",
            "bias variance",
            "bias",
            "tuning",
            "fine tuning",
            "language models",
            "bias variance tradeoff",
            "ensemble methods",
            "justification",
            "neural networks"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent empirical work and theoretical analysis with over-parameterized neural networks challenges the classic bias-variance trade-off notion suggesting that no such trade-off holds: as the width of the network grows, bias monotonically decreases while variance initially increases followed by a decrease.\"\n\nThis paper mentions a limitation of LLMs in passing, related to the bias-variance trade-off, but does not explore",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Recent empirical work and theoretical analysis with over-parameterized neural networks challenges the classic bias-variance trade-off notion suggesting that no such trade-off holds: as the width of the network grows, bias monotonically decreases while variance initially increases followed by a decrease.\"\n\nThis paper mentions a limitation of LLMs in passing, related to the bias-variance trade-off, but does not explore"
    },
    {
        "title": "A Comparative Study on the Impact of Model Compression Techniques on Fairness in Language Models",
        "authors": [
            "Krithika Ramesh",
            "Arnav Chavan",
            "Shrey Pandit",
            "Sunayana Sitaram"
        ],
        "published": "2023",
        "summary": "Compression techniques for deep learning have become increasingly popular, particularly in settings where latency and memory constraints are imposed. Several methods, such as pruning, distillation, and quantization, have been adopted for compressing models, each providing distinct advantages. However, existing literature demonstrates that compressing deep learning models could affect their fairness. Our analysis involves a comprehensive evaluation of pruned, distilled, and quantized language models, which we benchmark across a range of intrinsic and extrinsic metrics for measuring bias in text classification. We also investigate the impact of using multilingual models and evaluation measures. Our findings highlight the significance of considering both the pre-trained model and the chosen compression strategy in developing equitable language technologies. The results also indicate that compression strategies can have an adverse effect on fairness measures.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.878.pdf",
        "keywords": [
            "fairness",
            "language models",
            "model compression techniques"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our findings highlight the significance of considering both the pre-trained model and the chosen compression strategy in developing equitable language technologies. The results also indicate that compression strategies can have an adverse effect on fairness measures.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Our findings highlight the significance of considering both the pre-trained model and the chosen compression strategy in developing equitable language technologies. The results also indicate that compression strategies can have an adverse effect on fairness measures.\""
    },
    {
        "title": "Few-shot Reranking for Multi-hop QA via Language Model Prompting",
        "authors": [
            "Muhammad Khalifa",
            "Lajanugen Logeswaran",
            "Moontae Lee",
            "Honglak Lee",
            "Lu Wang"
        ],
        "published": "2023",
        "summary": "We study few-shot reranking for multi-hop QA (MQA) with open-domain questions. To alleviate the need for a large number of labeled question-document pairs for retriever training, we propose PromptRank, which relies on language model prompting for multi-hop path reranking. PromptRank first constructs an instruction-based prompt that includes a candidate document path and then computes the relevance score between a given question and the path based on the conditional likelihood of the question given the path prompt according to a language model. PromptRank yields strong retrieval performance on HotpotQA with only 128 training examples compared to state-of-the-art methods trained on thousands of examples — 73.6 recall@10 by PromptRank vs. 77.8 by PathRetriever and 77.5 by multi-hop dense retrieval.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.885.pdf",
        "keywords": [
            "shot reranking",
            "multi hop qa",
            "multi hop path reranking",
            "language model",
            "few shot reranking",
            "language model prompting",
            "relevance score"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations",
        "authors": [
            "Yusen Zhang",
            "Jun Wang",
            "Zhiguo Wang",
            "Rui Zhang"
        ],
        "published": "2023",
        "summary": "Cross-Lingual Semantic Parsing (CLSP) aims to translate queries in multiple natural languages (NLs) into meaning representations (MRs) such as SQL, lambda calculus, and logic forms. However, existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications, impeding a comprehensive and unified evaluation of CLSP on a diverse range of NLs and MRs. To this end, we present XSemPLR, a unified benchmark for cross-lingual semantic parsing featured with 22 natural languages and 8 meaning representations by examining and selecting 9 existing datasets to cover 5 tasks and 164 domains. We use XSemPLR to conduct a comprehensive benchmark study on a wide range of multilingual language models including encoder-based models (mBERT, XLM-R), encoder-decoder models (mBART, mT5), and decoder-based models (Codex, BLOOM). We design 6 experiment settings covering various lingual combinations (monolingual, multilingual, cross-lingual) and numbers of learning samples (full dataset, few-shot, and zero-shot). Our experiments show that encoder-decoder models (mT5) achieve the highest performance compared with other popular models, and multilingual training can further improve the average performance. Notably, multilingual large language models (e.g., BLOOM) are still inadequate to perform CLSP tasks. We also find that the performance gap between monolingual training and cross-lingual transfer learning is still significant for multilingual models, though it can be mitigated by cross-lingual few-shot training. Our dataset and code are available at https://github.com/psunlpgroup/XSemPLR.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.887.pdf",
        "keywords": [
            "natural languages",
            "multiple natural languages",
            "meaning representations",
            "semantic parsing",
            "cross lingual semantic parsing"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Notably, multilingual large language models (e.g., BLOOM) are still inadequate to perform CLSP tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Notably, multilingual large language models (e.g., BLOOM) are still inadequate to perform CLSP tasks.\""
    },
    {
        "title": "Cross-Modal Attribute Insertions for Assessing the Robustness of Vision-and-Language Learning",
        "authors": [
            "Shivaen Ramshetty",
            "Gaurav Verma",
            "Srijan Kumar"
        ],
        "published": "2023",
        "summary": "The robustness of multimodal deep learning models to realistic changes in the input text is critical for applicability on important tasks such as text-to-image retrieval and cross-modal entailment. To measure robustness, several existing approaches edit the text data, but without leveraging the cross-modal information present in multimodal data. Such information from the visual modality, such as color, size, and shape, provides additional attributes that users can include in their inputs. Thus, we propose cross-modal attribute insertions as a realistic perturbation strategy for vision-and-language data that inserts visual attributes of the objects in the image into the corresponding text (e.g., “girl on a chair” to “little girl on a wooden chair”). Our proposed approach for cross-modal attribute insertions is modular, controllable, and task-agnostic. We find that augmenting input text using cross-modal insertions causes state-of-the-art approaches for text-to-image retrieval and cross-modal entailment to perform poorly, resulting in relative drops of ~15% in MRR and ~20% in F1 score, respectively. Crowd-sourced annotations demonstrate that cross-modal insertions lead to higher quality augmentations for multimodal data than augmentations using text-only data, and are equivalent in quality to original examples. We release the code to encourage robustness evaluations of deep vision-and-language models: https://github.com/claws-lab/multimodal-robustness-xmai",
        "pdf_link": "https://aclanthology.org/2023.acl-long.890.pdf",
        "keywords": [
            "cross modal entailment",
            "cross modal insertions",
            "language models",
            "cross modal attribute insertions",
            "language learning",
            "robustness",
            "language data",
            "multimodal robustness",
            "to"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that augmenting input text using cross-modal insertions causes state-of-the-art approaches for text-to-image retrieval and cross-modal entailment to perform poorly, resulting in relative drops of ~15% in MRR and ~20% in F1 score, respectively.\"\n\nThis abstract mentions a limitation of vision-and-language models (which include multimodal models integrating language with other modalities",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We find that augmenting input text using cross-modal insertions causes state-of-the-art approaches for text-to-image retrieval and cross-modal entailment to perform poorly, resulting in relative drops of ~15% in MRR and ~20% in F1 score, respectively.\"\n\nThis abstract mentions a limitation of vision-and-language models (which include multimodal models integrating language with other modalities"
    },
    {
        "title": "Crosslingual Generalization through Multitask Finetuning",
        "authors": [
            "Niklas Muennighoff",
            "Thomas Wang",
            "Lintang Sutawika",
            "Adam Roberts",
            "Stella Biderman",
            "Teven Le Scao",
            "M Saiful Bari",
            "Sheng Shen",
            "Zheng Xin Yong",
            "Hailey Schoelkopf",
            "Xiangru Tang",
            "Dragomir Radev",
            "Alham Fikri Aji",
            "Khalid Almubarak",
            "Samuel Albanie",
            "Zaid Alyafeai",
            "Albert Webson",
            "Edward Raff",
            "Colin Raffel"
        ],
        "published": "2023",
        "summary": "Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task genrealization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.891.pdf",
        "keywords": [
            "finetuning",
            "multitask",
            "crosslingual generalization"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains",
        "authors": [
            "Yanis Labrak",
            "Adrien Bazoge",
            "Richard Dufour",
            "Mickael Rouvier",
            "Emmanuel Morin",
            "Béatrice Daille",
            "Pierre-Antoine Gourraud"
        ],
        "published": "2023",
        "summary": "In recent years, pre-trained language models (PLMs) achieve the best performance on a wide range of natural language processing (NLP) tasks. While the first models were trained on general domain data, specialized ones have emerged to more effectively treat specific domains. In this paper, we propose an original study of PLMs in the medical domain on French language. We compare, for the first time, the performance of PLMs trained on both public data from the web and private data from healthcare establishments. We also evaluate different learning strategies on a set of biomedical tasks. In particular, we show that we can take advantage of already existing biomedical PLMs in a foreign language by further pre-train it on our targeted data. Finally, we release the first specialized PLMs for the biomedical field in French, called DrBERT, as well as the largest corpus of medical data under free license on which these models are trained.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.896.pdf",
        "keywords": [
            "pre trained language models"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the abstract discusses the performance of pre-trained language models and proposes a new model, but does not mention any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the abstract discusses the performance of pre-trained language models and proposes a new model, but does not mention any limitations of LLMs."
    },
    {
        "title": "What Do NLP Researchers Believe? Results of the NLP Community Metasurvey",
        "authors": [
            "Julian Michael",
            "Ari Holtzman",
            "Alicia Parrish",
            "Aaron Mueller",
            "Alex Wang",
            "Angelica Chen",
            "Divyam Madaan",
            "Nikita Nangia",
            "Richard Yuanzhe Pang",
            "Jason Phang",
            "Samuel R. Bowman"
        ],
        "published": "2023",
        "summary": "We present the results of the NLP Community Metasurvey. Run from May to June 2022, it elicited opinions on controversial issues, including industry influence in the field, concerns about AGI, and ethics. Our results put concrete numbers to several controversies: For example, respondents are split in half on the importance of artificial general intelligence, whether language models understand language, and the necessity of linguistic structure and inductive bias for solving NLP problems. In addition, the survey posed meta-questions, asking respondents to predict the distribution of survey responses. This allows us to uncover false sociological beliefs where the community’s predictions don’t match reality. Among other results, we find that the community greatly overestimates its own belief in the usefulness of benchmarks and the potential for scaling to solve real-world problems, while underestimating its belief in the importance of linguistic structure, inductive bias, and interdisciplinary science.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.903.pdf",
        "keywords": [
            "inductive bias",
            "metasurvey",
            "language",
            "language models",
            "ethics",
            "sociological beliefs",
            "industry influence",
            "nlp",
            "artificial general intelligence"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"whether language models understand language\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"whether language models understand language\""
    },
    {
        "title": "LENS: A Learnable Evaluation Metric for Text Simplification",
        "authors": [
            "Mounica Maddela",
            "Yao Dou",
            "David Heineman",
            "Wei Xu"
        ],
        "published": "2023",
        "summary": "Training learnable metrics using modern language models has recently emerged as a promising method for the automatic evaluation of machine translation. However, existing human evaluation datasets for text simplification have limited annotations that are based on unitary or outdated models, making them unsuitable for this approach. To address these issues, we introduce the SimpEval corpus that contains: SimpEval_past, comprising 12K human ratings on 2.4K simplifications of 24 past systems, and SimpEval_2022, a challenging simplification benchmark consisting of over 1K human ratings of 360 simplifications including GPT-3.5 generated text. Training on SimpEval, we present LENS, a Learnable Evaluation Metric for Text Simplification. Extensive empirical results show that LENS correlates much better with human judgment than existing metrics, paving the way for future progress in the evaluation of text simplification. We also introduce Rank & Rate, a human evaluation framework that rates simplifications from several models in a list-wise manner using an interactive interface, which ensures both consistency and accuracy in the evaluation process and is used to create the SimpEval datasets.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.905.pdf",
        "keywords": [
            "text simplification",
            "learnable evaluation metric",
            "learnable metrics"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"including GPT-3.5 generated text.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"including GPT-3.5 generated text.\""
    },
    {
        "title": "UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective",
        "authors": [
            "Yang Ping",
            "JunYu Lu",
            "Ruyi Gan",
            "Junjie Wang",
            "Yuxiang Zhang",
            "Pingjian Zhang",
            "Jiaxing Zhang"
        ],
        "published": "2023",
        "summary": "We propose a new paradigm for universal information extraction (IE) that is compatible with any schema format and applicable to a list of IE tasks, such as named entity recognition, relation extraction, event extraction and sentiment analysis. Our approach converts the text-based IE tasks as the token-pair problem, which uniformly disassembles all extraction targets into joint span detection, classification and association problems with a unified extractive framework, namely UniEX. UniEX can synchronously encode schema-based prompt and textual information, and collaboratively learn the generalized knowledge from pre-defined information using the auto-encoder language models. We develop a traffine attention mechanism to integrate heterogeneous factors including tasks, labels and inside tokens, and obtain the extraction target via a scoring matrix. Experiment results show that UniEX can outperform generative universal IE models in terms of performance and inference-speed on 14 benchmarks IE datasets with the supervised setting. The state-of-the-art performance in low-resource scenarios also verifies the transferability and effectiveness of UniEX.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.907.pdf",
        "keywords": [
            "uniex",
            "extractive",
            "scoring matrix",
            "universal information extraction",
            "sentiment analysis",
            "unified information extraction"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"collaboratively learn the generalized knowledge from pre-defined information using the auto-encoder language models.\"\n\nThis paper mentions LLMs (auto-encoder language models) but does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"collaboratively learn the generalized knowledge from pre-defined information using the auto-encoder language models.\"\n\nThis paper mentions LLMs (auto-encoder language models) but does not discuss any limitations of LLMs."
    },
    {
        "title": "A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text",
        "authors": [
            "Yunxin Li",
            "Baotian Hu",
            "Yuxin Ding",
            "Lin Ma",
            "Min Zhang"
        ],
        "published": "2023",
        "summary": "Pretrained Vision-Language Models (VLMs) have achieved remarkable performance in image retrieval from text. However, their performance drops drastically when confronted with linguistically complex texts that they struggle to comprehend. Inspired by the Divide-and-Conquer algorithm and dual-process theory, in this paper, we regard linguistically complex texts as compound proposition texts composed of multiple simple proposition sentences and propose an end-to-end Neural Divide-and-Conquer Reasoning framework, dubbed NDCR. It contains three main components: 1) Divide: a proposition generator divides the compound proposition text into simple proposition sentences and produces their corresponding representations, 2) Conquer: a pretrained VLMs-based visual-linguistic interactor achieves the interaction between decomposed proposition sentences and images, 3) Combine: a neural-symbolic reasoner combines the above reasoning states to obtain the final solution via a neural logic reasoning approach. According to the dual-process theory, the visual-linguistic interactor and neural-symbolic reasoner could be regarded as analogical reasoning System 1 and logical reasoning System 2. We conduct extensive experiments on a challenging image retrieval from contextual descriptions data set. Experimental results and analyses indicate NDCR significantly improves performance in the complex image-text reasoning problem.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.909.pdf",
        "keywords": [
            "analogical reasoning",
            "image retrieval",
            "neural symbolic reasoner",
            "neural logic reasoning",
            "divide and conquer algorithm",
            "logical reasoning",
            "dual process theory",
            "linguistically",
            "text"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, their performance drops drastically when confronted with linguistically complex texts that they struggle to comprehend.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, their performance drops drastically when confronted with linguistically complex texts that they struggle to comprehend.\""
    },
    {
        "title": "RARR: Researching and Revising What Language Models Say, Using Language Models",
        "authors": [
            "Luyu Gao",
            "Zhuyun Dai",
            "Panupong Pasupat",
            "Anthony Chen",
            "Arun Tejasvi Chaganty",
            "Yicheng Fan",
            "Vincent Zhao",
            "Ni Lao",
            "Hongrae Lee",
            "Da-Cheng Juan",
            "Kelvin Guu"
        ],
        "published": "2023",
        "summary": "Language models (LMs) now excel at many tasks such as question answering, reasoning, and dialog. However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence. To enable attribution while still preserving all the powerful advantages of recent generation models, we propose RARR (Retrofit Attribution using Research and Revision), a system that 1) automatically finds attribution for the output of any text generation model, and 2) post-edits the output to fix unsupported content while preserving the original output as much as possible. When applied to the output of several state-of-the-art LMs on a diverse set of generation tasks, we find that RARR significantly improves attribution while otherwise preserving the original input to a much greater degree than previously explored edit models. Furthermore, the implementation of RARR requires only a handful of training examples, a large language model, and standard web search.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.910.pdf",
        "keywords": [
            "language models",
            "revising what language models",
            "revision"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, they sometimes generate unsupported or misleading content.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, they sometimes generate unsupported or misleading content.\""
    },
    {
        "title": "Should you marginalize over possible tokenizations?",
        "authors": [
            "Nadezhda Chirkova",
            "Germán Kruszewski",
            "Jos Rozen",
            "Marc Dymetman"
        ],
        "published": "2023",
        "summary": "Autoregressive language models (LMs) map token sequences to probabilities. The usual practice for computing the probability of any character string (e.g. English sentences) is to first transform it into a sequence of tokens that is scored by the model. However, there are exponentially many token sequences that represent any given string. To truly compute the probability of a string one should marginalize over all tokenizations, which is typically intractable. Here, we analyze whether the practice of ignoring the marginalization is justified. To this end, we devise an importance-sampling-based algorithm that allows us to compute estimates of the marginal probabilities and compare them to the default procedure in a range of state-of-the-art models and datasets. Our results show that the gap in log-likelihood is no larger than 0.5% in most cases, but that it becomes more pronounced for data with long complex words.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.1.pdf",
        "keywords": [
            "importance sampling",
            "token sequences",
            "tokenizations",
            "autoregressive language models"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, there are exponentially many token sequences that represent any given string. To truly compute the probability of a string one should marginalize over all tokenizations, which is typically intractable.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, there are exponentially many token sequences that represent any given string. To truly compute the probability of a string one should marginalize over all tokenizations, which is typically intractable.\""
    },
    {
        "title": "Ellipsis-Dependent Reasoning: a New Challenge for Large Language Models",
        "authors": [
            "Daniel Hardt"
        ],
        "published": "2023",
        "summary": "We propose a novel challenge for large language models: ellipsis-dependent reasoning. We define several structures of paired examples, where an ellipsis example is matched to its non-ellipsis counterpart, and a question is posed which requires resolution of the ellipsis. Test results show that the best models perform well on non-elliptical examples but struggle with all but the simplest ellipsis structures.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.4.pdf",
        "keywords": [
            "ellipsis",
            "language models",
            "large language models",
            "non ellipsis",
            "reasoning"
        ],
        "Rate_Llama-3.1-70b": "5",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Test results show that the best models perform well on non-elliptical examples but struggle with all but the simplest ellipsis structures.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 5\nEvidence: \"Test results show that the best models perform well on non-elliptical examples but struggle with all but the simplest ellipsis structures.\""
    },
    {
        "title": "Analyzing Text Representations by Measuring Task Alignment",
        "authors": [
            "Cesar Gonzalez-Gutierrez",
            "Audi Primadhanty",
            "Francesco Cazzaro",
            "Ariadna Quattoni"
        ],
        "published": "2023",
        "summary": "Textual representations based on pre-trained language models are key, especially in few-shot learning scenarios. What makes a representation good for text classification? Is it due to the geometric properties of the space or because it is well aligned with the task? We hypothesize the second claim. To test it, we develop a task alignment score based on hierarchical clustering that measures alignment at different levels of granularity. Our experiments on text classification validate our hypothesis by showing that task alignment can explain the classification performance of a given representation.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.7.pdf",
        "keywords": [
            "task alignment",
            "text classification",
            "language models",
            "text representations",
            "clustering"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Scaling in Cognitive Modelling: a Multilingual Approach to Human Reading Times",
        "authors": [
            "Andrea de Varda",
            "Marco Marelli"
        ],
        "published": "2023",
        "summary": "Neural language models are increasingly valued in computational psycholinguistics, due to their ability to provide conditional probability distributions over the lexicon that are predictive of human processing times. Given the vast array of available models, it is of both theoretical and methodological importance to assess what features of a model influence its psychometric quality. In this work we focus on parameter size, showing that larger Transformer-based language models generate probabilistic estimates that are less predictive of early eye-tracking measurements reflecting lexical access and early semantic integration. However, relatively bigger models show an advantage in capturing late eye-tracking measurements that reflect the full semantic and syntactic integration of a word into the current language context. Our results are supported by eye movement data in ten languages and consider four models, spanning from 564M to 4.5B parameters.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.14.pdf",
        "keywords": [
            "cognitive modelling",
            "language models",
            "neural language models",
            "human reading times",
            "eye tracking",
            "scaling",
            "parameter size"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"larger Transformer-based language models generate probabilistic estimates that are less predictive of early eye-tracking measurements reflecting lexical access and early semantic integration.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"larger Transformer-based language models generate probabilistic estimates that are less predictive of early eye-tracking measurements reflecting lexical access and early semantic integration.\""
    },
    {
        "title": "Improving Generalization in Language Model-based Text-to-SQL Semantic Parsing: Two Simple Semantic Boundary-based Techniques",
        "authors": [
            "Daking Rai",
            "Bailin Wang",
            "Yilun Zhou",
            "Ziyu Yao"
        ],
        "published": "2023",
        "summary": "Compositional and domain generalization present significant challenges in semantic parsing, even for state-of-the-art semantic parsers based on pre-trained language models (LMs). In this study, we empirically investigate improving an LM’s generalization in semantic parsing with two simple techniques: at the token level, we introduce a token preprocessing method to preserve the semantic boundaries of tokens produced by LM tokenizers; at the sequence level, we propose to use special tokens to mark the boundaries of components aligned between input and output. Our experimental results on two text-to-SQL semantic parsing datasets show that our token preprocessing, although simple, can substantially improve the LM performance on both types of generalization, and our component boundary marking method is particularly helpful for compositional generalization.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.15.pdf",
        "keywords": [
            "semantic parsing",
            "generalization",
            "domain generalization",
            "language",
            "language model",
            "parsers"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Compositional and domain generalization present significant challenges in semantic parsing, even for state-of-the-art semantic parsers based on pre-trained language models (LMs).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Compositional and domain generalization present significant challenges in semantic parsing, even for state-of-the-art semantic parsers based on pre-trained language models (LMs).\""
    },
    {
        "title": "HiPool: Modeling Long Documents Using Graph Neural Networks",
        "authors": [
            "Irene Li",
            "Aosong Feng",
            "Dragomir Radev",
            "Rex Ying"
        ],
        "published": "2023",
        "summary": "Encoding long sequences in Natural Language Processing (NLP) is a challenging problem. Though recent pretraining language models achieve satisfying performances in many NLP tasks, they are still restricted by a pre-defined maximum length, making them challenging to be extended to longer sequences. So some recent works utilize hierarchies to model long sequences. However, most of them apply sequential models for upper hierarchies, suffering from long dependency issues. In this paper, we alleviate these issues through a graph-based method. We first chunk the sequence with a fixed length to model the sentence-level information. We then leverage graphs to model intra- and cross-sentence correlations with a new attention mechanism. Additionally, due to limited standard benchmarks for long document classification (LDC), we propose a new challenging benchmark, totaling six datasets with up to 53k samples and 4034 average tokens’ length. Evaluation shows our model surpasses competitive baselines by 2.6% in F1 score, and 4.8% on the longest sequence dataset. Our method is shown to outperform hierarchical sequential models with better performance and scalability, especially for longer sequences.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.16.pdf",
        "keywords": [
            "graph neural networks",
            "natural language processing"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Though recent pretraining language models achieve satisfying performances in many NLP tasks, they are still restricted by a pre-defined maximum length, making them challenging to be extended to longer sequences.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Though recent pretraining language models achieve satisfying performances in many NLP tasks, they are still restricted by a pre-defined maximum length, making them challenging to be extended to longer sequences.\""
    },
    {
        "title": "BOLT: Fast Energy-based Controlled Text Generation with Tunable Biases",
        "authors": [
            "Xin Liu",
            "Muhammad Khalifa",
            "Lu Wang"
        ],
        "published": "2023",
        "summary": "Energy-based models (EBMs) have gained popularity for controlled text generation due to their high applicability to a wide range of constraints. However, sampling from EBMs is non-trivial, as it often requires a large number of iterations to converge to plausible text, which slows down the decoding process and makes it less practical for real-world applications. In this work, we propose BOLT, which relies on tunable biases to directly adjust the language model’s output logits. Unlike prior work, BOLT maintains the generator’s autoregressive nature to assert a strong control on token-wise conditional dependencies and overall fluency, and thus converges faster. When compared with state-of-the-arts on controlled generation tasks using both soft constraints (e.g., sentiment control) and hard constraints (e.g., keyword-guided topic control), BOLT demonstrates significantly improved efficiency and fluency. On sentiment control, BOLT is 7x faster than competitive baselines, and more fluent in 74.4% of the evaluation samples according to human judges.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.18.pdf",
        "keywords": [
            "text generation",
            "energy",
            "controlled text generation",
            "fluency",
            "energy based models",
            "sentiment control",
            "tunable biases"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Detoxifying Text with MaRCo: Controllable Revision with Experts and Anti-Experts",
        "authors": [
            "Skyler Hallinan",
            "Alisa Liu",
            "Yejin Choi",
            "Maarten Sap"
        ],
        "published": "2023",
        "summary": "Text detoxification has the potential to mitigate the harms of toxicity by rephrasing text to remove offensive meaning, but subtle toxicity remains challenging to tackle. We introduce MaRCo, a detoxification algorithm that combines controllable generation and text rewriting methods using a Product of Experts with autoencoder language models (LMs). MaRCo uses likelihoods under a non-toxic LM (expert) and a toxic LM (anti-expert) to find candidate words to mask and potentially replace. We evaluate our method on several subtle toxicity and microaggressions datasets, and show that it not only outperforms baselines on automatic metrics, but MaRCo’s rewrites are preferred 2.1 times more in human evaluation. Its applicability to instances of subtle toxicity is especially promising, demonstrating a path forward for addressing increasingly elusive online hate.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.21.pdf",
        "keywords": [
            "detoxification",
            "text detoxification",
            "autoencoder language models"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the abstract mentions the use of a \"toxic LM (anti-expert)\" which implies the existence of a limitation, but it is not explicitly discussed.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the abstract mentions the use of a \"toxic LM (anti-expert)\" which implies the existence of a limitation, but it is not explicitly discussed."
    },
    {
        "title": "Parameter-efficient Weight Ensembling Facilitates Task-level Knowledge Transfer",
        "authors": [
            "Xingtai Lv",
            "Ning Ding",
            "Yujia Qin",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published": "2023",
        "summary": "Recent studies show that large-scale pre-trained language models could be efficaciously adapted to particular tasks in a parameter-efficient manner. The trained lightweight set of parameters, such as adapters, can be easily stored and shared as a capability equipped with the corresponding models. Owning many lightweight parameters, we focus on transferring them between tasks to acquire an improvement in performance of new tasks, the key point of which is to obtain the similarity between tasks. In this paper, we explore 5 parameter-efficient weight ensembling methods to achieve such transferability and verify the effectiveness of them. These methods extract the information of datasets and trained lightweight parameters from different perspectives to obtain the similarity between tasks, and weight the existing lightweight parameters according to the comparability to acquire a suitable module for the initialization of new tasks. We apply them to three parameter-efficient tuning methods and test them on a wide set of downstream tasks. Experimental results show that our methods show an improvement of 5%~8% over baselines and could largely facilitate task-level knowledge transfer.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.24.pdf",
        "keywords": [
            "transferability",
            "knowledge transfer",
            "ensembling",
            "efficient"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations, but discusses adapting pre-trained language models in a parameter-efficient manner, implying potential limitations in the original models.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations, but discusses adapting pre-trained language models in a parameter-efficient manner, implying potential limitations in the original models."
    },
    {
        "title": "COGEN: Abductive Commonsense Language Generation",
        "authors": [
            "Rohola Zandie",
            "Diwanshu Shekhar",
            "Mohammad Mahoor"
        ],
        "published": "2023",
        "summary": "Reasoning is one of the most important elements in achieving Artificial General Intelligence (AGI), specifically when it comes to Abductive and counterfactual reasoning. In order to introduce these capabilities of reasoning in Natural Language Processing (NLP) models, there have been recent advances towards training NLP models to better perform on two main tasks - Abductive Natural Language Inference (alphaNLI) and Abductive Natural Language Generation Task (alphaNLG). This paper proposes CoGen, a model for both alphaNLI and alphaNLG tasks that employ a novel approach of combining the temporal commonsense reasoning for each observation (before and after a real hypothesis) from pre-trained models with contextual filtering for training. Additionally, we use state-of-the-art semantic entailment to filter out the contradictory hypothesis during the inference. Our experimental results show that CoGen outperforms current models and set a new state of the art in regards to alphaNLI and alphaNLG tasks. We make the source code of CoGen model publicly available for reproducibility and to facilitate relevant future research.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.26.pdf",
        "keywords": [
            "abductive",
            "abductive commonsense language generation",
            "reasoning",
            "abductive natural language generation",
            "abductive natural language inference",
            "temporal commonsense reasoning",
            "natural language processing"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions",
        "authors": [
            "Himanshu Thakur",
            "Atishay Jain",
            "Praneetha Vaddamanu",
            "Paul Pu Liang",
            "Louis-Philippe Morency"
        ],
        "published": "2023",
        "summary": "Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.30.pdf",
        "keywords": [
            "gender",
            "language models",
            "data intervention strategies",
            "gender bias",
            "experimentation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people.\""
    },
    {
        "title": "PLUE: Language Understanding Evaluation Benchmark for Privacy Policies in English",
        "authors": [
            "Jianfeng Chi",
            "Wasi Uddin Ahmad",
            "Yuan Tian",
            "Kai-Wei Chang"
        ],
        "published": "2023",
        "summary": "Privacy policies provide individuals with information about their rights and how their personal information is handled. Natural language understanding (NLU) technologies can support individuals and practitioners to understand better privacy practices described in lengthy and complex documents. However, existing efforts that use NLU technologies are limited by processing the language in a way exclusive to a single task focusing on certain privacy practices. To this end, we introduce the Privacy Policy Language Understanding Evaluation (PLUE) benchmark, a multi-task benchmark for evaluating the privacy policy language understanding across various tasks. We also collect a large corpus of privacy policies to enable privacy policy domain-specific language model pre-training. We evaluate several generic pre-trained language models and continue pre-training them on the collected corpus. We demonstrate that domain-specific continual pre-training offers performance improvements across all tasks. The code and models are released at https://github.com/JFChi/PLUE.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.31.pdf",
        "keywords": [
            "privacy",
            "privacy policies",
            "language",
            "natural language",
            "language models",
            "specific language model",
            "evaluation",
            "language understanding evaluation benchmark",
            "english"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing efforts that use NLU technologies are limited by processing the language in a way exclusive to a single task focusing on certain privacy practices.\"\n\nThis rating is given because the abstract mentions a limitation of existing NLU technologies (which include LLMs), but it is a brief mention and not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, existing efforts that use NLU technologies are limited by processing the language in a way exclusive to a single task focusing on certain privacy practices.\"\n\nThis rating is given because the abstract mentions a limitation of existing NLU technologies (which include LLMs), but it is a brief mention and not the primary focus of the paper."
    },
    {
        "title": "Stop Pre-Training: Adapt Visual-Language Models to Unseen Languages",
        "authors": [
            "Yasmine Karoui",
            "Rémi Lebret",
            "Negar Foroutan Eghlidi",
            "Karl Aberer"
        ],
        "published": "2023",
        "summary": "Vision-Language Pre-training (VLP) has advanced the performance of many vision-language tasks, such as image-text retrieval, visual entailment, and visual reasoning. The pre-training mostly utilizes lexical databases and image queries in English. Previous work has demonstrated that the pre-training in English does not transfer well to other languages in a zero-shot setting. However, multilingual pre-trained language models (MPLM) have excelled at a variety of single-modal language tasks. In this paper, we propose a simple yet efficient approach to adapt VLP to unseen languages using MPLM.We utilize a cross-lingual contextualised token embeddings alignment approach to train text encoders for non-English languages. Our approach does not require image input and primarily uses machine translation, eliminating the need for target language data. Our evaluation across three distinct tasks (image-text retrieval, visual entailment, and natural language visual reasoning) demonstrates that this approach outperforms the state-of-the-art multilingual vision-language models without requiring large parallel corpora. Our code is available at https://github.com/Yasminekaroui/CliCoTea.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.32.pdf",
        "keywords": [
            "adapt visual language",
            "adapt",
            "visual entailment",
            "natural language visual reasoning",
            "pre training",
            "unseen languages",
            "image text retrieval",
            "stop pre training",
            "pre trained language models",
            "multilingual vision language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, multilingual pre-trained language models (MPLM) have excelled at a variety of single-modal language tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, multilingual pre-trained language models (MPLM) have excelled at a variety of single-modal language tasks.\""
    },
    {
        "title": "BUCA: A Binary Classification Approach to Unsupervised Commonsense Question Answering",
        "authors": [
            "Jie He",
            "Simon U",
            "Victor Gutierrez-Basulto",
            "Jeff Pan"
        ],
        "published": "2023",
        "summary": "Unsupervised commonsense reasoning (UCR) is becoming increasingly popular as the construction of commonsense reasoning datasets is expensive, and they are inevitably limited in their scope. A popular approach to UCR is to fine-tune language models with external knowledge (e.g., knowledge graphs), but this usually requires a large number of training examples. In this paper, we propose to transform the downstream multiple choice question answering task into a simpler binary classification task by ranking all candidate answers according to their reasonableness. To this end, for training the model, we convert the knowledge graph triples into reasonable and unreasonable texts. Extensive experimental results show the effectiveness of our approach on various multiple choice question answering benchmarks. Furthermore, compared with existing UCR approaches using KGs, ours is less data hungry.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.33.pdf",
        "keywords": [
            "commonsense",
            "commonsense question answering",
            "question answering",
            "binary classification",
            "unsupervised commonsense reasoning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"A popular approach to UCR is to fine-tune language models with external knowledge (e.g., knowledge graphs), but this usually requires a large number of training examples.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"A popular approach to UCR is to fine-tune language models with external knowledge (e.g., knowledge graphs), but this usually requires a large number of training examples.\""
    },
    {
        "title": "Nichelle and Nancy: The Influence of Demographic Attributes and Tokenization Length on First Name Biases",
        "authors": [
            "Haozhe An",
            "Rachel Rudinger"
        ],
        "published": "2023",
        "summary": "Through the use of first name substitution experiments, prior research has demonstrated the tendency of social commonsense reasoning models to systematically exhibit social biases along the dimensions of race, ethnicity, and gender (An et al., 2023). Demographic attributes of first names, however, are strongly correlated with corpus frequency and tokenization length, which may influence model behavior independent of or in addition to demographic factors. In this paper, we conduct a new series of first name substitution experiments that measures the influence of these factors while controlling for the others. We find that demographic attributes of a name (race, ethnicity, and gender) and name tokenization length are both factors that systematically affect the behavior of social commonsense reasoning models.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.34.pdf",
        "keywords": [
            "demographic attributes",
            "name tokenization",
            "first name substitution",
            "tokenization",
            "social commonsense reasoning",
            "first name biases",
            "corpus"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Improving Syntactic Probing Correctness and Robustness with Control Tasks",
        "authors": [
            "Weicheng Ma",
            "Brian Wang",
            "Hefan Zhang",
            "Lili Wang",
            "Rolando Coto-Solano",
            "Saeed Hassanpour",
            "Soroush Vosoughi"
        ],
        "published": "2023",
        "summary": "Syntactic probing methods have been used to examine whether and how pre-trained language models (PLMs) encode syntactic features. However, the probing methods are usually biased by the PLMs’ memorization of common word co-occurrences, even if they do not form syntactic relations. This paper presents a random-word-substitution and random-label-matching control task to reduce these biases and improve the robustness of syntactic probing methods. Our control tasks are also shown to notably improve the consistency of probing results between different probing methods and make the methods more robust with respect to the text attributes of the probing instances. Our control tasks make syntactic probing methods better at reconstructing syntactic features and more generalizable to unseen text domains. Our experiments show that our proposed control tasks are effective on different PLMs, probing methods, and syntactic features.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.35.pdf",
        "keywords": [
            "robustness",
            "probing",
            "syntactic probing",
            "syntactic features",
            "co occurrences",
            "language models",
            "control tasks",
            "label matching",
            "random word substitution",
            "consistency"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the probing methods are usually biased by the PLMs’ memorization of common word co-occurrences, even if they do not form syntactic relations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the probing methods are usually biased by the PLMs’ memorization of common word co-occurrences, even if they do not form syntactic relations.\""
    },
    {
        "title": "Credible without Credit: Domain Experts Assess Generative Language Models",
        "authors": [
            "Denis Peskoff",
            "Brandon Stewart"
        ],
        "published": "2023",
        "summary": "Language models have recently broken into the public consciousness with the release of the wildly popular ChatGPT. Commentators have argued that language models could replace search engines, make college essays obsolete, or even write academic research papers. All of these tasks rely on accuracy of specialized information which can be difficult to assess for non-experts. Using 10 domain experts across science and culture, we provide an initial assessment of the coherence, conciseness, accuracy, and sourcing of two language models across 100 expert-written questions. While we find the results are consistently cohesive and concise, we find that they are mixed in their accuracy. These results raise questions of the role language models should play in general-purpose and expert knowledge seeking.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.37.pdf",
        "keywords": [
            "language models",
            "generative language models",
            "role language models",
            "expert knowledge seeking",
            "expert",
            "accuracy"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While we find the results are consistently cohesive and concise, we find that they are mixed in their accuracy.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While we find the results are consistently cohesive and concise, we find that they are mixed in their accuracy.\""
    },
    {
        "title": "Grokking of Hierarchical Structure in Vanilla Transformers",
        "authors": [
            "Shikhar Murty",
            "Pratyusha Sharma",
            "Jacob Andreas",
            "Christopher Manning"
        ],
        "published": "2023",
        "summary": "For humans, language production and comprehension is sensitive to the hierarchical structure of sentences. In natural language processing, past work has questioned how effectively neural sequence models like transformers capture this hierarchical structure when generalizing to structurally novel inputs. We show that transformer language models can learn to generalize hierarchically after training for extremely long periods—far beyond the point when in-domain accuracy has saturated. We call this phenomenon structural grokking. On multiple datasets, structural grokking exhibits inverted U-shaped scaling in model depth: intermediate-depth models generalize better than both very deep and very shallow transformers. When analyzing the relationship between model-internal properties and grokking, we find that optimal depth for grokking can be identified using the tree-structuredness metric of CITATION. Overall, our work provides strong evidence that, with extended training, vanilla transformers discover and use hierarchical structure.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.38.pdf",
        "keywords": [
            "hierarchical structure",
            "structure",
            "vanilla transformers",
            "transformers",
            "generalize",
            "tree structuredness metric",
            "transformer language",
            "domain accuracy",
            "vanilla transformers discover"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"past work has questioned how effectively neural sequence models like transformers capture this hierarchical structure when generalizing to structurally novel inputs.\"\n\nThis rating is given because the abstract mentions a limitation of transformers (a type of LLM) in capturing hierarchical structure, but it is a minor detail and the focus of the paper is on the phenomenon of structural grokking and the ability of transformers to learn",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"past work has questioned how effectively neural sequence models like transformers capture this hierarchical structure when generalizing to structurally novel inputs.\"\n\nThis rating is given because the abstract mentions a limitation of transformers (a type of LLM) in capturing hierarchical structure, but it is a minor detail and the focus of the paper is on the phenomenon of structural grokking and the ability of transformers to learn"
    },
    {
        "title": "Efficient Diagnosis Assignment Using Unstructured Clinical Notes",
        "authors": [
            "Louis Blankemeier",
            "Jason Fries",
            "Robert Tinn",
            "Joseph Preston",
            "Nigam Shah",
            "Akshay Chaudhari"
        ],
        "published": "2023",
        "summary": "Electronic phenotyping entails using electronic health records (EHRs) to identify patients with specific health outcomes and determine when those outcomes occurred. Unstructured clinical notes, which contain a vast amount of information, are a valuable resource for electronic phenotyping. However, traditional methods, such as rule-based labeling functions or neural networks, require significant manual effort to tune and may not generalize well to multiple indications. To address these challenges, we propose HyDE (hybrid diagnosis extractor). HyDE is a simple framework for electronic phenotyping that integrates labeling functions and a disease-agnostic neural network to assign diagnoses to patients. By training HyDE’s model to correct predictions made by labeling functions, we are able to disambiguate hypertension true positives and false positives with a supervised area under the precision-recall curve (AUPRC) of 0.85. We extend this hypertension-trained model to zero-shot evaluation of four other diseases, generating AUPRC values ranging from 0.82 - 0.95 and outperforming a labeling function baseline by 44 points in F1 score and a Word2Vec baseline by 24 points in F1 score on average. Furthermore, we demonstrate a speedup of >4x by pruning the length of inputs into our language model to ~2.3% of the full clinical notes, with negligible impact to the AUPRC. HyDE has the potential to improve the efficiency and efficacy of interpreting large-scale unstructured clinical notes for accurate EHR phenotyping.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.42.pdf",
        "keywords": [
            "ehr phenotyping",
            "ehrs",
            "electronic phenotyping",
            "electronic health records",
            "hybrid diagnosis",
            "efficient diagnosis assignment",
            "clinical notes",
            "unstructured clinical notes"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Furthermore, we demonstrate a speedup of >4x by pruning the length of inputs into our language model to ~2.3% of the full clinical notes, with negligible impact to the AUPRC.\"\n\nThis abstract mentions a limitation of LLMs in the context of processing long clinical notes, but it is a minor detail and not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Furthermore, we demonstrate a speedup of >4x by pruning the length of inputs into our language model to ~2.3% of the full clinical notes, with negligible impact to the AUPRC.\"\n\nThis abstract mentions a limitation of LLMs in the context of processing long clinical notes, but it is a minor detail and not the primary focus of the paper."
    },
    {
        "title": "MetaVL: Transferring In-Context Learning Ability From Language Models to Vision-Language Models",
        "authors": [
            "Masoud Monajatipoor",
            "Liunian Harold Li",
            "Mozhdeh Rouhsedaghat",
            "Lin Yang",
            "Kai-Wei Chang"
        ],
        "published": "2023",
        "summary": "Large-scale language models have shown the ability to adapt to a new task via conditioning on a few demonstrations (i.e., in-context learning). However, in the vision-language domain, most large-scale pre-trained vision-language (VL) models do not possess the ability to conduct in-context learning. How can we enable in-context learning for VL models? In this paper, we study an interesting hypothesis: can we transfer the in-context learning ability from the language domain to the VL domain? Specifically, we first meta-trains a language model to perform in-context learning on NLP tasks (as in MetaICL); then we transfer this model to perform VL tasks by attaching a visual encoder. Our experiments suggest that indeed in-context learning ability can be transferred cross modalities: our model considerably improves the in-context learning capability on VL tasks and can even compensate for the size of the model significantly. On VQA, OK-VQA, and GQA, our method could outperform the baseline model while having ~20 times fewer parameters.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.43.pdf",
        "keywords": [
            "language models",
            "metavl",
            "context learning",
            "in context learning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, in the vision-language domain, most large-scale pre-trained vision-language (VL) models do not possess the ability to conduct in-context learning.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of large-scale pre-trained vision-language models (a type of LLM) in the context of in-context learning, but it is not the primary focus of the paper",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, in the vision-language domain, most large-scale pre-trained vision-language (VL) models do not possess the ability to conduct in-context learning.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of large-scale pre-trained vision-language models (a type of LLM) in the context of in-context learning, but it is not the primary focus of the paper"
    },
    {
        "title": "Surface-Based Retrieval Reduces Perplexity of Retrieval-Augmented Language Models",
        "authors": [
            "Ehsan Doostmohammadi",
            "Tobias Norlund",
            "Marco Kuhlmann",
            "Richard Johansson"
        ],
        "published": "2023",
        "summary": "Augmenting language models with a retrieval mechanism has been shown to significantly improve their performance while keeping the number of parameters low. Retrieval-augmented models commonly rely on a semantic retrieval mechanism based on the similarity between dense representations of the query chunk and potential neighbors. In this paper, we study the state-of-the-art Retro model and observe that its performance gain is better explained by surface-level similarities, such as token overlap. Inspired by this, we replace the semantic retrieval in Retro with a surface-level method based on BM25, obtaining a significant reduction in perplexity. As full BM25 retrieval can be computationally costly for large datasets, we also apply it in a re-ranking scenario, gaining part of the perplexity reduction with minimal computational overhead.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.45.pdf",
        "keywords": [
            "perplexity",
            "surface",
            "surface based retrieval"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"As full BM25 retrieval can be computationally costly for large datasets\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"As full BM25 retrieval can be computationally costly for large datasets\""
    },
    {
        "title": "MIReAD: Simple Method for Learning High-quality Representations from Scientific Documents",
        "authors": [
            "Anastasiia Razdaibiedina",
            "Aleksandr Brechalov"
        ],
        "published": "2023",
        "summary": "Learning semantically meaningful representations from scientific documents can facilitate academic literature search and improve performance of recommendation systems. Pretrained language models have been shown to learn rich textual representations, yet they cannot provide powerful document-level representations for scientific articles. We propose MIReAD, a simple method that learns highquality representations of scientific papers by fine-tuning transformer model to predict the target journal class based on the abstract. We train MIReAD on more than 500,000 PubMed and arXiv abstracts across over 2,000 journal classes. We show that MIReAD produces representations that can be used for similar papers retrieval, topic categorization and literature search. Our proposed approach outperforms six existing models for representation learning on scientific documents across four evaluation standards.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.46.pdf",
        "keywords": [
            "language",
            "representations"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Pretrained language models have been shown to learn rich textual representations, yet they cannot provide powerful document-level representations for scientific articles.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Pretrained language models have been shown to learn rich textual representations, yet they cannot provide powerful document-level representations for scientific articles.\""
    },
    {
        "title": "Measuring the Effect of Influential Messages on Varying Personas",
        "authors": [
            "Chenkai Sun",
            "Jinning Li",
            "Hou Pong Chan",
            "ChengXiang Zhai",
            "Heng Ji"
        ],
        "published": "2023",
        "summary": "Predicting how a user responds to news events enables important applications such as allowing intelligent agents or content producers to estimate the effect on different communities and revise unreleased messages to prevent unexpected bad outcomes such as social conflict and moral injury. We present a new task, Response Forecasting on Personas for News Media, to estimate the response a persona (characterizing an individual or a group) might have upon seeing a news message. Compared to the previous efforts which only predict generic comments to news, the proposed task not only introduces personalization in the modeling but also predicts the sentiment polarity and intensity of each response. This enables more accurate and comprehensive inference on the mental state of the persona. Meanwhile, the generated sentiment dimensions make the evaluation and application more reliable. We create the first benchmark dataset, which consists of 13,357 responses to 3,847 news headlines from Twitter. We further evaluate the SOTA neural language models with our dataset. The empirical results suggest that the included persona attributes are helpful for the performance of all response dimensions. Our analysis shows that the best-performing models are capable of predicting responses that are consistent with the personas, and as a byproduct, the task formulation also enables many interesting applications in the analysis of social network groups and their opinions, such as the discovery of extreme opinion groups.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.48.pdf",
        "keywords": [
            "social network",
            "personalization",
            "news message",
            "news",
            "neural language models",
            "response forecasting"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We further evaluate the SOTA neural language models with our dataset.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"We further evaluate the SOTA neural language models with our dataset.\""
    },
    {
        "title": "Going Beyond Sentence Embeddings: A Token-Level Matching Algorithm for Calculating Semantic Textual Similarity",
        "authors": [
            "Hongwei Wang",
            "Dong Yu"
        ],
        "published": "2023",
        "summary": "Semantic Textual Similarity (STS) measures the degree to which the underlying semantics of paired sentences are equivalent. State-of-the-art methods for STS task use language models to encode sentences into embeddings. However, these embeddings are limited in representing semantics because they mix all the semantic information together in fixed-length vectors, which are difficult to recover and lack explainability. This paper presents a token-level matching inference algorithm, which can be applied on top of any language model to improve its performance on STS task. Our method calculates pairwise token-level similarity and token matching scores, and then aggregates them with pretrained token weights to produce sentence similarity. Experimental results on seven STS datasets show that our method improves the performance of almost all language models, with up to 12.7% gain in Spearman’s correlation. We also demonstrate that our method is highly explainable and computationally efficient.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.49.pdf",
        "keywords": [
            "sentence similarity",
            "textual similarity",
            "matching",
            "token level matching",
            "semantic textual similarity",
            "token matching",
            "token weights"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these embeddings are limited in representing semantics because they mix all the semantic information together in fixed-length vectors, which are difficult to recover and lack explainability.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these embeddings are limited in representing semantics because they mix all the semantic information together in fixed-length vectors, which are difficult to recover and lack explainability.\""
    },
    {
        "title": "Probing Physical Reasoning with Counter-Commonsense Context",
        "authors": [
            "Kazushi Kondo",
            "Saku Sugawara",
            "Akiko Aizawa"
        ],
        "published": "2023",
        "summary": "In this study, we create a CConS (Counter-commonsense Contextual Size comparison) dataset to investigate how physical commonsense affects the contextualized size comparison task; the proposed dataset consists of both contexts that fit physical commonsense and those that do not. This dataset tests the ability of language models to predict the size relationship between objects under various contexts generated from our curated noun list and templates. We measure the ability of several masked language models and encoder-decoder models. The results show that while large language models can use prepositions such as “in” and “into” in the provided context to infer size relationships, they fail to use verbs and thus make incorrect judgments led by their prior physical commonsense.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.53.pdf",
        "keywords": [
            "commonsense",
            "physical commonsense",
            "probing physical reasoning",
            "prior physical commonsense",
            "counter commonsense context",
            "commonsense contextual size comparison",
            "language models",
            "ccons"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"they fail to use verbs and thus make incorrect judgments led by their prior physical commonsense.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"they fail to use verbs and thus make incorrect judgments led by their prior physical commonsense.\""
    },
    {
        "title": "Learning Neuro-Symbolic World Models with Conversational Proprioception",
        "authors": [
            "Don Joven Agravante",
            "Daiki Kimura",
            "Michiaki Tatsubori",
            "Asim Munawar",
            "Alexander Gray"
        ],
        "published": "2023",
        "summary": "The recent emergence of Neuro-Symbolic Agent (NeSA) approaches to natural language-based interactions calls for the investigation of model-based approaches. In contrast to model-free approaches, which existing NeSAs take, learning an explicit world model has an interesting potential especially in the explainability, which is one of the key selling points of NeSA. To learn useful world models, we leverage one of the recent neuro-symbolic architectures, Logical Neural Networks (LNN). Here, we describe a method that can learn neuro-symbolic world models on the TextWorld-Commonsense set of games. We then show how this can be improved further by taking inspiration from the concept of proprioception, but for conversation. This is done by enhancing the internal logic state with a memory of previous actions while also guiding future actions by augmenting the learned model with constraints based on this memory. This greatly improves the game-solving agents performance in a TextWorld setting, where the advantage over the baseline is an 85% average steps reduction and x2.3 average score.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.57.pdf",
        "keywords": [
            "proprioception",
            "world model",
            "world",
            "neuro symbolic agent",
            "logical neural networks"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "In and Out-of-Domain Text Adversarial Robustness via Label Smoothing",
        "authors": [
            "Yahan Yang",
            "Soham Dan",
            "Dan Roth",
            "Insup Lee"
        ],
        "published": "2023",
        "summary": "Recently it has been shown that state-of-the-art NLP models are vulnerable to adversarial attacks, where the predictions of a model can be drastically altered by slight modifications to the input (such as synonym substitutions). While several defense techniques have been proposed, and adapted, to the discrete nature of text adversarial attacks, the benefits of general-purpose regularization methods such as label smoothing for language models, have not been studied. In this paper, we study the adversarial robustness provided by label smoothing strategies in foundational models for diverse NLP tasks in both in-domain and out-of-domain settings. Our experiments show that label smoothing significantly improves adversarial robustness in pre-trained models like BERT, against various popular attacks. We also analyze the relationship between prediction confidence and robustness, showing that label smoothing reduces over-confident errors on adversarial examples.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.58.pdf",
        "keywords": [
            "label smoothing",
            "robustness",
            "adversarial robustness",
            "text adversarial attacks",
            "language"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recently it has been shown that state-of-the-art NLP models are vulnerable to adversarial attacks, where the predictions of a model can be drastically altered by slight modifications to the input (such as synonym substitutions).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Recently it has been shown that state-of-the-art NLP models are vulnerable to adversarial attacks, where the predictions of a model can be drastically altered by slight modifications to the input (such as synonym substitutions).\""
    },
    {
        "title": "LM-CPPF: Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based Few-Shot Fine-Tuning",
        "authors": [
            "Amirhossein Abaskohi",
            "Sascha Rothe",
            "Yadollah Yaghoobzadeh"
        ],
        "published": "2023",
        "summary": "In recent years, there has been significant progress in developing pre-trained language models for NLP. However, these models often struggle when fine-tuned on small datasets. To address this issue, researchers have proposed various adaptation approaches. Prompt-based tuning is arguably the most common way, especially for larger models. Previous research shows that adding contrastive learning to prompt-based fine-tuning is effective as it helps the model generate embeddings that are more distinguishable between classes, and it can also be more sample-efficient as the model learns from positive and negative examples simultaneously. One of the most important components of contrastive learning is data augmentation, but unlike computer vision, effective data augmentation for NLP is still challenging. This paper proposes LM-CPPF, Contrastive Paraphrasing-guided Prompt-based Fine-tuning of Language Models, which leverages prompt-based few-shot paraphrasing using generative language models, especially large language models such as GPT-3 and OPT-175B, for data augmentation. Our experiments on multiple text classification benchmarks show that this augmentation method outperforms other methods, such as easy data augmentation, back translation, and multiple templates.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.59.pdf",
        "keywords": [
            "fine tuned",
            "lm cppf",
            "data augmentation",
            "contrastive prompt",
            "text classification benchmarks",
            "paraphrasing guided data augmentation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these models often struggle when fine-tuned on small datasets.\"\n\nThis paper mentions a limitation of LLMs (struggling with fine-tuning on small datasets) but does not explore it in depth, instead focusing on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these models often struggle when fine-tuned on small datasets.\"\n\nThis paper mentions a limitation of LLMs (struggling with fine-tuning on small datasets) but does not explore it in depth, instead focusing on the proposed solution."
    },
    {
        "title": "The Role of Global and Local Context in Named Entity Recognition",
        "authors": [
            "Arthur Amalvy",
            "Vincent Labatut",
            "Richard Dufour"
        ],
        "published": "2023",
        "summary": "Pre-trained transformer-based models have recently shown great performance when applied to Named Entity Recognition (NER). As the complexity of their self-attention mechanism prevents them from processing long documents at once, these models are usually applied in a sequential fashion. Such an approach unfortunately only incorporates local context and prevents leveraging global document context in long documents such as novels, which might hinder performance. In this article, we explore the impact of global document context, and its relationships with local context. We find that correctly retrieving global document context has a greater impact on performance than only leveraging local context, prompting for further research on how to better retrieve that context.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.62.pdf",
        "keywords": [
            "named entity recognition",
            "local context"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"As the complexity of their self-attention mechanism prevents them from processing long documents at once, these models are usually applied in a sequential fashion. Such an approach unfortunately only incorporates local context and prevents leveraging global document context in long documents such as novels, which might hinder performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"As the complexity of their self-attention mechanism prevents them from processing long documents at once, these models are usually applied in a sequential fashion. Such an approach unfortunately only incorporates local context and prevents leveraging global document context in long documents such as novels, which might hinder performance.\""
    },
    {
        "title": "Modular Visual Question Answering via Code Generation",
        "authors": [
            "Sanjay Subramanian",
            "Medhini Narasimhan",
            "Kushal Khangaonkar",
            "Kevin Yang",
            "Arsha Nagrani",
            "Cordelia Schmid",
            "Andy Zeng",
            "Trevor Darrell",
            "Dan Klein"
        ],
        "published": "2023",
        "summary": "We present a framework that formulates visual question answering as modular code generation. In contrast to prior work on modular approaches to VQA, our approach requires no additional training and relies on pre-trained language models (LMs), visual models pre-trained on image-caption pairs, and fifty VQA examples used for in-context learning. The generated Python programs invoke and compose the outputs of the visual models using arithmetic and conditional logic. Our approach improves accuracy on the COVR dataset by at least 3% and on the GQA dataset by 2% compared to the few-shot baseline that does not employ code generation.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.65.pdf",
        "keywords": [
            "code generation",
            "modular code generation",
            "generation",
            "visual",
            "visual question answering"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"In contrast to prior work on modular approaches to VQA, our approach requires no additional training and relies on pre-trained language models (LMs)\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"In contrast to prior work on modular approaches to VQA, our approach requires no additional training and relies on pre-trained language models (LMs)\""
    },
    {
        "title": "Unsupervised Subtitle Segmentation with Masked Language Models",
        "authors": [
            "David Ponce",
            "Thierry Etchegoyhen",
            "Victor Ruiz"
        ],
        "published": "2023",
        "summary": "We describe a novel unsupervised approach to subtitle segmentation, based on pretrained masked language models, where line endings and subtitle breaks are predicted according to the likelihood of punctuation to occur at candidate segmentation points. Our approach obtained competitive results in terms of segmentation accuracy across metrics, while also fully preserving the original text and complying with length constraints. Although supervised models trained on in-domain data and with access to source audio information can provide better segmentation accuracy, our approach is highly portable across languages and domains and may constitute a robust off-the-shelf solution for subtitle segmentation.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.67.pdf",
        "keywords": [
            "subtitle segmentation",
            "unsupervised subtitle segmentation",
            "masked language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although supervised models trained on in-domain data and with access to source audio information can provide better segmentation accuracy\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although supervised models trained on in-domain data and with access to source audio information can provide better segmentation accuracy\""
    },
    {
        "title": "Exploring Continual Learning for Code Generation Models",
        "authors": [
            "Prateek Yadav",
            "Qing Sun",
            "Hantian Ding",
            "Xiaopeng Li",
            "Dejiao Zhang",
            "Ming Tan",
            "Parminder Bhatia",
            "Xiaofei Ma",
            "Ramesh Nallapati",
            "Murali Krishna Ramanathan",
            "Mohit Bansal",
            "Bing Xiang"
        ],
        "published": "2023",
        "summary": "Large-scale code generation models such as Copilot and CodeT5 have achieved impressive performance. However, libraries are upgraded or deprecated very frequently and re-training large-scale language models is computationally expensive. Therefore, Continual Learning (CL) is an important aspect that remains under-explored in the code domain. In this paper, we introduce a benchmark called CodeTask-CL that covers a wide range of tasks, including code generation, translation, summarization, and refinement, with different input and output programming languages. Next, on our CodeTask-CL benchmark, we compare popular CL techniques from NLP and Vision domains. We find that effective methods like Prompt Pooling (PP) suffer from catastrophic forgetting due to the unstable training of the prompt selection mechanism caused by stark distribution shifts in coding tasks. We address this issue with our proposed method, Prompt Pooling with Teacher Forcing (PP-TF), that stabilizes training by enforcing constraints on the prompt selection mechanism and leads to a 21.54% improvement over Prompt Pooling. Along with the benchmark, we establish a training pipeline that can be used for CL on code models, which we believe can motivate further development of CL methods for code models.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.68.pdf",
        "keywords": [
            "code models",
            "code generation",
            "continual learning",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, libraries are upgraded or deprecated very frequently and re-training large-scale language models is computationally expensive\"; \"We find that effective methods like Prompt Pooling (PP) suffer from catastrophic forgetting due to the unstable training of the prompt selection mechanism caused by stark distribution shifts in coding tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, libraries are upgraded or deprecated very frequently and re-training large-scale language models is computationally expensive\"; \"We find that effective methods like Prompt Pooling (PP) suffer from catastrophic forgetting due to the unstable training of the prompt selection mechanism caused by stark distribution shifts in coding tasks.\""
    },
    {
        "title": "Counterfactual reasoning: Testing language models’ understanding of hypothetical scenarios",
        "authors": [
            "Jiaxuan Li",
            "Lang Yu",
            "Allyson Ettinger"
        ],
        "published": "2023",
        "summary": "Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on the understanding of real world. We tease these factors apart by leveraging counterfactual conditionals, which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from five pre-trained language models. We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge—however, we also find that for most models this effect appears largely to be driven by simple lexical cues. When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we find that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.70.pdf",
        "keywords": [
            "scenarios",
            "counterfactual scenarios",
            "counterfactual reasoning",
            "language",
            "counterfactual conditionals",
            "counterfactual predictions",
            "logical reasoning",
            "force language models",
            "counterfactuals"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge—however, we also find that for most models this effect appears largely to be driven by simple lexical cues.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge—however, we also find that for most models this effect appears largely to be driven by simple lexical cues.\""
    },
    {
        "title": "Gradient Ascent Post-training Enhances Language Model Generalization",
        "authors": [
            "Dongkeun Yoon",
            "Joel Jang",
            "Sungdong Kim",
            "Minjoon Seo"
        ],
        "published": "2023",
        "summary": "In this work, we empirically show that updating pretrained LMs (350M, 1.3B, 2.7B) with just a few steps of Gradient Ascent Post-training (GAP) on random, unlabeled text corpora enhances its zero-shot generalization capabilities across diverse NLP tasks. Specifically, we show that GAP can allow LMs to become comparable to 2-3x times larger LMs across 12 different NLP tasks. We also show that applying GAP on out-of-distribution corpora leads to the most reliable performance improvements. Our findings indicate that GAP can be a promising method for improving the generalization capability of LMs without any task-specific fine-tuning.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.74.pdf",
        "keywords": [
            "gradient ascent",
            "language model generalization",
            "shot generalization"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation is mentioned, but the abstract implies that LMs have limited generalization capabilities, which the proposed method aims to improve.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation is mentioned, but the abstract implies that LMs have limited generalization capabilities, which the proposed method aims to improve."
    },
    {
        "title": "Are Pre-trained Language Models Useful for Model Ensemble in Chinese Grammatical Error Correction?",
        "authors": [
            "Chenming Tang",
            "Xiuyu Wu",
            "Yunfang Wu"
        ],
        "published": "2023",
        "summary": "Model ensemble has been in widespread use for Grammatical Error Correction (GEC), boosting model performance. We hypothesize that model ensemble based on the perplexity (PPL) computed by pre-trained language models (PLMs) should benefit the GEC system. To this end, we explore several ensemble strategies based on strong PLMs with four sophisticated single models. However, the performance does not improve but even gets worse after the PLM-based ensemble. This surprising result sets us doing a detailed analysis on the data and coming up with some insights on GEC. The human references of correct sentences is far from sufficient in the test data, and the gap between a correct sentence and an idiomatic one is worth our attention. Moreover, the PLM-based ensemble strategies provide an effective way to extend and improve GEC benchmark data. Our source code is available at https://github.com/JamyDon/PLM-based-CGEC-Model-Ensemble.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.77.pdf",
        "keywords": [
            "grammatical error correction",
            "ensemble",
            "model ensemble",
            "language models",
            "pre trained language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the performance does not improve but even gets worse after the PLM-based ensemble.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the performance does not improve but even gets worse after the PLM-based ensemble.\""
    },
    {
        "title": "With a Little Push, NLI Models can Robustly and Efficiently Predict Faithfulness",
        "authors": [
            "Julius Steen",
            "Juri Opitz",
            "Anette Frank",
            "Katja Markert"
        ],
        "published": "2023",
        "summary": "Conditional language models still generate unfaithful output that is not supported by their input. These unfaithful generations jeopardize trust in real-world applications such as summarization or human-machine interaction, motivating a need for automatic faithfulness metrics. To implement such metrics, NLI models seem attractive, since they solve a strongly related task that comes with a wealth of prior research and data. But recent research suggests that NLI models require costly additional machinery to perform reliably across datasets, e.g., by running inference on a cartesian product of input and generated sentences, or supporting them with a question-generation/answering step. In this work we show that pure NLI models _can_ outperform more complex metrics when combining task-adaptive data augmentation with robust inference procedures. We propose: (1) Augmenting NLI training data toadapt NL inferences to the specificities of faithfulness prediction in dialogue;(2) Making use of both entailment and contradiction probabilities in NLI, and(3) Using Monte-Carlo dropout during inference. Applied to the TRUE benchmark, which combines faithfulness datasets across diverse domains and tasks, our approach strongly improves a vanilla NLI model and significantly outperforms previous work, while showing favourable computational cost.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.79.pdf",
        "keywords": [
            "faithfulness",
            "conditional language",
            "nli"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Conditional language models still generate unfaithful output that is not supported by their input.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Conditional language models still generate unfaithful output that is not supported by their input.\""
    },
    {
        "title": "A Better Way to Do Masked Language Model Scoring",
        "authors": [
            "Carina Kauf",
            "Anna Ivanova"
        ],
        "published": "2023",
        "summary": "Estimating the log-likelihood of a given sentence under an autoregressive language model is straightforward: one can simply apply the chain rule and sum the log-likelihood values for each successive token. However, for masked language models (MLMs), there is no direct way to estimate the log-likelihood of a sentence. To address this issue, Salazar et al. (2020) propose to estimate sentence pseudo-log-likelihood (PLL) scores, computed by successively masking each sentence token, retrieving its score using the rest of the sentence as context, and summing the resulting values. Here, we demonstrate that the original PLL method yields inflated scores for out-of-vocabulary words and propose an adapted metric, in which we mask not only the target token, but also all within-word tokens to the right of the target. We show that our adapted metric (PLL-word-l2r) outperforms both the original PLL metric and a PLL metric in which all within-word tokens are masked. In particular, it better satisfies theoretical desiderata and better correlates with scores from autoregressive models. Finally, we show that the choice of metric affects even tightly controlled, minimal pair evaluation benchmarks (such as BLiMP), underscoring the importance of selecting an appropriate scoring metric for evaluating MLM properties.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.80.pdf",
        "keywords": [
            "masked",
            "scoring metric"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "ChatGPT for Zero-shot Dialogue State Tracking: A Solution or an Opportunity?",
        "authors": [
            "Michael Heck",
            "Nurul Lubis",
            "Benjamin Ruppik",
            "Renato Vukovic",
            "Shutong Feng",
            "Christian Geishauser",
            "Hsien-chin Lin",
            "Carel van Niekerk",
            "Milica Gasic"
        ],
        "published": "2023",
        "summary": "Recent research on dialog state tracking (DST) focuses on methods that allow few- and zero-shot transfer to new domains or schemas. However, performance gains heavily depend on aggressive data augmentation and fine-tuning of ever larger language model based architectures. In contrast, general purpose language models, trained on large amounts of diverse data, hold the promise of solving any kind of task without task-specific training. We present preliminary experimental results on the ChatGPT research preview, showing that ChatGPT achieves state-of-the-art performance in zero-shot DST. Despite our findings, we argue that properties inherent to general purpose models limit their ability to replace specialized systems. We further theorize that the in-context learning capabilities of such models will likely become powerful tools to support the development of dedicated dialog state trackers and enable dynamic methods.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.81.pdf",
        "keywords": [
            "dialog state",
            "dialog state tracking"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite our findings, we argue that properties inherent to general purpose models limit their ability to replace specialized systems.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Despite our findings, we argue that properties inherent to general purpose models limit their ability to replace specialized systems.\""
    },
    {
        "title": "Controllable Mixed-Initiative Dialogue Generation through Prompting",
        "authors": [
            "Maximillian Chen",
            "Xiao Yu",
            "Weiyan Shi",
            "Urvi Awasthi",
            "Zhou Yu"
        ],
        "published": "2023",
        "summary": "Mixed-initiative dialogue tasks involve repeated exchanges of information and conversational control. Conversational agents gain control by generating responses that follow particular dialogue intents or strategies, prescribed by a policy planner. The standard approach has been fine-tuning pre-trained language models to perform generation conditioned on these intents. However, these supervised generation models are limited by the cost and quality of data annotation. We instead prompt large language models as a drop-in replacement to fine-tuning on conditional generation. We formalize prompt construction for controllable mixed-initiative dialogue. Our findings show improvements over fine-tuning and ground truth responses according to human evaluation and automatic metrics for two tasks: PersuasionForGood and Emotional Support Conversations.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.82.pdf",
        "keywords": [
            "agents",
            "prompting",
            "emotional",
            "dialogue generation",
            "controllable mixed",
            "strategies"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these supervised generation models are limited by the cost and quality of data annotation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these supervised generation models are limited by the cost and quality of data annotation.\""
    },
    {
        "title": "NollySenti: Leveraging Transfer Learning and Machine Translation for Nigerian Movie Sentiment Classification",
        "authors": [
            "Iyanuoluwa Shode",
            "David Ifeoluwa Adelani",
            "JIng Peng",
            "Anna Feldman"
        ],
        "published": "2023",
        "summary": "Africa has over 2000 indigenous languages but they are under-represented in NLP research due to lack of datasets. In recent years, there have been progress in developing labelled corpora for African languages. However, they are often available in a single domain and may not generalize to other domains. In this paper, we focus on the task of sentiment classification for cross-domain adaptation. We create a new dataset, Nollywood movie reviews for five languages widely spoken in Nigeria (English, Hausa, Igbo, Nigerian Pidgin, and Yoruba). We provide an extensive empirical evaluation using classical machine learning methods and pre-trained language models. By leveraging transfer learning, we compare the performance of cross-domain adaptation from Twitter domain, and cross-lingual adaptation from English language. Our evaluation shows that transfer from English in the same target domain leads to more than 5% improvement in accuracy compared to transfer from Twitter in the same language. To further mitigate the domain difference, we leverage machine translation from English to other Nigerian languages, which leads to a further improvement of 7% over cross-lingual evaluation. While machine translation to low-resource languages are often of low quality, our analysis shows that sentiment related words are often preserved.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.85.pdf",
        "keywords": [
            "cross domain adaptation",
            "transfer learning",
            "nigerian languages",
            "machine translation",
            "languages",
            "sentiment classification",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While machine translation to low-resource languages are often of low quality, our analysis shows that sentiment related words are often preserved.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of machine translation (a component related to LLMs) in low-resource languages, but it is not the primary focus of the paper and is only briefly mentioned.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While machine translation to low-resource languages are often of low quality, our analysis shows that sentiment related words are often preserved.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of machine translation (a component related to LLMs) in low-resource languages, but it is not the primary focus of the paper and is only briefly mentioned."
    },
    {
        "title": "Do GPTs Produce Less Literal Translations?",
        "authors": [
            "Vikas Raunak",
            "Arul Menezes",
            "Matt Post",
            "Hany Hassan"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose language models capable of addressing many natural language generation or understanding tasks. On the task of Machine Translation (MT), multiple works have investigated few-shot prompting mechanisms to elicit better translations from LLMs. However, there has been relatively little investigation on how such translations differ qualitatively from the translations generated by standard Neural Machine Translation (NMT) models. In this work, we investigate these differences in terms of the literalness of translations produced by the two systems. Using literalness measures involving word alignment and monotonicity, we find that translations out of English (E-X) from GPTs tend to be less literal, while exhibiting similar or better scores on MT quality metrics. We demonstrate that this finding is borne out in human evaluations as well. We then show that these differences are especially pronounced when translating sentences that contain idiomatic expressions.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.90.pdf",
        "keywords": [
            "translations",
            "neural machine translation",
            "literalness measures"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Using literalness measures involving word alignment and monotonicity, we find that translations out of English (E-X) from GPTs tend to be less literal, while exhibiting similar or better scores on MT quality metrics.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Using literalness measures involving word alignment and monotonicity, we find that translations out of English (E-X) from GPTs tend to be less literal, while exhibiting similar or better scores on MT quality metrics.\""
    },
    {
        "title": "Black-box language model explanation by context length probing",
        "authors": [
            "Ondřej Cífka",
            "Antoine Liutkus"
        ],
        "published": "2023",
        "summary": "The increasingly widespread adoption of large language models has highlighted the need for improving their explainability. We present *context length probing*, a novel explanation technique for causal language models, based on tracking the predictions of a model as a function of the length of available context, and allowing to assign *differential importance scores* to different contexts. The technique is model-agnostic and does not rely on access to model internals beyond computing token-level probabilities. We apply context length probing to large pre-trained language models and offer some initial analyses and insights, including the potential for studying long-range dependencies. The [source code](https://github.com/cifkao/context-probing/) and an [interactive demo](https://cifkao.github.io/context-probing/) of the method are available.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.92.pdf",
        "keywords": [
            "context",
            "context probing",
            "context length probing",
            "box language model explanation",
            "causal language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The increasingly widespread adoption of large language models has highlighted the need for improving their explainability.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The increasingly widespread adoption of large language models has highlighted the need for improving their explainability.\""
    },
    {
        "title": "ReAugKD: Retrieval-Augmented Knowledge Distillation For Pre-trained Language Models",
        "authors": [
            "Jianyi Zhang",
            "Aashiq Muhamed",
            "Aditya Anantharaman",
            "Guoyin Wang",
            "Changyou Chen",
            "Kai Zhong",
            "Qingjun Cui",
            "Yi Xu",
            "Belinda Zeng",
            "Trishul Chilimbi",
            "Yiran Chen"
        ],
        "published": "2023",
        "summary": "Knowledge Distillation (KD) is one of the most effective approaches to deploying large-scale pre-trained language models in low-latency environments by transferring the knowledge contained in the large-scale models to smaller student models. Prior KD approaches use the soft labels and intermediate activations generated by the teacher to transfer knowledge to the student model parameters alone. In this paper, we show that having access to non-parametric memory in the form of a knowledge base with the teacher’s soft labels and predictions can further improve student generalization. To enable the student to retrieve from the knowledge base effectively, we propose a new framework and loss function that preserves the semantic similarities of teacher and student training examples. We show through extensive experiments that our retrieval mechanism can achieve state-of-the-art performance for task-specific knowledge distillation on the GLUE benchmark.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.97.pdf",
        "keywords": [
            "knowledge distillation",
            "loss function",
            "pre trained language models",
            "augmented knowledge distillation",
            "prior kd"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit evidence, but the motivation for knowledge distillation implies that large-scale pre-trained language models may not be suitable for low-latency environments, which can be seen as a limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit evidence, but the motivation for knowledge distillation implies that large-scale pre-trained language models may not be suitable for low-latency environments, which can be seen as a limitation."
    },
    {
        "title": "Deriving Language Models from Masked Language Models",
        "authors": [
            "Lucas Torroba Hennigen",
            "Yoon Kim"
        ],
        "published": "2023",
        "summary": "Masked language models (MLM) do not explicitly define a distribution over language, i.e., they are not language models per se. However, recent work has implicitly treated them as such for the purposes of generation and scoring. This paper studies methods for deriving explicit joint distributions from MLMs, focusing on distributions over two tokens, which makes it possible to calculate exact distributional properties. We find that an approach based on identifying joints whose conditionals are closest to those of the MLM works well and outperforms existing Markov random field-based approaches. We further find that this derived model’s conditionals can even occasionally outperform the original MLM’s conditionals.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.99.pdf",
        "keywords": [
            "language",
            "language models",
            "masked language"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Masked language models (MLM) do not explicitly define a distribution over language, i.e., they are not language models per se.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Masked language models (MLM) do not explicitly define a distribution over language, i.e., they are not language models per se.\""
    },
    {
        "title": "UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation",
        "authors": [
            "Zhiming Mao",
            "Huimin Wang",
            "Yiming Du",
            "Kam-Fai Wong"
        ],
        "published": "2023",
        "summary": "Prior study has shown that pretrained language models (PLM) can boost the performance of text-based recommendation. In contrast to previous works that either use PLM to encode user history as a whole input text, or impose an additional aggregation network to fuse multi-turn history representations, we propose a unified local- and global-attention Transformer encoder to better model two-level contexts of user history. Moreover, conditioned on user history encoded by Transformer encoders, our framework leverages Transformer decoders to estimate the language perplexity of candidate text items, which can serve as a straightforward yet significant contrastive signal for user-item text matching. Based on this, our framework, UniTRec, unifies the contrastive objectives of discriminative matching scores and candidate text perplexity to jointly enhance text-based recommendation. Extensive evaluation shows that UniTRec delivers SOTA performance on three text-based recommendation tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.100.pdf",
        "keywords": [
            "text perplexity",
            "text based recommendation",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "Reasoning Implicit Sentiment with Chain-of-Thought Prompting",
        "authors": [
            "Hao Fei",
            "Bobo Li",
            "Qian Liu",
            "Lidong Bing",
            "Fei Li",
            "Tat-Seng Chua"
        ],
        "published": "2023",
        "summary": "While sentiment analysis systems try to determine the sentiment polarities of given targets based on the key opinion expressions in input texts, in implicit sentiment analysis (ISA) the opinion cues come in an implicit and obscure manner. Thus detecting implicit sentiment requires the common-sense and multi-hop reasoning ability to infer the latent intent of opinion. Inspired by the recent chain-of-thought (CoT) idea, in this work we introduce a Three-hop Reasoning (THOR) CoT framework to mimic the human-like reasoning process for ISA. We design a three-step prompting principle for THOR to step-by-step induce the implicit aspect, opinion, and finally the sentiment polarity. Our THOR+Flan-T5 (11B) pushes the state-of-the-art (SoTA) by over 6% F1 on supervised setup. More strikingly, THOR+GPT3 (175B) boosts the SoTA by over 50% F1 on zero-shot setting.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.101.pdf",
        "keywords": [
            "sentiment analysis",
            "implicit sentiment analysis",
            "prompting",
            "chain of thought prompting",
            "sentiment polarity"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations, but the introduction of a new framework to address implicit sentiment analysis implies that existing LLMs may struggle with this task.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations, but the introduction of a new framework to address implicit sentiment analysis implies that existing LLMs may struggle with this task."
    },
    {
        "title": "Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings",
        "authors": [
            "Ta-Chung Chi",
            "Ting-Han Fan",
            "Li-Wei Chen",
            "Alexander Rudnicky",
            "Peter Ramadge"
        ],
        "published": "2023",
        "summary": "The use of positional embeddings in transformer language models is widely accepted. However, recent research has called into question the necessity of such embeddings. We further extend this inquiry by demonstrating that a randomly initialized and frozen transformer language model, devoid of positional embeddings, inherently encodes strong positional information through the shrinkage of self-attention variance. To quantify this variance, we derive the underlying distribution of each step within a transformer layer. Through empirical validation using a fully pretrained model, we show that the variance shrinkage effect still persists after extensive gradient updates. Our findings serve to justify the decision to discard positional embeddings and thus facilitate more efficient pretraining of transformer language models.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.102.pdf",
        "keywords": [
            "transformer language models",
            "positional embeddings",
            "positional information",
            "self attention variance",
            "variance shrinkage"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, recent research has called into question the necessity of such embeddings.\"\n\nThis paper discusses LLMs but does not mention any explicit limitation of the models in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, recent research has called into question the necessity of such embeddings.\"\n\nThis paper discusses LLMs but does not mention any explicit limitation of the models in the abstract."
    },
    {
        "title": "Learning Multi-Step Reasoning by Solving Arithmetic Tasks",
        "authors": [
            "Tianduo Wang",
            "Wei Lu"
        ],
        "published": "2023",
        "summary": "Mathematical reasoning is regarded as a necessary ability for Language Models (LMs). Recent works demonstrate large LMs’ impressive performance in solving math problems. The success is attributed to their Chain-of-Thought (CoT) reasoning abilities, i.e., the ability to decompose complex questions into step-by-step reasoning chains, but such ability seems only to emerge from models with abundant parameters. This work investigates how to incorporate relatively small LMs with the capabilities of multi-step reasoning. We propose to inject such abilities by continually pre-training LMs on a synthetic dataset MsAT which is composed of Multi-step Arithmetic Tasks. Our experiments on four math word problem datasets show the effectiveness of the proposed method in enhancing LMs’ math reasoning abilities.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.106.pdf",
        "keywords": [
            "multi step reasoning",
            "multi step arithmetic tasks",
            "models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"but such ability seems only to emerge from models with abundant parameters.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"but such ability seems only to emerge from models with abundant parameters.\""
    },
    {
        "title": "Towards Adaptive Prefix Tuning for Parameter-Efficient Language Model Fine-tuning",
        "authors": [
            "Zhen-Ru Zhang",
            "Chuanqi Tan",
            "Haiyang Xu",
            "Chengyu Wang",
            "Jun Huang",
            "Songfang Huang"
        ],
        "published": "2023",
        "summary": "Fine-tuning large pre-trained language models on various downstream tasks with whole parameters is prohibitively expensive. Hence, Parameter-efficient fine-tuning has attracted attention that only optimizes a few task-specific parameters with the frozen pre-trained model. In this work, we focus on prefix tuning, which only optimizes continuous prefix vectors (i.e. pseudo tokens) inserted into Transformer layers. Based on the observation that the learned syntax and semantics representation varies a lot at different layers, we argue that the adaptive prefix will be further tailored to each layer than the fixed one, enabling the fine-tuning more effective and efficient. Thus, we propose Adaptive Prefix Tuning (APT) to adjust the prefix in terms of both fine-grained token level and coarse-grained layer level with a gate mechanism. Experiments on the SuperGLUE and NER datasets show the effectiveness of APT. In addition, taking the gate as a probing, we validate the efficiency and effectiveness of the variable prefix.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.107.pdf",
        "keywords": [
            "prefix",
            "prefix tuning",
            "tuning",
            "fine tuning",
            "adaptive prefix tuning",
            "variable prefix",
            "adaptive prefix"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Fine-tuning large pre-trained language models on various downstream tasks with whole parameters is prohibitively expensive.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Fine-tuning large pre-trained language models on various downstream tasks with whole parameters is prohibitively expensive.\""
    },
    {
        "title": "Improving Gender Fairness of Pre-Trained Language Models without Catastrophic Forgetting",
        "authors": [
            "Zahra Fatemi",
            "Chen Xing",
            "Wenhao Liu",
            "Caimming Xiong"
        ],
        "published": "2023",
        "summary": "Existing studies addressing gender bias of pre-trained language models, usually build a small gender-neutral data set and conduct a second phase pre-training on the model with such data. However, given the limited size and concentrated focus of the gender-neutral data, catastrophic forgetting would occur during second-phase pre-training. Forgetting information in the original training data may damage the model’s downstream performance by a large margin. In this work, we empirically show that catastrophic forgetting occurs in such methods by evaluating them with general NLP tasks in GLUE. Then, we propose a new method, GEnder Equality Prompt (GEEP), to improve gender fairness of pre-trained models with less forgetting. GEEP freezes the pre-trained model and learns gender-related prompts with gender-neutral data. Empirical results show that GEEP not only achieves SOTA performances on gender fairness tasks, but also forgets less and performs better on GLUE by a large margin.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.108.pdf",
        "keywords": [
            "gender equality",
            "gender fairness",
            "gender",
            "forgetting",
            "gender neutral",
            "pre trained language models",
            "gender bias"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, given the limited size and concentrated focus of the gender-neutral data, catastrophic forgetting would occur during second-phase pre-training. Forgetting information in the original training data may damage the model’s downstream performance by a large margin.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, given the limited size and concentrated focus of the gender-neutral data, catastrophic forgetting would occur during second-phase pre-training. Forgetting information in the original training data may damage the model’s downstream performance by a large margin.\""
    },
    {
        "title": "Class-Incremental Learning based on Label Generation",
        "authors": [
            "Yijia Shao",
            "Yiduo Guo",
            "Dongyan Zhao",
            "Bing Liu"
        ],
        "published": "2023",
        "summary": "Despite the great success of pre-trained language models, it is still a challenge to use these models for continual learning, especially for the class-incremental learning (CIL) setting due to catastrophic forgetting (CF). This paper reports our finding that if we formulate CIL as a continual label generation problem, CF is drastically reduced and the generalizable representations of pre-trained models can be better retained. We thus propose a new CIL method (VAG) that also leverages the sparsity of vocabulary to focus the generation and creates pseudo-replay samples by using label semantics. Experimental results show that VAG outperforms baselines by a large margin.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.109.pdf",
        "keywords": [
            "label generation",
            "class incremental learning",
            "class incremental",
            "continual label generation",
            "replay"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the great success of pre-trained language models, it is still a challenge to use these models for continual learning, especially for the class-incremental learning (CIL) setting due to catastrophic forgetting (CF).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite the great success of pre-trained language models, it is still a challenge to use these models for continual learning, especially for the class-incremental learning (CIL) setting due to catastrophic forgetting (CF).\""
    },
    {
        "title": "Evaluating pragmatic abilities of image captioners on A3DS",
        "authors": [
            "Polina Tsvilodub",
            "Michael Franke"
        ],
        "published": "2023",
        "summary": "Evaluating grounded neural language model performance with respect to pragmatic qualities like the trade off between truthfulness, contrastivity and overinformativity of generated utterances remains a challenge in absence of data collected from humans. To enable such evaluation, we present a novel open source image-text dataset “Annotated 3D Shapes” (A3DS) comprising over nine million exhaustive natural language annotations and over 12 million variable-granularity captions for the 480,000 images provided by Burgess & Kim (2018).We showcase the evaluation of pragmatic abilities developed by a task-neutral image captioner fine-tuned in a multi-agent communication setting to produce contrastive captions. The evaluation is enabled by the dataset because the exhaustive annotations allow to quantify the presence of contrastive features in the model’s generations. We show that the model develops human-like patterns (informativity, brevity, over-informativity for specific features (e.g., shape, color biases)).",
        "pdf_link": "https://aclanthology.org/2023.acl-short.110.pdf",
        "keywords": [
            "captioners",
            "captioner fine tuned",
            "truthfulness",
            "pragmatic abilities",
            "pragmatic qualities",
            "evaluating pragmatic abilities"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Evaluating grounded neural language model performance with respect to pragmatic qualities like the trade off between truthfulness, contrastivity and overinformativity of generated utterances remains a challenge\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Evaluating grounded neural language model performance with respect to pragmatic qualities like the trade off between truthfulness, contrastivity and overinformativity of generated utterances remains a challenge\""
    },
    {
        "title": "The Art of Prompting: Event Detection based on Type Specific Prompts",
        "authors": [
            "Sijia Wang",
            "Mo Yu",
            "Lifu Huang"
        ],
        "published": "2023",
        "summary": "We compare various forms of prompts to represent event types and develop a unified framework to incorporate the event type specific prompts for supervised, few-shot, and zero-shot event detection. The experimental results demonstrate that a well-defined and comprehensive event type prompt can significantly improve event detection performance, especially when the annotated data is scarce (few-shot event detection) or not available (zero-shot event detection). By leveraging the semantics of event types, our unified framework shows up to 22.2% F-score gain over the previous state-of-the-art baselines.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.111.pdf",
        "keywords": [
            "event types",
            "event type specific prompts",
            "event detection",
            "type specific prompts",
            "prompting",
            "shot event detection",
            "art"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Do Models Really Learn to Follow Instructions? An Empirical Study of Instruction Tuning",
        "authors": [
            "Po-Nien Kung",
            "Nanyun Peng"
        ],
        "published": "2023",
        "summary": "Recent works on instruction tuning (IT) have achieved great performance with zero-shot generalizability to unseen tasks. With additional context (e.g., task definition, examples) provided to models for fine-tuning, they achieved much higher performance than untuned models. Despite impressive performance gains, what models learn from IT remains understudied. In this work, we analyze how models utilize instructions during IT by comparing model training with altered vs. original instructions. Specifically, we create simplified task definitions by removing all semantic components and only leaving the output space information, and delusive examples that contain incorrect input-output mapping. Our experiments show that models trained on simplified task definition or delusive examples can achieve comparable performance to the ones trained on the original instructions and examples. Furthermore, we introduce a random baseline to perform zeroshot classification tasks, and find it achieves similar performance (42.6% exact-match) as IT does (43% exact-match) in low resource setting, while both methods outperform naive T5 significantly (30% per exact-match). Our analysis provides evidence that the impressive performance gain of current IT models can come from picking up superficial patterns, such as learning the output format and guessing. Our study highlights the urgent need for more reliable IT methods and evaluation.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.113.pdf",
        "keywords": [
            "instruction tuning"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our analysis provides evidence that the impressive performance gain of current IT models can come from picking up superficial patterns, such as learning the output format and guessing.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Our analysis provides evidence that the impressive performance gain of current IT models can come from picking up superficial patterns, such as learning the output format and guessing.\""
    },
    {
        "title": "Self-Distilled Quantization: Achieving High Compression Rates in Transformer-Based Language Models",
        "authors": [
            "James O’Neill",
            "Sourav Dutta"
        ],
        "published": "2023",
        "summary": "We investigate the effects of post-training quantization and quantization-aware training on the generalization of Transformer language models. We present a new method called self-distilled quantization (SDQ) that minimizes accumulative quantization errors and outperforms baselines. We apply SDQ to multilingual models XLM-RBase and InfoXLMBase and demonstrate that both models can be reduced from 32-bit floating point weights to 8-bit integer weights while maintaining a high level of performance on the XGLUE benchmark. Our results also highlight the challenges of quantizing multilingual models, which must generalize to languages they were not fine-tuned on.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.114.pdf",
        "keywords": [
            "transformer language models",
            "self distilled quantization",
            "transformer based language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our results also highlight the challenges of quantizing multilingual models, which must generalize to languages they were not fine-tuned on.\"\n\nThis rating is given because the abstract mentions a challenge related to LLMs, but it is not the primary focus of the paper and is only briefly mentioned.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Our results also highlight the challenges of quantizing multilingual models, which must generalize to languages they were not fine-tuned on.\"\n\nThis rating is given because the abstract mentions a challenge related to LLMs, but it is not the primary focus of the paper and is only briefly mentioned."
    },
    {
        "title": "Text-to-SQL Error Correction with Language Models of Code",
        "authors": [
            "Ziru Chen",
            "Shijie Chen",
            "Michael White",
            "Raymond Mooney",
            "Ali Payani",
            "Jayanth Srinivasa",
            "Yu Su",
            "Huan Sun"
        ],
        "published": "2023",
        "summary": "Despite recent progress in text-to-SQL parsing, current semantic parsers are still not accurate enough for practical use. In this paper, we investigate how to build automatic text-to-SQL error correction models. Noticing that token-level edits are out of context and sometimes ambiguous, we propose building clause-level edit models instead. Besides, while most language models of code are not specifically pre-trained for SQL, they know common data structures and their operations in programming languages such as Python. Thus, we propose a novel representation for SQL queries and their edits that adheres more closely to the pre-training corpora of language models of code. Our error correction model improves the exact set match accuracy of different parsers by 2.4-6.5 and obtains up to 4.3 point absolute improvement over two strong baselines.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.117.pdf",
        "keywords": [
            "error correction",
            "language models",
            "parsers",
            "text to sql parsing",
            "semantic parsers"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Noticing that token-level edits are out of context and sometimes ambiguous\"\n\nThis paper discusses language models of code, which can be considered a type of LLM. However, it does not explicitly discuss the limitations of LLMs, but rather uses the ambiguity of token-level edits as a motivation for proposing a clause-level edit model.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Noticing that token-level edits are out of context and sometimes ambiguous\"\n\nThis paper discusses language models of code, which can be considered a type of LLM. However, it does not explicitly discuss the limitations of LLMs, but rather uses the ambiguity of token-level edits as a motivation for proposing a clause-level edit model."
    },
    {
        "title": "Summarizing, Simplifying, and Synthesizing Medical Evidence using GPT-3 (with Varying Success)",
        "authors": [
            "Chantal Shaib",
            "Millicent Li",
            "Sebastian Joseph",
            "Iain Marshall",
            "Junyi Jessy Li",
            "Byron Wallace"
        ],
        "published": "2023",
        "summary": "Large language models, particularly GPT-3, are able to produce high quality summaries ofgeneral domain news articles in few- and zero-shot settings. However, it is unclear if such models are similarly capable in more specialized domains such as biomedicine. In this paper we enlist domain experts (individuals with medical training) to evaluate summaries of biomedical articles generated by GPT-3, given no supervision. We consider bothsingle- and multi-document settings. In the former, GPT-3 is tasked with generating regular and plain-language summaries of articles describing randomized controlled trials; in thelatter, we assess the degree to which GPT-3 is able to synthesize evidence reported acrossa collection of articles. We design an annotation scheme for evaluating model outputs, withan emphasis on assessing the factual accuracy of generated summaries. We find that whileGPT-3 is able to summarize and simplify single biomedical articles faithfully, it strugglesto provide accurate aggregations of findings over multiple documents. We release all data,code, and annotations used in this work.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.119.pdf",
        "keywords": [
            "gpt 3",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it is unclear if such models are similarly capable in more specialized domains such as biomedicine... We find that while GPT-3 is able to summarize and simplify single biomedical articles faithfully, it struggles to provide accurate aggregations of findings over multiple documents.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, it is unclear if such models are similarly capable in more specialized domains such as biomedicine... We find that while GPT-3 is able to summarize and simplify single biomedical articles faithfully, it struggles to provide accurate aggregations of findings over multiple documents.\""
    },
    {
        "title": "Prefix Propagation: Parameter-Efficient Tuning for Long Sequences",
        "authors": [
            "Jonathan Li",
            "Will Aitken",
            "Rohan Bhambhoria",
            "Xiaodan Zhu"
        ],
        "published": "2023",
        "summary": "Parameter-efficient tuning aims to mitigate the large memory requirements of adapting pretrained language models for downstream tasks. For example, one popular method, prefix-tuning, prepends trainable tokens to sequences while freezing the rest of the model’s parameters. Although such models attain comparable performance with fine-tuning when applied to sequences with short to moderate lengths, we show their inferior performance when modelling long sequences. To bridge this gap, we propose prefix-propagation, a simple but effective approach that conditions prefixes on previous hidden states. We empirically demonstrate that prefix-propagation outperforms prefix-tuning across long-document tasks, while using 50% fewer parameters. To further investigate the proposed architecture, we also show its advantage in calibration, and perform additional study on its relationship with kernel attention. To the best of our knowledge, this work is the first to focus on parameter-efficient learning for long-sequence language tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.120.pdf",
        "keywords": [
            "prefixes",
            "parameter efficient tuning",
            "prefix propagation",
            "prefix tuning",
            "parameter efficient learning",
            "attention"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although such models attain comparable performance with fine-tuning when applied to sequences with short to moderate lengths, we show their inferior performance when modelling long sequences.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although such models attain comparable performance with fine-tuning when applied to sequences with short to moderate lengths, we show their inferior performance when modelling long sequences.\""
    },
    {
        "title": "Hexatagging: Projective Dependency Parsing as Tagging",
        "authors": [
            "Afra Amini",
            "Tianyu Liu",
            "Ryan Cotterell"
        ],
        "published": "2023",
        "summary": "We introduce a novel dependency parser, the hexatagger, that constructs dependency trees by tagging the words in a sentence with elements from a finite set of possible tags. In contrast to many approaches to dependency parsing, our approach is fully parallelizable at training time, i.e., the structure-building actions needed to build a dependency parse can be predicted in parallel to each other. Additionally, exact decoding is linear in time and space complexity. Furthermore, we derive a probabilistic dependency parser that predicts hexatags using no more than a linear model with features from a pretrained language model, i.e., we forsake a bespoke architecture explicitly designed for the task. Despite the generality and simplicity of our approach, we achieve state-of-the-art performance of 96.4 LAS and 97.4 UAS on the Penn Treebank test set. Additionally, our parser’s linear time complexity and parallelism significantly improve computational efficiency, with a roughly 10-times speed-up over previous state-of-the-art models during decoding.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.124.pdf",
        "keywords": [
            "dependency parsing",
            "dependency parser",
            "hexatagging",
            "dependency trees"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Understanding Demonstration-based Learning from a Causal Perspective",
        "authors": [
            "Ruiyi Zhang",
            "Tong Yu"
        ],
        "published": "2023",
        "summary": "Demonstration-based learning has shown impressive performance in exploiting pretrained language models under few-shot learning settings. It is interesting to see that demonstrations, even those composed of random tokens, can still improve performance. In this paper, we build a Structural Causal Model (SCM) to understand demonstration-based learning from causal perspectives and interpret random demonstrations as interventions on the demonstration variable within the causal model. We investigate the causal effects and find that the concurrence of specific words in the demonstration will induce bias, while randomly sampled tokens in the demonstration do not. Based on this finding, we further propose simple ways to construct random demonstrations, which even outperform hand-crafted, meaningful demonstrations on public sequence labeling benchmarks.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.125.pdf",
        "keywords": [
            "demonstration based learning",
            "causal model"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the concurrence of specific words in the demonstration will induce bias\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the concurrence of specific words in the demonstration will induce bias\""
    },
    {
        "title": "RAMP: Retrieval and Attribute-Marking Enhanced Prompting for Attribute-Controlled Translation",
        "authors": [
            "Gabriele Sarti",
            "Phu Mon Htut",
            "Xing Niu",
            "Benjamin Hsu",
            "Anna Currey",
            "Georgiana Dinu",
            "Maria Nadejde"
        ],
        "published": "2023",
        "summary": "Attribute-controlled translation (ACT) is a subtask of machine translation that involves controlling stylistic or linguistic attributes (like formality and gender) of translation outputs. While ACT has garnered attention in recent years due to its usefulness in real-world applications, progress in the task is currently limited by dataset availability, since most prior approaches rely on supervised methods. To address this limitation, we propose Retrieval and Attribute-Marking enhanced Prompting (RAMP), which leverages large multilingual language models to perform ACT in few-shot and zero-shot settings. RAMP improves generation accuracy over the standard prompting approach by (1) incorporating a semantic similarity retrieval component for selecting similar in-context examples, and (2) marking in-context examples with attribute annotations. Our comprehensive experiments show that RAMP is a viable approach in both zero-shot and few-shot settings.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.126.pdf",
        "keywords": [
            "attribute marking",
            "attribute controlled",
            "prompting",
            "attribute controlled translation",
            "act"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"progress in the task is currently limited by dataset availability, since most prior approaches rely on supervised methods.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"progress in the task is currently limited by dataset availability, since most prior approaches rely on supervised methods.\""
    },
    {
        "title": "Discourse-Level Representations can Improve Prediction of Degree of Anxiety",
        "authors": [
            "Swanie Juhng",
            "Matthew Matero",
            "Vasudha Varadarajan",
            "Johannes Eichstaedt",
            "Adithya V Ganesan",
            "H. Andrew Schwartz"
        ],
        "published": "2023",
        "summary": "Anxiety disorders are the most common of mental illnesses, but relatively little is known about how to detect them from language. The primary clinical manifestation of anxiety is worry associated cognitive distortions, which are likely expressed at the discourse-level of semantics. Here, we investigate the development of a modern linguistic assessment for degree of anxiety, specifically evaluating the utility of discourse-level information in addition to lexical-level large language model embeddings. We find that a combined lexico-discourse model outperforms models based solely on state-of-the-art contextual embeddings (RoBERTa), with discourse-level representations derived from Sentence-BERT and DiscRE both providing additional predictive power not captured by lexical-level representations. Interpreting the model, we find that discourse patterns of causal explanations, among others, were used significantly more by those scoring high in anxiety, dovetailing with psychological literature.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.128.pdf",
        "keywords": [
            "anxiety",
            "language",
            "degree",
            "prediction",
            "discourse",
            "discourse model",
            "worry",
            "semantics",
            "representations",
            "linguistic"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that a combined lexico-discourse model outperforms models based solely on state-of-the-art contextual embeddings (RoBERTa), with discourse-level representations derived from Sentence-BERT and DiscRE both providing additional predictive power not captured by lexical-level representations.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (in this case, the limitation of lexical-level",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We find that a combined lexico-discourse model outperforms models based solely on state-of-the-art contextual embeddings (RoBERTa), with discourse-level representations derived from Sentence-BERT and DiscRE both providing additional predictive power not captured by lexical-level representations.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (in this case, the limitation of lexical-level"
    },
    {
        "title": "Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning",
        "authors": [
            "Mustafa Ozdayi",
            "Charith Peris",
            "Jack FitzGerald",
            "Christophe Dupuy",
            "Jimit Majmudar",
            "Haidar Khan",
            "Rahil Parikh",
            "Rahul Gupta"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) are known to memorize significant portions of their training data. Parts of this memorized content have been shown to be extractable by simply querying the model, which poses a privacy risk. We present a novel approach which uses prompt-tuning to control the extraction rates of memorized content in LLMs. We present two prompt training strategies to increase and decrease extraction rates, which correspond to an attack and a defense, respectively. We demonstrate the effectiveness of our techniques by using models from the GPT-Neo family on a public benchmark. For the 1.3B parameter GPT-Neo model, our attack yields a 9.3 percentage point increase in extraction rate compared to our baseline. Our defense can be tuned to achieve different privacy-utility trade-offs by a user-specified hyperparameter. We achieve an extraction rate reduction of up to 97.7% relative to our baseline, with a perplexity increase of 16.9%.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.129.pdf",
        "keywords": [
            "memorize",
            "language models",
            "perplexity",
            "large language models",
            "controlling"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Parts of this memorized content have been shown to be extractable by simply querying the model, which poses a privacy risk.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Parts of this memorized content have been shown to be extractable by simply querying the model, which poses a privacy risk.\""
    },
    {
        "title": "MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting",
        "authors": [
            "Tatsuro Inaba",
            "Hirokazu Kiyomaru",
            "Fei Cheng",
            "Sadao Kurohashi"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have achieved impressive performance on various reasoning tasks. To further improve the performance, we propose MultiTool-CoT, a novel framework that leverages chain-of-thought (CoT) prompting to incorporate multiple external tools, such as a calculator and a knowledge retriever, during the reasoning process. We apply MultiTool-CoT to the Task 2 dataset of NumGLUE, which requires both numerical reasoning and domain-specific knowledge. The experiments show that our method significantly outperforms strong baselines and achieves state-of-the-art performance.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.130.pdf",
        "keywords": [
            "thought",
            "multitool cot",
            "chain of thought prompting",
            "numerical reasoning",
            "chain",
            "multiple external tools",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"To further improve the performance\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"To further improve the performance\""
    },
    {
        "title": "mPMR: A Multilingual Pre-trained Machine Reader at Scale",
        "authors": [
            "Weiwen Xu",
            "Xin Li",
            "Wai Lam",
            "Lidong Bing"
        ],
        "published": "2023",
        "summary": "We present multilingual Pre-trained Machine Reader (mPMR), a novel method for multilingual machine reading comprehension (MRC)-style pre-training. mPMR aims to guide multilingual pre-trained language models (mPLMs) to perform natural language understanding (NLU) including both sequence classification and span extraction in multiple languages. To achieve cross-lingual generalization when only source-language fine-tuning data is available, existing mPLMs solely transfer NLU capability from a source language to target languages. In contrast, mPMR allows the direct inheritance of multilingual NLU capability from the MRC-style pre-training to downstream tasks. Therefore, mPMR acquires better NLU capability for target languages. mPMR also provides a unified solver for tackling cross-lingual span extraction and sequence classification, thereby enabling the extraction of rationales to explain the sentence-pair classification process.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.131.pdf",
        "keywords": [
            "natural language understanding",
            "machine reading comprehension",
            "pre trained machine reader"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"To achieve cross-lingual generalization when only source-language fine-tuning data is available, existing mPLMs solely transfer NLU capability from a source language to target languages.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"To achieve cross-lingual generalization when only source-language fine-tuning data is available, existing mPLMs solely transfer NLU capability from a source language to target languages.\""
    },
    {
        "title": "LI-RAGE: Late Interaction Retrieval Augmented Generation with Explicit Signals for Open-Domain Table Question Answering",
        "authors": [
            "Weizhe Lin",
            "Rexhina Blloshmi",
            "Bill Byrne",
            "Adria de Gispert",
            "Gonzalo Iglesias"
        ],
        "published": "2023",
        "summary": "Recent open-domain TableQA models are typically implemented as retriever-reader pipelines. The retriever component is usually a variant of the Dense Passage Retriever, which computes the similarities between questions and tables based on a single representation of each. These fixed vectors can be insufficient to capture fine-grained features of potentially very big tables with heterogeneous row/column information. We address this limitation by 1) applying late interaction models which enforce a finer-grained interaction between question and table embeddings at retrieval time. In addition, we 2) incorporate a joint training scheme of the retriever and reader with explicit table-level signals, and 3) embed a binary relevance token as a prefix to the answer generated by the reader, so we can determine at inference time whether the table used to answer the question is reliable and filter accordingly. The combined strategies set a new state-to-the-art performance on two public open-domain TableQA datasets.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.133.pdf",
        "keywords": [
            "interaction",
            "table question answering",
            "late interaction models",
            "retriever",
            "augmented generation"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.\n\nNote: Although the paper discusses limitations of retriever models, it does not specifically talk about Large Language Models (LLMs).",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs.\n\nNote: Although the paper discusses limitations of retriever models, it does not specifically talk about Large Language Models (LLMs)."
    },
    {
        "title": "MolXPT: Wrapping Molecules with Text for Generative Pre-training",
        "authors": [
            "Zequn Liu",
            "Wei Zhang",
            "Yingce Xia",
            "Lijun Wu",
            "Shufang Xie",
            "Tao Qin",
            "Ming Zhang",
            "Tie-Yan Liu"
        ],
        "published": "2023",
        "summary": "Generative pre-trained Transformer (GPT) has demonstrates its great success in natural language processing and related techniques have been adapted into molecular modeling. Considering that text is the most important record for scientific discovery, in this paper, we propose MolXPT, a unified language model of text and molecules pre-trained on SMILES (a sequence representation of molecules) wrapped by text. Briefly, we detect the molecule names in each sequence and replace them to the corresponding SMILES. In this way, the SMILES could leverage the information from surrounding text, and vice versa. The above wrapped sequences, text sequences from PubMed and SMILES sequences from PubChem are all fed into a language model for pre-training. Experimental results demonstrate that MolXPT outperforms strong baselines of molecular property prediction on MoleculeNet, performs comparably to the best model in text-molecule translation while using less than half of its parameters, and enables zero-shot molecular generation without finetuning.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.138.pdf",
        "keywords": [
            "text",
            "molxpt"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation of LLMs is mentioned in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation of LLMs is mentioned in the abstract."
    },
    {
        "title": "NarrowBERT: Accelerating Masked Language Model Pretraining and Inference",
        "authors": [
            "Haoxin Li",
            "Phillip Keung",
            "Daniel Cheng",
            "Jungo Kasai",
            "Noah A. Smith"
        ],
        "published": "2023",
        "summary": "Large-scale language model pretraining is a very successful form of self-supervised learning in natural language processing, but it is increasingly expensive to perform as the models and pretraining corpora have become larger over time. We propose NarrowBERT, a modified transformer encoder that increases the throughput for masked language model pretraining by more than 2x. NarrowBERT sparsifies the transformer model such that the self-attention queries and feedforward layers only operate on the masked tokens of each sentence during pretraining, rather than all of the tokens as with the usual transformer encoder. We also show that NarrowBERT increases the throughput at inference time by as much as 3.5x with minimal (or no) performance degradation on sentence encoding tasks like MNLI. Finally, we examine the performance of NarrowBERT on the IMDB and Amazon reviews classification and CoNLL NER tasks and show that it is also comparable to standard BERT performance.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.146.pdf",
        "keywords": [
            "pretraining",
            "masked",
            "feedforward",
            "masked language model pretraining",
            "inference"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large-scale language model pretraining is a very successful form of self-supervised learning in natural language processing, but it is increasingly expensive to perform as the models and pretraining corpora have become larger over time.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Large-scale language model pretraining is a very successful form of self-supervised learning in natural language processing, but it is increasingly expensive to perform as the models and pretraining corpora have become larger over time.\""
    },
    {
        "title": "S3HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid Question Answering",
        "authors": [
            "Fangyu Lei",
            "Xiang Li",
            "Yifan Wei",
            "Shizhu He",
            "Yiming Huang",
            "Jun Zhao",
            "Kang Liu"
        ],
        "published": "2023",
        "summary": "Answering multi-hop questions over hybrid factual knowledge from the given text and table (TextTableQA) is a challenging task. Existing models mainly adopt a retriever-reader framework, which have several deficiencies, such as noisy labeling in training retriever, insufficient utilization of heterogeneous information over text and table, and deficient ability for different reasoning operations. In this paper, we propose a three-stage TextTableQA framework S3HQA, which comprises of retriever, selector, and reasoner. We use a retriever with refinement training to solve the noisy labeling problem. Then, a hybrid selector considers the linked relationships between heterogeneous data to select the most relevant factual knowledge. For the final stage, instead of adapting a reading comprehension module like in previous methods, we employ a generation-based reasoner to obtain answers. This includes two approaches: a row-wise generator and an LLM prompting generator (first time used in this task). The experimental results demonstrate that our method achieves competitive results in the few-shot setting. When trained on the full dataset, our approach outperforms all baseline methods, ranking first on the HybridQA leaderboard.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.147.pdf",
        "keywords": [
            "retriever",
            "hybrid question answering"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"an LLM prompting generator (first time used in this task)\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"an LLM prompting generator (first time used in this task)\""
    },
    {
        "title": "Towards Fewer Hallucinations in Knowledge-Grounded Dialogue Generation via Augmentative and Contrastive Knowledge-Dialogue",
        "authors": [
            "Bin Sun",
            "Yitong Li",
            "Fei Mi",
            "Fanhu Bie",
            "Yiwei Li",
            "Kan Li"
        ],
        "published": "2023",
        "summary": "Existing knowledge-grounded open-domain dialogue generation models often face the hallucination problem, i.e. the dialogue generative model will persist in an inappropriate knowledge and generate responses that inconsistent with the facts. We argue that this problem mainly stems from the polarized optimization objectives and weak knowledge generation ability. To mitigate the hallucination, we take inspiration from human communicating that people will replay euphemistic responses for the unclear or unrecognizable knowledge, and propose an Augmentative and Contrastive Knowledge Dialogue Expansion Framework (ACK-DEF). ACK-DEF constructs the augmentative and contrastive knowledge dialogue samples, which consist of the knowledge of different degrees of errors and the response of manual design, to expand the original training set and smooth the polarized optimization objective that enables models to generate ground-truth with or without gold knowledge. Not only the knowledge, ACK-DEF also provides the tactful responses of manual design corresponding to the incomplete correct knowledge. Experimental results on the Wikipedia of Wizard dataset show that employing the ACK-DEF is effective to alleviate the hallucination problem.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.148.pdf",
        "keywords": [
            "hallucination",
            "wikipedia",
            "polarized",
            "knowledge dialogue",
            "replay",
            "knowledge grounded dialogue"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing knowledge-grounded open-domain dialogue generation models often face the hallucination problem, i.e. the dialogue generative model will persist in an inappropriate knowledge and generate responses that inconsistent with the facts.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Existing knowledge-grounded open-domain dialogue generation models often face the hallucination problem, i.e. the dialogue generative model will persist in an inappropriate knowledge and generate responses that inconsistent with the facts.\""
    },
    {
        "title": "AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models",
        "authors": [
            "Siheng Li",
            "Cheng Yang",
            "Yichun Yin",
            "Xinyu Zhu",
            "Zesen Cheng",
            "Lifeng Shang",
            "Xin Jiang",
            "Qun Liu",
            "Yujiu Yang"
        ],
        "published": "2023",
        "summary": "Information-seeking conversation, which aims to help users gather information through conversation, has achieved great progress in recent years. However, the research is still stymied by the scarcity of training data. To alleviate this problem, we propose AutoConv for synthetic conversation generation, which takes advantage of the few-shot learning ability and generation capacity of large language models (LLM). Specifically, we formulate the conversation generation problem as a language modeling task, then finetune an LLM with a few human conversations to capture the characteristics of the information-seeking process and use it for generating synthetic conversations with high quality. Experimental results on two frequently-used datasets verify that AutoConv has substantial improvements over strong baselines and alleviates the dependence on human annotation. In addition, we also provide several analysis studies to promote future research.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.149.pdf",
        "keywords": [
            "conversation",
            "synthetic conversations",
            "language models",
            "modeling",
            "autoconv"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation of LLMs is mentioned in the abstract, but the context implies that one limitation is the need for large amounts of training data, which the paper aims to alleviate.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation of LLMs is mentioned in the abstract, but the context implies that one limitation is the need for large amounts of training data, which the paper aims to alleviate."
    },
    {
        "title": "Teaching Small Language Models to Reason",
        "authors": [
            "Lucie Charlotte Magister",
            "Jonathan Mallinson",
            "Jakub Adamek",
            "Eric Malmi",
            "Aliaksei Severyn"
        ],
        "published": "2023",
        "summary": "Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with at least tens of billions of parameters. In this paper, we explore the transfer of such reasoning capabilities to smaller models via knowledge distillation, also investigating model and dataset size trade-off. Specifically, we finetune a student model on the chain of thought outputs generated by a larger teacher model. Our experiments show that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets. For example, the accuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% and 18.42% when finetuned on PaLM 540B and GPT-3 175B generated chains of thought, respectively.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.151.pdf",
        "keywords": [
            "knowledge distillation",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these reasoning capabilities only appear to emerge in models with at least tens of billions of parameters.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these reasoning capabilities only appear to emerge in models with at least tens of billions of parameters.\""
    },
    {
        "title": "A Simple and Effective Framework for Strict Zero-Shot Hierarchical Classification",
        "authors": [
            "Rohan Bhambhoria",
            "Lei Chen",
            "Xiaodan Zhu"
        ],
        "published": "2023",
        "summary": "In recent years, large language models (LLMs) have achieved strong performance on benchmark tasks, especially in zero or few-shot settings. However, these benchmarks often do not adequately address the challenges posed in the real-world, such as that of hierarchical classification. In order to address this challenge, we propose refactoring conventional tasks on hierarchical datasets into a more indicative long-tail prediction task. We observe LLMs are more prone to failure in these cases. To address these limitations, we propose the use of entailment-contradiction prediction in conjunction with LLMs, which allows for strong performance in a strict zero-shot setting. Importantly, our method does not require any parameter updates, a resource-intensive process and achieves strong performance across multiple datasets.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.152.pdf",
        "keywords": [
            "refactoring"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these benchmarks often do not adequately address the challenges posed in the real-world, such as that of hierarchical classification. We observe LLMs are more prone to failure in these cases.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, these benchmarks often do not adequately address the challenges posed in the real-world, such as that of hierarchical classification. We observe LLMs are more prone to failure in these cases.\""
    },
    {
        "title": "ScoNe: Benchmarking Negation Reasoning in Language Models With Fine-Tuning and In-Context Learning",
        "authors": [
            "Jingyuan S. She",
            "Christopher Potts",
            "Samuel R. Bowman",
            "Atticus Geiger"
        ],
        "published": "2023",
        "summary": "A number of recent benchmarks seek to assess how well models handle natural language negation. However, these benchmarks lack the controlled example paradigms that would allow us to infer whether a model had truly learned how negation morphemes semantically scope. To fill these analytical gaps, we present the Scoped Negation NLI (ScoNe-NLI) benchmark, which contains contrast sets of six examples with up to two negations where either zero, one, or both negative morphemes affect the NLI label. We use ScoNe-NLI to assess fine-tuning and in-context learning strategies. We find that RoBERTa and DeBERTa models solve ScoNe-NLI after many shot fine-tuning. For in-context learning, we test the latest InstructGPT models and find that most prompt strategies are not successful, including those using step-by-step reasoning. To better understand this result, we extend ScoNe with ScoNe-NLG, a sentence completion test set that embeds negation reasoning in short narratives. Here, InstructGPT is successful, which reveals the model can correctly reason about negation, but struggles to do so on NLI examples outside of its core pretraining regime.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.154.pdf",
        "keywords": [
            "negation",
            "negation reasoning",
            "language negation",
            "scoped negation",
            "language models",
            "benchmarking negation reasoning",
            "benchmarks",
            "fine tuning"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"For in-context learning, we test the latest InstructGPT models and find that most prompt strategies are not successful, including those using step-by-step reasoning... which reveals the model can correctly reason about negation, but struggles to do so on NLI examples outside of its core pretraining regime.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"For in-context learning, we test the latest InstructGPT models and find that most prompt strategies are not successful, including those using step-by-step reasoning... which reveals the model can correctly reason about negation, but struggles to do so on NLI examples outside of its core pretraining regime.\""
    },
    {
        "title": "Revisiting Automated Prompting: Are We Actually Doing Better?",
        "authors": [
            "Yulin Zhou",
            "Yiren Zhao",
            "Ilia Shumailov",
            "Robert Mullins",
            "Yarin Gal"
        ],
        "published": "2023",
        "summary": "Current literature demonstrates that Large Language Models (LLMs) are great few-shot learners, and prompting significantly increases their performance on a range of downstream tasks in a few-shot learning setting. An attempt to automate human-led prompting followed, with some progress achieved. In particular, subsequent work demonstrates that automation can outperform fine-tuning in certain K-shot learning scenarios. In this paper, we revisit techniques for automated prompting on six different downstream tasks and a larger range of K-shot learning settings. We find that automated prompting does not consistently outperform simple manual prompting. Our work suggests that, in addition to fine-tuning, manual prompting should be used as a baseline in this line of research.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.155.pdf",
        "keywords": [
            "prompting",
            "automated prompting",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that automated prompting does not consistently outperform simple manual prompting.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We find that automated prompting does not consistently outperform simple manual prompting.\""
    },
    {
        "title": "Linear Classifier: An Often-Forgotten Baseline for Text Classification",
        "authors": [
            "Yu-Chen Lin",
            "Si-An Chen",
            "Jie-Jyun Liu",
            "Chih-Jen Lin"
        ],
        "published": "2023",
        "summary": "Large-scale pre-trained language models such as BERT are popular solutions for text classification. Due to the superior performance of these advanced methods, nowadays, people often directly train them for a few epochs and deploy the obtained model. In this opinion paper, we point out that this way may only sometimes get satisfactory results. We argue the importance of running a simple baseline like linear classifiers on bag-of-words features along with advanced methods. First, for many text data, linear methods show competitive performance, high efficiency, and robustness. Second, advanced models such as BERT may only achieve the best results if properly applied. Simple baselines help to confirm whether the results of advanced models are acceptable. Our experimental results fully support these points.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.160.pdf",
        "keywords": [
            "text classification",
            "linear classifier"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We argue the importance of running a simple baseline like linear classifiers on bag-of-words features along with advanced methods... Second, advanced models such as BERT may only achieve the best results if properly applied.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We argue the importance of running a simple baseline like linear classifiers on bag-of-words features along with advanced methods... Second, advanced models such as BERT may only achieve the best results if properly applied.\""
    },
    {
        "title": "Table and Image Generation for Investigating Knowledge of Entities in Pre-trained Vision and Language Models",
        "authors": [
            "Hidetaka Kamigaito",
            "Katsuhiko Hayashi",
            "Taro Watanabe"
        ],
        "published": "2023",
        "summary": "In this paper, we propose a table and image generation task to verify how the knowledge about entities acquired from natural language is retained in Vision & Language (V & L) models. This task consists of two parts: the first is to generate a table containing knowledge about an entity and its related image, and the second is to generate an image from an entity with a caption and a table containing related knowledge of the entity. In both tasks, the model must know the entities used to perform the generation properly. We created the Wikipedia Table and Image Generation (WikiTIG) dataset from about 200,000 infoboxes in English Wikipedia articles to perform the proposed tasks. We evaluated the performance on the tasks with respect to the above research question using the V & L model OFA, which has achieved state-of-the-art results in multiple tasks. Experimental results show that OFA forgets part of its entity knowledge by pre-training as a complement to improve the performance of image related tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.162.pdf",
        "keywords": [
            "image generation",
            "language models",
            "pre trained vision",
            "wikipedia"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Experimental results show that OFA forgets part of its entity knowledge by pre-training as a complement to improve the performance of image related tasks.\"\n\nThis rating is given because the paper mentions a limitation of the model (forgetting part of its entity knowledge) but does not elaborate on it in detail, and the primary focus of the paper is on the proposed task and the experimental results.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Experimental results show that OFA forgets part of its entity knowledge by pre-training as a complement to improve the performance of image related tasks.\"\n\nThis rating is given because the paper mentions a limitation of the model (forgetting part of its entity knowledge) but does not elaborate on it in detail, and the primary focus of the paper is on the proposed task and the experimental results."
    },
    {
        "title": "Investigating Glyph-Phonetic Information for Chinese Spell Checking: What Works and What’s Next?",
        "authors": [
            "Xiaotian Zhang",
            "Yanjun Zheng",
            "Hang Yan",
            "Xipeng Qiu"
        ],
        "published": "2023",
        "summary": "While pre-trained Chinese language models have demonstrated impressive performance on a wide range of NLP tasks, the Chinese Spell Checking (CSC) task remains a challenge. Previous research has explored using information such as glyphs and phonetics to improve the ability of CSC models to distinguish misspelled characters, with good results at the accuracy level on public datasets. However, the generalization ability of these CSC models has not been well understood: it is unclear whether they incorporate glyph-phonetic information and, if so, whether this information is fully utilized. In this paper, we aim to better understand the role of glyph-phonetic information in the CSC task and suggest directions for improvement. Additionally, we propose a new, more challenging, and practical setting for testing the generalizability of CSC models. All code is made publicly available.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.1.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While pre-trained Chinese language models have demonstrated impressive performance on a wide range of NLP tasks, the Chinese Spell Checking (CSC) task remains a challenge.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While pre-trained Chinese language models have demonstrated impressive performance on a wide range of NLP tasks, the Chinese Spell Checking (CSC) task remains a challenge.\""
    },
    {
        "title": "A Self-Supervised Integration Method of Pretrained Language Models and Word Definitions",
        "authors": [
            "Hwiyeol Jo"
        ],
        "published": "2023",
        "summary": "We investigate the representation of pretrained language models and humans, using the idea of word definition modeling–how well a word is represented by its definition, and vice versa. Our analysis shows that a word representation in pretrained language models does not successfully map its human-written definition and its usage in example sentences. We then present a simple method DefBERT that integrates pretrained models with word semantics in dictionaries. We show its benefits on newly-proposed tasks of definition ranking and definition sense disambiguation. Furthermore, we present the results on standard word similarity tasks and short text classification tasks where models are required to encode semantics with only a few words. The results demonstrate the effectiveness of integrating word definitions and pretrained language models.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.2.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our analysis shows that a word representation in pretrained language models does not successfully map its human-written definition and its usage in example sentences.\"\n\nThis evidence suggests that the paper mentions a limitation of LLMs, specifically that they do not successfully map word representations to human-written definitions and usage in example sentences, but it does not elaborate on this limitation in detail and focuses more on the proposed solution",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Our analysis shows that a word representation in pretrained language models does not successfully map its human-written definition and its usage in example sentences.\"\n\nThis evidence suggests that the paper mentions a limitation of LLMs, specifically that they do not successfully map word representations to human-written definitions and usage in example sentences, but it does not elaborate on this limitation in detail and focuses more on the proposed solution"
    },
    {
        "title": "Conformal Nucleus Sampling",
        "authors": [
            "Shauli Ravfogel",
            "Yoav Goldberg",
            "Jacob Goldberger"
        ],
        "published": "2023",
        "summary": "Language models generate text based on successively sampling the next word. A decoding procedure based on nucleus (top-p) sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p. In this work, we assess whether a top-p set is indeed aligned with its probabilistic meaning in various linguistic contexts.We employ conformal prediction, a calibration procedure that focuses on the construction of minimal prediction sets according to a desired confidence level, to calibrate the parameter p as a function of the entropy of the next word distribution. We find that OPT models are overconfident, and that calibration shows a moderate inverse scaling with model size.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.3.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that OPT models are overconfident, and that calibration shows a moderate inverse scaling with model size.\"\n\nThis rating is given because the paper briefly mentions a limitation (overconfidence) of LLMs, but it is not the primary focus of the paper, which is on the calibration procedure.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We find that OPT models are overconfident, and that calibration shows a moderate inverse scaling with model size.\"\n\nThis rating is given because the paper briefly mentions a limitation (overconfidence) of LLMs, but it is not the primary focus of the paper, which is on the calibration procedure."
    },
    {
        "title": "DiscoPrompt: Path Prediction Prompt Tuning for Implicit Discourse Relation Recognition",
        "authors": [
            "Chunkit Chan",
            "Xin Liu",
            "Jiayang Cheng",
            "Zihan Li",
            "Yangqiu Song",
            "Ginny Wong",
            "Simon See"
        ],
        "published": "2023",
        "summary": "Implicit Discourse Relation Recognition (IDRR) is a sophisticated and challenging task to recognize the discourse relations between the arguments with the absence of discourse connectives. The sense labels for each discourse relation follow a hierarchical classification scheme in the annotation process (Prasad et al., 2008), forming a hierarchy structure. Most existing works do not well incorporate the hierarchy structure but focus on the syntax features and the prior knowledge of connectives in the manner of pure text classification. We argue that it is more effective to predict the paths inside the hierarchical tree (e.g., “Comparison -> Contrast -> however”) rather than flat labels (e.g., Contrast) or connectives (e.g., however). We propose a prompt-based path prediction method to utilize the interactive information and intrinsic senses among the hierarchy in IDRR. This is the first work that injects such structure information into pre-trained language models via prompt tuning, and the performance of our solution shows significant and consistent improvement against competitive baselines.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.4.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Most existing works do not well incorporate the hierarchy structure but focus on the syntax features and the prior knowledge of connectives in the manner of pure text classification.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Most existing works do not well incorporate the hierarchy structure but focus on the syntax features and the prior knowledge of connectives in the manner of pure text classification.\""
    },
    {
        "title": "Abstractive Text Summarization Using the BRIO Training Paradigm",
        "authors": [
            "Khang Lam",
            "Thieu Doan",
            "Khang Pham",
            "Jugal Kalita"
        ],
        "published": "2023",
        "summary": "Summary sentences produced by abstractive summarization models may be coherent and comprehensive, but they lack control and rely heavily on reference summaries. The BRIO training paradigm assumes a non-deterministic distribution to reduce the model’s dependence on reference summaries, and improve model performance during inference. This paper presents a straightforward but effective technique to improve abstractive summaries by fine-tuning pre-trained language models, and training them with the BRIO paradigm. We build a text summarization dataset for Vietnamese, called VieSum. We perform experiments with abstractive summarization models trained with the BRIO paradigm on the CNNDM and the VieSum datasets. The results show that the models, trained on basic hardware, outperform all existing abstractive summarization models, especially for Vietnamese.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.7.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but \"pre-trained language models\" implies LLMs, and the lack of control and dependence on reference summaries can be seen as a limitation, although it's not explicitly stated as a limitation of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but \"pre-trained language models\" implies LLMs, and the lack of control and dependence on reference summaries can be seen as a limitation, although it's not explicitly stated as a limitation of LLMs."
    },
    {
        "title": "Pre-training Language Model as a Multi-perspective Course Learner",
        "authors": [
            "Beiduo Chen",
            "Shaohan Huang",
            "Zihan Zhang",
            "Wu Guo",
            "Zhenhua Ling",
            "Haizhen Huang",
            "Furu Wei",
            "Weiwei Deng",
            "Qi Zhang"
        ],
        "published": "2023",
        "summary": "ELECTRA, the generator-discriminator pre-training framework, has achieved impressive semantic construction capability among various downstream tasks. Despite the convincing performance, ELECTRA still faces the challenges of monotonous training and deficient interaction. Generator with only masked language modeling (MLM) leads to biased learning and label imbalance for discriminator, decreasing learning efficiency; no explicit feedback loop from discriminator to generator results in the chasm between these two components, underutilizing the course learning. In this study, a multi-perspective course learning (MCL) method is proposed to fetch a many degrees and visual angles for sample-efficient pre-training, and to fully leverage the relationship between generator and discriminator. Concretely, three self-supervision courses are designed to alleviate inherent flaws of MLM and balance the label in a multi-perspective way. Besides, two self-correction courses are proposed to bridge the chasm between the two encoders by creating a “correction notebook” for secondary-supervision. Moreover, a course soups trial is conducted to solve the “tug-of-war” dynamics problem of MCL, evolving a stronger pre-trained model. Experimental results show that our method significantly improves ELECTRA’s average performance by 2.8% and 3.2% absolute points respectively on GLUE and SQuAD 2.0 benchmarks, and overshadows recent advanced ELECTRA-style models under the same settings. The pre-trained MCL model is available at https://huggingface.co/McmanusChen/MCL-base.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.9.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Generator with only masked language modeling (MLM) leads to biased learning and label imbalance for discriminator, decreasing learning efficiency; no explicit feedback loop from discriminator to generator results in the chasm between these two components, underutilizing the course learning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Generator with only masked language modeling (MLM) leads to biased learning and label imbalance for discriminator, decreasing learning efficiency; no explicit feedback loop from discriminator to generator results in the chasm between these two components, underutilizing the course learning.\""
    },
    {
        "title": "Layerwise universal adversarial attack on NLP models",
        "authors": [
            "Olga Tsymboi",
            "Danil Malaev",
            "Andrei Petrovskii",
            "Ivan Oseledets"
        ],
        "published": "2023",
        "summary": "In this work, we examine the vulnerability of language models to universal adversarial triggers (UATs). We propose a new white-box approach to the construction of layerwise UATs (LUATs), which searches the triggers by perturbing hidden layers of a network. On the example of three transformer models and three datasets from the GLUE benchmark, we demonstrate that our method provides better transferability in a model-to-model setting with an average gain of 9.3% in the fooling rate over the baseline. Moreover, we investigate triggers transferability in the task-to-task setting. Using small subsets from the datasets similar to the target tasks for choosing a perturbed layer, we show that LUATs are more efficient than vanilla UATs by 7.1% in the fooling rate.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.10.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We examine the vulnerability of language models to universal adversarial triggers (UATs).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We examine the vulnerability of language models to universal adversarial triggers (UATs).\""
    },
    {
        "title": "Boost Transformer-based Language Models with GPU-Friendly Sparsity and Quantization",
        "authors": [
            "Chong Yu",
            "Tao Chen",
            "Zhongxue Gan"
        ],
        "published": "2023",
        "summary": "Along with the performance improvement in NLP domain, the sizes of transformer-based language models (TLM) are also dramatically increased. Some prior works intend to compress TLM models into more compact forms, but do not fully consider the hardware characters may not support the efficient execution for these forms, leading to the deployment of TLM on hardware with noticeable acceleration is still challenging. This paper thoroughly designs a compression scheme named GPUSQ-TLM to maximally utilize the GPU-friendly 2:4 fine-grained structured sparsity and quantization characters. Especially, a dense TLM model is first pruned to meet the GPU’s acceleration constraint of sparse patterns with FP16 type, then it is further quantized into a fixed-point one by quantization-aware training, to provide an extra speedup for integer tensors on GPU. A mixed-strategy knowledge distillation of labels, logits and feature maps is used for best accuracy compensation during pruning and quantization process. Experiment results show GPUSQ-TLM scheme achieves state-of-the-art compression on TLM model of various encoder and decoder blocks with negligible accuracy degradation on SQuAD, GLUE, CNN-DM & XSum and WikiText benchmarking tasks. Moreover, GPUSQ-TLM can boost actual deployment performance by up to 4.08-4.25x latency and 6.18-6.79x throughput on A100 GPU.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.15.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"but do not fully consider the hardware characters may not support the efficient execution for these forms, leading to the deployment of TLM on hardware with noticeable acceleration is still challenging.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"but do not fully consider the hardware characters may not support the efficient execution for these forms, leading to the deployment of TLM on hardware with noticeable acceleration is still challenging.\""
    },
    {
        "title": "Zero-Shot Prompting for Implicit Intent Prediction and Recommendation with Commonsense Reasoning",
        "authors": [
            "Hui-Chi Kuo",
            "Yun-Nung Chen"
        ],
        "published": "2023",
        "summary": "The current generation of intelligent assistants require explicit user requests to perform tasks or services, often leading to lengthy and complex conversations. In contrast, human assistants can infer multiple implicit intents from utterances via their commonsense knowledge, thereby simplifying interactions. To bridge this gap, this paper proposes a framework for multi-domain dialogue systems. This framework automatically infers implicit intents from user utterances, and prompts a large pre-trained language model to suggest suitable task-oriented bots. By leveraging commonsense knowledge, our framework recommends associated bots in a zero-shot manner, enhancing interaction efficiency and effectiveness. This approach substantially reduces interaction complexity, seamlessly integrates various domains and tasks, and represents a significant step towards creating more human-like intelligent assistants that can reason about implicit intents, offering a superior user experience.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.17.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"prompts a large pre-trained language model\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"prompts a large pre-trained language model\""
    },
    {
        "title": "MTGP: Multi-turn Target-oriented Dialogue Guided by Generative Global Path with Flexible Turns",
        "authors": [
            "Anqi Liu",
            "Bo Wang",
            "Yue Tan",
            "Dongming Zhao",
            "Kun Huang",
            "Ruifang He",
            "Yuexian Hou"
        ],
        "published": "2023",
        "summary": "Target-oriented dialogue guides the dialogue to a target quickly and smoothly. The latest approaches focus on global planning, which plans toward the target before the conversation instead of adopting a greedy strategy during the conversation. However, the global plan in existing works is fixed to certain turns by generating paths with certain nodes, which limits the optimization of turns and coherence of the target-oriented process. Toward flexible global planning, we propose to generate a global path as a natural language sentence instead of a sequence of nodes. With this path, the dialog is guided to the target with flexible turns of dialog. For model training, we also extract targetoriented dialogues from the chit-chat corpus with a knowledge graph. We conduct experiments on three datasets and simulate scenarios with and without user participation. The results show that our method has fewer turns, more coherent semantics, and a higher success rate in reaching the target than baselines.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.18.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "The Larger they are, the Harder they Fail: Language Models do not Recognize Identifier Swaps in Python",
        "authors": [
            "Antonio Valerio Miceli Barone",
            "Fazl Barez",
            "Shay B. Cohen",
            "Ioannis Konstas"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have successfully been applied to code generation tasks, raising the question of how well these models understand programming. Typical programming languages have invariances and equivariances in their semantics that human programmers intuitively understand and exploit, such as the (near) invariance to the renaming of identifiers. We show that LLMs not only fail to properly generate correct Python code when default function names are swapped, but some of them even become more confident in their incorrect predictions as the model size increases, an instance of the recently discovered phenomenon of Inverse Scaling, which runs contrary to the commonly observed trend of increasing prediction quality with increasing model size. Our findings indicate that, despite their astonishing typical-case performance, LLMs still lack a deep, abstract understanding of the content they manipulate, making them unsuitable for tasks that statistically deviate from their training data, and that mere scaling is not enough to achieve such capability.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.19.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "5",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We show that LLMs not only fail to properly generate correct Python code when default function names are swapped, but some of them even become more confident in their incorrect predictions as the model size increases... Our findings indicate that, despite their astonishing typical-case performance, LLMs still lack a deep, abstract understanding of the content they manipulate, making them unsuitable for tasks that statistically",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 5\nEvidence: \"We show that LLMs not only fail to properly generate correct Python code when default function names are swapped, but some of them even become more confident in their incorrect predictions as the model size increases... Our findings indicate that, despite their astonishing typical-case performance, LLMs still lack a deep, abstract understanding of the content they manipulate, making them unsuitable for tasks that statistically"
    },
    {
        "title": "On Evaluating and Mitigating Gender Biases in Multilingual Settings",
        "authors": [
            "Aniket Vashishtha",
            "Kabir Ahuja",
            "Sunayana Sitaram"
        ],
        "published": "2023",
        "summary": "While understanding and removing gender biases in language models has been a long-standing problem in Natural Language Processing, prior research work has primarily been limited to English. In this work, we investigate some of the challenges with evaluating and mitigating biases in multilingual settings which stem from a lack of existing benchmarks and resources for bias evaluation beyond English especially for non-western context. In this paper, we first create a benchmark for evaluating gender biases in pre-trained masked language models by extending DisCo to different Indian languages using human annotations. We extend various debiasing methods to work beyond English and evaluate their effectiveness for SOTA massively multilingual models on our proposed metric. Overall, our work highlights the challenges that arise while studying social biases in multilingual settings and provides resources as well as mitigation techniques to take a step toward scaling to more languages.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.21.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While understanding and removing gender biases in language models has been a long-standing problem in Natural Language Processing, prior research work has primarily been limited to English... In this paper, we first create a benchmark for evaluating gender biases in pre-trained masked language models... Overall, our work highlights the challenges that arise while studying social biases in multilingual settings\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"While understanding and removing gender biases in language models has been a long-standing problem in Natural Language Processing, prior research work has primarily been limited to English... In this paper, we first create a benchmark for evaluating gender biases in pre-trained masked language models... Overall, our work highlights the challenges that arise while studying social biases in multilingual settings\""
    },
    {
        "title": "G3R: A Graph-Guided Generate-and-Rerank Framework for Complex and Cross-domain Text-to-SQL Generation",
        "authors": [
            "Yanzheng Xiang",
            "Qian-Wen Zhang",
            "Xu Zhang",
            "Zejie Liu",
            "Yunbo Cao",
            "Deyu Zhou"
        ],
        "published": "2023",
        "summary": "We present a framework called G3R for complex and cross-domain Text-to-SQL generation. G3R aims to address two limitations of current approaches: (1) The structure of the abstract syntax tree (AST) is not fully explored during the decoding process which is crucial for complex SQL generation; (2) Domain knowledge is not incorporated to enhance their ability to generalise to unseen domains. G3R consists of a graph-guided SQL generator and a knowledge-enhanced re-ranking mechanism. Firstly, during the decoding process, An AST-Grammar bipartite graph is constructed for both the AST and corresponding grammar rules of the generated partial SQL query. The graph-guided SQL generator captures its structural information and fuses heterogeneous information to predict the action sequence which can construct the AST for the corresponding SQL query uniquely. Then, in the inference stage, a knowledge-enhanced re-ranking mechanism is proposed to introduce domain knowledge to re-rank candidate SQL queries from the beam output and choose the final answer. The SQL ranker is based on pre-trained language models (PLM) and contrastive learning with hybrid prompt tuning is incorporated to stimulate the knowledge of PLMs and make it more discriminative. The proposed approach achieves state-of-the-art results on the Spider and Spider-DK benchmarks, which are challenging complex and cross-domain benchmarks for Text-to-SQL semantic analysis.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.23.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"address two limitations of current approaches\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"address two limitations of current approaches\""
    },
    {
        "title": "A Unified Knowledge Graph Augmentation Service for Boosting Domain-specific NLP Tasks",
        "authors": [
            "Ruiqing Ding",
            "Xiao Han",
            "Leye Wang"
        ],
        "published": "2023",
        "summary": "By focusing the pre-training process on domain-specific corpora, some domain-specific pre-trained language models (PLMs) have achieved state-of-the-art results. However, it is under-investigated to design a unified paradigm to inject domain knowledge in the PLM fine-tuning stage. We propose KnowledgeDA, a unified domain language model development service to enhance the task-specific training procedure with domain knowledge graphs. Given domain-specific task texts input, KnowledgeDA can automatically generate a domain-specific language model following three steps: (i) localize domain knowledge entities in texts via an embedding-similarity approach; (ii) generate augmented samples by retrieving replaceable domain entity pairs from two views of both knowledge graph and training data; (iii) select high-quality augmented samples for fine-tuning via confidence-based assessment. We implement a prototype of KnowledgeDA to learn language models for two domains, healthcare and software development. Experiments on domain-specific text classification and QA tasks verify the effectiveness and generalizability of KnowledgeDA.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.24.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicit, but \"However, it is under-investigated to design a unified paradigm to inject domain knowledge in the PLM fine-tuning stage\" implies that there may be limitations in the current approaches to domain-specific pre-trained language models.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None explicit, but \"However, it is under-investigated to design a unified paradigm to inject domain knowledge in the PLM fine-tuning stage\" implies that there may be limitations in the current approaches to domain-specific pre-trained language models."
    },
    {
        "title": "Dialogue Planning via Brownian Bridge Stochastic Process for Goal-directed Proactive Dialogue",
        "authors": [
            "Jian Wang",
            "Dongding Lin",
            "Wenjie Li"
        ],
        "published": "2023",
        "summary": "Goal-directed dialogue systems aim to proactively reach a pre-determined target through multi-turn conversations. The key to achieving this task lies in planning dialogue paths that smoothly and coherently direct conversations towards the target. However, this is a challenging and under-explored task. In this work, we propose a coherent dialogue planning approach that uses a stochastic process to model the temporal dynamics of dialogue paths. We define a latent space that captures the coherence of goal-directed behavior using a Brownian bridge process, which allows us to incorporate user feedback flexibly in dialogue planning. Based on the derived latent trajectories, we generate dialogue paths explicitly using pre-trained language models. We finally employ these paths as natural language prompts to guide dialogue generation. Our experiments show that our approach generates more coherent utterances and achieves the goal with a higher success rate.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.25.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Based on the derived latent trajectories, we generate dialogue paths explicitly using pre-trained language models.\"\n\nThis paper discusses LLMs, but only mentions their use in a specific context without discussing any limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Based on the derived latent trajectories, we generate dialogue paths explicitly using pre-trained language models.\"\n\nThis paper discusses LLMs, but only mentions their use in a specific context without discussing any limitations."
    },
    {
        "title": "Learning Joint Structural and Temporal Contextualized Knowledge Embeddings for Temporal Knowledge Graph Completion",
        "authors": [
            "Yifu Gao",
            "Yongquan He",
            "Zhigang Kan",
            "Yi Han",
            "Linbo Qiao",
            "Dongsheng Li"
        ],
        "published": "2023",
        "summary": "Temporal knowledge graph completion that predicts missing links for incomplete temporal knowledge graphs (TKG) is gaining increasing attention. Most existing works have achieved good results by incorporating time information into static knowledge graph embedding methods. However, they ignore the contextual nature of the TKG structure, i.e., query-specific subgraph contains both structural and temporal neighboring facts. This paper presents the SToKE, a novel method that employs the pre-trained language model (PLM) to learn joint Structural and Temporal Contextualized Knowledge Embeddings.Specifically, we first construct an event evolution tree (EET) for each query to enable PLMs to handle the TKG, which can be seen as a structured event sequence recording query-relevant structural and temporal contexts. We then propose a novel temporal embedding and structural matrix to learn the time information and structural dependencies of facts in EET.Finally, we formulate TKG completion as a mask prediction problem by masking the missing entity of the query to fine-tune pre-trained language models. Experimental results on three widely used datasets show the superiority of our model.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.28.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"to fine-tune pre-trained language models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"to fine-tune pre-trained language models.\""
    },
    {
        "title": "A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets",
        "authors": [
            "Md Tahmid Rahman Laskar",
            "M Saiful Bari",
            "Mizanur Rahman",
            "Md Amran Hossen Bhuiyan",
            "Shafiq Joty",
            "Jimmy Huang"
        ],
        "published": "2023",
        "summary": "The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT’s performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instructions that we mostly found in ChatGPT and other instruction-tuned models. Our extensive evaluation shows that even though ChatGPT is capable of performing a wide variety of tasks, and may obtain impressive performance in several benchmark datasets, it is still far from achieving the ability to reliably solve many challenging tasks. By providing a thorough assessment of ChatGPT’s performance across diverse NLP tasks, this paper sets the stage for a targeted deployment of ChatGPT-like LLMs in real-world applications.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.29.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"even though ChatGPT is capable of performing a wide variety of tasks, and may obtain impressive performance in several benchmark datasets, it is still far from achieving the ability to reliably solve many challenging tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"even though ChatGPT is capable of performing a wide variety of tasks, and may obtain impressive performance in several benchmark datasets, it is still far from achieving the ability to reliably solve many challenging tasks.\""
    },
    {
        "title": "Generating Deep Questions with Commonsense Reasoning Ability from the Text by Disentangled Adversarial Inference",
        "authors": [
            "Jianxing Yu",
            "Shiqi Wang",
            "Libin Zheng",
            "Qinliang Su",
            "Wei Liu",
            "Baoquan Zhao",
            "Jian Yin"
        ],
        "published": "2023",
        "summary": "This paper proposes a new task of commonsense question generation, which aims to yield deep-level and to-the-point questions from the text. Their answers need to reason over disjoint relevant contexts and external commonsense knowledge, such as encyclopedic facts and causality. The knowledge may not be explicitly mentioned in the text but is used by most humans for problem-shooting. Such complex reasoning with hidden contexts involves deep semantic understanding. Thus, this task has great application value, such as making high-quality quizzes in advanced exams. Due to the lack of modeling complexity, existing methods may produce shallow questions that can be answered by simple word matching. To address these challenges, we propose a new QG model by simultaneously considering asking contents, expressive ways, and answering complexity. We first retrieve text-related commonsense context. Then we disentangle the key factors that control questions in terms of reasoning content and verbalized way. Independence priors and constraints are imposed to facilitate disentanglement. We further develop a discriminator to promote the deep results by considering their answering complexity. Through adversarial inference, we learn the latent factors from data. By sampling the expressive factor from the data distributions, diverse questions can be yielded. Evaluations of two typical data sets show the effectiveness of our approach.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.30.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs or their limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs or their limitations."
    },
    {
        "title": "TADA: Efficient Task-Agnostic Domain Adaptation for Transformers",
        "authors": [
            "Chia-Chien Hung",
            "Lukas Lange",
            "Jannik Strötgen"
        ],
        "published": "2023",
        "summary": "Intermediate training of pre-trained transformer-based language models on domain-specific data leads to substantial gains for downstream tasks. To increase efficiency and prevent catastrophic forgetting alleviated from full domain-adaptive pre-training, approaches such as adapters have been developed. However, these require additional parameters for each layer, and are criticized for their limited expressiveness. In this work, we introduce TADA, a novel task-agnostic domain adaptation method which is modular, parameter-efficient, and thus, data-efficient. Within TADA, we retrain the embeddings to learn domain-aware input representations and tokenizers for the transformer encoder, while freezing all other parameters of the model. Then, task-specific fine-tuning is performed. We further conduct experiments with meta-embeddings and newly introduced meta-tokenizers, resulting in one model per task in multi-domain use cases. Our broad evaluation in 4 downstream tasks for 14 domains across single- and multi-domain setups and high- and low-resource scenarios reveals that TADA is an effective and efficient alternative to full domain-adaptive pre-training and adapters for domain adaptation, while not introducing additional parameters or complex training steps.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.31.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these require additional parameters for each layer, and are criticized for their limited expressiveness.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these require additional parameters for each layer, and are criticized for their limited expressiveness.\""
    },
    {
        "title": "Farewell to Aimless Large-scale Pretraining: Influential Subset Selection for Language Model",
        "authors": [
            "Xiao Wang",
            "Weikang Zhou",
            "Qi Zhang",
            "Jie Zhou",
            "SongYang Gao",
            "Junzhe Wang",
            "Menghan Zhang",
            "Xiang Gao",
            "Yun Wen Chen",
            "Tao Gui"
        ],
        "published": "2023",
        "summary": "Pretrained language models have achieved remarkable success in various natural language processing tasks. However, pretraining has recently shifted toward larger models and larger data, which has resulted in significant computational and energy costs. In this paper, we propose Influence Subset Selection (ISS) for language model, which explicitly utilizes end-task knowledge to select a tiny subset of the pretraining corpus. Specifically, the ISS selects the samples that will provide the most positive influence on the performance of the end task. Furthermore, we design a gradient matching-based influence estimation method, which can drastically reduce the computation time of influence. With only 0.45% of the data and a three-orders-of-magnitude lower computational cost, ISS outperformed pretrained models (e.g., RoBERTa) on eight datasets covering four domains.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.35.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, pretraining has recently shifted toward larger models and larger data, which has resulted in significant computational and energy costs.\"\n\nThis evidence suggests that the paper mentions a limitation of large-scale pretraining for language models, specifically the high computational and energy costs, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, pretraining has recently shifted toward larger models and larger data, which has resulted in significant computational and energy costs.\"\n\nThis evidence suggests that the paper mentions a limitation of large-scale pretraining for language models, specifically the high computational and energy costs, but it is not the primary focus of the paper."
    },
    {
        "title": "Transferring General Multimodal Pretrained Models to Text Recognition",
        "authors": [
            "Junyang Lin",
            "Xuancheng Ren",
            "Yichang Zhang",
            "Gao Liu",
            "Peng Wang",
            "An Yang",
            "Chang Zhou"
        ],
        "published": "2023",
        "summary": "This paper proposes a new method, OFA-OCR, to transfer multimodal pretrained models to text recognition. Specifically, we recast text recognition as image captioning and directly transfer a unified vision-language pretrained model to the end task. Without pretraining on large-scale annotated or synthetic text recognition data, OFA-OCR outperforms the baselines and achieves state-of-the-art performance in the Chinese text recognition benchmark. Additionally, we construct an OCR pipeline with OFA-OCR, and we demonstrate that it can achieve competitive performance with the product-level API.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.37.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations, but \"transfer multimodal pretrained models\" implies that there might be limitations in directly applying these models to text recognition tasks.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations, but \"transfer multimodal pretrained models\" implies that there might be limitations in directly applying these models to text recognition tasks."
    },
    {
        "title": "Automatic Named Entity Obfuscation in Speech",
        "authors": [
            "Judita Preiss"
        ],
        "published": "2023",
        "summary": "Sharing data containing personal information often requires its anonymization, even when consent for sharing was obtained from the data originator. While approaches exist for automated anonymization of text, the area is not as thoroughly explored in speech. This work focuses on identifying, replacing and inserting replacement named entities synthesized using voice cloning into original audio thereby retaining prosodic information while reducing the likelihood of deanonymization. The approach employs a novel named entity recognition (NER) system built directly on speech by training HuBERT (Hsu et al, 2021) using the English speech NER dataset (Yadav et al, 2020). Name substitutes are found using a masked language model and are synthesized using text to speech voice cloning (Eren and team, 2021), upon which the substitute named entities are re-inserted into the original text. The approach is prototyped on a sample of the LibriSpeech corpus (Panyatov et al, 2015) with each step evaluated individually.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.39.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Name substitutes are found using a masked language model\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Name substitutes are found using a masked language model\""
    },
    {
        "title": "Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models",
        "authors": [
            "Soochan Lee",
            "Gunhee Kim"
        ],
        "published": "2023",
        "summary": "Generating intermediate steps, or Chain of Thought (CoT), is an effective way to significantly improve language models’ (LM) multi-step reasoning capability. However, the CoT lengths can grow rapidly with the problem complexity, easily exceeding the maximum context size. Instead of increasing the context limit, which has already been heavily investigated, we explore an orthogonal direction: making LMs divide a problem into multiple contexts. We propose a new inference framework, called Recursion of Thought (RoT), which introduces several special tokens that the models can output to trigger context-related operations. Extensive experiments with multiple architectures including GPT-3 show that RoT dramatically improves LMs’ inference capability to solve problems, whose solution consists of hundreds of thousands of tokens.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.40.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the CoT lengths can grow rapidly with the problem complexity, easily exceeding the maximum context size.\"\n\nThis abstract mentions a limitation of LLMs in passing, specifically the maximum context size limitation, but does not explore it in depth.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the CoT lengths can grow rapidly with the problem complexity, easily exceeding the maximum context size.\"\n\nThis abstract mentions a limitation of LLMs in passing, specifically the maximum context size limitation, but does not explore it in depth."
    },
    {
        "title": "Can Language Models Be Specific? How?",
        "authors": [
            "Jie Huang",
            "Kevin Chen-Chuan Chang",
            "Jinjun Xiong",
            "Wen-mei Hwu"
        ],
        "published": "2023",
        "summary": "“He is a person”, “Paris is located on the earth”. Both statements are correct but meaningless - due to lack of specificity. In this paper, we propose to measure how specific the language of pre-trained language models (PLMs) is. To achieve this, we introduce a novel approach to build a benchmark for specificity testing by forming masked token prediction tasks with prompts. For instance, given “Toronto is located in [MASK].”, we want to test whether a more specific answer will be better filled in by PLMs, e.g., Ontario instead of Canada. From our evaluations, we show that existing PLMs have only a slight preference for more specific answers. We identify underlying factors affecting the specificity and design two prompt-based methods to improve the specificity. Results show that the specificity of the models can be improved by the proposed methods without additional training. We hope this work can bring to awareness the notion of specificity of language models and encourage the research community to further explore this important but understudied problem.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.45.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"From our evaluations, we show that existing PLMs have only a slight preference for more specific answers.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"From our evaluations, we show that existing PLMs have only a slight preference for more specific answers.\""
    },
    {
        "title": "The Web Can Be Your Oyster for Improving Language Models",
        "authors": [
            "Junyi Li",
            "Tianyi Tang",
            "Wayne Xin Zhao",
            "Jingyuan Wang",
            "Jian-Yun Nie",
            "Ji-Rong Wen"
        ],
        "published": "2023",
        "summary": "Pretrained language models (PLMs) encode a large amount of world knowledge. However, as such knowledge is frozen at the time of model training, the models become static and limited by the training data at that time. In order to further improve the capacity of PLMs for knowledge-intensive tasks, we consider augmenting PLMs with the large-scale web using search engine. Unlike previous augmentation sources (e.g., Wikipedia data dump), the web provides broader, more comprehensive and constantly updated information. In this paper, we present a web-augmented PLM – UniWeb, which is trained over 16 knowledge-intensive tasks in a unified text-to-text format. Instead of simply using the retrieved contents from web, our approach has made two major improvements. Firstly, we propose an adaptive search engine assisted learning method that can self-evaluate the confidence level of PLM’s predictions, and adaptively determine when to refer to the web for more data, which can avoid useless or noisy augmentation from web. Secondly, we design a pretraining task, i.e., continual knowledge learning, based on salient spans prediction, to reduce the discrepancy between the encoded and retrieved knowledge. Experiments on a wide range of knowledge-intensive tasks show that our model significantly outperforms previous retrieval-augmented methods.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.46.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, as such knowledge is frozen at the time of model training, the models become static and limited by the training data at that time.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, as such knowledge is frozen at the time of model training, the models become static and limited by the training data at that time.\""
    },
    {
        "title": "Enhancing Few-shot Cross-lingual Transfer with Target Language Peculiar Examples",
        "authors": [
            "Hwichan Kim",
            "Mamoru Komachi"
        ],
        "published": "2023",
        "summary": "Few-shot cross-lingual transfer, fine-tuning Multilingual Masked Language Model (MMLM) with source language labeled data and a small amount of target language labeled data, provides excellent performance in the target language. However, if no labeled data in the target language are available, they need to be created through human annotations. In this study, we devise a metric to select annotation candidates from an unlabeled data pool that efficiently enhance accuracy for few-shot cross-lingual transfer. It is known that training a model with hard examples is important to improve the model’s performance. Therefore, we first identify examples that MMLM cannot solve in a zero-shot cross-lingual transfer setting and demonstrate that it is hard to predict peculiar examples in the target language, i.e., the examples distant from the source language examples in cross-lingual semantic space of the MMLM.We then choose high peculiarity examples as annotation candidates and perform few-shot cross-lingual transfer. In comprehensive experiments with 20 languages and 6 tasks, we demonstrate that the high peculiarity examples improve the target language accuracy compared to other candidate selection methods proposed in previous studies.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.47.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"It is known that training a model with hard examples is important to improve the model’s performance. Therefore, we first identify examples that MMLM cannot solve in a zero-shot cross-lingual transfer setting and demonstrate that it is hard to predict peculiar examples in the target language, i.e., the examples distant from the source language examples in cross-lingual semantic space of the",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"It is known that training a model with hard examples is important to improve the model’s performance. Therefore, we first identify examples that MMLM cannot solve in a zero-shot cross-lingual transfer setting and demonstrate that it is hard to predict peculiar examples in the target language, i.e., the examples distant from the source language examples in cross-lingual semantic space of the"
    },
    {
        "title": "Aligning Instruction Tasks Unlocks Large Language Models as Zero-Shot Relation Extractors",
        "authors": [
            "Kai Zhang",
            "Bernal Jimenez Gutierrez",
            "Yu Su"
        ],
        "published": "2023",
        "summary": "Recent work has shown that fine-tuning large language models (LLMs) on large-scale instruction-following datasets substantially improves their performance on a wide range of NLP tasks, especially in the zero-shot setting. However, even advanced instruction-tuned LLMs still fail to outperform small LMs on relation extraction (RE), a fundamental information extraction task. We hypothesize that instruction-tuning has been unable to elicit strong RE capabilities in LLMs due to RE’s low incidence in instruction-tuning datasets, making up less than 1% of all tasks (Wang et al. 2022). To address this limitation, we propose QA4RE, a framework that aligns RE with question answering (QA), a predominant task in instruction-tuning datasets. Comprehensive zero-shot RE experiments over four datasets with two series of instruction-tuned LLMs (six LLMs in total) demonstrate that our QA4RE framework consistently improves LLM performance, strongly verifying our hypothesis and enabling LLMs to outperform strong zero-shot baselines by a large margin. Additionally, we provide thorough experiments and discussions to show the robustness, few-shot effectiveness, and strong transferability of our QA4RE framework. This work illustrates a promising way of adapting LLMs to challenging and underrepresented tasks by aligning these tasks with more common instruction-tuning tasks like QA.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.50.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, even advanced instruction-tuned LLMs still fail to outperform small LMs on relation extraction (RE), a fundamental information extraction task.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, even advanced instruction-tuned LLMs still fail to outperform small LMs on relation extraction (RE), a fundamental information extraction task.\""
    },
    {
        "title": "TADA : Task Agnostic Dialect Adapters for English",
        "authors": [
            "William Held",
            "Caleb Ziems",
            "Diyi Yang"
        ],
        "published": "2023",
        "summary": "Large Language Models, the dominant starting point for Natural Language Processing (NLP) applications, fail at a higher rate for speakers of English dialects other than Standard American English (SAE). Prior work addresses this using task specific data or synthetic data augmentation, both of which require intervention for each dialect and task pair. This poses a scalability issue that prevents the broad adoption of robust dialectal English NLP. We introduce a simple yet effective method for task-agnostic dialect adaptation by aligning non-SAE dialects using adapters and composing them with task-specific adapters from SAE. Task-Agnostic Dialect Adapters (TADA) improve dialectal robustness on 4 dialectal variants of the GLUE benchmark without task-specific supervision.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.51.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large Language Models, the dominant starting point for Natural Language Processing (NLP) applications, fail at a higher rate for speakers of English dialects other than Standard American English (SAE).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large Language Models, the dominant starting point for Natural Language Processing (NLP) applications, fail at a higher rate for speakers of English dialects other than Standard American English (SAE).\""
    },
    {
        "title": "Generative Zero-Shot Prompt Learning for Cross-Domain Slot Filling with Inverse Prompting",
        "authors": [
            "Xuefeng Li",
            "Liwen Wang",
            "Guanting Dong",
            "Keqing He",
            "Jinzheng Zhao",
            "Hao Lei",
            "Jiachi Liu",
            "Weiran Xu"
        ],
        "published": "2023",
        "summary": "Zero-shot cross-domain slot filling aims to transfer knowledge from the labeled source domain to the unlabeled target domain. Existing models either encode slot descriptions and examples or design handcrafted question templates using heuristic rules, suffering from poor generalization capability or robustness. In this paper, we propose a generative zero-shot prompt learning framework for cross-domain slot filling, both improving generalization and robustness than previous work. Besides, we introduce a novel inverse prompting strategy to distinguish different slot types to avoid the multiple prediction problem, and an efficient prompt tuning strategy to boost higher performance only training fewer prompt parameters. Experiments and analysis demonstrate the effectiveness of our proposed framework, especially huge improvements (+13.44% F1) on the unseen slots.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.52.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Re-appraising the Schema Linking for Text-to-SQL",
        "authors": [
            "Yujian Gan",
            "Xinyun Chen",
            "Matthew Purver"
        ],
        "published": "2023",
        "summary": "Most text-to-SQL models, even though based on the same grammar decoder, generate the SQL structure first and then fill in the SQL slots with the correct schema items. This second step depends on schema linking: aligning the entity references in the question with the schema columns or tables. This is generally approached via Exact Match based Schema Linking (EMSL) within a neural network-based schema linking module. EMSL has become standard in text-to-SQL: many state-of-the-art models employ EMSL, with performance dropping significantly when the EMSL component is removed. In this work, however, we show that EMSL reduces robustness, rendering models vulnerable to synonym substitution and typos. Instead of relying on EMSL to make up for deficiencies in question-schema encoding, we show that using a pre-trained language model as an encoder can improve performance without using EMSL, giving a more robust model. We also study the design choice of the schema linking module, finding that a suitable design benefits performance and interoperability. Finally, based on the above study of schema linking, we introduce the grammar linking to help model align grammar references in the question with the SQL keywords.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.53.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Instead of relying on EMSL to make up for deficiencies in question-schema encoding, we show that using a pre-trained language model as an encoder can improve performance without using EMSL, giving a more robust model.\"\n\nThis paper discusses the limitations of a specific schema linking approach (EMSL) in text-to-SQL models, but also mentions the potential of pre-trained language models to improve",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Instead of relying on EMSL to make up for deficiencies in question-schema encoding, we show that using a pre-trained language model as an encoder can improve performance without using EMSL, giving a more robust model.\"\n\nThis paper discusses the limitations of a specific schema linking approach (EMSL) in text-to-SQL models, but also mentions the potential of pre-trained language models to improve"
    },
    {
        "title": "When Gradient Descent Meets Derivative-Free Optimization: A Match Made in Black-Box Scenario",
        "authors": [
            "Chengcheng Han",
            "Liqing Cui",
            "Renyu Zhu",
            "Jianing Wang",
            "Nuo Chen",
            "Qiushi Sun",
            "Xiang Li",
            "Ming Gao"
        ],
        "published": "2023",
        "summary": "Large pre-trained language models (PLMs) have garnered significant attention for their versatility and potential for solving a wide spectrum of natural language processing (NLP) tasks. However, the cost of running these PLMs may be prohibitive. Furthermore, PLMs may not be open-sourced due to commercial considerations and potential risks of misuse, such as GPT-3. The parameters and gradients of PLMs are unavailable in this scenario. To solve the issue, black-box tuning has been proposed, which utilizes derivative-free optimization (DFO), instead of gradient descent, for training task-specific continuous prompts. However, these gradient-free methods still exhibit a significant gap compared to gradient-based methods. In this paper, we introduce gradient descent into black-box tuning scenario through knowledge distillation. Furthermore, we propose a novel method GDFO, which integrates gradient descent and derivative-free optimization to optimize task-specific continuous prompts in a harmonized manner. Experimental results show that GDFO can achieve significant performance gains over previous state-of-the-art methods.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.55.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the cost of running these PLMs may be prohibitive. Furthermore, PLMs may not be open-sourced due to commercial considerations and potential risks of misuse, such as GPT-3.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the cost of running these PLMs may be prohibitive. Furthermore, PLMs may not be open-sourced due to commercial considerations and potential risks of misuse, such as GPT-3.\""
    },
    {
        "title": "The Dangers of trusting Stochastic Parrots: Faithfulness and Trust in Open-domain Conversational Question Answering",
        "authors": [
            "Sabrina Chiesurin",
            "Dimitris Dimakopoulos",
            "Marco Antonio Sobrevilla Cabezudo",
            "Arash Eshghi",
            "Ioannis Papaioannou",
            "Verena Rieser",
            "Ioannis Konstas"
        ],
        "published": "2023",
        "summary": "Large language models are known to produce output which sounds fluent and convincing, but is also often wrong, e.g. “unfaithful” with respect to a rationale as retrieved from a knowledge base. In this paper, we show that task-based systems which exhibit certain advanced linguistic dialog behaviors, such as lexical alignment (repeating what the user said), are in fact preferred and trusted more, whereas other phenomena, such as pronouns and ellipsis are dis-preferred. We use open-domain question answering systems as our test-bed for task based dialog generation and compare several open- and closed-book models. Our results highlight the danger of systems that appear to be trustworthy by parroting user input while providing an unfaithful response.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.60.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models are known to produce output which sounds fluent and convincing, but is also often wrong, e.g. “unfaithful” with respect to a rationale as retrieved from a knowledge base.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Large language models are known to produce output which sounds fluent and convincing, but is also often wrong, e.g. “unfaithful” with respect to a rationale as retrieved from a knowledge base.\""
    },
    {
        "title": "Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker",
        "authors": [
            "Sukmin Cho",
            "Soyeong Jeong",
            "Jeong yeon Seo",
            "Jong Park"
        ],
        "published": "2023",
        "summary": "Re-rankers, which order retrieved documents with respect to the relevance score on the given query, have gained attention for the information retrieval (IR) task. Rather than fine-tuning the pre-trained language model (PLM), the large-scale language model (LLM) is utilized as a zero-shot re-ranker with excellent results. While LLM is highly dependent on the prompts, the impact and the optimization of the prompts for the zero-shot re-ranker are not explored yet. Along with highlighting the impact of optimization on the zero-shot re-ranker, we propose a novel discrete prompt optimization method, Constrained Prompt generation (Co-Prompt), with the metric estimating the optimum for re-ranking. Co-Prompt guides the generated texts from PLM toward optimal prompts based on the metric without parameter update. The experimental results demonstrate that Co-Prompt leads to outstanding re-ranking performance against the baselines. Also, Co-Prompt generates more interpretable prompts for humans against other prompt optimization methods.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.61.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While LLM is highly dependent on the prompts, the impact and the optimization of the prompts for the zero-shot re-ranker are not explored yet.\"\n\nThis paper discusses LLMs and mentions a limitation related to their dependence on prompts, but it does not elaborate on this limitation and instead focuses on proposing a solution to optimize prompts.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"While LLM is highly dependent on the prompts, the impact and the optimization of the prompts for the zero-shot re-ranker are not explored yet.\"\n\nThis paper discusses LLMs and mentions a limitation related to their dependence on prompts, but it does not elaborate on this limitation and instead focuses on proposing a solution to optimize prompts."
    },
    {
        "title": "Triggering Multi-Hop Reasoning for Question Answering in Language Models using Soft Prompts and Random Walks",
        "authors": [
            "Kanishka Misra",
            "Cicero Nogueira dos Santos",
            "Siamak Shakeri"
        ],
        "published": "2023",
        "summary": "Despite readily memorizing world knowledge about entities, pre-trained language models (LMs) struggle to compose together two or more facts to perform multi-hop reasoning in question-answering tasks. In this work, we propose techniques that improve upon this limitation by relying on random-walks over structured knowledge graphs. Specifically, we use soft-prompts to guide LMs to chain together their encoded knowledge by learning to map multi-hop questions to random-walk paths that lead to the answer. Applying our methods on two T5 LMs shows substantial improvements over standard tuning approaches in answering questions that require multi-hop reasoning.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.62.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite readily memorizing world knowledge about entities, pre-trained language models (LMs) struggle to compose together two or more facts to perform multi-hop reasoning in question-answering tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Despite readily memorizing world knowledge about entities, pre-trained language models (LMs) struggle to compose together two or more facts to perform multi-hop reasoning in question-answering tasks.\""
    },
    {
        "title": "Label Agnostic Pre-training for Zero-shot Text Classification",
        "authors": [
            "Christopher Clarke",
            "Yuzhao Heng",
            "Yiping Kang",
            "Krisztian Flautner",
            "Lingjia Tang",
            "Jason Mars"
        ],
        "published": "2023",
        "summary": "Conventional approaches to text classification typically assume the existence of a fixed set of predefined labels to which a given text can be classified. However, in real-world applications, there exists an infinite label space for describing a given text. In addition, depending on the aspect (sentiment, topic, etc.) and domain of the text (finance, legal, etc.), the interpretation of the label can vary greatly. This makes the task of text classification, particularly in the zero-shot scenario, extremely challenging. In this paper, we investigate the task of zero-shot text classification with the aim of improving the ability of pre-trained language models (PLMs) to generalize to both seen and unseen data across varying aspects and domains. To solve this we introduce two new simple yet effective pre-training strategies, Implicit and Explicit pre-training. These methods inject aspect-level understanding into the model at train time with the goal of conditioning the model to build task-level understanding. To evaluate this, we construct and release UTCD, a new benchmark dataset for evaluating text classification in zero-shot settings. Experimental results on UTCD show that our approach achieves improved zero-shot generalization on a suite of challenging datasets across an array of zero-shot formalizations.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.64.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, in real-world applications, there exists an infinite label space for describing a given text. In addition, depending on the aspect (sentiment, topic, etc.) and domain of the text (finance, legal, etc.), the interpretation of the label can vary greatly.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, in real-world applications, there exists an infinite label space for describing a given text. In addition, depending on the aspect (sentiment, topic, etc.) and domain of the text (finance, legal, etc.), the interpretation of the label can vary greatly.\""
    },
    {
        "title": "Click: Controllable Text Generation with Sequence Likelihood Contrastive Learning",
        "authors": [
            "Chujie Zheng",
            "Pei Ke",
            "Zheng Zhang",
            "Minlie Huang"
        ],
        "published": "2023",
        "summary": "It has always been an important yet challenging problem to control language models to avoid generating texts with undesirable attributes, such as toxic language and unnatural repetition. We introduce Leo for controllable text generation, which needs no modification to the model architecture and facilitates out-of-the-box use of trained models. It employs a contrastive loss on sequence likelihood, which fundamentally decreases the generation probability of negative samples (i.e., generations with undesirable attributes). It also adopts a novel likelihood ranking-based strategy to construct contrastive samples from model generations. On the tasks of language detoxification, sentiment steering, and repetition reduction, we show that Leo outperforms strong baselines of controllable text generation and demonstrate the superiority of Leo’s sample construction strategy.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.65.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"It has always been an important yet challenging problem to control language models to avoid generating texts with undesirable attributes, such as toxic language and unnatural repetition.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"It has always been an important yet challenging problem to control language models to avoid generating texts with undesirable attributes, such as toxic language and unnatural repetition.\""
    },
    {
        "title": "Towards Reasoning in Large Language Models: A Survey",
        "authors": [
            "Jie Huang",
            "Kevin Chen-Chuan Chang"
        ],
        "published": "2023",
        "summary": "Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.67.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it is not yet clear to what extent LLMs are capable of reasoning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, it is not yet clear to what extent LLMs are capable of reasoning.\""
    },
    {
        "title": "Adaptation Approaches for Nearest Neighbor Language Models",
        "authors": [
            "Rishabh Bhardwaj",
            "George Polovets",
            "Monica Sunkara"
        ],
        "published": "2023",
        "summary": "Semi-parametric Nearest Neighbor Language Models (kNN-LMs) have produced impressive gains over purely parametric LMs, by leveraging large-scale neighborhood retrieval over external memory datastores. However, there has been little investigation into adapting such models for new domains. This work attempts to fill that gap and suggests the following approaches for adapting kNN-LMs — 1) adapting the underlying LM (using Adapters), 2) expanding neighborhood retrieval over an additional adaptation datastore, and 3) adapting the weights (scores) of retrieved neighbors using a learned Rescorer module. We study each adaptation strategy separately, as well as the combined performance improvement through ablation experiments and an extensive set of evaluations run over seven adaptation domains. Our combined adaptation approach consistently outperforms purely parametric adaptation and zero-shot (kNN-LM) baselines that construct datastores from the adaptation data. On average, we see perplexity improvements of 17.1% and 16% for these respective baselines, across domains.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.73.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, there has been little investigation into adapting such models for new domains.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, there has been little investigation into adapting such models for new domains.\""
    },
    {
        "title": "Language Models for German Text Simplification: Overcoming Parallel Data Scarcity through Style-specific Pre-training",
        "authors": [
            "Miriam Anschütz",
            "Joshua Oehms",
            "Thomas Wimmer",
            "Bartłomiej Jezierski",
            "Georg Groh"
        ],
        "published": "2023",
        "summary": "Automatic text simplification systems help to reduce textual information barriers on the internet. However, for languages other than English, only few parallel data to train these systems exists. We propose a two-step approach to overcome this data scarcity issue. First, we fine-tuned language models on a corpus of German Easy Language, a specific style of German. Then, we used these models as decoders in a sequence-to-sequence simplification task. We show that the language models adapt to the style characteristics of Easy Language and output more accessible texts. Moreover, with the style-specific pre-training, we reduced the number of trainable parameters in text simplification models. Hence, less parallel data is sufficient for training. Our results indicate that pre-training on unaligned data can reduce the required parallel data while improving the performance on downstream tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.74.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, for languages other than English, only few parallel data to train these systems exists.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, for languages other than English, only few parallel data to train these systems exists.\""
    },
    {
        "title": "Client-Customized Adaptation for Parameter-Efficient Federated Learning",
        "authors": [
            "Yeachan Kim",
            "Junho Kim",
            "Wing-Lam Mok",
            "Jun-Hyung Park",
            "SangKeun Lee"
        ],
        "published": "2023",
        "summary": "Despite the versatility of pre-trained language models (PLMs) across domains, their large memory footprints pose significant challenges in federated learning (FL), where the training model has to be distributed between a server and clients. One potential solution to bypass such constraints might be the use of parameter-efficient fine-tuning (PEFT) in the context of FL. However, we have observed that typical PEFT tends to severely suffer from heterogeneity among clients in FL scenarios, resulting in unstable and slow convergence. In this paper, we propose Client-Customized Adaptation (C2A), a novel hypernetwork-based FL framework that generates client-specific adapters by conditioning the client information. With the effectiveness of the hypernetworks in generating customized weights through learning to adopt the different characteristics of inputs, C2A can maximize the utility of shared model parameters while minimizing the divergence caused by client heterogeneity. To verify the efficacy of C2A, we perform extensive evaluations on FL scenarios involving heterogeneity in label and language distributions. Comprehensive evaluation results clearly support the superiority of C2A in terms of both efficiency and effectiveness in FL scenarios.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.75.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the versatility of pre-trained language models (PLMs) across domains, their large memory footprints pose significant challenges in federated learning (FL), where the training model has to be distributed between a server and clients.\"\n\nThis abstract mentions a limitation of pre-trained language models (large memory footprints) but does not explore it in depth, instead focusing on a proposed solution to address",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite the versatility of pre-trained language models (PLMs) across domains, their large memory footprints pose significant challenges in federated learning (FL), where the training model has to be distributed between a server and clients.\"\n\nThis abstract mentions a limitation of pre-trained language models (large memory footprints) but does not explore it in depth, instead focusing on a proposed solution to address"
    },
    {
        "title": "FolkScope: Intention Knowledge Graph Construction for E-commerce Commonsense Discovery",
        "authors": [
            "Changlong Yu",
            "Weiqi Wang",
            "Xin Liu",
            "Jiaxin Bai",
            "Yangqiu Song",
            "Zheng Li",
            "Yifan Gao",
            "Tianyu Cao",
            "Bing Yin"
        ],
        "published": "2023",
        "summary": "Understanding users’ intentions in e-commerce platforms requires commonsense knowledge. In this paper, we present FolkScope, an intention knowledge graph construction framework, to reveal the structure of humans’ minds about purchasing items. As commonsense knowledge is usually ineffable and not expressed explicitly, it is challenging to perform information extraction. Thus, we propose a new approach that leverages the generation power of large language models (LLMs) and human-in-the-loop annotation to semi-automatically construct the knowledge graph. LLMs first generate intention assertions via e-commerce specific prompts to explain shopping behaviors, where the intention can be an open reason or a predicate falling into one of 18 categories aligning with ConceptNet, e.g., IsA, MadeOf, UsedFor, etc. Then we annotate plausibility and typicality labels of sampled intentions as training data in order to populate human judgments to all automatic generations. Last, to structurize the assertions, we propose pattern mining and conceptualization to form more condensed and abstract knowledge. Extensive evaluations and study demonstrate that our constructed knowledge graph can well model e-commerce knowledge and have many potential applications.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.76.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"As commonsense knowledge is usually ineffable and not expressed explicitly, it is challenging to perform information extraction.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"As commonsense knowledge is usually ineffable and not expressed explicitly, it is challenging to perform information extraction.\""
    },
    {
        "title": "Risks and NLP Design: A Case Study on Procedural Document QA",
        "authors": [
            "Nikita Haduong",
            "Alice Gao",
            "Noah A. Smith"
        ],
        "published": "2023",
        "summary": "As NLP systems are increasingly deployed at scale, concerns about their potential negative impacts have attracted the attention of the research community, yet discussions of risk have mostly been at an abstract level and focused on generic AI or NLP applications. We argue that clearer assessments of risks and harms to users—and concrete strategies to mitigate them—will be possible when we specialize the analysis to more concrete applications and their plausible users. As an illustration, this paper is grounded in cooking recipe procedural document question answering (ProcDocQA), where there are well-defined risks to users such as injuries or allergic reactions. Our case study shows that an existing language model, applied in “zero-shot” mode, quantitatively answers real-world questions about recipes as well or better than the humans who have answered the questions on the web. Using a novel questionnaire informed by theoretical work on AI risk, we conduct a risk-oriented error analysis that could then inform the design of a future system to be deployed with lower risk of harm and better performance.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.81.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our case study shows that an existing language model, applied in “zero-shot” mode, quantitatively answers real-world questions about recipes as well or better than the humans who have answered the questions on the web.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Our case study shows that an existing language model, applied in “zero-shot” mode, quantitatively answers real-world questions about recipes as well or better than the humans who have answered the questions on the web.\""
    },
    {
        "title": "The Diminishing Returns of Masked Language Models to Science",
        "authors": [
            "Zhi Hong",
            "Aswathy Ajith",
            "James Pauloski",
            "Eamon Duede",
            "Kyle Chard",
            "Ian Foster"
        ],
        "published": "2023",
        "summary": "Transformer-based masked language models such as BERT, trained on general corpora, have shown impressive performance on downstream tasks. It has also been demonstrated that the downstream task performance of such models can be improved by pretraining larger models for longer on more data. In this work, we empirically evaluate the extent to which these results extend to tasks in science. We use 14 domain-specific transformer-based models (including ScholarBERT, a new 770Mparameter science-focused masked language model pretrained on up to 225B tokens) to evaluate the impact of training data, model size, pretraining and finetuning time on 12 downstream scientific tasks. Interestingly, we find that increasing model size, training data, or compute time does not always lead to significant improvements (i.e., >1% F1), if any, in scientific information extraction tasks. We offer possible explanations for this surprising result.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.82.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Interestingly, we find that increasing model size, training data, or compute time does not always lead to significant improvements (i.e., >1% F1), if any, in scientific information extraction tasks.\"\n\nThis abstract extensively discusses the limitations of LLMs in the context of scientific information extraction tasks, specifically highlighting that increasing model size, training data, or compute time does not always",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Interestingly, we find that increasing model size, training data, or compute time does not always lead to significant improvements (i.e., >1% F1), if any, in scientific information extraction tasks.\"\n\nThis abstract extensively discusses the limitations of LLMs in the context of scientific information extraction tasks, specifically highlighting that increasing model size, training data, or compute time does not always"
    },
    {
        "title": "Learning to Generalize for Cross-domain QA",
        "authors": [
            "Yingjie Niu",
            "Linyi Yang",
            "Ruihai Dong",
            "Yue Zhang"
        ],
        "published": "2023",
        "summary": "There have been growing concerns regarding the out-of-domain generalization ability of natural language processing (NLP) models, particularly in question-answering (QA) tasks. Current synthesized data augmentation methods for QA are hampered by increased training costs. To address this issue, we propose a novel approach that combines prompting methods and linear probing with fine-tuning strategy, which does not entail additional cost. Our method has been theoretically and empirically shown to be effective in enhancing the generalization ability of both generative and discriminative models. Our approach outperforms state-of-the-art baselines, with an average increase in F1 score of 4.5%-7.9%. Furthermore, our method can be easily integrated into any pre-trained models and offers a promising solution to the under-explored cross-domain QA task.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.84.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"There have been growing concerns regarding the out-of-domain generalization ability of natural language processing (NLP) models, particularly in question-answering (QA) tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"There have been growing concerns regarding the out-of-domain generalization ability of natural language processing (NLP) models, particularly in question-answering (QA) tasks.\""
    },
    {
        "title": "Enhancing Cross-lingual Natural Language Inference by Soft Prompting with Multilingual Verbalizer",
        "authors": [
            "Shuang Li",
            "Xuming Hu",
            "Aiwei Liu",
            "Yawen Yang",
            "Fukun Ma",
            "Philip S. Yu",
            "Lijie Wen"
        ],
        "published": "2023",
        "summary": "Cross-lingual natural language inference is a fundamental problem in cross-lingual language understanding. Many recent works have used prompt learning to address the lack of annotated parallel corpora in XNLI.However, these methods adopt discrete prompting by simply translating the templates to the target language and need external expert knowledge to design the templates. Besides, discrete prompts of human-designed template words are not trainable vectors and can not be migrated to target languages in the inference stage flexibly. In this paper, we propose a novel Soft prompt learning framework with the Multilingual Verbalizer (SoftMV) for XNLI. SoftMV first constructs cloze-style question with soft prompts for the input sample. Then we leverage bilingual dictionaries to generate an augmented multilingual question for the original question. SoftMV adopts a multilingual verbalizer to align the representations of original and augmented multilingual questions into a unified semantic space with consistency regularization. Experimental results on XNLI demonstrate that SoftMV can achieve state-of-the-art performance and significantly outperform the previous methods under the few-shot and full-shot cross-lingual transfer settings.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.88.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Towards Zero-Shot Persona Dialogue Generation with In-Context Learning",
        "authors": [
            "Xinchao Xu",
            "Zeyang Lei",
            "Wenquan Wu",
            "Zheng-Yu Niu",
            "Hua Wu",
            "Haifeng Wang"
        ],
        "published": "2023",
        "summary": "Much work has been done to improve persona consistency by finetuning a pretrained dialogue model on high-quality human-annoated persona datasets. However, these methods still face the challenges of high cost and poor scalability. To this end, we propose a simple-yet-effective approach to significantly improve zero-shot persona consistency via in-context learning. Specifically, we first pre-train a persona-augmented dialogue generation model and then utilize in-context prompting mechanism to realize zero-shot persona customization. Experimental results demonstrate that our method can dramatically improve persona consistency without compromising coherence and informativeness in zero-shot settings.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.90.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these methods still face the challenges of high cost and poor scalability.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, these methods still face the challenges of high cost and poor scalability.\""
    },
    {
        "title": "Exploiting Rich Textual User-Product Context for Improving Personalized Sentiment Analysis",
        "authors": [
            "Chenyang Lyu",
            "Linyi Yang",
            "Yue Zhang",
            "Yvette Graham",
            "Jennifer Foster"
        ],
        "published": "2023",
        "summary": "User and product information associated with a review is useful for sentiment polarity prediction. Typical approaches incorporating such information focus on modeling users and products as implicitly learned representation vectors. Most do not exploit the potential of historical reviews, or those that currently do require unnecessary modifications to model architectureor do not make full use of user/product associations. The contribution of this work is twofold: i) a method to explicitly employ historical reviews belonging to the same user/product in initializing representations, and ii) efficient incorporation of textual associations between users and products via a user-product cross-context module. Experiments on the IMDb, Yelp-2013 and Yelp-2014 English benchmarks with BERT, SpanBERT and Longformer pretrained language models show that our approach substantially outperforms previous state-of-the-art.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.92.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Most do not exploit the potential of historical reviews, or those that currently do require unnecessary modifications to model architecture or do not make full use of user/product associations.\"\n\nThis rating is chosen because the abstract mentions limitations of previous approaches using LLMs, but does not discuss the limitations of LLMs themselves in detail.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Most do not exploit the potential of historical reviews, or those that currently do require unnecessary modifications to model architecture or do not make full use of user/product associations.\"\n\nThis rating is chosen because the abstract mentions limitations of previous approaches using LLMs, but does not discuss the limitations of LLMs themselves in detail."
    },
    {
        "title": "Pseudo Outlier Exposure for Out-of-Distribution Detection using Pretrained Transformers",
        "authors": [
            "Jaeyoung Kim",
            "Kyuheon Jung",
            "Dongbin Na",
            "Sion Jang",
            "Eunbin Park",
            "Sungchul Choi"
        ],
        "published": "2023",
        "summary": "For real-world language applications, detecting an out-of-distribution (OOD) sample is helpful to alert users or reject such unreliable samples. However, modern over-parameterized language models often produce overconfident predictions for both in-distribution (ID) and OOD samples. In particular, language models suffer from OOD samples with a similar semantic representation to ID samples since these OOD samples lie near the ID manifold.A rejection network can be trained with ID and diverse outlier samples to detect test OOD samples, but explicitly collecting auxiliary OOD datasets brings an additional burden for data collection. In this paper, we propose a simple but effective method called Pseudo Outlier Exposure (POE) that constructs a surrogate OOD dataset by sequentially masking tokens related to ID classes. The surrogate OOD sample introduced by POE shows a similar representation to ID data, which is most effective in training a rejection network. Our method does not require any external OOD data and can be easily implemented within off-the-shelf Transformers.A comprehensive comparison with state-of-the-art algorithms demonstrates POE’s competitiveness on several text classification benchmarks.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.95.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "not extracted",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, modern over-parameterized language models often produce overconfident predictions for both in-distribution (ID) and OOD samples. In particular, language models suffer from OOD samples with a similar semantic representation to ID samples since these OOD samples lie near the ID manifold.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs:  \nEvidence: \"However, modern over-parameterized language models often produce overconfident predictions for both in-distribution (ID) and OOD samples. In particular, language models suffer from OOD samples with a similar semantic representation to ID samples since these OOD samples lie near the ID manifold.\""
    },
    {
        "title": "SERENGETI: Massively Multilingual Language Models for Africa",
        "authors": [
            "Ife Adebara",
            "AbdelRahim Elmadany",
            "Muhammad Abdul-Mageed",
            "Alcides Alcoba Inciarte"
        ],
        "published": "2023",
        "summary": "Multilingual pretrained language models (mPLMs) acquire valuable, generalizable linguistic information during pretraining and have advanced the state of the art on task-specific finetuning. To date, only ~31 out of ~2,000 African languages are covered in existing language models. We ameliorate this limitation by developing SERENGETI, a set of massively multilingual language model that covers 517 African languages and language varieties. We evaluate our novel models on eight natural language understanding tasks across 20 datasets, comparing to 4 mPLMs that cover 4-23 African languages. SERENGETI outperforms other models on 11 datasets across the eights tasks, achieving 82.27 average F_1. We also perform analyses of errors from our models, which allows us to investigate the influence of language genealogy and linguistic similarity when the models are applied under zero-shot settings. We will publicly release our models for research. Anonymous link",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.97.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"To date, only ~31 out of ~2,000 African languages are covered in existing language models.\"\n\nThis paper mentions a limitation of existing language models (coverage of African languages) but does not explore it in depth. The primary focus of the paper is on the proposed solution, SERENGETI, and its evaluation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"To date, only ~31 out of ~2,000 African languages are covered in existing language models.\"\n\nThis paper mentions a limitation of existing language models (coverage of African languages) but does not explore it in depth. The primary focus of the paper is on the proposed solution, SERENGETI, and its evaluation."
    },
    {
        "title": "AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation",
        "authors": [
            "Chujie Zheng",
            "Sahand Sabour",
            "Jiaxin Wen",
            "Zheng Zhang",
            "Minlie Huang"
        ],
        "published": "2023",
        "summary": "Crowdsourced dialogue corpora are usually limited in scale and topic coverage due to the expensive cost of data curation. This would hinder the generalization of downstream dialogue models to open-domain topics. In this work, we leverage large language models for dialogue augmentation in the task of emotional support conversation (ESC). By treating dialogue augmentation as a dialogue completion task, we prompt a fine-tuned language model to complete full dialogues from available dialogue posts of various topics, which are then postprocessed based on heuristics. Applying this approach, we construct AugESC, an augmented dataset for the ESC task, which largely extends the scale and topic coverage of the crowdsourced ESConv corpus. Through comprehensive human evaluation, we demonstrate that our approach is superior to strong baselines of dialogue augmentation and that AugESC has comparable dialogue quality to the crowdsourced corpus. We also conduct human interactive evaluation and prove that post-training on AugESC improves downstream dialogue models’ generalization ability to open-domain topics. These results suggest the utility of AugESC and highlight the potential of large language models in improving data-scarce dialogue generation tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.99.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "LMCap: Few-shot Multilingual Image Captioning by Retrieval Augmented Language Model Prompting",
        "authors": [
            "Rita Ramos",
            "Bruno Martins",
            "Desmond Elliott"
        ],
        "published": "2023",
        "summary": "Multilingual image captioning has recently been tackled by training with large-scale machine translated data, which is an expensive, noisy, and time-consuming process. Without requiring any multilingual caption data, we propose LMCap, an image-blind few-shot multilingual captioning model that works by prompting a language model with retrieved captions. Specifically, instead of following the standard encoder-decoder paradigm, given an image, LMCap first retrieves the captions of similar images using a multilingual CLIP encoder. These captions are then combined into a prompt for an XGLM decoder, in order to generate captions in the desired language. In other words, the generation model does not directly process the image, instead it processes retrieved captions. Experiments on the XM3600 dataset of geographically diverse images show that our model is competitive with fully-supervised multilingual captioning models, without requiring any supervised training on any captioning data.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.104.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation of LLMs is mentioned in the abstract, but the proposed method relies on \"prompting a language model\" which indirectly suggests that the standard language model may not be sufficient for the task.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation of LLMs is mentioned in the abstract, but the proposed method relies on \"prompting a language model\" which indirectly suggests that the standard language model may not be sufficient for the task."
    },
    {
        "title": "Boosting Text Augmentation via Hybrid Instance Filtering Framework",
        "authors": [
            "Heng Yang",
            "Ke Li"
        ],
        "published": "2023",
        "summary": "Text augmentation is an effective technique for addressing the problem of insufficient data in natural language processing. However, existing text augmentation methods tend to focus on few-shot scenarios and usually perform poorly on large public datasets. Our research indicates that existing augmentation methods often generate instances with shifted feature spaces, which leads to a drop in performance on the augmented data (for example, EDA generally loses approximately 2% in aspect-based sentiment classification). To address this problem, we propose a hybrid instance-filtering framework (BoostAug) based on pre-trained language models that can maintain a similar feature space with natural datasets. BoostAug is transferable to existing text augmentation methods (such as synonym substitution and back translation) and significantly improves the augmentation performance by 2-3% in classification accuracy. Our experimental results on three classification tasks and nine public datasets show that BoostAug addresses the performance drop problem and outperforms state-of-the-art text augmentation methods. Additionally, we release the code to help improve existing augmentation methods on large datasets.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.105.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing text augmentation methods often generate instances with shifted feature spaces, which leads to a drop in performance on the augmented data (for example, EDA generally loses approximately 2% in aspect-based sentiment classification).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, existing text augmentation methods often generate instances with shifted feature spaces, which leads to a drop in performance on the augmented data (for example, EDA generally loses approximately 2% in aspect-based sentiment classification).\""
    },
    {
        "title": "Zero-Shot Text Classification via Self-Supervised Tuning",
        "authors": [
            "Chaoqun Liu",
            "Wenxuan Zhang",
            "Guizhen Chen",
            "Xiaobao Wu",
            "Anh Tuan Luu",
            "Chip Hong Chang",
            "Lidong Bing"
        ],
        "published": "2023",
        "summary": "Existing solutions to zero-shot text classification either conduct prompting with pre-trained language models, which is sensitive to the choices of templates, or rely on large-scale annotated data of relevant tasks for meta-tuning. In this work, we propose a new paradigm based on self-supervised learning to solve zero-shot text classification tasks by tuning the language models with unlabeled data, called self-supervised tuning. By exploring the inherent structure of free texts, we propose a new learning objective called first sentence prediction to bridge the gap between unlabeled data and text classification tasks. After tuning the model to learn to predict the first sentence in a paragraph based on the rest, the model is able to conduct zero-shot inference on unseen tasks such as topic classification and sentiment analysis. Experimental results show that our model outperforms the state-of-the-art baselines on 7 out of 10 tasks. Moreover, the analysis reveals that our model is less sensitive to the prompt design. Our code and pre-trained models are publicly available at https://github.com/DAMO-NLP-SG/SSTuning.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.110.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing solutions to zero-shot text classification either conduct prompting with pre-trained language models, which is sensitive to the choices of templates\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing solutions to zero-shot text classification either conduct prompting with pre-trained language models, which is sensitive to the choices of templates\""
    },
    {
        "title": "Logical Transformers: Infusing Logical Structures into Pre-Trained Language Models",
        "authors": [
            "Borui Wang",
            "Qiuyuan Huang",
            "Budhaditya Deb",
            "Aaron Halfaker",
            "Liqun Shao",
            "Daniel McDuff",
            "Ahmed Hassan Awadallah",
            "Dragomir Radev",
            "Jianfeng Gao"
        ],
        "published": "2023",
        "summary": "Natural language contains rich logical structures and logical information, and correctly detecting and accurately understanding these logical structures and information underlying natural language texts is very crucial for NLP models’ performance on many important NLU and NLG tasks. Existing pre-trained language models based on the transformer architecture mostly adopt a classical design for constructing their input embeddings that ignores the logical structures underlying natural language texts, thus limiting their ability to better capture and encode key logical information in the input sequences. To overcome such limitations, in this paper we first propose a novel approach to construct logic-aware input embeddings for transformer language models through a combination of logic detection, logic mapping and hierarchical logical projections, and then develop a corresponding new modeling paradigm that can upgrade existing transformer language models into logical transformers to boost their performance on different NLU and NLG tasks. Our empirical experiments on four important and challenging NLU and NLG tasks demonstrate that our proposed logical transformer language models can achieve superior performance over their baseline transformer models through a deeper understanding of the logical structures of texts.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.111.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing pre-trained language models based on the transformer architecture mostly adopt a classical design for constructing their input embeddings that ignores the logical structures underlying natural language texts, thus limiting their ability to better capture and encode key logical information in the input sequences.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Existing pre-trained language models based on the transformer architecture mostly adopt a classical design for constructing their input embeddings that ignores the logical structures underlying natural language texts, thus limiting their ability to better capture and encode key logical information in the input sequences.\""
    },
    {
        "title": "Large Language Models with Controllable Working Memory",
        "authors": [
            "Daliang Li",
            "Ankit Singh Rawat",
            "Manzil Zaheer",
            "Xin Wang",
            "Michal Lukasik",
            "Andreas Veit",
            "Felix Yu",
            "Sanjiv Kumar"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have led to a series of breakthroughs in natural language processing (NLP), partly owing to the massive amounts of world knowledge they memorize during pretraining. While many downstream applications provide the model with an informational context to aid its underlying task, how the model’s world knowledge interacts with the factual information presented in the context remains under explored. As a desirable behavior, an LLM should give precedence to the context whenever it contains task-relevant information that conflicts with the model’s memorized knowledge. This enables model predictions to be grounded in the context, which then facilitates updating specific model predictions without frequently retraining the model. By contrast, when the context is irrelevant to the task, the model should ignore it and fall back on its internal knowledge. In this paper, we undertake a first joint study of the aforementioned two properties, namely controllability and robustness, in the context of LLMs. We demonstrate that state-of-the-art T5 and PaLM models (both pretrained and finetuned) could exhibit low controllability and robustness that does not improve with increasing the model size. As a solution, we propose a simple yet effective method – knowledge aware finetuning (KAFT) – to strengthen both controllability and robustness by injecting counterfactual and irrelevant contexts to standard supervised datasets. Our comprehensive evaluation showcases the utility of KAFT across model architectures and sizes.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.112.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We demonstrate that state-of-the-art T5 and PaLM models (both pretrained and finetuned) could exhibit low controllability and robustness that does not improve with increasing the model size.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We demonstrate that state-of-the-art T5 and PaLM models (both pretrained and finetuned) could exhibit low controllability and robustness that does not improve with increasing the model size.\""
    },
    {
        "title": "A Language-First Approach for Procedure Planning",
        "authors": [
            "Jiateng Liu",
            "Sha Li",
            "Zhenhailong Wang",
            "Manling Li",
            "Heng Ji"
        ],
        "published": "2023",
        "summary": "Procedure planning, or the ability to predict a series of steps that can achieve a given goal conditioned on the current observation, is critical for building intelligent embodied agents that can assist users in everyday tasks. Encouraged by the recent success of language models (LMs) for zero-shot and few-shot planning, we hypothesize that LMs may be equipped with stronger priors for planning compared to their visual counterparts. To this end, we propose a language-first procedure planning framework with a modularized design: we first align the current and goal observations with corresponding steps and then use a pre-trained LM to predict the intermediate steps. Under this framework, we find that using an image captioning model for alignment can already match state-of-the-art performance and by designing a double retrieval model conditioned over current and goal observations jointly, we can achieve large improvements (19.2%-98.9% relatively higher success rate than state-of-the-art) on both COIN and CrossTask benchmarks. Our work verifies the planning ability of LMs and demonstrates how LMs can serve as a powerful “reasoning engine” even when the input is provided in another modality.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.122.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the abstract only discusses the potential benefits and applications of LMs in procedure planning.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the abstract only discusses the potential benefits and applications of LMs in procedure planning."
    },
    {
        "title": "TempLM: Distilling Language Models into Template-Based Generators",
        "authors": [
            "Tianyi Zhang",
            "Mina Lee",
            "Xiang Lisa Li",
            "Ende Shen",
            "Tatsunori Hashimoto"
        ],
        "published": "2023",
        "summary": "While pretrained language models (PLMs) have greatly improved text generation, they have also been known to produce unfaithful or inappropriate content. In contrast, classic template-based systems provide strong guarantees of faithfulness at the cost of fluency. We propose TempLM, which achieves the best of both worlds by distilling a PLM into a template-based generator. On the E2E and SynthBio data-to-text datasets, we show that TempLM is more faithful than the original PLM and is more fluent than prior template systems. Notably, on an out-of-domain evaluation, TempLM reduces a finetuned BART model’s unfaithfulness rate from 83% to 0%. In a human study, we find that TempLM’s templates substantially improve upon human-written ones in BERTScore.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.124.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While pretrained language models (PLMs) have greatly improved text generation, they have also been known to produce unfaithful or inappropriate content.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While pretrained language models (PLMs) have greatly improved text generation, they have also been known to produce unfaithful or inappropriate content.\""
    },
    {
        "title": "Incorporating Graph Information in Transformer-based AMR Parsing",
        "authors": [
            "Pavlo Vasylenko",
            "Pere Lluís Huguet Cabot",
            "Abelardo Carlos Martínez Lorenzo",
            "Roberto Navigli"
        ],
        "published": "2023",
        "summary": "Abstract Meaning Representation (AMR) is a Semantic Parsing formalism that aims at providing a semantic graph abstraction representing a given text. Current approaches are based on autoregressive language models such as BART or T5, fine-tuned through Teacher Forcing to obtain a linearized version of the AMR graph from a sentence. In this paper, we present LeakDistill, a model and method that explores a modification to the Transformer architecture, using structural adapters to explicitly incorporate graph information into the learned representations and improve AMR parsing performance. Our experiments show how, by employing word-to-node alignment to embed graph structural information into the encoder at training time, we can obtain state-of-the-art AMR parsing through self-knowledge distillation, even without the use of additional data. We release the code at [http://www.github.com/sapienzanlp/LeakDistill](http://www.github.com/sapienzanlp/LeakDistill).",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.125.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Current approaches are based on autoregressive language models such as BART or T5, fine-tuned through Teacher Forcing to obtain a linearized version of the AMR graph from a sentence.\"\n\nThis paper mentions LLMs (autoregressive language models) but does not discuss their limitations, it rather presents a new approach to improve AMR parsing performance.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Current approaches are based on autoregressive language models such as BART or T5, fine-tuned through Teacher Forcing to obtain a linearized version of the AMR graph from a sentence.\"\n\nThis paper mentions LLMs (autoregressive language models) but does not discuss their limitations, it rather presents a new approach to improve AMR parsing performance."
    },
    {
        "title": "Structural Contrastive Pretraining for Cross-Lingual Comprehension",
        "authors": [
            "Nuo Chen",
            "Linjun Shou",
            "Tengtao Song",
            "Ming Gong",
            "Jian Pei",
            "Jianhui Chang",
            "Daxin Jiang",
            "Jia Li"
        ],
        "published": "2023",
        "summary": "To present, multilingual language models trained using various pre-training tasks like mask language modeling (MLM) have yielded encouraging results on a wide range of downstream tasks. Despite the promising performances, structural knowledge in cross-lingual corpus is less explored in current works, leading to the semantic misalignment. In this paper, we propose a new pre-training task named Structural Contrast Pretraining (SCP) to align the structural words in a parallel sentence, enhancing the models’ ability to comprehend cross-lingual representations. Concretely, each structural word in source and target languages is regarded as a positive pair in SCP. Since contrastive learning compares positive and negative pairs, an increase in the frequency of negative pairings could enhance the performance of the resulting model. Therefore, we further propose Cross-lingual Momentum Contrast (CL-MoCo) to increase the number of negative pairs by maintaining a large size of the queue. CL-MoCo extends the original Moco approach into cross-lingual training and jointly optimizes the source-to-target language and target-to-source language representations, resulting in a more suitable encoder for cross-lingual transfer. We conduct extensive experiments to validate the proposed approach on three cross-lingual tasks across five datasets such as MLQA, WikiAnn, etc, and results prove the effectiveness of our method.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.128.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the promising performances, structural knowledge in cross-lingual corpus is less explored in current works, leading to the semantic misalignment.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite the promising performances, structural knowledge in cross-lingual corpus is less explored in current works, leading to the semantic misalignment.\""
    },
    {
        "title": "Reducing Sensitivity on Speaker Names for Text Generation from Dialogues",
        "authors": [
            "Qi Jia",
            "Haifeng Tang",
            "Kenny Zhu"
        ],
        "published": "2023",
        "summary": "Changing speaker names consistently throughout a dialogue should not affect its meaning and corresponding outputs for text generation from dialogues. However, pre-trained language models, serving as the backbone for dialogue-processing tasks, have shown to be sensitive to nuances. This may result in unfairness in real-world applications. No comprehensive analysis of this problem has been done in the past. In this work, we propose to quantitatively measure a model’s sensitivity on speaker names, and comprehensively evaluate a number of known methods for reducing speaker name sensitivity, including a novel approach of our own. Extensive experiments on multiple datasets provide a benchmark for this problem and show the favorable performance of our approach in sensitivity reduction and quality of generation.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.129.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, pre-trained language models... have shown to be sensitive to nuances... This may result in unfairness in real-world applications.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, pre-trained language models... have shown to be sensitive to nuances... This may result in unfairness in real-world applications.\""
    },
    {
        "title": "Exploiting Abstract Meaning Representation for Open-Domain Question Answering",
        "authors": [
            "Cunxiang Wang",
            "Zhikun Xu",
            "Qipeng Guo",
            "Xiangkun Hu",
            "Xuefeng Bai",
            "Zheng Zhang",
            "Yue Zhang"
        ],
        "published": "2023",
        "summary": "The Open-Domain Question Answering (ODQA) task involves retrieving and subsequently generating answers from fine-grained relevant passages within a database. Current systems leverage Pretrained Language Models (PLMs) to model the relationship between questions and passages. However, the diversity in surface form expressions can hinder the model’s ability to capture accurate correlations, especially within complex contexts. Therefore, we utilize Abstract Meaning Representation (AMR) graphs to assist the model in understanding complex semantic information. We introduce a method known as Graph-as-Token (GST) to incorporate AMRs into PLMs. Results from Natural Questions (NQ) and TriviaQA (TQ) demonstrate that our GST method can significantly improve performance, resulting in up to 2.44/3.17 Exact Match score improvements on NQ/TQ respectively. Furthermore, our method enhances robustness and outperforms alternative Graph Neural Network (GNN) methods for integrating AMRs. To the best of our knowledge, we are the first to employ semantic graphs in ODQA.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.131.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the diversity in surface form expressions can hinder the model’s ability to capture accurate correlations, especially within complex contexts.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the diversity in surface form expressions can hinder the model’s ability to capture accurate correlations, especially within complex contexts.\""
    },
    {
        "title": "Nonparametric Masked Language Modeling",
        "authors": [
            "Sewon Min",
            "Weijia Shi",
            "Mike Lewis",
            "Xilun Chen",
            "Wen-tau Yih",
            "Hannaneh Hajishirzi",
            "Luke Zettlemoyer"
        ],
        "published": "2023",
        "summary": "Existing language models (LMs) predict tokens with a softmax over a finite vocabulary, which can make it difficult to predict rare tokens or phrases. We introduce NPM, the first nonparametric masked language model that replaces this softmax with a nonparametric distribution over every phrase in a reference corpus. NPM fills in the [MASK] solely from retrieving a token from a text corpus. We show that NPM can be efficiently trained with a contrastive objective and an in-batch approximation to full corpus retrieval. Zero-shot evaluation on 16 tasks including classification, fact probing and question answering demonstrates that NPM outperforms significantly larger parametric models, either with or without a retrieve-and-generate approach. It is particularly better at dealing with rare patterns (word senses or facts) and predicting rare or nearly unseen words (e.g., non-Latin script). We release the model and code at github.com/facebookresearch/NPM.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.132.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing language models (LMs) predict tokens with a softmax over a finite vocabulary, which can make it difficult to predict rare tokens or phrases.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing language models (LMs) predict tokens with a softmax over a finite vocabulary, which can make it difficult to predict rare tokens or phrases.\""
    },
    {
        "title": "Speaking Multiple Languages Affects the Moral Bias of Language Models",
        "authors": [
            "Katharina Hämmerl",
            "Bjoern Deiseroth",
            "Patrick Schramowski",
            "Jindřich Libovický",
            "Constantin Rothkopf",
            "Alexander Fraser",
            "Kristian Kersting"
        ],
        "published": "2023",
        "summary": "Pre-trained multilingual language models (PMLMs) are commonly used when dealing with data from multiple languages and cross-lingual transfer. However, PMLMs are trained on varying amounts of data for each language. In practice this means their performance is often much better on English than many other languages. We explore to what extent this also applies to moral norms. Do the models capture moral norms from English and impose them on other languages? Do the models exhibit random and thus potentially harmful beliefs in certain languages? Both these issues could negatively impact cross-lingual transfer and potentially lead to harmful outcomes. In this paper, we (1) apply the MORALDIRECTION framework to multilingual models, comparing results in German, Czech, Arabic, Chinese, and English, (2) analyse model behaviour on filtered parallel subtitles corpora, and (3) apply the models to a Moral Foundations Questionnaire, comparing with human responses from different countries. Our experiments demonstrate that, indeed, PMLMs encode differing moral biases, but these do not necessarily correspond to cultural differences or commonalities in human opinions. We release our code and models.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.134.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, PMLMs are trained on varying amounts of data for each language. In practice this means their performance is often much better on English than many other languages.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, PMLMs are trained on varying amounts of data for each language. In practice this means their performance is often much better on English than many other languages.\""
    },
    {
        "title": "NormNet: Normalize Noun Phrases for More Robust NLP",
        "authors": [
            "Minlong Peng",
            "Mingming Sun"
        ],
        "published": "2023",
        "summary": "A critical limitation of deep NLP models is their over-fitting over spurious features. Previous work has proposed several approaches to debunk such features and reduce their impact on the learned models. In this work, a normalization strategy is proposed to eliminate the false features caused by the textual surfaces of noun phrases. The motivation for this strategy is that noun phrases often play the role of slots in textual expressions and their exact forms are often not that important for performing the final task. As an intuitive example, consider the expression ”x like eating y\". There are a huge number of suitable instantiations for x and y in the locale. However, humans can already infer the sentiment polarity of x toward y without knowing their exact forms.Based on this intuition, we introduce NormNet, a pretrained language model based network, to implement the normalization strategy. NormNet learns to replace as many noun phrases in the input sentence as possible with pre-defined base forms. The output of NormNet is then fed as input to a prompt-based learning model to perform label prediction. To evaluate the effectiveness of our strategy, we conducted experimental studies on several tasks, including aspect sentiment classification (ASC), semantic text similarity (STS), and natural language inference (NLI). The experimental results confirm the effectiveness of our strategy.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.136.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"A critical limitation of deep NLP models is their over-fitting over spurious features.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"A critical limitation of deep NLP models is their over-fitting over spurious features.\""
    },
    {
        "title": "This prompt is measuring <mask>: evaluating bias evaluation in language models",
        "authors": [
            "Seraphina Goldfarb-Tarrant",
            "Eddie Ungless",
            "Esma Balkir",
            "Su Lin Blodgett"
        ],
        "published": "2023",
        "summary": "Bias research in NLP seeks to analyse models for social biases, thus helping NLP practitioners uncover, measure, and mitigate social harms. We analyse the body of work that uses prompts and templates to assess bias in language models. We draw on a measurement modelling framework to create a taxonomy of attributes that capture what a bias test aims to measure and how that measurement is carried out. By applying this taxonomy to 90 bias tests, we illustrate qualitatively and quantitatively that core aspects of bias test conceptualisations and operationalisations are frequently unstated or ambiguous, carry implicit assumptions, or be mismatched. Our analysis illuminates the scope of possible bias types the field is able to measure, and reveals types that are as yet under-researched. We offer guidance to enable the community to explore a wider section of the possible bias space, and to better close the gap between desired outcomes and experimental design, both for bias and for evaluating language models more broadly.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.139.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"core aspects of bias test conceptualisations and operationalisations are frequently unstated or ambiguous, carry implicit assumptions, or be mismatched.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"core aspects of bias test conceptualisations and operationalisations are frequently unstated or ambiguous, carry implicit assumptions, or be mismatched.\""
    },
    {
        "title": "AVATAR: A Parallel Corpus for Java-Python Program Translation",
        "authors": [
            "Wasi Uddin Ahmad",
            "Md Golam Rahman Tushar",
            "Saikat Chakraborty",
            "Kai-Wei Chang"
        ],
        "published": "2023",
        "summary": "Program translation refers to migrating source code from one programming language to another. It has tremendous practical value in software development, as porting software across languages is time-consuming and costly. Automating program translation is of paramount importance in software migration, and recently researchers explored unsupervised approaches due to the unavailability of parallel corpora. However, the availability of pre-trained language models for programming languages enables supervised fine-tuning with a small number of labeled examples. Therefore, we present AVATAR, a collection of 9,515 programming problems and their solutions written in two popular languages, Java and Python. AVATAR is collected from competitive programming sites, online platforms, and open-source repositories. Furthermore, AVATAR includes unit tests for 250 examples to facilitate functional correctness evaluation. We benchmark several pre-trained language models fine-tuned on AVATAR. Experiment results show that the models lack in generating functionally accurate code.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.143.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Experiment results show that the models lack in generating functionally accurate code.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Experiment results show that the models lack in generating functionally accurate code.\""
    },
    {
        "title": "On Dataset Transferability in Active Learning for Transformers",
        "authors": [
            "Fran Jelenić",
            "Josip Jukić",
            "Nina Drobac",
            "Jan Snajder"
        ],
        "published": "2023",
        "summary": "Active learning (AL) aims to reduce labeling costs by querying the examples most beneficial for model learning. While the effectiveness of AL for fine-tuning transformer-based pre-trained language models (PLMs) has been demonstrated, it is less clear to what extent the AL gains obtained with one model transfer to others. We consider the problem of transferability of actively acquired datasets in text classification and investigate whether AL gains persist when a dataset built using AL coupled with a specific PLM is used to train a different PLM. We link the AL dataset transferability to the similarity of instances queried by the different PLMs and show that AL methods with similar acquisition sequences produce highly transferable datasets regardless of the models used. Additionally, we show that the similarity of acquisition sequences is influenced more by the choice of the AL method than the choice of the model.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.144.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While the effectiveness of AL for fine-tuning transformer-based pre-trained language models (PLMs) has been demonstrated, it is less clear to what extent the AL gains obtained with one model transfer to others.\"\n\nThis abstract mentions a limitation of LLMs in passing, specifically the uncertainty of transferring AL gains from one model to another, but does not elaborate on it further.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While the effectiveness of AL for fine-tuning transformer-based pre-trained language models (PLMs) has been demonstrated, it is less clear to what extent the AL gains obtained with one model transfer to others.\"\n\nThis abstract mentions a limitation of LLMs in passing, specifically the uncertainty of transferring AL gains from one model to another, but does not elaborate on it further."
    },
    {
        "title": "Generate then Select: Open-ended Visual Question Answering Guided by World Knowledge",
        "authors": [
            "Xingyu Fu",
            "Sheng Zhang",
            "Gukyeong Kwon",
            "Pramuditha Perera",
            "Henghui Zhu",
            "Yuhao Zhang",
            "Alexander Hanbo Li",
            "William Yang Wang",
            "Zhiguo Wang",
            "Vittorio Castelli",
            "Patrick Ng",
            "Dan Roth",
            "Bing Xiang"
        ],
        "published": "2023",
        "summary": "The open-ended Visual Question Answering (VQA) task requires AI models to jointly reason over visual and natural language inputs using world knowledge. Recently, pre-trained Language Models (PLM) such as GPT-3 have been applied to the task and shown to be powerful world knowledge sources. However, these methods suffer from low knowledge coverage caused by PLM bias – the tendency to generate certain tokens over other tokens regardless of prompt changes, and high dependency on the PLM quality – only models using GPT-3 can achieve the best result. To address the aforementioned challenges, we propose RASO: a new VQA pipeline that deploys a generate-then-select strategy guided by world knowledge for the first time. Rather than following the de facto standard to train a multi-modal model that directly generates the VQA answer, {pasted macro ‘MODEL’}name first adopts PLM to generate all the possible answers, and then trains a lightweight answer selection model for the correct answer. As proved in our analysis, RASO expands the knowledge coverage from in-domain training data by a large margin. We provide extensive experimentation and show the effectiveness of our pipeline by advancing the state-of-the-art by 4.1% on OK-VQA, without additional computation cost.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.147.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these methods suffer from low knowledge coverage caused by PLM bias – the tendency to generate certain tokens over other tokens regardless of prompt changes, and high dependency on the PLM quality – only models using GPT-3 can achieve the best result.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these methods suffer from low knowledge coverage caused by PLM bias – the tendency to generate certain tokens over other tokens regardless of prompt changes, and high dependency on the PLM quality – only models using GPT-3 can achieve the best result.\""
    },
    {
        "title": "Hence, Socrates is mortal: A Benchmark for Natural Language Syllogistic Reasoning",
        "authors": [
            "Yongkang Wu",
            "Meng Han",
            "Yutao Zhu",
            "Lei Li",
            "Xinyu Zhang",
            "Ruofei Lai",
            "Xiaoguang Li",
            "Yuanhang Ren",
            "Zhicheng Dou",
            "Zhao Cao"
        ],
        "published": "2023",
        "summary": "Syllogistic reasoning, a typical form of deductive reasoning, is a critical capability widely required in natural language understanding tasks, such as text entailment and question answering. To better facilitate research on syllogistic reasoning, we develop a benchmark called SylloBase that differs from existing syllogistic datasets in three aspects: (1) Covering a complete taxonomy of syllogism reasoning patterns; (2) Containing both automatically and manually constructed samples; and (3) Involving both the generation and understanding tasks. We automatically construct 50k template-based syllogism samples by mining syllogism patterns from Wikidata and ConceptNet. To improve our dataset’s naturalness and challenge, we apply GPT-3 to paraphrase the template-based data and further manually rewrite 1,000 samples as the test set. State-of-the-art pre-trained language models can achieve the best generation ROUGE-L of 38.72 by T5 and the best multi-choice accuracy of 72.77% by RoBERTa on SylloBase, which indicates the great challenge of learning diverse syllogistic reasoning types on SylloBase. Our datasets are released at https://github.com/casually-PYlearner/SYLLOBASE.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.148.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"which indicates the great challenge of learning diverse syllogistic reasoning types on SylloBase.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"which indicates the great challenge of learning diverse syllogistic reasoning types on SylloBase.\""
    },
    {
        "title": "Attribute Controlled Dialogue Prompting",
        "authors": [
            "Runcheng Liu",
            "Ahmad Rashid",
            "Ivan Kobyzev",
            "Mehdi Rezagholizadeh",
            "Pascal Poupart"
        ],
        "published": "2023",
        "summary": "Prompt-tuning has become an increasingly popular parameter-efficient method for adapting large pretrained language models to downstream tasks. However, both discrete prompting and continuous prompting assume fixed prompts for all data samples within a task, neglecting the fact that inputs vary greatly in some tasks such as open-domain dialogue generation. In this paper, we present a novel, instance-specific prompt-tuning algorithm for dialogue generation. Specifically, we generate prompts based on instance-level control code, rather than the conversation history, to explore their impact on controlled dialogue generation. Experiments on popular open-domain dialogue datasets, evaluated on both automated metrics and human evaluation, demonstrate that our method is superior to prompting baselines and comparable to fine-tuning with only 5%-6% of total parameters.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.150.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, both discrete prompting and continuous prompting assume fixed prompts for all data samples within a task, neglecting the fact that inputs vary greatly in some tasks such as open-domain dialogue generation.\"\n\nThis evidence suggests that the paper mentions a limitation of traditional prompting methods for LLMs, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, both discrete prompting and continuous prompting assume fixed prompts for all data samples within a task, neglecting the fact that inputs vary greatly in some tasks such as open-domain dialogue generation.\"\n\nThis evidence suggests that the paper mentions a limitation of traditional prompting methods for LLMs, but it is not the primary focus of the paper."
    },
    {
        "title": "Open-World Factually Consistent Question Generation",
        "authors": [
            "Himanshu Maheshwari",
            "Sumit Shekhar",
            "Apoorv Saxena",
            "Niyati Chhaya"
        ],
        "published": "2023",
        "summary": "Question generation methods based on pre-trained language models often suffer from factual inconsistencies and incorrect entities and are not answerable from the input paragraph. Domain shift – where the test data is from a different domain than the training data - further exacerbates the problem of hallucination. This is a critical issue for any natural language application doing question generation. In this work, we propose an effective data processing technique based on de-lexicalization for consistent question generation across domains. Unlike existing approaches for remedying hallucination, the proposed approach does not filter training data and is generic across question-generation models. Experimental results across six benchmark datasets show that our model is robust to domain shift and produces entity-level factually consistent questions without significant impact on traditional metrics.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.151.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Question generation methods based on pre-trained language models often suffer from factual inconsistencies and incorrect entities and are not answerable from the input paragraph. Domain shift – where the test data is from a different domain than the training data - further exacerbates the problem of hallucination.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Question generation methods based on pre-trained language models often suffer from factual inconsistencies and incorrect entities and are not answerable from the input paragraph. Domain shift – where the test data is from a different domain than the training data - further exacerbates the problem of hallucination.\""
    },
    {
        "title": "Contrastive Learning of Sociopragmatic Meaning in Social Media",
        "authors": [
            "Chiyu Zhang",
            "Muhammad Abdul-Mageed",
            "Ganesh Jawahar"
        ],
        "published": "2023",
        "summary": "Recent progress in representation and contrastive learning in NLP has not widely considered the class of sociopragmatic meaning (i.e., meaning in interaction within different language communities). To bridge this gap, we propose a novel framework for learning task-agnostic representations transferable to a wide range of sociopragmatic tasks (e.g., emotion, hate speech, humor, sarcasm). Our framework outperforms other contrastive learning frameworks for both in-domain and out-of-domain data, across both the general and few-shot settings. For example, compared to two popular pre-trained language models, our model obtains an improvement of 11.66 average F1 on 16 datasets when fine-tuned on only 20 training samples per dataset. We also show that our framework improves uniformity and preserves the semantic structure of representations. Our code is available at: https://github.com/UBC-NLP/infodcl",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.152.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"compared to two popular pre-trained language models\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"compared to two popular pre-trained language models\""
    },
    {
        "title": "Noisy Positive-Unlabeled Learning with Self-Training for Speculative Knowledge Graph Reasoning",
        "authors": [
            "Ruijie Wang",
            "Baoyu Li",
            "Yichen Lu",
            "Dachun Sun",
            "Jinning Li",
            "Yuchen Yan",
            "Shengzhong Liu",
            "Hanghang Tong",
            "Tarek Abdelzaher"
        ],
        "published": "2023",
        "summary": "This paper studies speculative reasoning task on real-world knowledge graphs (KG) that contain both false negative issue (i.e., potential true facts being excluded) and false positive issue (i.e., unreliable or outdated facts being included). State-of-the-art methods fall short in the speculative reasoning ability, as they assume the correctness of a fact is solely determined by its presence in KG, making them vulnerable to false negative/positive issues. The new reasoning task is formulated as a noisy Positive-Unlabeled learning problem. We propose a variational framework, namely nPUGraph, that jointly estimates the correctness of both collected and uncollected facts (which we call label posterior) and updates model parameters during training. The label posterior estimation facilitates speculative reasoning from two perspectives. First, it improves the robustness of a label posterior-aware graph encoder against false positive links. Second, it identifies missing facts to provide high-quality grounds of reasoning. They are unified in a simple yet effective self-training procedure. Empirically, extensive experiments on three benchmark KG and one Twitter dataset with various degrees of false negative/positive cases demonstrate the effectiveness of nPUGraph.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.153.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Unsupervised Keyphrase Extraction by Learning Neural Keyphrase Set Function",
        "authors": [
            "Mingyang Song",
            "Haiyun Jiang",
            "Lemao Liu",
            "Shuming Shi",
            "Liping Jing"
        ],
        "published": "2023",
        "summary": "We create a paradigm shift concerning building unsupervised keyphrase extraction systems in this paper. Instead of modeling the relevance between an individual candidate phrase and the document as in the commonly used framework, we formulate the unsupervised keyphrase extraction task as a document-set matching problem from a set-wise perspective, in which the document and the candidate set are globally matched in the semantic space to particularly take into account the interactions among all candidate phrases. Since it is intractable to exactly extract the keyphrase set by the matching function during the inference, we propose an approximate approach, which obtains the candidate subsets via a set extractor agent learned by reinforcement learning. Exhaustive experimental results demonstrate the effectiveness of our model, which outperforms the recent state-of-the-art unsupervised keyphrase extraction baselines by a large margin.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.156.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Diffusion Theory as a Scalpel: Detecting and Purifying Poisonous Dimensions in Pre-trained Language Models Caused by Backdoor or Bias",
        "authors": [
            "Zhiyuan Zhang",
            "Deli Chen",
            "Hao Zhou",
            "Fandong Meng",
            "Jie Zhou",
            "Xu Sun"
        ],
        "published": "2023",
        "summary": "Pre-trained Language Models (PLMs) may be poisonous with backdoors or bias injected by the suspicious attacker during the fine-tuning process. A core challenge of purifying potentially poisonous PLMs is precisely finding poisonous dimensions. To settle this issue, we propose the Fine-purifying approach, which utilizes the diffusion theory to study the dynamic process of fine-tuning for finding potentially poisonous dimensions. According to the relationship between parameter drifts and Hessians of different dimensions, we can detect poisonous dimensions with abnormal dynamics, purify them by resetting them to clean pre-trained weights, and then fine-tune the purified weights on a small clean dataset. To the best of our knowledge, we are the first to study the dynamics guided by the diffusion theory for safety or defense purposes. Experimental results validate the effectiveness of Fine-purifying even with a small clean dataset.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.157.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Pre-trained Language Models (PLMs) may be poisonous with backdoors or bias injected by the suspicious attacker during the fine-tuning process.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Pre-trained Language Models (PLMs) may be poisonous with backdoors or bias injected by the suspicious attacker during the fine-tuning process.\""
    },
    {
        "title": "Retrieving Multimodal Prompts for Generative Visual Question Answering",
        "authors": [
            "Timothy Ossowski",
            "Junjie Hu"
        ],
        "published": "2023",
        "summary": "Recent years have witnessed impressive results of pre-trained vision-language models on knowledge-intensive tasks such as visual question answering (VQA). Despite the recent advances in VQA, existing methods mainly adopt a discriminative formulation that predicts answers within a pre-defined label set, leading to easy overfitting on low-resource domains (e.g., medicine) and poor generalization under domain shift to another dataset. To tackle this limitation, we propose a novel generative model enhanced by multimodal prompt retrieval (MPR) that integrates retrieved prompts and multimodal features to generate answers in free text. Our generative model enables rapid zero-shot dataset adaptation to unseen data distributions and open-set answer labels across datasets. Our experiments on medical VQA tasks show that MPR outperforms its non-retrieval counterpart by up to 30% accuracy points in a few-shot domain adaptation setting.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.158.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"existing methods mainly adopt a discriminative formulation that predicts answers within a pre-defined label set, leading to easy overfitting on low-resource domains (e.g., medicine) and poor generalization under domain shift to another dataset.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"existing methods mainly adopt a discriminative formulation that predicts answers within a pre-defined label set, leading to easy overfitting on low-resource domains (e.g., medicine) and poor generalization under domain shift to another dataset.\""
    },
    {
        "title": "An Investigation of Evaluation Methods in Automatic Medical Note Generation",
        "authors": [
            "Asma Ben Abacha",
            "Wen-wai Yim",
            "George Michalopoulos",
            "Thomas Lin"
        ],
        "published": "2023",
        "summary": "Recent studies on automatic note generation have shown that doctors can save significant amounts of time when using automatic clinical note generation (Knoll et al., 2022). Summarization models have been used for this task to generate clinical notes as summaries of doctor-patient conversations (Krishna et al., 2021; Cai et al., 2022). However, assessing which model would best serve clinicians in their daily practice is still a challenging task due to the large set of possible correct summaries, and the potential limitations of automatic evaluation metrics. In this paper we study evaluation methods and metrics for the automatic generation of clinical notes from medical conversation. In particular, we propose new task-specific metrics and we compare them to SOTA evaluation metrics in text summarization and generation, including: (i) knowledge-graph embedding-based metrics, (ii) customized model-based metrics with domain-specific weights, (iii) domain-adapted/fine-tuned metrics, and (iv) ensemble metrics. To study the correlation between the automatic metrics and manual judgments, we evaluate automatic notes/summaries by comparing the system and reference facts and computing the factual correctness, and the hallucination and omission rates for critical medical facts. This study relied on seven datasets manually annotated by domain experts. Our experiments show that automatic evaluation metrics can have substantially different behaviors on different types of clinical notes datasets. However, the results highlight one stable subset of metrics as the most correlated with human judgments with a relevant aggregation of different evaluation criteria.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.161.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Focusing, Bridging and Prompting for Few-shot Nested Named Entity Recognition",
        "authors": [
            "Yuanyuan Xu",
            "Zeng Yang",
            "Linhai Zhang",
            "Deyu Zhou",
            "Tiandeng Wu",
            "Rong Zhou"
        ],
        "published": "2023",
        "summary": "Few-shot named entity recognition (NER), identifying named entities with a small number of labeled data, has attracted much attention. Frequently, entities are nested within each other. However, most of the existing work on few-shot NER addresses flat entities instead of nested entities. To tackle nested NER in a few-shot setting, it is crucial to utilize the limited labeled data to mine unique features of nested entities, such as the relationship between inner and outer entities and contextual position information. Therefore, in this work, we propose a novel method based on focusing, bridging and prompting for few-shot nested NER without using source domain data. Both focusing and bridging components provide accurate candidate spans for the prompting component. The prompting component leverages the unique features of nested entities to classify spans based on soft prompts and contrastive learning. Experimental results show that the proposed approach achieves state-of-the-art performance consistently on the four benchmark datasets (ACE2004, ACE2005, GENIA and KBP2017) and outperforms several competing baseline models on F1-score by 9.33% on ACE2004, 6.17% on ACE2005, 9.40% on GENIA and 5.12% on KBP2017 on the 5-shot setting.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.164.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Large Language Models are Built-in Autoregressive Search Engines",
        "authors": [
            "Noah Ziems",
            "Wenhao Yu",
            "Zhihan Zhang",
            "Meng Jiang"
        ],
        "published": "2023",
        "summary": "Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool. However, the training cost of such autoregressive search engines rises sharply as the number of candidate documents increases. In this paper, we find that large language models (LLMs) can follow human instructions to directly generate URLs for document retrieval. Surprisingly, when providing a few Query-URL pairs as in-context demonstrations, LLMs can generate Web URLs where nearly 90% of the corresponding documents contain correct answers to open-domain questions. In this way, LLMs can be thought of as built-in search engines, since they have not been explicitly trained to map questions to document identifiers. Experiments demonstrate that our method can consistently achieve better retrieval performance than existing retrieval approaches by a significant margin on three open-domain question answering benchmarks, under both zero and few-shot settings. The code for this work can be found at https://github.com/Ziems/llm-url.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.167.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them.\""
    },
    {
        "title": "From chocolate bunny to chocolate crocodile: Do Language Models Understand Noun Compounds?",
        "authors": [
            "Albert Coil",
            "Vered Shwartz"
        ],
        "published": "2023",
        "summary": "Noun compound interpretation is the task of expressing a noun compound (e.g. chocolate bunny) in a free-text paraphrase that makes the relationship between the constituent nouns explicit (e.g. bunny-shaped chocolate). We propose modifications to the data and evaluation setup of the standard task (Hendrickx et al., 2013), and show that GPT-3 solves it almost perfectly. We then investigate the task of noun compound conceptualization, i.e. paraphrasing a novel or rare noun compound. E.g., chocolate crocodile is a crocodile-shaped chocolate. This task requires creativity, commonsense, and the ability to generalize knowledge about similar concepts. While GPT-3’s performance is not perfect, it is better than that of humans—likely thanks to its access to vast amounts of knowledge, and because conceptual processing is effortful for people (Connell and Lynott, 2012). Finally, we estimate the extent to which GPT-3 is reasoning about the world vs. parroting its training data. We find that the outputs from GPT-3 often have significant overlap with a large web corpus, but that the parroting strategy is less beneficial for novel noun compounds.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.169.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While GPT-3’s performance is not perfect\", \"We find that the outputs from GPT-3 often have significant overlap with a large web corpus, but that the parroting strategy is less beneficial for novel noun compounds.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While GPT-3’s performance is not perfect\", \"We find that the outputs from GPT-3 often have significant overlap with a large web corpus, but that the parroting strategy is less beneficial for novel noun compounds.\""
    },
    {
        "title": "Playing the Part of the Sharp Bully: Generating Adversarial Examples for Implicit Hate Speech Detection",
        "authors": [
            "Nicolás Benjamín Ocampo",
            "Elena Cabrio",
            "Serena Villata"
        ],
        "published": "2023",
        "summary": "Research on abusive content detection on social media has primarily focused on explicit forms of hate speech (HS), that are often identifiable by recognizing hateful words and expressions. Messages containing linguistically subtle and implicit forms of hate speech still constitute an open challenge for automatic hate speech detection. In this paper, we propose a new framework for generating adversarial implicit HS short-text messages using Auto-regressive Language Models. Moreover, we propose a strategy to group the generated implicit messages in complexity levels (EASY, MEDIUM, and HARD categories) characterizing how challenging these messages are for supervised classifiers. Finally, relying on (Dinan et al., 2019; Vidgen et al., 2021), we propose a “build it, break it, fix it”, training scheme using HARD messages showing how iteratively retraining on HARD messages substantially leverages SOTA models’ performances on implicit HS benchmarks.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.173.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicitly mentioned in the abstract, but the fact that the paper aims to generate adversarial examples to improve hate speech detection models implies that current LLMs may struggle with detecting implicit hate speech.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None explicitly mentioned in the abstract, but the fact that the paper aims to generate adversarial examples to improve hate speech detection models implies that current LLMs may struggle with detecting implicit hate speech."
    },
    {
        "title": "Subword Segmental Machine Translation: Unifying Segmentation and Target Sentence Generation",
        "authors": [
            "Francois Meyer",
            "Jan Buys"
        ],
        "published": "2023",
        "summary": "Subword segmenters like BPE operate as a preprocessing step in neural machine translation and other (conditional) language models. They are applied to datasets before training, so translation or text generation quality relies on the quality of segmentations. We propose a departure from this paradigm, called subword segmental machine translation (SSMT). SSMT unifies subword segmentation and MT in a single trainable model. It learns to segment target sentence words while jointly learning to generate target sentences. To use SSMT during inference we propose dynamic decoding, a text generation algorithm that adapts segmentations as it generates translations. Experiments across 6 translation directions show that SSMT improves chrF scores for morphologically rich agglutinative languages. Gains are strongest in the very low-resource scenario. SSMT also learns subwords that are closer to morphemes compared to baselines and proves more robust on a test set constructed for evaluating morphological compositional generalisation.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.175.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Subword segmenters like BPE operate as a preprocessing step in neural machine translation and other (conditional) language models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Subword segmenters like BPE operate as a preprocessing step in neural machine translation and other (conditional) language models.\""
    },
    {
        "title": "AraMUS: Pushing the Limits of Data and Model Scale for Arabic Natural Language Processing",
        "authors": [
            "Asaad Alghamdi",
            "Xinyu Duan",
            "Wei Jiang",
            "Zhenhai Wang",
            "Yimeng Wu",
            "Qingrong Xia",
            "Zhefeng Wang",
            "Yi Zheng",
            "Mehdi Rezagholizadeh",
            "Baoxing Huai",
            "Peilun Cheng",
            "Abbas Ghaddar"
        ],
        "published": "2023",
        "summary": "Developing monolingual large Pre-trained Language Models (PLMs) is shown to be very successful in handling different tasks in Natural Language Processing (NLP). In this work, we present AraMUS, the largest Arabic PLM with 11B parameters trained on 529GB of high-quality Arabic textual data. AraMUS achieves state-of-the-art performances on a diverse set of Arabic classification and generative tasks. Moreover, AraMUS shows impressive few-shot learning abilities compared with the best existing Arabic PLMs.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.181.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper does not mention any limitations of LLMs in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper does not mention any limitations of LLMs in the abstract."
    },
    {
        "title": "Leveraging Explicit Procedural Instructions for Data-Efficient Action Prediction",
        "authors": [
            "Julia White",
            "Arushi Raghuvanshi",
            "Yada Pruksachatkun"
        ],
        "published": "2023",
        "summary": "Task-oriented dialogues often require agents to enact complex, multi-step procedures in order to meet user requests. While large language models have found success automating these dialogues in constrained environments, their widespread deployment is limited by the substantial quantities of task-specific data required for training. The following paper presents a data-efficient solution to constructing dialogue systems, leveraging explicit instructions derived from agent guidelines, such as company policies or customer service manuals. Our proposed Knowledge-Augmented Dialogue System (KADS) combines a large language model with a knowledge retrieval module that pulls documents outlining relevant procedures from a predefined set of policies, given a user-agent interaction. To train this system, we introduce a semi-supervised pre-training scheme that employs dialogue-document matching and action-oriented masked language modeling with partial parameter freezing. We evaluate the effectiveness of our approach on prominent task-oriented dialogue datasets, Action-Based Conversations Dataset and Schema-Guided Dialogue, for two dialogue tasks: action state tracking and workflow discovery. Our results demonstrate that procedural knowledge augmentation improves accuracy predicting in- and out-of-distribution actions while preserving high performance in settings with low or sparse data.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.182.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While large language models have found success automating these dialogues in constrained environments, their widespread deployment is limited by the substantial quantities of task-specific data required for training.\"\n\nThis paper mentions a limitation of LLMs, specifically that they require substantial quantities of task-specific data for training, but does not explore this limitation in depth.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2**\nEvidence: \"While large language models have found success automating these dialogues in constrained environments, their widespread deployment is limited by the substantial quantities of task-specific data required for training.\"\n\nThis paper mentions a limitation of LLMs, specifically that they require substantial quantities of task-specific data for training, but does not explore this limitation in depth."
    },
    {
        "title": "Learning to Predict Persona Information for Dialogue Personalization without Explicit Persona Description",
        "authors": [
            "Wangchunshu Zhou",
            "Qifei Li",
            "Chenle Li"
        ],
        "published": "2023",
        "summary": "Personalizing dialogue agents is important for dialogue systems to generate more specific,consistent, and engaging responses. However, most current dialogue personalization approaches rely on explicit persona descriptions during inference, which severely restricts its application. In this paper, we propose a novel approach that learns to predict persona information based on the dialogue history to personalize the dialogue agent without relying on any explicit persona descriptions during inference. Experimental results on the PersonaChat dataset show that the proposed method can improve the consistency of generated responses when conditioning on the predicted profile of the dialogue agent (i.e. “self persona”), and improve the engagingness of the generated responses when conditioning on the predicted persona of the dialogue partner (i.e. “their persona”). We also find that a trained persona prediction model can be successfully transferred to other datasets and help generate more relevant responses.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.186.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Exploring the Relationship between Alignment and Cross-lingual Transfer in Multilingual Transformers",
        "authors": [
            "Felix Gaschi",
            "Patricio Cerda",
            "Parisa Rastin",
            "Yannick Toussaint"
        ],
        "published": "2023",
        "summary": "Without any explicit cross-lingual training data, multilingual language models can achieve cross-lingual transfer. One common way to improve this transfer is to perform realignment steps before fine-tuning, i.e., to train the model to build similar representations for pairs of words from translated sentences. But such realignment methods were found to not always improve results across languages and tasks, which raises the question of whether aligned representations are truly beneficial for cross-lingual transfer. We provide evidence that alignment is actually significantly correlated with cross-lingual transfer across languages, models and random seeds. We show that fine-tuning can have a significant impact on alignment, depending mainly on the downstream task and the model. Finally, we show that realignment can, in some instances, improve cross-lingual transfer, and we identify conditions in which realignment methods provide significant improvements. Namely, we find that realignment works better on tasks for which alignment is correlated with cross-lingual transfer when generalizing to a distant language and with smaller models, as well as when using a bilingual dictionary rather than FastAlign to extract realignment pairs. For example, for POS-tagging, between English and Arabic, realignment can bring a +15.8 accuracy improvement on distilmBERT, even outperforming XLM-R Large by 1.7. We thus advocate for further research on realignment methods for smaller multilingual models as an alternative to scaling.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.189.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"But such realignment methods were found to not always improve results across languages and tasks, which raises the question of whether aligned representations are truly beneficial for cross-lingual transfer.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"But such realignment methods were found to not always improve results across languages and tasks, which raises the question of whether aligned representations are truly beneficial for cross-lingual transfer.\""
    },
    {
        "title": "Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming",
        "authors": [
            "Hanlin Zhang",
            "Jiani Huang",
            "Ziyang Li",
            "Mayur Naik",
            "Eric Xing"
        ],
        "published": "2023",
        "summary": "Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality. In this work, we tackle this challenge through the lens of symbolic programming. We propose DSR-LM, a Differentiable Symbolic Reasoning framework where pre-trained LMs govern the perception of factual knowledge, and a symbolic module performs deductive reasoning. In contrast to works that rely on hand-crafted logic rules, our differentiable symbolic reasoning framework efficiently learns weighted rules and applies semantic loss to further improve LMs. DSR-LM is scalable, interpretable, and allows easy integration of prior knowledge, thereby supporting extensive symbolic programming to robustly derive a logical conclusion. The results of our experiments suggest that DSR-LM improves the logical reasoning abilities of pre-trained language models, resulting in a significant increase in accuracy of over 20% on deductive reasoning benchmarks. Furthermore, DSR-LM outperforms a variety of competitive baselines when faced with systematic changes in sequence length.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.191.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (struggling with logical reasoning) but does not explore it in depth and instead focuses on the proposed solution (DSR-LM framework) to address this limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (struggling with logical reasoning) but does not explore it in depth and instead focuses on the proposed solution (DSR-LM framework) to address this limitation."
    },
    {
        "title": "Eliciting Affective Events from Language Models by Multiple View Co-prompting",
        "authors": [
            "Yuan Zhuang",
            "Ellen Riloff"
        ],
        "published": "2023",
        "summary": "Prior research on affective event classification showed that exploiting weakly labeled data for training can improve model performance. In this work, we propose a simpler and more effective approach for generating training data by automatically acquiring and labeling affective events with Multiple View Co-prompting, which leverages two language model prompts that provide independent views of an event. The approach starts with a modest amount of gold data and prompts pre-trained language models to generate new events. Next, information about the probable affective polarity of each event is collected from two complementary language model prompts and jointly used to assign polarity labels. Experimental results on two datasets show that the newly acquired events improve a state-of-the-art affective event classifier. We also present analyses which show that using multiple views produces polarity labels of higher quality than either view on its own.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.199.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the abstract does not mention any limitations of LLMs, it only discusses a method for improving a state-of-the-art affective event classifier using pre-trained language models.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the abstract does not mention any limitations of LLMs, it only discusses a method for improving a state-of-the-art affective event classifier using pre-trained language models."
    },
    {
        "title": "ZeroAE: Pre-trained Language Model based Autoencoder for Transductive Zero-shot Text Classification",
        "authors": [
            "Kaihao Guo",
            "Hang Yu",
            "Cong Liao",
            "Jianguo Li",
            "Haipeng Zhang"
        ],
        "published": "2023",
        "summary": "Many text classification tasks require handling unseen domains with plenty of unlabeled data, thus giving rise to the self-adaption or the so-called transductive zero-shot learning (TZSL) problem. However, current methods based solely on encoders or decoders overlook the possibility that these two modules may promote each other. As a first effort to bridge this gap, we propose an autoencoder named ZeroAE. Specifically, the text is encoded with two separate BERT-based encoders into two disentangled spaces, i.e., label-relevant (for classification) and label-irrelevant respectively. The two latent spaces are then decoded by prompting GPT-2 to recover the text as well as to further generate text with labels in the unseen domains to train the encoder in turn. To better exploit the unlabeled data, a novel indirect uncertainty-aware sampling (IUAS) approach is proposed to train ZeroAE. Extensive experiments show that ZeroAE largely surpasses the SOTA methods by 15.93% and 8.70% on average respectively in the label-partially-unseen and label-fully-unseen scenario. Notably, the label-fully-unseen ZeroAE even possesses superior performance to the label-partially-unseen SOTA methods.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.200.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but \"GPT-2\" is mentioned as a component used in the proposed method, indicating the involvement of LLMs without discussing their limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but \"GPT-2\" is mentioned as a component used in the proposed method, indicating the involvement of LLMs without discussing their limitations."
    },
    {
        "title": "PRAM: An End-to-end Prototype-based Representation Alignment Model for Zero-resource Cross-lingual Named Entity Recognition",
        "authors": [
            "Yucheng Huang",
            "Wenqiang Liu",
            "Xianli Zhang",
            "Jun Lang",
            "Tieliang Gong",
            "Chen Li"
        ],
        "published": "2023",
        "summary": "Zero-resource cross-lingual named entity recognition (ZRCL-NER) aims to leverage rich labeled source language data to address the NER problem in the zero-resource target language. Existing methods are built either based on data transfer or representation transfer. However, the former usually leads to additional computation costs, and the latter lacks explicit optimization specific to the NER task. To overcome the above limitations, we propose a novel prototype-based representation alignment model (PRAM) for the challenging ZRCL-NER task. PRAM models the cross-lingual (CL) NER task and transfers knowledge from source languages to target languages in a unified neural network, and performs end-to-end training, avoiding additional computation costs. Moreover, PRAM borrows the CL inference ability of multilingual language models and enhances it with a novel training objective—attribution-prediction consistency (APC)—for explicitly enforcing the entity-level alignment between entity representations and predictions, as well as that across languages using prototypes as bridges. The experimental results show that PRAM significantly outperforms existing state-of-the-art methods, especially in some challenging scenarios.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.201.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the former usually leads to additional computation costs, and the latter lacks explicit optimization specific to the NER task.\"\n\nThis rating is given because the abstract mentions limitations of existing methods, which are related to multilingual language models, but does not extensively discuss these limitations and instead focuses on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the former usually leads to additional computation costs, and the latter lacks explicit optimization specific to the NER task.\"\n\nThis rating is given because the abstract mentions limitations of existing methods, which are related to multilingual language models, but does not extensively discuss these limitations and instead focuses on the proposed solution."
    },
    {
        "title": "WYWEB: A NLP Evaluation Benchmark For Classical Chinese",
        "authors": [
            "Bo Zhou",
            "Qianglong Chen",
            "Tianyu Wang",
            "Xiaomi Zhong",
            "Yin Zhang"
        ],
        "published": "2023",
        "summary": "To fully evaluate the overall performance of different NLP models in a given domain, many evaluation benchmarks are proposed, such as GLUE, SuperGLUE and CLUE. The field of natural language understanding has traditionally focused on benchmarks for various tasks in languages such as Chinese, English, and multilingual, however, there has been a lack of attention given to the area of classical Chinese, also known as \"wen yan wen (文言文)\", which has a rich history spanning thousands of years and holds significant cultural and academic value. For the prosperity of the NLP community, in this paper, we introduce the WYWEB evaluation benchmark, which consists of nine NLP tasks in classical Chinese, implementing sentence classification, sequence labeling, reading comprehension, and machine translation. We evaluate the existing pre-trained language models, which are all struggling with this benchmark. We also introduce a number of supplementary datasets and additional tools to help facilitate further progress on classical Chinese NLU. The github repository is https://github.com/baudzhou/WYWEB.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.204.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We evaluate the existing pre-trained language models, which are all struggling with this benchmark.\"\n\nThis paper mentions a limitation of pre-trained language models (struggling with the WYWEB benchmark), but it is a minor detail and not the primary focus of the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We evaluate the existing pre-trained language models, which are all struggling with this benchmark.\"\n\nThis paper mentions a limitation of pre-trained language models (struggling with the WYWEB benchmark), but it is a minor detail and not the primary focus of the abstract."
    },
    {
        "title": "Two Examples are Better than One: Context Regularization for Gradient-based Prompt Tuning",
        "authors": [
            "Hyeonmin Ha",
            "Soyoung Jung",
            "Jinsol Park",
            "Minjoon Seo",
            "Seung-won Hwang",
            "Byung-Gon Chun"
        ],
        "published": "2023",
        "summary": "Prompting has gained tremendous attention as an efficient method for the adaptation of large-scale language models. However, prompts often act against human intuition and report unstable performances, which has motivated methods that automatically find effective prompts. One popular approach is gradient-based search, which iteratively updates a (randomly) initialized prompt towards the optimal one with the guide of gradients. We propose a novel regularization method, CoRe, for gradient-based prompt tuning techniques, which guides a prompt to produce a task context properly. CoRe realizes two regularization effects — context attuning and context filtering — that improve prediction performance in a zero-shot in-context learning setting where a model makes inferences only with the prompt tuned by CoRe, without any demonstration examples for in-context learning. Context attuning guides the context generated by the input and the tuned prompt toward embedding the appropriate context for the task. In our theoretical analysis, regularizing the context extends to improving zero-shot in-context learning performance. Context filtering steers the prompt to select only the task-related context so that context attuning solely focuses on creating and sending the right task context. We evaluate CoRe on natural language understanding datasets and two large language models, GPT2-XL and GPT-J.Our training scheme shows performance improvements up to 11.9% on GPT2-XL, and up to 6.3% on GPT-J in zero-shot settings.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.206.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, prompts often act against human intuition and report unstable performances, which has motivated methods that automatically find effective prompts.\"\n\nThis paper mentions a limitation of LLMs in passing, specifically that prompts often act against human intuition and report unstable performances, but it does not elaborate on this limitation further.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, prompts often act against human intuition and report unstable performances, which has motivated methods that automatically find effective prompts.\"\n\nThis paper mentions a limitation of LLMs in passing, specifically that prompts often act against human intuition and report unstable performances, but it does not elaborate on this limitation further."
    },
    {
        "title": "Graph Reasoning for Question Answering with Triplet Retrieval",
        "authors": [
            "Shiyang Li",
            "Yifan Gao",
            "Haoming Jiang",
            "Qingyu Yin",
            "Zheng Li",
            "Xifeng Yan",
            "Chao Zhang",
            "Bing Yin"
        ],
        "published": "2023",
        "summary": "Answering complex questions often requires reasoning over knowledge graphs (KGs). State-of-the-art methods often utilize entities in questions to retrieve local subgraphs, which are then fed into KG encoder, e.g. graph neural networks (GNNs), to model their local structures and integrated into language models for question answering. However, this paradigm constrains retrieved knowledge in local subgraphs and discards more diverse triplets buried in KGs that are disconnected but useful for question answering. In this paper, we propose a simple yet effective method to first retrieve the most relevant triplets from KGs and then rerank them, which are then concatenated with questions to be fed into language models. Extensive results on both CommonsenseQA and OpenbookQA datasets show that our method can outperform state-of-the-art up to 4.6% absolute accuracy.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.208.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, this paradigm constrains retrieved knowledge in local subgraphs and discards more diverse triplets buried in KGs that are disconnected but useful for question answering.\"\n\n(Note: Although the paper mentions a limitation of the paradigm used by state-of-the-art methods, it is not a direct limitation of LLMs, but rather a limitation of how LLMs are used in question answering",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, this paradigm constrains retrieved knowledge in local subgraphs and discards more diverse triplets buried in KGs that are disconnected but useful for question answering.\"\n\n(Note: Although the paper mentions a limitation of the paradigm used by state-of-the-art methods, it is not a direct limitation of LLMs, but rather a limitation of how LLMs are used in question answering"
    },
    {
        "title": "Unsupervised Task Graph Generation from Instructional Video Transcripts",
        "authors": [
            "Lajanugen Logeswaran",
            "Sungryull Sohn",
            "Yunseok Jang",
            "Moontae Lee",
            "Honglak Lee"
        ],
        "published": "2023",
        "summary": "This work explores the problem of generating task graphs of real-world activities. Different from prior formulations, we consider a setting where text transcripts of instructional videos performing a real-world activity (e.g., making coffee) are provided and the goal is to identify the key steps relevant to the task as well as the dependency relationship between these key steps. We propose a novel task graph generation approach that combines the reasoning capabilities of instruction-tuned language models along with clustering and ranking components to generate accurate task graphs in a completely unsupervised manner. We show that the proposed approach generates more accurate task graphs compared to a supervised learning approach on tasks from the ProceL and CrossTask datasets.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.210.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"combines the reasoning capabilities of instruction-tuned language models\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"combines the reasoning capabilities of instruction-tuned language models\""
    },
    {
        "title": "Language Model Analysis for Ontology Subsumption Inference",
        "authors": [
            "Yuan He",
            "Jiaoyan Chen",
            "Ernesto Jimenez-Ruiz",
            "Hang Dong",
            "Ian Horrocks"
        ],
        "published": "2023",
        "summary": "Investigating whether pre-trained language models (LMs) can function as knowledge bases (KBs) has raised wide research interests recently. However, existing works focus on simple, triple-based, relational KBs, but omit more sophisticated, logic-based, conceptualised KBs such as OWL ontologies. To investigate an LM’s knowledge of ontologies, we propose OntoLAMA, a set of inference-based probing tasks and datasets from ontology subsumption axioms involving both atomic and complex concepts. We conduct extensive experiments on ontologies of different domains and scales, and our results demonstrate that LMs encode relatively less background knowledge of Subsumption Inference (SI) than traditional Natural Language Inference (NLI) but can improve on SI significantly when a small number of samples are given. We will open-source our code and datasets.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.213.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing works focus on simple, triple-based, relational KBs, but omit more sophisticated, logic-based, conceptualised KBs such as OWL ontologies.\"; \"We conduct extensive experiments on ontologies of different domains and scales, and our results demonstrate that LMs encode relatively less background knowledge of Subsumption Inference (SI) than traditional Natural Language Inference (",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, existing works focus on simple, triple-based, relational KBs, but omit more sophisticated, logic-based, conceptualised KBs such as OWL ontologies.\"; \"We conduct extensive experiments on ontologies of different domains and scales, and our results demonstrate that LMs encode relatively less background knowledge of Subsumption Inference (SI) than traditional Natural Language Inference ("
    },
    {
        "title": "Exploring Automatically Perturbed Natural Language Explanations in Relation Extraction",
        "authors": [
            "Wanyun Cui",
            "Xingran Chen"
        ],
        "published": "2023",
        "summary": "Previous research has demonstrated that natural language explanations provide valuable inductive biases that guide models, thereby improving the generalization ability and data efficiency. In this paper, we undertake a systematic examination of the effectiveness of these explanations. Remarkably, we find that corrupted explanations with diminished inductive biases can achieve competitive or superior performance compared to the original explanations. Our findings furnish novel insights into the characteristics of natural language explanations in the following ways: (1) the impact of explanations varies across different training styles and datasets, with previously believed improvements primarily observed in frozen language models. (2) While previous research has attributed the effect of explanations solely to their inductive biases, our study shows that the effect persists even when the explanations are completely corrupted. We propose that the main effect is due to the provision of additional context space. (3) Utilizing the proposed automatic perturbed context, we were able to attain comparable results to annotated explanations, but with a significant increase in computational efficiency, 20-30 times faster.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.214.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"with previously believed improvements primarily observed in frozen language models.\"\n\n(Note: Although this paper does not directly discuss the limitations of LLMs, it mentions the variable impact of explanations across different models, including frozen language models, which can be seen as a limitation of LLMs in certain contexts.)",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"with previously believed improvements primarily observed in frozen language models.\"\n\n(Note: Although this paper does not directly discuss the limitations of LLMs, it mentions the variable impact of explanations across different models, including frozen language models, which can be seen as a limitation of LLMs in certain contexts.)"
    },
    {
        "title": "Varta: A Large-Scale Headline-Generation Dataset for Indic Languages",
        "authors": [
            "Rahul Aralikatte",
            "Ziling Cheng",
            "Sumanth Doddapaneni",
            "Jackie Chi Kit Cheung"
        ],
        "published": "2023",
        "summary": "We present Varta, a large-scale multilingual dataset for headline generation in Indic languages. This dataset includes more than 41 million pairs of headlines and articles in 14 different Indic languages (and English), which come from a variety of high-quality news sources. To the best of our knowledge, this is the largest collection of curated news articles for Indic languages currently available. We use the collected data in a series of experiments to answer important questions related to Indic NLP and multilinguality research in general. We show that the dataset is challenging even for state-of-the-art abstractive models and that they perform only slightly better than extractive baselines. Owing to its size, we also show that the dataset can be used to pre-train strong language models that outperform competitive baselines in both NLU and NLG benchmarks.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.215.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We show that the dataset is challenging even for state-of-the-art abstractive models and that they perform only slightly better than extractive baselines.\"\n\nThis evidence indicates that the paper mentions a limitation of state-of-the-art abstractive models (a type of LLM) in that they struggle with the dataset, but it is not a major focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We show that the dataset is challenging even for state-of-the-art abstractive models and that they perform only slightly better than extractive baselines.\"\n\nThis evidence indicates that the paper mentions a limitation of state-of-the-art abstractive models (a type of LLM) in that they struggle with the dataset, but it is not a major focus of the paper."
    },
    {
        "title": "Better Zero-Shot Reasoning with Self-Adaptive Prompting",
        "authors": [
            "Xingchen Wan",
            "Ruoxi Sun",
            "Hanjun Dai",
            "Sercan Arik",
            "Tomas Pfister"
        ],
        "published": "2023",
        "summary": "Modern large language models (LLMs) have demonstrated impressive capabilities at sophisticated tasks, often through step-by-step reasoning similar to humans. This is made possible by their strong few- and zero-shot abilities – they can effectively learn from a handful of handcrafted, completed responses (“in-context examples”), or are prompted to reason spontaneously through specially designed triggers. Nonetheless, some limitations have been observed. First, performance in the few-shot setting is sensitive to the choice of the examples, whose design requires significant human effort. Moreover, given the diverse downstream tasks of LLMs, it may be difficult or laborious to handcraft per-task labels. Second, while the zero-shot setting does not require handcrafting, its performance is limited due to the lack of guidance to the LLMs. To address these limitations, we propose Consistency-based Self-adaptive Prompting (COSP), a novel prompt design method for LLMs. Requiring neither handcrafted responses nor ground-truth labels, COSP selects and builds the set of examples from the LLM zero-shot outputs via carefully designed criteria combining consistency, diversity and repetition. In the zero-shot setting for three different LLMs, we show that using only LLM predictions, COSP significantly improves performance up to 15% compared to zero-shot baselines and matches or exceeds few-shot baselines at a range of reasoning tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.216.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"some limitations have been observed. First, performance in the few-shot setting is sensitive to the choice of the examples, whose design requires significant human effort. Moreover, given the diverse downstream tasks of LLMs, it may be difficult or laborious to handcraft per-task labels. Second, while the zero-shot setting does not require handcrafting, its performance is limited due to",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"some limitations have been observed. First, performance in the few-shot setting is sensitive to the choice of the examples, whose design requires significant human effort. Moreover, given the diverse downstream tasks of LLMs, it may be difficult or laborious to handcraft per-task labels. Second, while the zero-shot setting does not require handcrafting, its performance is limited due to"
    },
    {
        "title": "ANALOGICAL - A Novel Benchmark for Long Text Analogy Evaluation in Large Language Models",
        "authors": [
            "Thilini Wijesiriwardene",
            "Ruwan Wickramarachchi",
            "Bimal Gajera",
            "Shreeyash Gowaikar",
            "Chandan Gupta",
            "Aman Chadha",
            "Aishwarya Naresh Reganti",
            "Amit Sheth",
            "Amitava Das"
        ],
        "published": "2023",
        "summary": "Over the past decade, analogies, in the form of word-level analogies, have played a significant role as an intrinsic measure of evaluating the quality of word embedding methods such as word2vec. Modern large language models (LLMs), however, are primarily evaluated on extrinsic measures based on benchmarks such as GLUE and SuperGLUE, and there are only a few investigations on whether LLMs can draw analogies between long texts. In this paper, we present ANALOGICAL, a new benchmark to intrinsically evaluate LLMs across a taxonomy of analogies of long text with six levels of complexity – (i) word, (ii) word vs. sentence, (iii) syntactic, (iv) negation, (v) entailment, and (vi) metaphor. Using thirteen datasets and three different distance measures, we evaluate the abilities of eight LLMs in identifying analogical pairs in the semantic vector space. Our evaluation finds that it is increasingly challenging for LLMs to identify analogies when going up the analogy taxonomy.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.218.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our evaluation finds that it is increasingly challenging for LLMs to identify analogies when going up the analogy taxonomy.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Our evaluation finds that it is increasingly challenging for LLMs to identify analogies when going up the analogy taxonomy.\""
    },
    {
        "title": "Multilingual Summarization with Factual Consistency Evaluation",
        "authors": [
            "Roee Aharoni",
            "Shashi Narayan",
            "Joshua Maynez",
            "Jonathan Herzig",
            "Elizabeth Clark",
            "Mirella Lapata"
        ],
        "published": "2023",
        "summary": "Abstractive summarization has enjoyed renewed interest in recent years, thanks to pre-trained language models and the availability of large-scale datasets. Despite promising results, current models still suffer from generating factually inconsistent summaries, reducing their utility for real-world application. Several recent efforts attempt to address this by devising models that automatically detect factual inconsistencies in machine generated summaries. However, they focus exclusively on English, a language with abundant resources. In this work, we leverage factual consistency evaluation models to improve multilingual summarization. We explore two intuitive approaches to mitigate hallucinations based on the signal provided by a multilingual NLI model, namely data filtering and controlled generation. Experimental results in the 45 languages from the XLSum dataset show gains over strong baselines in both automatic and human evaluation. We release models and human judgements of summaries to foster progress towards more factually consistent multilingual summarization.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.220.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite promising results, current models still suffer from generating factually inconsistent summaries, reducing their utility for real-world application.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite promising results, current models still suffer from generating factually inconsistent summaries, reducing their utility for real-world application.\""
    },
    {
        "title": "Encoder and Decoder, Not One Less for Pre-trained Language Model Sponsored NMT",
        "authors": [
            "Sufeng Duan",
            "Hai Zhao"
        ],
        "published": "2023",
        "summary": "Well pre-trained contextualized representations from pre-trained language model (PLM) have been shown helpful for enhancing various natural language processing tasks, surely including neural machine translation (NMT). However, existing methods either consider encoder-only enhancement or rely on specific multilingual PLMs, which leads to a much larger model or give up potentially helpful knowledge from target PLMs. In this paper, we propose a new monolingual PLM-sponsored NMT model to let both encoder and decoder enjoy PLM enhancement to alleviate such obvious inconvenience. Especially, incorporating a newly proposed frequency-weighted embedding transformation algorithm, PLM embeddings can be effectively exploited in terms of the representations of the NMT decoder. We evaluate our model on IWSLT14 En-De, De-En, WMT14 En-De, and En-Fr tasks, and the results show that our proposed PLM enhancement gives significant improvement and even helps achieve new state-of-the-art.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.222.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing methods either consider encoder-only enhancement or rely on specific multilingual PLMs, which leads to a much larger model or give up potentially helpful knowledge from target PLMs.\"\n\nThis paper mentions a limitation of existing methods using pre-trained language models, but it is not a major focus and is used to motivate the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, existing methods either consider encoder-only enhancement or rely on specific multilingual PLMs, which leads to a much larger model or give up potentially helpful knowledge from target PLMs.\"\n\nThis paper mentions a limitation of existing methods using pre-trained language models, but it is not a major focus and is used to motivate the proposed solution."
    },
    {
        "title": "NewsDialogues: Towards Proactive News Grounded Conversation",
        "authors": [
            "Siheng Li",
            "Yichun Yin",
            "Cheng Yang",
            "Wangjie Jiang",
            "Yiwei Li",
            "Zesen Cheng",
            "Lifeng Shang",
            "Xin Jiang",
            "Qun Liu",
            "Yujiu Yang"
        ],
        "published": "2023",
        "summary": "Hot news is one of the most popular topics in daily conversations. However, news grounded conversation has long been stymied by the lack of well-designed task definition and scarce data. In this paper, we propose a novel task, Proactive News Grounded Conversation, in which a dialogue system can proactively lead the conversation based on some key topics of the news. In addition, both information-seeking and chit-chat scenarios are included realistically, where the user may ask a series of questions about the news details or express their opinions and be eager to chat. To further develop this novel task, we collect a human-to-human Chinese dialogue dataset NewsDialogues, which includes 1K conversations with a total of 14.6K utterances and detailed annotations for target topics and knowledge spans. Furthermore, we propose a method named Predict-Generate-Rank, consisting of a generator for grounded knowledge prediction and response generation, and a ranker for the ranking of multiple responses to alleviate the exposure bias. We conduct comprehensive experiments to demonstrate the effectiveness of the proposed method and further present several key findings and challenges to prompt future research.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.224.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Task-aware Retrieval with Instructions",
        "authors": [
            "Akari Asai",
            "Timo Schick",
            "Patrick Lewis",
            "Xilun Chen",
            "Gautier Izacard",
            "Sebastian Riedel",
            "Hannaneh Hajishirzi",
            "Wen-tau Yih"
        ],
        "published": "2023",
        "summary": "We study the problem of retrieval with instructions, where users provide explicit descriptions of their intent along with their queries to guide a retrieval system. Our solution is a general-purpose task-aware retrieval system, trained using multi-task instruction tuning and can follow human-written instructions to find relevant documents to a given query. We introduce the first large-scale collection of 37 retrieval datasets with instructions, BERRI, and present TART, a single multi-task retrieval system trained on BERRI with instructions that can adapt to a new task without any parameter updates. TART advances the state of the art on two zero-shot retrieval benchmarks, BEIR and LOTTE, outperforming models up to three times larger. We further introduce a new evaluation setup, X2-Retrieval, to better reflect real-world scenarios in which diverse domains and tasks are pooled. TART significantly outperforms competitive baselines in this setup, further highlighting the effectiveness of guiding retrieval with instructions.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.225.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Define, Evaluate, and Improve Task-Oriented Cognitive Capabilities for Instruction Generation Models",
        "authors": [
            "Lingjun Zhao",
            "Khanh Nguyen",
            "Hal Daumé III"
        ],
        "published": "2023",
        "summary": "Recent work studies the cognitive capabilities of language models through psychological tests designed for humans. While these studies are helpful for understanding the general capabilities of these models, there is no guarantee that a model possessing sufficient capabilities to pass those tests would actually use those capabilities in performing real-life tasks. In this work, we formulate task-oriented cognitive capabilities, which are human-like cognitive capabilities that language models leverage to perform tasks. These capabilities are (i) the ability to quickly generate good candidate utterances (the search capability) (ii) the ability to predict how a listener interprets those utterances and choose the most appropriate one (the pragmatic capability). We design an evaluation scheme for comparing these capabilities of a language model with those of a human. Applying this scheme to examine various models in a navigation instruction generation problem, we find that their pragmatic capability is severely lacking. This insight leads us to augment them with better models of the listener and obtain a significant boost of 11% in success rate in guiding real humans. Our work advocates for having a principled procedure for aligning language models with humans that involves (i) formulating task-oriented capabilities, (ii) devising a method to quantify their deficiency, and (iii) iteratively improving them.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.227.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Applying this scheme to examine various models in a navigation instruction generation problem, we find that their pragmatic capability is severely lacking.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Applying this scheme to examine various models in a navigation instruction generation problem, we find that their pragmatic capability is severely lacking.\""
    },
    {
        "title": "Not The End of Story: An Evaluation of ChatGPT-Driven Vulnerability Description Mappings",
        "authors": [
            "Xin Liu",
            "Yuan Tan",
            "Zhenghang Xiao",
            "Jianwei Zhuge",
            "Rui Zhou"
        ],
        "published": "2023",
        "summary": "As the number of vulnerabilities increases day by day, security management requires more and more structured data. In addition to textual descriptions of vulnerabilities, security engineers must classify and assess vulnerabilities and clarify their associated techniques. Vulnerability Description Mapping (VDM) refers to mapping vulnerabilities to Common Weakness Enumeration (CWE), Common Attack Pattern Enumeration and Classification, ATT&CK Techniques, and other classifications. Accurate VDM is necessary to reduce the pressure of security management and improve the speed of security emergency response. ChatGPT is the latest state-of-the-art closed-source conversational large language model (LLM), which performs excellently on many tasks. This paper explores the application of closed-source LLMs to real-world security management scenarios by evaluating ChatGPT’s performance on VDM tasks. The results show that although ChatGPT may be close to the level of human experts on some tasks, it still cannot replace the critical role of professional security engineers in vulnerability analysis. In a word, closed-source LLM is not the end of story.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.229.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"although ChatGPT may be close to the level of human experts on some tasks, it still cannot replace the critical role of professional security engineers in vulnerability analysis.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"although ChatGPT may be close to the level of human experts on some tasks, it still cannot replace the critical role of professional security engineers in vulnerability analysis.\""
    },
    {
        "title": "Automatic Table Union Search with Tabular Representation Learning",
        "authors": [
            "Xuming Hu",
            "Shen Wang",
            "Xiao Qin",
            "Chuan Lei",
            "Zhengyuan Shen",
            "Christos Faloutsos",
            "Asterios Katsifodimos",
            "George Karypis",
            "Lijie Wen",
            "Philip S. Yu"
        ],
        "published": "2023",
        "summary": "Given a data lake of tabular data as well as a query table, how can we retrieve all the tables in the data lake that can be unioned with the query table? Table union search constitutes an essential task in data discovery and preparation as it enables data scientists to navigate massive open data repositories. Existing methods identify uniability based on column representations (word surface forms or token embeddings) and column relation represented by column representation similarity. However, the semantic similarity obtained between column representations is often insufficient to reveal latent relational features to describe the column relation between pair of columns and not robust to the table noise. To address these issues, in this paper, we propose a multi-stage self-supervised table union search framework called AutoTUS, which represents column relation as a vector– column relational representation and learn column relational representation in a multi-stage manner that can better describe column relation for unionability prediction. In particular, the large language model powered contextualized column relation encoder is updated by adaptive clustering and pseudo label classification iteratively so that the better column relational representation can be learned. Moreover, to improve the robustness of the model against table noises, we propose table noise generator to add table noise to the training table data. Experiments on real-world datasets as well as synthetic test set augmented with table noise show that AutoTUS achieves 5.2% performance gain over the SOTA baseline.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.233.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the semantic similarity obtained between column representations is often insufficient to reveal latent relational features to describe the column relation between pair of columns\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"the semantic similarity obtained between column representations is often insufficient to reveal latent relational features to describe the column relation between pair of columns\""
    },
    {
        "title": "Not Enough Data to Pre-train Your Language Model? MT to the Rescue!",
        "authors": [
            "Gorka Urbizu",
            "Iñaki San Vicente",
            "Xabier Saralegi",
            "Ander Corral"
        ],
        "published": "2023",
        "summary": "In recent years, pre-trained transformer-based language models (LM) have become a key resource for implementing most NLP tasks. However, pre-training such models demands large text collections not available in most languages. In this paper, we study the use of machine-translated corpora for pre-training LMs. We answer the following research questions: RQ1: Is MT-based data an alternative to real data for learning a LM?; RQ2: Can real data be complemented with translated data and improve the resulting LM? In order to validate these two questions, several BERT models for Basque have been trained, combining real data and synthetic data translated from Spanish.The evaluation carried out on 9 NLU tasks indicates that models trained exclusively on translated data offer competitive results. Furthermore, models trained with real data can be improved with synthetic data, although further research is needed on the matter.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.235.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, pre-training such models demands large text collections not available in most languages.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, pre-training such models demands large text collections not available in most languages.\""
    },
    {
        "title": "Maximum Entropy Loss, the Silver Bullet Targeting Backdoor Attacks in Pre-trained Language Models",
        "authors": [
            "Zhengxiao Liu",
            "Bowen Shen",
            "Zheng Lin",
            "Fali Wang",
            "Weiping Wang"
        ],
        "published": "2023",
        "summary": "Pre-trained language model (PLM) can be stealthily misled to target outputs by backdoor attacks when encountering poisoned samples, without performance degradation on clean samples. The stealthiness of backdoor attacks is commonly attained through minimal cross-entropy loss fine-tuning on a union of poisoned and clean samples. Existing defense paradigms provide a workaround by detecting and removing poisoned samples at pre-training or inference time. On the contrary, we provide a new perspective where the backdoor attack is directly reversed. Specifically, maximum entropy loss is incorporated in training to neutralize the minimal cross-entropy loss fine-tuning on poisoned data. We defend against a range of backdoor attacks on classification tasks and significantly lower the attack success rate. In extension, we explore the relationship between intended backdoor attacks and unintended dataset bias, and demonstrate the feasibility of the maximum entropy principle in de-biasing.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.237.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Pre-trained language model (PLM) can be stealthily misled to target outputs by backdoor attacks when encountering poisoned samples, without performance degradation on clean samples.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Pre-trained language model (PLM) can be stealthily misled to target outputs by backdoor attacks when encountering poisoned samples, without performance degradation on clean samples.\""
    },
    {
        "title": "Improving Named Entity Recognition via Bridge-based Domain Adaptation",
        "authors": [
            "Jingyun Xu",
            "Changmeng Zheng",
            "Yi Cai",
            "Tat-Seng Chua"
        ],
        "published": "2023",
        "summary": "Recent studies have shown remarkable success in cross-domain named entity recognition (cross-domain NER). Despite the promising results, existing methods mainly utilize pre-training language models like BERT to represent words. As such, the original chaotic representations may challenge them to distinguish entity types of entities, leading to entity type misclassification. To this end, we attempt to utilize contrastive learning to refine the original representations and propose a model-agnostic framework named MoCL for cross-domain NER. Additionally, we respectively combine MoCL with two distinctive cross-domain NER methods and two pre-training language models to explore its generalization ability. Empirical results on seven domains show the effectiveness and good generalization ability of MoCL.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.238.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"existing methods mainly utilize pre-training language models like BERT to represent words. As such, the original chaotic representations may challenge them to distinguish entity types of entities, leading to entity type misclassification.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"existing methods mainly utilize pre-training language models like BERT to represent words. As such, the original chaotic representations may challenge them to distinguish entity types of entities, leading to entity type misclassification.\""
    },
    {
        "title": "A Simple Yet Strong Domain-Agnostic De-bias Method for Zero-Shot Sentiment Classification",
        "authors": [
            "Yang Zhao",
            "Tetsuya Nasukawa",
            "Masayasu Muraoka",
            "Bishwaranjan Bhattacharjee"
        ],
        "published": "2023",
        "summary": "Zero-shot prompt-based learning has made much progress in sentiment analysis, and considerable effort has been dedicated to designing high-performing prompt templates. However, two problems exist; First, large language models are often biased to their pre-training data, leading to poor performance in prompt templates that models have rarely seen. Second, in order to adapt to different domains, re-designing prompt templates is usually required, which is time-consuming and inefficient. To remedy both shortcomings, we propose a simple yet strong data construction method to de-bias a given prompt template, yielding a large performance improvement in sentiment analysis tasks across different domains, pre-trained language models, and prompt templates. Also, we demonstrate the advantage of using domain-agnostic generic responses over the in-domain ground-truth data.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.242.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"large language models are often biased to their pre-training data, leading to poor performance in prompt templates that models have rarely seen.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"large language models are often biased to their pre-training data, leading to poor performance in prompt templates that models have rarely seen.\""
    },
    {
        "title": "A Benchmark on Extremely Weakly Supervised Text Classification: Reconcile Seed Matching and Prompting Approaches",
        "authors": [
            "Zihan Wang",
            "Tianle Wang",
            "Dheeraj Mekala",
            "Jingbo Shang"
        ],
        "published": "2023",
        "summary": "Extremely Weakly Supervised Text Classification (XWS-TC) refers to text classification based on minimal high-level human guidance, such as a few label-indicative seed words or classification instructions. There are two mainstream approaches for XWS-TC, however, never being rigorously compared: (1) training classifiers based on pseudo-labels generated by (softly) matching seed words (Seed) and (2) prompting (and calibrating) language models using classification instruction (and raw texts) to decode label words (Prompt). This paper presents the first XWS-TC benchmark to compare the two approaches on fair grounds, where the datasets, supervisions, and hyperparameter choices are standardized across methods. Our benchmarking results suggest that (1) Both Seed and Prompt approaches are competitive and there is no clear winner; (2) Seed is empirically more tolerant than Prompt to human guidance (e.g., seed words, classification instructions, and label words) changes; (3) Seed is empirically more selective than Prompt to the pre-trained language models; (4) Recent Seed and Prompt methods have close connections and a clustering post-processing step based on raw in-domain texts is a strong performance booster to both. We hope this benchmark serves as a guideline in selecting XWS-TC methods in different scenarios and stimulate interest in developing guidance- and model-robust XWS-TC methods.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.244.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Seed is empirically more selective than Prompt to the pre-trained language models\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Seed is empirically more selective than Prompt to the pre-trained language models\""
    },
    {
        "title": "Zemi: Learning Zero-Shot Semi-Parametric Language Models from Multiple Tasks",
        "authors": [
            "Zhenhailong Wang",
            "Xiaoman Pan",
            "Dian Yu",
            "Dong Yu",
            "Jianshu Chen",
            "Heng Ji"
        ],
        "published": "2023",
        "summary": "Although large language models have exhibited impressive zero-shot ability, the huge model size generally incurs high cost. Recently, semi-parametric language models, which augment a smaller language model with retrieved related background knowledge, alleviate the need for storing everything into the model parameters. Although existing semi-parametric language models have demonstrated promising language modeling capabilities, it remains unclear whether they can exhibit competitive zero-shot abilities as their fully-parametric counterparts. In this work, we introduce Zemi, a semi-parametric language model for zero-shot task generalization. To our best knowledge, this is the first semi-parametric language model that can demonstrate strong zero-shot performance on a wide range of held-out unseen tasks. We train Zemi with semi-parametric multitask training, which shows significant improvement compared with the parametric multitask training as proposed by T0. Specifically, during both training and inference, Zemi is equipped with a retrieval system based on the unlabeled pretraining corpus of our backbone model. To address the unique challenges from large-scale retrieval, we further propose a novel retrieval-augmentation fusion module that can effectively incorporate noisy retrieved documents. Finally, we show detailed analysis and ablation studies on the key ingredients towards building effective zero-shot semi-parametric language models. Notably, our proposed Zemi_Large model outperforms T0-3B by 16% across seven diverse evaluation tasks while being 3.8x smaller in scale.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.246.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although large language models have exhibited impressive zero-shot ability, the huge model size generally incurs high cost.\"\n\nThis abstract mentions a limitation of LLMs (high cost due to huge model size) but does not elaborate on it and focuses on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although large language models have exhibited impressive zero-shot ability, the huge model size generally incurs high cost.\"\n\nThis abstract mentions a limitation of LLMs (high cost due to huge model size) but does not elaborate on it and focuses on the proposed solution."
    },
    {
        "title": "Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers",
        "authors": [
            "Damai Dai",
            "Yutao Sun",
            "Li Dong",
            "Yaru Hao",
            "Shuming Ma",
            "Zhifang Sui",
            "Furu Wei"
        ],
        "published": "2023",
        "summary": "Large pretrained language models have shown surprising in-context learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without parameter updates. Despite the great success in performance, its working mechanism still remains an open question. In this paper, we explain language models as meta-optimizers and understand in-context learning as implicit finetuning. Theoretically, we figure out that Transformer attention has a dual form of gradient descent. On top of it, we understand ICL as follows: GPT first produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. We comprehensively compare the behaviors of in-context learning and explicit finetuning on real tasks to provide empirical evidence that supports our understanding. Experimental results show that in-context learning behaves similarly to explicit finetuning from multiple perspectives. Inspired by the dual form between Transformer attention and gradient descent, we design a momentum-based attention by analogy with gradient descent with momentum. The improved performance over vanilla attention further supports our understanding from another perspective, and more importantly, shows the potential to utilize our understanding for future model design. The code is available at https://aka.ms/icl.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.247.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the abstract does not mention any limitations of LLMs, it rather explains their in-context learning ability and proposes a new understanding of their working mechanism.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the abstract does not mention any limitations of LLMs, it rather explains their in-context learning ability and proposes a new understanding of their working mechanism."
    },
    {
        "title": "Injecting Comparison Skills in Task-Oriented Dialogue Systems for Database Search Results Disambiguation",
        "authors": [
            "Yongil Kim",
            "Yerin Hwang",
            "Joongbo Shin",
            "Hyunkyung Bae",
            "Kyomin Jung"
        ],
        "published": "2023",
        "summary": "In task-oriented dialogue (TOD) systems designed to aid users accomplish specific goals in one or more domains, the agent retrieves entities that satisfy user constraints from the database. However, when multiple database search results exist, an ambiguity occurs regarding which results to select and present to the user. Existing TOD systems handle this ambiguity by randomly selecting one or few results and presenting their names to the user. However, in a real scenario, users do not always accept a randomly recommended entity, and users should have access to more comprehensive information about the search results. To address this limitation, we propose a novel task called Comparison-Based database search Ambiguity handling (CBA), which handles ambiguity in database search results by comparing the properties of multiple entities to enable users to choose according to their preferences. Accordingly, we introduce a new framework for automatically collecting high-quality dialogue data along with the Disambiguating Schema-guided Dialogue (DSD) dataset, an augmented version of the SGD dataset. Experimental studies on the DSD dataset demonstrate that training baseline models with the dataset effectively address the CBA task. Our dataset and code will be publicized.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.249.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Universal Information Extraction with Meta-Pretrained Self-Retrieval",
        "authors": [
            "Xin Cong",
            "Bowen Yu",
            "Mengcheng Fang",
            "Tingwen Liu",
            "Haiyang Yu",
            "Zhongkai Hu",
            "Fei Huang",
            "Yongbin Li",
            "Bin Wang"
        ],
        "published": "2023",
        "summary": "Universal Information Extraction (Universal IE) aims to solve different extraction tasks in a uniform text-to-structure generation manner. Such a generation procedure tends to struggle when there exist complex information structures to be extracted. Retrieving knowledge from external knowledge bases may help models to overcome this problem but it is impossible to construct a knowledge base suitable for various IE tasks. Inspired by the fact that large amount of knowledge are stored in the pretrained language models (PLM) and can be retrieved explicitly, in this paper, we propose MetaRetriever to retrieve task-specific knowledge from PLMs to enhance universal IE. As different IE tasks need different knowledge, we further propose a Meta-Pretraining Algorithm which allows MetaRetriever to quicktly achieve maximum task-specific retrieval performance when fine-tuning on downstream IE tasks. Experimental results show that MetaRetriever achieves the new state-of-the-art on 4 IE tasks, 12 datasets under fully-supervised, low-resource and few-shot scenarios.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.251.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the abstract implies that the model struggles with complex information structures, which is a limitation, however it is not explicitly stated as a limitation of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the abstract implies that the model struggles with complex information structures, which is a limitation, however it is not explicitly stated as a limitation of LLMs."
    },
    {
        "title": "SETI: Systematicity Evaluation of Textual Inference",
        "authors": [
            "Xiyan Fu",
            "Anette Frank"
        ],
        "published": "2023",
        "summary": "We propose SETI (Systematicity Evaluation of Textual Inference), a novel and comprehensive benchmark designed for evaluating pre-trained language models (PLMs) for their systematicity capabilities in the domain of textual inference. Specifically, SETI offers three different NLI tasks and corresponding datasets to evaluate various types of systematicity in reasoning processes. In order to solve these tasks, models are required to perform compositional inference based on known primitive constituents. We conduct experiments of SETI on six widely used PLMs. Results show that various PLMs are able to solve unseen compositional inferences when having encountered the knowledge of how to combine primitives, with good performance. However, they are considerably limited when this knowledge is unknown to the model (40-100 % points decrease). Furthermore, we find that PLMs are able to improve dramatically once exposed to crucial compositional knowledge in minimalistic shots. These findings position SETI as the first benchmark for measuring the future progress of PLMs in achieving systematicity generalization in the textual inference.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.252.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, they are considerably limited when this knowledge is unknown to the model (40-100 % points decrease).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, they are considerably limited when this knowledge is unknown to the model (40-100 % points decrease).\""
    },
    {
        "title": "Self-Evolution Learning for Discriminative Language Model Pretraining",
        "authors": [
            "Qihuang Zhong",
            "Liang Ding",
            "Juhua Liu",
            "Bo Du",
            "Dacheng Tao"
        ],
        "published": "2023",
        "summary": "Masked language modeling, widely used in discriminative language model (e.g., BERT) pretraining, commonly adopts a random masking strategy. However, random masking does not consider the importance of the different words in the sentence meaning, where some of them are more worthy to be predicted. Therefore, various masking strategies (e.g., entity-level masking) are proposed, but most of them require expensive prior knowledge and generally train from scratch without reusing existing model weights. In this paper, we present Self-Evolution learning (SE), a simple and effective token masking and learning method to fully and wisely exploit the knowledge from data. SE focuses on learning the informative yet under-explored tokens and adaptively regularizes the training by introducing a novel Token-specific Label Smoothing approach. Experiments on 10 tasks show that our SE brings consistent and significant improvements (+1.43 2.12 average scores) upon different PLMs. In-depth analyses demonstrate that SE improves linguistic knowledge learning and generalization.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.254.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, random masking does not consider the importance of the different words in the sentence meaning, where some of them are more worthy to be predicted.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, random masking does not consider the importance of the different words in the sentence meaning, where some of them are more worthy to be predicted.\""
    },
    {
        "title": "QueryForm: A Simple Zero-shot Form Entity Query Framework",
        "authors": [
            "Zifeng Wang",
            "Zizhao Zhang",
            "Jacob Devlin",
            "Chen-Yu Lee",
            "Guolong Su",
            "Hao Zhang",
            "Jennifer Dy",
            "Vincent Perot",
            "Tomas Pfister"
        ],
        "published": "2023",
        "summary": "Zero-shot transfer learning for document understanding is a crucial yet under-investigated scenario to help reduce the high cost involved in annotating document entities. We present a novel query-based framework, QueryForm, that extracts entity values from form-like documents in a zero-shot fashion. QueryForm contains a dual prompting mechanism that composes both the document schema and a specific entity type into a query, which is used to prompt a Transformer model to perform a single entity extraction task. Furthermore, we propose to leverage large-scale query-entity pairs generated from form-like webpages with weak HTML annotations to pre-train QueryForm. By unifying pre-training and fine-tuning into the same query-based framework, QueryForm enables models to learn from structured documents containing various entities and layouts, leading to better generalization to target document types without the need for target-specific training data. QueryForm sets new state-of-the-art average F1 score on both the XFUND (+4.6% 10.1%) and the Payment (+3.2% 9.5%) zero-shot benchmark, with a smaller model size and no additional image input.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.255.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Speaking the Language of Your Listener: Audience-Aware Adaptation via Plug-and-Play Theory of Mind",
        "authors": [
            "Ece Takmaz",
            "Nicolo’ Brandizzi",
            "Mario Giulianelli",
            "Sandro Pezzelle",
            "Raquel Fernandez"
        ],
        "published": "2023",
        "summary": "Dialogue participants may have varying levels of knowledge about the topic under discussion. In such cases, it is essential for speakers to adapt their utterances by taking their audience into account. Yet, it is an open question how such adaptation can be modelled in computational agents. In this paper, we model a visually grounded referential game between a knowledgeable speaker and a listener with more limited visual and linguistic experience. Inspired by psycholinguistic theories, we endow our speaker with the ability to adapt its referring expressions via a simulation module that monitors the effectiveness of planned utterances from the listener’s perspective. We propose an adaptation mechanism building on plug-and-play approaches to controlled language generation, where utterance generation is steered on the fly by the simulator without finetuning the speaker’s underlying language model. Our results and analyses show that our approach is effective: the speaker’s utterances become closer to the listener’s domain of expertise, which leads to higher communicative success.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.258.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "AMR-TST: Abstract Meaning Representation-based Text Style Transfer",
        "authors": [
            "Kaize Shi",
            "Xueyao Sun",
            "Li He",
            "Dingxian Wang",
            "Qing Li",
            "Guandong Xu"
        ],
        "published": "2023",
        "summary": "Abstract Meaning Representation (AMR) is a semantic representation that can enhance natural language generation (NLG) by providing a logical semantic input. In this paper, we propose the AMR-TST, an AMR-based text style transfer (TST) technique. The AMR-TST converts the source text to an AMR graph and generates the transferred text based on the AMR graph modified by a TST policy named style rewriting. Our method combines both the explainability and diversity of explicit and implicit TST methods. The experiments show that the proposed method achieves state-of-the-art results compared with other baseline models in automatic and human evaluations. The generated transferred text in qualitative evaluation proves the AMR-TST have significant advantages in keeping semantic features and reducing hallucinations. To the best of our knowledge, this work is the first to apply the AMR method focusing on node-level features to the TST task.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.260.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "RobustQA: Benchmarking the Robustness of Domain Adaptation for Open-Domain Question Answering",
        "authors": [
            "Rujun Han",
            "Peng Qi",
            "Yuhao Zhang",
            "Lan Liu",
            "Juliette Burger",
            "William Yang Wang",
            "Zhiheng Huang",
            "Bing Xiang",
            "Dan Roth"
        ],
        "published": "2023",
        "summary": "Open-domain question answering (ODQA) is a crucial task in natural language processing. A typical ODQA system relies on a retriever module to select relevant contexts from a large corpus for a downstream reading comprehension model. Existing ODQA datasets consist mainly of Wikipedia corpus, and are insufficient to study models’ generalizability across diverse domains as models are trained and evaluated on the same genre of data. We propose **RobustQA**, a novel benchmark consisting of datasets from 8 different domains, which facilitates the evaluation of ODQA’s domain robustness. To build **RobustQA**, we annotate QA pairs in retrieval datasets with rigorous quality control. We further examine improving QA performances by incorporating unsupervised learning methods with target-domain corpus and adopting large generative language models. These methods can effectively improve model performances on **RobustQA**. However, experimental results demonstrate a significant gap from in-domain training, suggesting that **RobustQA** is a challenging benchmark to evaluate ODQA domain robustness.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.263.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, experimental results demonstrate a significant gap from in-domain training, suggesting that **RobustQA** is a challenging benchmark to evaluate ODQA domain robustness.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of large generative language models, specifically their performance gap in out-of-domain training, but it is not the primary focus of the paper and is not explored",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, experimental results demonstrate a significant gap from in-domain training, suggesting that **RobustQA** is a challenging benchmark to evaluate ODQA domain robustness.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of large generative language models, specifically their performance gap in out-of-domain training, but it is not the primary focus of the paper and is not explored"
    },
    {
        "title": "Contextualized Soft Prompts for Extraction of Event Arguments",
        "authors": [
            "Chien Nguyen",
            "Hieu Man",
            "Thien Nguyen"
        ],
        "published": "2023",
        "summary": "Event argument extraction (EAE) is a sub-task of event extraction where the goal is to identify roles of entity mentions for events in text. The current state-of-the-art approaches for this problem explore prompt-based methods to prompt pre-trained language models for arguments over input context. However, existing prompt-based methods mainly rely on discrete and manually-designed prompts that cannot exploit specific context for each example to improve customization for optimal performance. In addition, the discrete nature of current prompts prevents the incorporation of relevant context from multiple external documents to enrich prompts for EAE. To this end, we propose a novel prompt-based method for EAE that introduces soft prompts to facilitate the encoding of individual example context and multiple relevant documents to boost EAE. We extensively evaluate the proposed method on benchmark datasets for EAE to demonstrate its benefits with state-of-the-art performance.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.266.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing prompt-based methods mainly rely on discrete and manually-designed prompts that cannot exploit specific context for each example to improve customization for optimal performance.\"\n\nThis rating is given because the paper mentions a limitation of existing prompt-based methods for LLMs, but does not elaborate on it and focuses on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, existing prompt-based methods mainly rely on discrete and manually-designed prompts that cannot exploit specific context for each example to improve customization for optimal performance.\"\n\nThis rating is given because the paper mentions a limitation of existing prompt-based methods for LLMs, but does not elaborate on it and focuses on the proposed solution."
    },
    {
        "title": "Towards Alleviating the Object Bias in Prompt Tuning-based Factual Knowledge Extraction",
        "authors": [
            "Yuhang Wang",
            "Dongyuan Lu",
            "Chao Kong",
            "Jitao Sang"
        ],
        "published": "2023",
        "summary": "Many works employed prompt tuning methods to automatically optimize prompt queries and extract the factual knowledge stored in Pre-trained Language Models. In this paper, we observe that the optimized prompts, including discrete prompts and continuous prompts, exhibit undesirable object bias. To handle this problem, we propose a novel prompt tuning method called MeCoD consisting of three modules: Prompt Encoder, Object Equalization and Biased Object Obstruction. Experimental results show that MeCoD can significantly reduce the object bias and at the same time improve accuracy of factual knowledge extraction.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.270.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"we observe that the optimized prompts, including discrete prompts and continuous prompts, exhibit undesirable object bias.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"we observe that the optimized prompts, including discrete prompts and continuous prompts, exhibit undesirable object bias.\""
    },
    {
        "title": "Complementary Explanations for Effective In-Context Learning",
        "authors": [
            "Xi Ye",
            "Srinivasan Iyer",
            "Asli Celikyilmaz",
            "Veselin Stoyanov",
            "Greg Durrett",
            "Ramakanth Pasunuru"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have exhibited remarkable capabilities in learning from expla- nations in prompts, but there has been limited understanding of exactly how these explana- tions function or why they are effective. This work aims to better understand the mechanisms by which explanations are used for in-context learning. We first study the impact of two dif- ferent factors on the performance of prompts with explanations: the computation trace (the way the solution is decomposed) and the natural language used to express the prompt. By per- turbing explanations on three controlled tasks, we show that both factors contribute to the ef- fectiveness of explanations. We further study how to form maximally effective sets of expla- nations for solving a given test query. We find that LLMs can benefit from the complemen- tarity of the explanation set: diverse reasoning skills shown by different exemplars can lead to better performance. Therefore, we propose a maximal marginal relevance-based exemplar selection approach for constructing exemplar sets that are both relevant as well as comple- mentary, which successfully improves the in- context learning performance across three real- world tasks on multiple LLMs.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.273.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models (LLMs) have exhibited remarkable capabilities in learning from explanations in prompts, but there has been limited understanding of exactly how these explanations function or why they are effective.\"\n\nNote: The abstract does not explicitly discuss limitations of LLMs but mentions a limitation in understanding how explanations function, which is related to the effectiveness of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large language models (LLMs) have exhibited remarkable capabilities in learning from explanations in prompts, but there has been limited understanding of exactly how these explanations function or why they are effective.\"\n\nNote: The abstract does not explicitly discuss limitations of LLMs but mentions a limitation in understanding how explanations function, which is related to the effectiveness of LLMs."
    },
    {
        "title": "MISMATCH: Fine-grained Evaluation of Machine-generated Text with Mismatch Error Types",
        "authors": [
            "Keerthiram Murugesan",
            "Sarathkrishna Swaminathan",
            "Soham Dan",
            "Subhajit Chaudhury",
            "Chulaka Gunasekara",
            "Maxwell Crouse",
            "Diwakar Mahajan",
            "Ibrahim Abdelaziz",
            "Achille Fokoue",
            "Pavan Kapanipathi",
            "Salim Roukos",
            "Alexander Gray"
        ],
        "published": "2023",
        "summary": "With the growing interest in large language models, the need for evaluating the quality of machine text compared to reference (typically human-generated) text has become focal attention. Most recent works focus either on task-specific evaluation metrics or study the properties of machine-generated text captured by the existing metrics. In this work, we propose a new evaluation scheme to model human judgments in 7 NLP tasks, based on the fine-grained mismatches between a pair of texts. Inspired by the recent efforts in several NLP tasks for fine-grained evaluation, we introduce a set of 13 mismatch error types such as spatial/geographic errors, entity errors, etc, to guide the model for better prediction of human judgments. We propose a neural framework for evaluating machine texts that uses these mismatch error types as auxiliary tasks and re-purposes the existing single-number evaluation metrics as additional scalar features, in addition to textual features extracted from the machine and reference texts. Our experiments reveal key insights about the existing metrics via the mismatch errors. We show that the mismatch errors between the sentence pairs on the held-out datasets from 7 NLP tasks align well with the human evaluation.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.274.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"With the growing interest in large language models, the need for evaluating the quality of machine text compared to reference (typically human-generated) text has become focal attention.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"With the growing interest in large language models, the need for evaluating the quality of machine text compared to reference (typically human-generated) text has become focal attention.\""
    },
    {
        "title": "RHO: Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding",
        "authors": [
            "Ziwei Ji",
            "Zihan Liu",
            "Nayeon Lee",
            "Tiezheng Yu",
            "Bryan Wilie",
            "Min Zeng",
            "Pascale Fung"
        ],
        "published": "2023",
        "summary": "Dialogue systems can leverage large pre-trained language models and knowledge to generate fluent and informative responses. However, these models are still prone to produce hallucinated responses not supported by the input source, which greatly hinders their application. The heterogeneity between external knowledge and dialogue context challenges representation learning and source integration, which further contributes to unfaithfulness. To handle this challenge and generate more faithful responses, this paper presents RHO (ρ) utilizing the representations of linked entities and relation predicates from a knowledge graph (KG). We propose (1) local knowledge grounding to combine textual embeddings with the corresponding KG embeddings; and (2) global knowledge grounding to equip RHO with multi-hop reasoning abilities via the attention mechanism. In addition, we devise a response re-ranking technique based on walks over KG sub-graphs for better conversational reasoning. Experimental results on OpenDialKG (Moon et al., 2019) show that our approach significantly outperforms state-of-the-art methods on both automatic and human evaluation by a large margin, especially in hallucination reduction (17.54% in FeQA (Durmus et al., 2020)).",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.275.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these models are still prone to produce hallucinated responses not supported by the input source, which greatly hinders their application.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these models are still prone to produce hallucinated responses not supported by the input source, which greatly hinders their application.\""
    },
    {
        "title": "Transformer Language Models Handle Word Frequency in Prediction Head",
        "authors": [
            "Goro Kobayashi",
            "Tatsuki Kuribayashi",
            "Sho Yokoi",
            "Kentaro Inui"
        ],
        "published": "2023",
        "summary": "Prediction head is a crucial component of Transformer language models. Despite its direct impact on prediction, this component has often been overlooked in analyzing Transformers.In this study, we investigate the inner workings of the prediction head, specifically focusing on bias parameters. Our experiments with BERT and GPT-2 models reveal that the biases in their word prediction heads play a significant role in the models’ ability to reflect word frequency in a corpus, aligning with the logit adjustment method commonly used in long-tailed learning. We also quantify the effect of controlling the biases in practical auto-regressive text generation scenarios;under a particular setting, more diverse text can be generated without compromising text quality.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.276.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our experiments with BERT and GPT-2 models reveal that the biases in their word prediction heads play a significant role in the models’ ability to reflect word frequency in a corpus, aligning with the logit adjustment method commonly used in long-tailed learning.\"\n\nThis abstract mentions a limitation of LLMs in passing, specifically the role of biases in word prediction heads in reflecting",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Our experiments with BERT and GPT-2 models reveal that the biases in their word prediction heads play a significant role in the models’ ability to reflect word frequency in a corpus, aligning with the logit adjustment method commonly used in long-tailed learning.\"\n\nThis abstract mentions a limitation of LLMs in passing, specifically the role of biases in word prediction heads in reflecting"
    },
    {
        "title": "Prompted LLMs as Chatbot Modules for Long Open-domain Conversation",
        "authors": [
            "Gibbeum Lee",
            "Volker Hartmann",
            "Jongho Park",
            "Dimitris Papailiopoulos",
            "Kangwook Lee"
        ],
        "published": "2023",
        "summary": "In this paper, we propose MPC (Modular Prompted Chatbot), a new approach for creating high-quality conversational agents without the need for fine-tuning. Our method utilizes pre-trained large language models (LLMs) as individual modules for long-term consistency and flexibility, by using techniques such as few-shot prompting, chain-of-thought (CoT), and external memory. Our human evaluation results show that MPC is on par with fine-tuned chatbot models in open-domain conversations, making it an effective solution for creating consistent and engaging chatbots.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.277.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the abstract does not mention any limitations of LLMs, but rather utilizes them as a component of the proposed approach.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the abstract does not mention any limitations of LLMs, but rather utilizes them as a component of the proposed approach."
    },
    {
        "title": "Prompt to be Consistent is Better than Self-Consistent? Few-Shot and Zero-Shot Fact Verification with Pre-trained Language Models",
        "authors": [
            "Fengzhu Zeng",
            "Wei Gao"
        ],
        "published": "2023",
        "summary": "Few-shot or zero-shot fact verification only relies on a few or no labeled training examples. In this paper, we propose a novel method called ProToCo, to Prompt pre-trained language models (PLMs) To be Consistent, for improving the factuality assessment capability of PLMs in the few-shot and zero-shot settings. Given a claim-evidence pair, ProToCo generates multiple variants of the claim with different relations and frames a simple consistency mechanism as constraints for making compatible predictions across these variants. We update PLMs by using parameter-efficient fine-tuning (PEFT), leading to more accurate predictions in few-shot and zero-shot fact verification tasks. Our experiments on three public verification datasets show that ProToCo significantly outperforms state-of-the-art few-shot fact verification baselines. With a small number of unlabeled instances, ProToCo also outperforms the strong zero-shot learner T0 on zero-shot verification. Compared to large PLMs using in-context learning (ICL) method, ProToCo outperforms OPT-30B and the Self-Consistency-enabled OPT-6.7B model in both few- and zero-shot settings.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.278.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicit, but implies limitations in few-shot and zero-shot fact verification tasks by proposing a novel method to improve the factuality assessment capability of PLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None explicit, but implies limitations in few-shot and zero-shot fact verification tasks by proposing a novel method to improve the factuality assessment capability of PLMs."
    },
    {
        "title": "Debiasing should be Good and Bad: Measuring the Consistency of Debiasing Techniques in Language Models",
        "authors": [
            "Robert Morabito",
            "Jad Kabbara",
            "Ali Emami"
        ],
        "published": "2023",
        "summary": "Debiasing methods that seek to mitigate the tendency of Language Models (LMs) to occasionally output toxic or inappropriate text have recently gained traction. In this paper, we propose a standardized protocol which distinguishes methods that yield not only desirable results, but are also consistent with their mechanisms and specifications. For example, we ask, given a debiasing method that is developed to reduce toxicity in LMs, if the definition of toxicity used by the debiasing method is reversed, would the debiasing results also be reversed? We used such considerations to devise three criteria for our new protocol: Specification Polarity, Specification Importance, and Domain Transferability. As a case study, we apply our protocol to a popular debiasing method, Self-Debiasing, and compare it to one we propose, called Instructive Debiasing, and demonstrate that consistency is as important an aspect to debiasing viability as is simply a desirable result. We show that our protocol provides essential insights into the generalizability and interpretability of debiasing methods that may otherwise go overlooked.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.280.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Debiasing methods that seek to mitigate the tendency of Language Models (LMs) to occasionally output toxic or inappropriate text\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Debiasing methods that seek to mitigate the tendency of Language Models (LMs) to occasionally output toxic or inappropriate text\""
    },
    {
        "title": "Critic-Guided Decoding for Controlled Text Generation",
        "authors": [
            "Minbeom Kim",
            "Hwanhee Lee",
            "Kang Min Yoo",
            "Joonsuk Park",
            "Hwaran Lee",
            "Kyomin Jung"
        ],
        "published": "2023",
        "summary": "Steering language generation towards objectives or away from undesired content has been a long-standing goal in utilizing language models (LM). Recent work has demonstrated reinforcement learning and weighted decoding as effective approaches to achieve a higher level of language control and quality with pros and cons. In this work, we propose a novel critic decoding method for controlled language generation (CriticControl) that combines the strengths of reinforcement learning and weighted decoding. Specifically, we adopt the actor-critic framework and train an LM-steering critic from reward models. Similar to weighted decoding, our method freezes the language model and manipulates the output token distribution using a critic to improve training efficiency and stability. Evaluation of our method on three controlled generation tasks, topic control, sentiment control, and detoxification, shows that our approach generates more coherent and well-controlled texts than previous methods. In addition, CriticControl demonstrates superior generalization ability in zero-shot settings. Human evaluation studies also corroborate our findings.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.281.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but mentions \"pros and cons\" of previous approaches which implies limitations, however, it does not explicitly discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but mentions \"pros and cons\" of previous approaches which implies limitations, however, it does not explicitly discuss any limitations of LLMs."
    },
    {
        "title": "SEAG: Structure-Aware Event Causality Generation",
        "authors": [
            "Zhengwei Tao",
            "Zhi Jin",
            "Xiaoying Bai",
            "Haiyan Zhao",
            "Chengfeng Dou",
            "Yongqiang Zhao",
            "Fang Wang",
            "Chongyang Tao"
        ],
        "published": "2023",
        "summary": "Extracting event causality underlies a broad spectrum of natural language processing applications. Cutting-edge methods break this task into Event Detection and Event Causality Identification. Although the pipelined solutions succeed in achieving acceptable results, the inherent nature of separating the task incurs limitations. On the one hand, it suffers from the lack of cross-task dependencies and may cause error propagation. On the other hand, it predicts events and relations separately, undermining the integrity of the event causality graph (ECG). To address such issues, in this paper, we propose an approach for Structure-Aware Event Causality Generation (SEAG). With a graph linearization module, we generate the ECG structure in a way of text2text generation based on a pre-trained language model. To foster the structural representation of the ECG, we introduce the novel Causality Structural Discrimination training paradigm in which we perform structural discriminative training alongside auto-regressive generation enabling the model to distinguish from constructed incorrect ECGs. We conduct experiments on three datasets. The experimental results demonstrate the effectiveness of structural event causality generation and the causality structural discrimination training.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.283.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although the pipelined solutions succeed in achieving acceptable results, the inherent nature of separating the task incurs limitations. On the one hand, it suffers from the lack of cross-task dependencies and may cause error propagation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although the pipelined solutions succeed in achieving acceptable results, the inherent nature of separating the task incurs limitations. On the one hand, it suffers from the lack of cross-task dependencies and may cause error propagation.\""
    },
    {
        "title": "Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning",
        "authors": [
            "Ruixiang Tang",
            "Dehan Kong",
            "Longtao Huang",
            "Hui Xue"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have recently shown great potential for in-context learning, where LLMs learn a new task simply by conditioning on a few input-label pairs (prompts). Despite their potential, our understanding of the factors influencing end-task performance and the robustness of in-context learning remains limited. This paper aims to bridge this knowledge gap by investigating the reliance of LLMs on shortcuts or spurious correlations within prompts. Through comprehensive experiments on classification and extraction tasks, we reveal that LLMs are “lazy learners” that tend to exploit such shortcuts. Additionally, we uncover a surprising finding that larger models are more likely to utilize shortcuts in prompts during inference. Our findings provide a new perspective on evaluating robustness in in-context learning and pose new challenges for detecting and mitigating the use of shortcuts in prompts.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.284.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"we reveal that LLMs are “lazy learners” that tend to exploit such shortcuts. Additionally, we uncover a surprising finding that larger models are more likely to utilize shortcuts in prompts during inference.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"we reveal that LLMs are “lazy learners” that tend to exploit such shortcuts. Additionally, we uncover a surprising finding that larger models are more likely to utilize shortcuts in prompts during inference.\""
    },
    {
        "title": "Unsupervised Paraphrasing of Multiword Expressions",
        "authors": [
            "Takashi Wada",
            "Yuji Matsumoto",
            "Timothy Baldwin",
            "Jey Han Lau"
        ],
        "published": "2023",
        "summary": "We propose an unsupervised approach to paraphrasing multiword expressions (MWEs) in context. Our model employs only monolingual corpus data and pre-trained language models (without fine-tuning), and does not make use of any external resources such as dictionaries. We evaluate our method on the SemEval 2022 idiomatic semantic text similarity task, and show that it outperforms all unsupervised systems and rivals supervised systems.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.290.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"employs only monolingual corpus data and pre-trained language models (without fine-tuning)\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"employs only monolingual corpus data and pre-trained language models (without fine-tuning)\""
    },
    {
        "title": "G-Tuning: Improving Generalization of Pre-trained Language Models with Generative Adversarial Network",
        "authors": [
            "Rongxiang Weng",
            "Wen Sen Cheng",
            "Min Zhang"
        ],
        "published": "2023",
        "summary": "The generalization ability of pre-trained language models (Plms) in downstream tasks is heavily influenced by fine-tuning. The objective of fine-tuning is to transform the latent representation of Plms from a universal space to a target space, allowing the model to be applied to downstream tasks with the capability of generalizing to unseen samples. However, the effect of Plms will be diminished when the training data coverage is insufficient, in which fine-tuning is inadequate to learn the complete mapping. In this study, we propose a new fine-tuning framework, referred to as G-Tuning, that aims to preserve the generalization ability of Plms in downstream tasks. Specifically, we integrate a generative adversarial network into the fine-tuning process to aid in the transformation of the latent representation in the entire space. Empirical evaluations on the GLUE benchmark, as well as two additional demanding scenarios involving domain and language generalization, demonstrate that G-Tuning can accurately map the universal representation to the target space, thus effectively enhancing the generalization performance of Plms across various downstream tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.291.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the effect of Plms will be diminished when the training data coverage is insufficient, in which fine-tuning is inadequate to learn the complete mapping.\"\n\nThis abstract briefly mentions one limitation of pre-trained language models, which is that fine-tuning may be inadequate when training data coverage is insufficient.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2**\nEvidence: \"However, the effect of Plms will be diminished when the training data coverage is insufficient, in which fine-tuning is inadequate to learn the complete mapping.\"\n\nThis abstract briefly mentions one limitation of pre-trained language models, which is that fine-tuning may be inadequate when training data coverage is insufficient."
    },
    {
        "title": "Unified Language Representation for Question Answering over Text, Tables, and Images",
        "authors": [
            "Bowen Yu",
            "Cheng Fu",
            "Haiyang Yu",
            "Fei Huang",
            "Yongbin Li"
        ],
        "published": "2023",
        "summary": "When trying to answer complex questions, people often rely on multiple sources of information, such as visual, textual, and tabular data. Previous approaches to this problem have focused on designing input features or model structure in the multi-modal space, which is inflexible for cross-modal reasoning or data-efficient training. In this paper, we call for an alternative paradigm, which transforms the images and tables into unified language representations, so that we can simplify the task into a simpler textual QA problem that can be solved using three steps: retrieval, ranking, and generation, all within a language space. This idea takes advantage of the power of pre-trained language models and is implemented in a framework called Solar. Our experimental results show that Solar outperforms all existing methods by 10.6-32.3 pts on two datasets, MultimodalQA and MMCoQA, across ten different metrics. Additionally, Solar achieves the best performance on the WebQA leaderboard.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.292.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"takes advantage of the power of pre-trained language models\"\n\nThis abstract mentions pre-trained language models, but does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"takes advantage of the power of pre-trained language models\"\n\nThis abstract mentions pre-trained language models, but does not discuss any limitations of LLMs."
    },
    {
        "title": "Predicting Numerals in Text Using Nearest Neighbor Language Models",
        "authors": [
            "Taku Sakamoto",
            "Akiko Aizawa"
        ],
        "published": "2023",
        "summary": "Commonsense about quantitative properties is essential for a deep understanding of texts containing numerals. However, naive language models (LMs) treat numerals as string tokens; therefore, they lack an understanding of the magnitudes of numerals, resulting in a difficulty in acquiring the commonsense. In this study, we apply the k-nearest neighbor LM (kNN-LM) to the masked numeral prediction (MNP) task, which measures the quantitative commonsense of LMs.kNN-LM extends pre-trained neural LMs with the k-nearest neighbor (kNN) search.Since it can utilize patterns that appear in the datastore for prediction, we expect an improvement in numeral prediction accuracy, which is associated with a high rate of occurrence of out-of-vocabulary (OOV) words.Through experiments, we verified that the retrieval-based method is effective for fine-grained predictions of numerals from context, especially for the OOV numerals.We also compared two different context spans for context representations to improve the accuracy of kNN search by using only the words that are closely related to the masked numeral: the mask and its surrounding words, and the mask and its subsequent words.Our results reveal that using only the embeddings of mask tokens for numerals in kNN search is the most effective approach for realizing MNP tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.295.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, naive language models (LMs) treat numerals as string tokens; therefore, they lack an understanding of the magnitudes of numerals, resulting in a difficulty in acquiring the commonsense.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, naive language models (LMs) treat numerals as string tokens; therefore, they lack an understanding of the magnitudes of numerals, resulting in a difficulty in acquiring the commonsense.\""
    },
    {
        "title": "Few Shot Rationale Generation using Self-Training with Dual Teachers",
        "authors": [
            "Aditya Srikanth Veerubhotla",
            "Lahari Poddar",
            "Jun Yin",
            "György Szarvas",
            "Sharanya Eswaran"
        ],
        "published": "2023",
        "summary": "Self-rationalizing models that also generate a free-text explanation for their predicted labels are an important tool to build trustworthy AI applications. Since generating explanations for annotated labels is a laborious and costly process, recent models rely on large pretrained language models (PLMs) as their backbone and few-shot learning. In this work we explore a self-training approach leveraging both labeled and unlabeled data to further improve few-shot models, under the assumption that neither human written rationales nor annotated task labels are available at scale. We introduce a novel dual-teacher learning framework, which learns two specialized teacher models for task prediction and rationalization using self-training and distills their knowledge into a multi-tasking student model that can jointly generate the task label and rationale. Furthermore, we formulate a new loss function, Masked Label Regularization(MLR) which promotes explanations to be strongly conditioned on predicted labels. Evaluation on three public datasets demonstrate that the proposed methods are effective in modeling task labels and generating faithful rationales.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.297.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"generating explanations for annotated labels is a laborious and costly process\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"generating explanations for annotated labels is a laborious and costly process\""
    },
    {
        "title": "Towards Accurate Translation via Semantically Appropriate Application of Lexical Constraints",
        "authors": [
            "Yujin Baek",
            "Koanho Lee",
            "Dayeon Ki",
            "Cheonbok Park",
            "Hyoung-Gyu Lee",
            "Jaegul Choo"
        ],
        "published": "2023",
        "summary": "Lexically-constrained NMT (LNMT) aims to incorporate user-provided terminology into translations. Despite its practical advantages, existing work has not evaluated LNMT models under challenging real-world conditions. In this paper, we focus on two important but understudied issues that lie in the current evaluation process of LNMT studies. The model needs to cope with challenging lexical constraints that are “homographs” or “unseen” during training. To this end, we first design a homograph disambiguation module to differentiate the meanings of homographs. Moreover, we propose PLUMCOT which integrates contextually rich information about unseen lexical constraints from pre-trained language models and strengthens a copy mechanism of the pointer network via direct supervision of a copying score. We also release HOLLY, an evaluation benchmark for assessing the ability of model to cope with “homographic” and “unseen” lexical constraints. Experiments on HOLLY and the previous test setup show the effectiveness of our method. The effects of PLUMCOT are shown to be remarkable in “unseen” constraints. Our dataset is available at https://github.com/papago-lab/HOLLY-benchmark.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.298.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "LMs stand their Ground: Investigating the Effect of Embodiment in Figurative Language Interpretation by Language Models",
        "authors": [
            "Philipp Wicke"
        ],
        "published": "2023",
        "summary": "Figurative language is a challenge for language models since its interpretation is based on the use of words in a way that deviates from their conventional order and meaning. Yet, humans can easily understand and interpret metaphors, similes or idioms as they can be derived from embodied metaphors. Language is a proxy for embodiment and if a metaphor is conventional and lexicalised, it becomes easier for a system without a body to make sense of embodied concepts. Yet, the intricate relation between embodiment and features such as concreteness or age of acquisition has not been studied in the context of figurative language interpretation concerning language models. Hence, the presented study shows how larger language models perform better at interpreting metaphoric sentences when the action of the metaphorical sentence is more embodied. The analysis rules out multicollinearity with other features (e.g. word length or concreteness) and provides initial evidence that larger language models conceptualise embodied concepts to a degree that facilitates figurative language understanding.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.302.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Figurative language is a challenge for language models since its interpretation is based on the use of words in a way that deviates from their conventional order and meaning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Figurative language is a challenge for language models since its interpretation is based on the use of words in a way that deviates from their conventional order and meaning.\""
    },
    {
        "title": "Code Execution with Pre-trained Language Models",
        "authors": [
            "Chenxiao Liu",
            "Shuai Lu",
            "Weizhu Chen",
            "Daxin Jiang",
            "Alexey Svyatkovskiy",
            "Shengyu Fu",
            "Neel Sundaresan",
            "Nan Duan"
        ],
        "published": "2023",
        "summary": "Code execution is a fundamental aspect of programming language semantics that reflects the exact behavior of the code. However, most pre-trained models for code intelligence ignore the execution trace and only rely on source code and syntactic structures. In this paper, we investigate how well pre-trained models can understand and perform code execution. We develop a mutation-based data augmentation technique to create a large-scale and realistic Python dataset and task for code execution, which challenges existing models such as Codex. We then present CodeExecutor, a Transformer model that leverages code execution pre-training and curriculum learning to enhance its semantic comprehension. We evaluate CodeExecutor on code execution and show its promising performance and limitations. We also demonstrate its potential benefits for code intelligence tasks such as zero-shot code-to-code search and text-to-code generation. Our analysis provides insights into the learning and generalization abilities of pre-trained models for code execution.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.308.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We evaluate CodeExecutor on code execution and show its promising performance and limitations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We evaluate CodeExecutor on code execution and show its promising performance and limitations.\""
    },
    {
        "title": "BertNet: Harvesting Knowledge Graphs with Arbitrary Relations from Pretrained Language Models",
        "authors": [
            "Shibo Hao",
            "Bowen Tan",
            "Kaiwen Tang",
            "Bin Ni",
            "Xiyan Shao",
            "Hengzhe Zhang",
            "Eric Xing",
            "Zhiting Hu"
        ],
        "published": "2023",
        "summary": "It is crucial to automatically construct knowledge graphs (KGs) of diverse new relations to support knowledge discovery and broad applications. Previous KG construction methods, based on either crowdsourcing or text mining, are often limited to a small predefined set of relations due to manual cost or restrictions in text corpus. Recent research proposed to use pretrained language models (LMs) as implicit knowledge bases that accept knowledge queries with prompts. Yet, the implicit knowledge lacks many desirable properties of a full-scale symbolic KG, such as easy access, navigation, editing, and quality assurance. In this paper, we propose a new approach of harvesting massive KGs of arbitrary relations from pretrained LMs. With minimal input of a relation definition (a prompt and a few shot of example entity pairs), the approach efficiently searches in the vast entity pair space to extract diverse accurate knowledge of the desired relation. We develop an effective search-and-rescore mechanism for improved efficiency and accuracy. We deploy the approach to harvest KGs of over 400 new relations, from LMs of varying capacities such as RoBERTaNet. Extensive human and automatic evaluations show our approach manages to extract diverse accurate knowledge, including tuples of complex relations (e.g., “A is capable of but not good at B”). The resulting KGs as a symbolic interpretation of the source LMs also reveal new insights into the LMs’ knowledge capacities.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.309.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Yet, the implicit knowledge lacks many desirable properties of a full-scale symbolic KG, such as easy access, navigation, editing, and quality assurance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Yet, the implicit knowledge lacks many desirable properties of a full-scale symbolic KG, such as easy access, navigation, editing, and quality assurance.\""
    },
    {
        "title": "Data-Efficient French Language Modeling with CamemBERTa",
        "authors": [
            "Wissam Antoun",
            "Benoît Sagot",
            "Djamé Seddah"
        ],
        "published": "2023",
        "summary": "Recent advances in NLP have significantly improved the performance of language models on a variety of tasks. While these advances are largely driven by the availability of large amounts of data and computational power, they also benefit from the development of better training methods and architectures. In this paper, we introduce CamemBERTa, a French DeBERTa model that builds upon the DeBERTaV3 architecture and training objective. We evaluate our model’s performance on a variety of French downstream tasks and datasets, including question answering, part-of-speech tagging, dependency parsing, named entity recognition, and the FLUE benchmark, and compare against CamemBERT, the state-of-the-art monolingual model for French. Our results show that, given the same amount of training tokens, our model outperforms BERT-based models trained with MLM on most tasks. Furthermore, our new model reaches similar or superior performance on downstream tasks compared to CamemBERT, despite being trained on only 30% of its total number of input tokens. In addition to our experimental results, we also publicly release the weights and code implementation of CamemBERTa, making it the first publicly available DeBERTaV3 model outside of the original paper and the first openly available implementation of a DeBERTaV3 training objective.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.320.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text",
        "authors": [
            "Zhun Yang",
            "Adam Ishay",
            "Joohyung Lee"
        ],
        "published": "2023",
        "summary": "While large language models (LLMs), such as GPT-3, appear to be robust and general, their reasoning ability is not at a level to compete with the best models trained for specific natural language reasoning problems. In this study, we observe that a large language model can serve as a highly effective few-shot semantic parser. It can convert natural language sentences into a logical form that serves as input for answer set programs, a logic-based declarative knowledge representation formalism. The combination results in a robust and general system that can handle multiple question-answering tasks without requiring retraining for each new task. It only needs a few examples to guide the LLM’s adaptation to a specific task, along with reusable ASP knowledge modules that can be applied to multiple tasks. We demonstrate that this method achieves state-of-the-art performance on several NLP benchmarks, including bAbI, StepGame, CLUTRR, and gSCAN. Additionally, it successfully tackles robot planning tasks that an LLM alone fails to solve.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.321.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While large language models (LLMs), such as GPT-3, appear to be robust and general, their reasoning ability is not at a level to compete with the best models trained for specific natural language reasoning problems.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While large language models (LLMs), such as GPT-3, appear to be robust and general, their reasoning ability is not at a level to compete with the best models trained for specific natural language reasoning problems.\""
    },
    {
        "title": "Evaluating the Factual Consistency of Large Language Models Through News Summarization",
        "authors": [
            "Derek Tam",
            "Anisha Mascarenhas",
            "Shiyue Zhang",
            "Sarah Kwan",
            "Mohit Bansal",
            "Colin Raffel"
        ],
        "published": "2023",
        "summary": "While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB (Factual Inconsistency Benchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as factually consistent. To generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as factually inconsistent. A model’s factual consistency is then measured according to its accuracy, i.e. the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of {pasted macro ‘BENCHMARK’}, we evaluate 23 large language models ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We find that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.322.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries.\"\n\nThis rating is given because the paper mentions a limitation of LLMs, specifically their tendency to assign higher scores to factually inconsistent summaries if they occur verbatim in the document, but it is not the",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries.\"\n\nThis rating is given because the paper mentions a limitation of LLMs, specifically their tendency to assign higher scores to factually inconsistent summaries if they occur verbatim in the document, but it is not the"
    },
    {
        "title": "Text Generation Model Enhanced with Semantic Information in Aspect Category Sentiment Analysis",
        "authors": [
            "Tu Tran",
            "Kiyoaki Shirai",
            "Natthawut Kertkeidkachorn"
        ],
        "published": "2023",
        "summary": "Aspect Category Sentiment Analysis (ACSA) is one of the main subtasks of sentiment analysis, which aims at predicting polarity over a given aspect category. Recently, generative methods emerge as an efficient way to utilize a pre-trained language model for solving ACSA. However, those methods fail to model relations of target words and opinion words in a sentence including multiple aspects. To tackle this problem, this paper proposes a method to incorporate Abstract Meaning Representation (AMR), which describes semantic representation of a sentence as a directed graph, into a text generation model. Furthermore, two regularizers are designed to guide cross attention weights allocation over AMR graphs. One is the identical regularizer that constrains attention weights of aligned nodes, the other is the entropy regularizer that helps the decoder generate tokens by heavily considering only a few related nodes in the AMR graph. Experimental results on three datasets show that the proposed method outperforms state-of-the-art methods, proving the effectiveness of our model.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.323.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Mind the Biases: Quantifying Cognitive Biases in Language Model Prompting",
        "authors": [
            "Ruixi Lin",
            "Hwee Tou Ng"
        ],
        "published": "2023",
        "summary": "We advocate the importance of exposing uncertainty on results of language model prompting which display bias modes resembling cognitive biases, and propose to help users grasp the level of uncertainty via simple quantifying metrics. Cognitive biases in the human decision making process can lead to flawed responses when we are under uncertainty. Not surprisingly, we have seen biases in language models resembling cognitive biases as a result of training on biased textual data, raising dangers in downstream tasks that are centered around people’s lives if users trust their results too much. In this work, we reveal two bias modes leveraging cognitive biases when we prompt BERT, accompanied by two bias metrics. On a drug-drug interaction extraction task, our bias measurements reveal an error pattern similar to the availability bias when the labels for training prompts are imbalanced, and show that a toning-down transformation of the drug-drug description in a prompt can elicit a bias similar to the framing effect, warning users to distrust when prompting language models for answers.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.324.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Cognitive biases in the human decision making process can lead to flawed responses when we are under uncertainty. Not surprisingly, we have seen biases in language models resembling cognitive biases as a result of training on biased textual data, raising dangers in downstream tasks that are centered around people’s lives if users trust their results too much.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Cognitive biases in the human decision making process can lead to flawed responses when we are under uncertainty. Not surprisingly, we have seen biases in language models resembling cognitive biases as a result of training on biased textual data, raising dangers in downstream tasks that are centered around people’s lives if users trust their results too much.\""
    },
    {
        "title": "CodePrompt: Task-Agnostic Prefix Tuning for Program and Language Generation",
        "authors": [
            "YunSeok Choi",
            "Jee-Hyong Lee"
        ],
        "published": "2023",
        "summary": "In order to solve the inefficient parameter update and storage issues of fine-tuning in Natural Language Generation (NLG) tasks, prompt-tuning methods have emerged as lightweight alternatives. Furthermore, efforts to reduce the gap between pre-training and fine-tuning have shown successful results in low-resource settings. As large Pre-trained Language Models (PLMs) for Program and Language Generation (PLG) tasks are constantly being developed, prompt tuning methods are necessary for the tasks. However, due to the gap between pre-training and fine-tuning different from PLMs for natural language, a prompt tuning method that reflects the traits of PLM for program language is needed. In this paper, we propose a Task-Agnostic prompt tuning method for the PLG tasks, CodePrompt, that combines Input-Dependent Prompt Template (to bridge the gap between pre-training and fine-tuning of PLMs for program and language) and Corpus-Specific Prefix Tuning (to update the parameters of PLMs for program and language efficiently).Also, we propose a method to provide richer prefix word information for limited prefix lengths. We prove that our method is effective in three PLG tasks, not only in the full-data setting but also in the low-resource setting and cross-domain setting.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.325.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, due to the gap between pre-training and fine-tuning different from PLMs for natural language, a prompt tuning method that reflects the traits of PLM for program language is needed.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of PLMs (the gap between pre-training and fine-tuning) but only briefly and as a minor detail, without further",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, due to the gap between pre-training and fine-tuning different from PLMs for natural language, a prompt tuning method that reflects the traits of PLM for program language is needed.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of PLMs (the gap between pre-training and fine-tuning) but only briefly and as a minor detail, without further"
    },
    {
        "title": "Honey, I Shrunk the Language: Language Model Behavior at Reduced Scale.",
        "authors": [
            "Vijeta Deshpande",
            "Dan Pechi",
            "Shree Thatte",
            "Vladislav Lialin",
            "Anna Rumshisky"
        ],
        "published": "2023",
        "summary": "In recent years, language models have drastically grown in size, and the abilities of these models have been shown to improve with scale. The majority of recent scaling laws studies focused on high-compute high-parameter count settings, leaving the question of when these abilities begin to emerge largely unanswered. In this paper, we investigate whether the effects of pre-training can be observed when the problem size is reduced, modeling a smaller, reduced-vocabulary language. We show the benefits of pre-training with masked language modeling (MLM) objective in models as small as 1.25M parameters, and establish a strong correlation between pre-training perplexity and downstream performance (GLUE benchmark). We examine downscaling effects, extending scaling laws to models as small as ~1M parameters. At this scale, we observe a break of the power law for compute-optimal models and show that the MLM loss does not scale smoothly with compute-cost (FLOPs) below 2.2 × 1015 FLOPs. We also find that adding layers does not always benefit downstream performance.Our filtered pre-training data, reduced English vocabulary, and code are available at https://github.com/text-machine-lab/mini_bertgithub.com/text-machine-lab/mini_bert",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.326.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"At this scale, we observe a break of the power law for compute-optimal models and show that the MLM loss does not scale smoothly with compute-cost (FLOPs) below 2.2 × 10^15 FLOPs. We also find that adding layers does not always benefit downstream performance.\"\n\nThis rating is given because the paper discusses the limitations of LLMs",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"At this scale, we observe a break of the power law for compute-optimal models and show that the MLM loss does not scale smoothly with compute-cost (FLOPs) below 2.2 × 10^15 FLOPs. We also find that adding layers does not always benefit downstream performance.\"\n\nThis rating is given because the paper discusses the limitations of LLMs"
    },
    {
        "title": "Communication Efficient Federated Learning for Multilingual Neural Machine Translation with Adapter",
        "authors": [
            "Yi Liu",
            "Xiaohan Bi",
            "Lei Li",
            "Sishuo Chen",
            "Wenkai Yang",
            "Xu Sun"
        ],
        "published": "2023",
        "summary": "Federated Multilingual Neural Machine Translation (Fed-MNMT) has emerged as a promising paradigm for institutions with limited language resources. This approach allows multiple institutions to act as clients and train a unified model through model synchronization, rather than collecting sensitive data for centralized training. This significantly reduces the cost of corpus collection and preserves data privacy. However, as pre-trained language models (PLMs) continue to increase in size, the communication cost for transmitting parameters during synchronization has become a training speed bottleneck. In this paper, we propose a communication-efficient Fed-MNMT framework that addresses this issue by keeping PLMs frozen and only transferring lightweight adapter modules between clients. Since different language pairs exhibit substantial discrepancies in data distributions, adapter parameters of clients may conflict with each other. To tackle this, we explore various clustering strategies to group parameters for integration and mitigate the negative effects of conflicting parameters. Experimental results demonstrate that our framework reduces communication cost by over 98% while achieving similar or even better performance compared to competitive baselines. Further analysis reveals that clustering strategies effectively solve the problem of linguistic discrepancy and pruning adapter modules further improves communication efficiency.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.327.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, as pre-trained language models (PLMs) continue to increase in size, the communication cost for transmitting parameters during synchronization has become a training speed bottleneck.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, as pre-trained language models (PLMs) continue to increase in size, the communication cost for transmitting parameters during synchronization has become a training speed bottleneck.\""
    },
    {
        "title": "Automatic Readability Assessment for Closely Related Languages",
        "authors": [
            "Joseph Marvin Imperial",
            "Ekaterina Kochmar"
        ],
        "published": "2023",
        "summary": "In recent years, the main focus of research on automatic readability assessment (ARA) has shifted towards using expensive deep learning-based methods with the primary goal of increasing models’ accuracy. This, however, is rarely applicable for low-resource languages where traditional handcrafted features are still widely used due to the lack of existing NLP tools to extract deeper linguistic representations. In this work, we take a step back from the technical component and focus on how linguistic aspects such as mutual intelligibility or degree of language relatedness can improve ARA in a low-resource setting. We collect short stories written in three languages in the Philippines—Tagalog, Bikol, and Cebuano—to train readability assessment models and explore the interaction of data and features in various cross-lingual setups. Our results show that the inclusion of CrossNGO, a novel specialized feature exploiting n-gram overlap applied to languages with high mutual intelligibility, significantly improves the performance of ARA models compared to the use of off-the-shelf large multilingual language models alone. Consequently, when both linguistic representations are combined, we achieve state-of-the-art results for Tagalog and Cebuano, and baseline scores for ARA in Bikol.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.331.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Boosting Distress Support Dialogue Responses with Motivational Interviewing Strategy",
        "authors": [
            "Anuradha Welivita",
            "Pearl Pu"
        ],
        "published": "2023",
        "summary": "AI-driven chatbots have become an emerging solution to address psychological distress. Due to the lack of psychotherapeutic data, researchers use dialogues scraped from online peer support forums to train them. But since the responses in such platforms are not given by professionals, they contain both conforming and non-conforming responses. In this work, we attempt to recognize these conforming and non-conforming response types present in online distress-support dialogues using labels adapted from a well-established behavioral coding scheme named Motivational Interviewing Treatment Integrity (MITI) code and show how some response types could be rephrased into a more MI adherent form that can, in turn, enable chatbot responses to be more compliant with the MI strategy. As a proof of concept, we build several rephrasers by fine-tuning Blender and GPT3 to rephrase MI non-adherent Advise without permission responses into Advise with permission. We show how this can be achieved with the construction of pseudo-parallel corpora avoiding costs for human labor. Through automatic and human evaluation we show that in the presence of less training data, techniques such as prompting and data augmentation can be used to produce substantially good rephrasings that reflect the intended style and preserve the content of the original text.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.334.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations, but mentions \"due to the lack of psychotherapeutic data\" which is related to the challenges of training LLMs, however, this is not the main focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations, but mentions \"due to the lack of psychotherapeutic data\" which is related to the challenges of training LLMs, however, this is not the main focus of the paper."
    },
    {
        "title": "Gender-tuning: Empowering Fine-tuning for Debiasing Pre-trained Language Models",
        "authors": [
            "Somayeh Ghanbarzadeh",
            "Yan Huang",
            "Hamid Palangi",
            "Radames Cruz Moreno",
            "Hamed Khanpour"
        ],
        "published": "2023",
        "summary": "Recent studies have revealed that the widely-used Pre-trained Language Models (PLMs) propagate societal biases from the large unmoderated pre-training corpora. Existing solutions require debiasing training processes and datasets for debiasing, which are resource-intensive and costly. Furthermore, these methods hurt the PLMs’ performance on downstream tasks. In this study, we propose Gender-tuning, which debiases the PLMs through fine-tuning on downstream tasks’ datasets. For this aim, Gender-tuning integrates Masked Language Modeling (MLM) training objectives into fine-tuning’s training process. Comprehensive experiments show that Gender-tuning outperforms the state-of-the-art baselines in terms of average gender bias scores in PLMs while improving PLMs’ performance on downstream tasks solely using the downstream tasks’ dataset. Also, Gender-tuning is a deployable debiasing tool for any PLM that works with original fine-tuning.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.336.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent studies have revealed that the widely-used Pre-trained Language Models (PLMs) propagate societal biases from the large unmoderated pre-training corpora.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Recent studies have revealed that the widely-used Pre-trained Language Models (PLMs) propagate societal biases from the large unmoderated pre-training corpora.\""
    },
    {
        "title": "TextObfuscator: Making Pre-trained Language Model a Privacy Protector via Obfuscating Word Representations",
        "authors": [
            "Xin Zhou",
            "Yi Lu",
            "Ruotian Ma",
            "Tao Gui",
            "Yuran Wang",
            "Yong Ding",
            "Yibo Zhang",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2023",
        "summary": "In real-world applications, pre-trained language models are typically deployed on the cloud, allowing clients to upload data and perform compute-intensive inference remotely. To avoid sharing sensitive data directly with service providers, clients can upload numerical representations rather than plain text to the cloud. However, recent text reconstruction techniques have demonstrated that it is possible to transform representations into original words, suggesting that privacy risk remains. In this paper, we propose TextObfuscator, a novel framework for protecting inference privacy by applying random perturbations to clustered representations. The random perturbations make the representations indistinguishable from surrounding clustered representations, thus obscuring word information while retaining the original word functionality. To achieve this, we utilize prototypes to learn clustered representation, where tokens of similar functionality are encouraged to be closer to the same prototype during training. Additionally, we design different methods to find prototypes for token-level and sentence-level tasks, which can improve performance by incorporating semantic and task information. Experimental results on token and sentence classification tasks show that TextObfuscator achieves improvement over compared methods without increasing inference cost.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.337.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, recent text reconstruction techniques have demonstrated that it is possible to transform representations into original words, suggesting that privacy risk remains.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, recent text reconstruction techniques have demonstrated that it is possible to transform representations into original words, suggesting that privacy risk remains.\""
    },
    {
        "title": "Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training",
        "authors": [
            "Kelly Marchisio",
            "Patrick Lewis",
            "Yihong Chen",
            "Mikel Artetxe"
        ],
        "published": "2023",
        "summary": "Prior work shows that it is possible to expand pretrained Masked Language Models (MLMs) to new languages by learning a new set of embeddings, while keeping the transformer body frozen. Despite learning a small subset of parameters, this approach is not compute-efficient, as training the new embeddings requires a full forward and backward pass over the entire model. We propose mini-model adaptation, a compute-efficient alternative that builds a shallow mini-model from a fraction of a large model’s parameters. New language-specific embeddings can then be efficiently trained over the mini-model and plugged into the aligned large model for rapid cross-lingual transfer. We explore two approaches to learn mini-models: MINIJOINT, which jointly pretrains the primary model and the mini-model using a single transformer with a secondary MLM head at a middle layer; and MINIPOST, where we start from a regular pretrained model, build a mini-model by extracting and freezing a few layers, and learn a small number of parameters on top. Experiments on XNLI, MLQA and PAWS-X show that mini-model adaptation matches the performance of the standard approach using up to 2.3x less compute on average.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.338.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Prior work shows that it is possible to expand pretrained Masked Language Models (MLMs) to new languages by learning a new set of embeddings, while keeping the transformer body frozen. Despite learning a small subset of parameters, this approach is not compute-efficient...\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Prior work shows that it is possible to expand pretrained Masked Language Models (MLMs) to new languages by learning a new set of embeddings, while keeping the transformer body frozen. Despite learning a small subset of parameters, this approach is not compute-efficient...\""
    },
    {
        "title": "DSP: Discriminative Soft Prompts for Zero-Shot Entity and Relation Extraction",
        "authors": [
            "Bo Lv",
            "Xin Liu",
            "Shaojie Dai",
            "Nayu Liu",
            "Fan Yang",
            "Ping Luo",
            "Yue Yu"
        ],
        "published": "2023",
        "summary": "Prompt-based methods have shown their efficacy in transferring general knowledge within pre-trained language models (PLMs) for low-resource scenarios. Typically, prompt-based methods convert downstream tasks to cloze-style problems and map all labels to verbalizers.However, when applied to zero-shot entity and relation extraction, vanilla prompt-based methods may struggle with the limited coverage of verbalizers to labels and the slow inference speed. In this work, we propose a novel Discriminate Soft Prompts (DSP) approach to take advantage of the prompt-based methods to strengthen the transmission of general knowledge. Specifically, we develop a discriminative prompt method, which reformulates zero-shot tasks into token discrimination tasks without having to construct verbalizers.Furthermore, to improve the inference speed of the prompt-based methods, we design a soft prompt co-reference strategy, which leverages soft prompts to approximately refer to the vector representation of text tokens. The experimental results show that, our model outperforms baselines on two zero-shot entity recognition datasets with higher inference speed, and obtains a 7.5% average relation F1-score improvement over previous state-of-the-art models on Wiki-ZSL and FewRel.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.339.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, when applied to zero-shot entity and relation extraction, vanilla prompt-based methods may struggle with the limited coverage of verbalizers to labels and the slow inference speed.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, when applied to zero-shot entity and relation extraction, vanilla prompt-based methods may struggle with the limited coverage of verbalizers to labels and the slow inference speed.\""
    },
    {
        "title": "Exploring Robust Overfitting for Pre-trained Language Models",
        "authors": [
            "Bin Zhu",
            "Yanghui Rao"
        ],
        "published": "2023",
        "summary": "We identify the robust overfitting issue for pre-trained language models by showing that the robust test loss increases as the epoch grows. Through comprehensive exploration of the robust loss on the training set, we attribute robust overfitting to the model’s memorization of the adversarial training data. We attempt to mitigate robust overfitting by combining regularization methods with adversarial training. Following the philosophy that prevents the model from memorizing the adversarial data, we find that flooding, a regularization method with loss scaling, can mitigate robust overfitting for pre-trained language models. Eventually, we investigate the effect of flooding levels and evaluate the models’ adversarial robustness under textual attacks. Extensive experiments demonstrate that our methods can mitigate robust overfitting upon three top adversarial training methods and further promote adversarial robustness.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.340.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We identify the robust overfitting issue for pre-trained language models by showing that the robust test loss increases as the epoch grows. Through comprehensive exploration of the robust loss on the training set, we attribute robust overfitting to the model’s memorization of the adversarial training data.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We identify the robust overfitting issue for pre-trained language models by showing that the robust test loss increases as the epoch grows. Through comprehensive exploration of the robust loss on the training set, we attribute robust overfitting to the model’s memorization of the adversarial training data.\""
    },
    {
        "title": "Language Anisotropic Cross-Lingual Model Editing",
        "authors": [
            "Yang Xu",
            "Yutai Hou",
            "Wanxiang Che",
            "Min Zhang"
        ],
        "published": "2023",
        "summary": "Multilingual pre-trained language models can learn task-specific abilities or memorize facts across multiple languages but inevitably make undesired predictions with specific inputs. Under similar observation, model editing aims to post-hoc calibrate a model targeted to specific inputs with keeping the model’s raw behavior. However, existing work only studies the monolingual scenario, which lacks the cross-lingual transferability to perform editing simultaneously across languages. In this work, we focus on cross-lingual model editing. Firstly, we define the cross-lingual model editing task and corresponding metrics, where an edit in one language propagates to the others. Next, we propose a framework to naturally adapt monolingual model editing approaches to the cross-lingual scenario using parallel corpus. Further, we propose language anisotropic editing to improve cross-lingual editing by amplifying different subsets of parameters for each language. On the newly defined cross-lingual model editing task, we empirically demonstrate the failure of monolingual baselines in propagating the edit to multiple languages and the effectiveness of the proposed language anisotropic model editing. Our code is publicly available at https://github.com/franklear/LiME.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.343.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Multilingual pre-trained language models... inevitably make undesired predictions with specific inputs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Multilingual pre-trained language models... inevitably make undesired predictions with specific inputs.\""
    },
    {
        "title": "Diverse Retrieval-Augmented In-Context Learning for Dialogue State Tracking",
        "authors": [
            "Brendan King",
            "Jeffrey Flanigan"
        ],
        "published": "2023",
        "summary": "There has been significant interest in zero and few-shot learning for dialogue state tracking (DST) due to the high cost of collecting and annotating task-oriented dialogues. Recent work has demonstrated that in-context learning requires very little data and zero parameter updates, and even outperforms trained methods in the few-shot setting. We propose RefPyDST, which advances the state of the art with three advancements to in-context learning for DST.First, we formulate DST as a Python programming task, explicitly modeling language coreference as variable reference in Python. Second, since in-context learning depends highly on the context examples, we propose a method to retrieve a diverse set of relevant examples to improve performance. Finally, we introduce a novel re-weighting method during decoding that takes into account probabilities of competing surface forms, and produces a more accurate dialogue state prediction. We evaluate our approach using MultiWOZ and achieve state-of-the-art multi-domain joint-goal accuracy in zero and few-shot settings.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.344.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Pre-Trained Language-Meaning Models for Multilingual Parsing and Generation",
        "authors": [
            "Chunliu Wang",
            "Huiyuan Lai",
            "Malvina Nissim",
            "Johan Bos"
        ],
        "published": "2023",
        "summary": "Pre-trained language models (PLMs) have achieved great success in NLP and have recently been used for tasks in computational semantics. However, these tasks do not fully benefit from PLMs since meaning representations are not explicitly included. We introduce multilingual pre-trained language-meaning models based on Discourse Representation Structures (DRSs), including meaning representations besides natural language texts in the same model, and design a new strategy to reduce the gap between the pre-training and fine-tuning objectives. Since DRSs are language neutral, cross-lingual transfer learning is adopted to further improve the performance of non-English tasks. Automatic evaluation results show that our approach achieves the best performance on both the multilingual DRS parsing and DRS-to-text generation tasks. Correlation analysis between automatic metrics and human judgements on the generation task further validates the effectiveness of our model. Human inspection reveals that out-of-vocabulary tokens are the main cause of erroneous results.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.345.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Human inspection reveals that out-of-vocabulary tokens are the main cause of erroneous results.\"\n\nThis paper discusses Large Language Models (LLMs) in the context of pre-trained language-meaning models and mentions a limitation related to out-of-vocabulary tokens, but it is a minor detail and not the primary focus of the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Human inspection reveals that out-of-vocabulary tokens are the main cause of erroneous results.\"\n\nThis paper discusses Large Language Models (LLMs) in the context of pre-trained language-meaning models and mentions a limitation related to out-of-vocabulary tokens, but it is a minor detail and not the primary focus of the abstract."
    },
    {
        "title": "Rethinking Semi-supervised Learning with Language Models",
        "authors": [
            "Zhengxiang Shi",
            "Francesco Tonolini",
            "Nikolaos Aletras",
            "Emine Yilmaz",
            "Gabriella Kazai",
            "Yunlong Jiao"
        ],
        "published": "2023",
        "summary": "Semi-supervised learning (SSL) is a popular setting aiming to effectively utilize unlabelled data to improve model performance in downstream natural language processing (NLP) tasks. Currently, there are two popular approaches to make use of the unlabelled data: Self-training (ST) and Task-adaptive pre-training (TAPT). ST uses a teacher model to assign pseudo-labels to the unlabelled data, while TAPT continues pre-training on the unlabelled data before fine-tuning. To the best of our knowledge, the effectiveness of TAPT in SSL tasks has not been systematically studied, and no previous work has directly compared TAPT and ST in terms of their ability to utilize the pool of unlabelled data. In this paper, we provide an extensive empirical study comparing five state-of-the-art ST approaches and TAPT across various NLP tasks and data sizes, including in- and out-of domain settings. Surprisingly, we find that TAPT is a strong and more robust SSL learner, even when using just a few hundred unlabelled samples or in the presence of domain shifts, compared to more sophisticated ST approaches, and tends to bring greater improvements in SSL than in fully-supervised settings. Our further analysis demonstrates the risks of using ST approaches when the size of labelled or unlabelled data is small or when domain shifts exist, and highlights TAPT as a potential solution.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.347.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our further analysis demonstrates the risks of using ST approaches when the size of labelled or unlabelled data is small or when domain shifts exist\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Our further analysis demonstrates the risks of using ST approaches when the size of labelled or unlabelled data is small or when domain shifts exist\""
    },
    {
        "title": "ECG-QALM: Entity-Controlled Synthetic Text Generation using Contextual Q&A for NER",
        "authors": [
            "Karan Aggarwal",
            "Henry Jin",
            "Aitzaz Ahmad"
        ],
        "published": "2023",
        "summary": "Named Entity Recognition (NER) state-of-the-art methods requires high-quality labeled datasets. Issues such as scarcity of labeled data, under-representation of entities, and privacy concerns with using sensitive data for training, can be significant barriers. Generating synthetic data to train models is a promising solution to mitigate these problems. We propose ECG-QALM, a contextual question and answering approach using pre-trained language models to synthetically generate entity-controlled text. Generated text is then used to augment small labeled datasets for downstream NER tasks. We evaluate our method on two publicly available datasets. We find ECG-QALM is capable of producing full text samples with desired entities appearing in a controllable way, while retaining sentence coherence closest to the real world data. Evaluations on NER tasks show significant improvements (75% - 140%) in low-labeled data regimes.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.349.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper uses pre-trained language models to synthetically generate entity-controlled text, implying that the limitations of LLMs are not the focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper uses pre-trained language models to synthetically generate entity-controlled text, implying that the limitations of LLMs are not the focus of the paper."
    },
    {
        "title": "Tokenization Impacts Multilingual Language Modeling: Assessing Vocabulary Allocation and Overlap Across Languages",
        "authors": [
            "Tomasz Limisiewicz",
            "Jiří Balhar",
            "David Mareček"
        ],
        "published": "2023",
        "summary": "Multilingual language models have recently gained attention as a promising solution for representing multiple languages in a single model. In this paper, we propose new criteria to evaluate the quality of lexical representation and vocabulary overlap observed in sub-word tokenizers.Our findings show that the overlap of vocabulary across languages can be actually detrimental to certain downstream tasks (POS, dependency tree labeling). In contrast, NER and sentence-level tasks (cross-lingual retrieval, NLI) benefit from sharing vocabulary. We also observe that the coverage of the language-specific tokens in the multilingual vocabulary significantly impacts the word-level tasks. Our study offers a deeper understanding of the role of tokenizers in multilingual language models and guidelines for future model developers to choose the most suitable tokenizer for their specific application before undertaking costly model pre-training.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.350.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our findings show that the overlap of vocabulary across languages can be actually detrimental to certain downstream tasks (POS, dependency tree labeling).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Our findings show that the overlap of vocabulary across languages can be actually detrimental to certain downstream tasks (POS, dependency tree labeling).\""
    },
    {
        "title": "The Whole Truth and Nothing But the Truth: Faithful and Controllable Dialogue Response Generation with Dataflow Transduction and Constrained Decoding",
        "authors": [
            "Hao Fang",
            "Anusha Balakrishnan",
            "Harsh Jhamtani",
            "John Bufe",
            "Jean Crawford",
            "Jayant Krishnamurthy",
            "Adam Pauls",
            "Jason Eisner",
            "Jacob Andreas",
            "Dan Klein"
        ],
        "published": "2023",
        "summary": "In a real-world dialogue system, generated text must be truthful and informative while remaining fluent and adhering to a prescribed style. Satisfying these constraints simultaneously isdifficult for the two predominant paradigms in language generation: neural language modeling and rule-based generation. We describe a hybrid architecture for dialogue response generation that combines the strengths of both paradigms. The first component of this architecture is a rule-based content selection model defined using a new formal framework called dataflow transduction, which uses declarative rules to transduce a dialogue agent’s actions and their results (represented as dataflow graphs) into context-free grammars representing the space of contextually acceptable responses. The second component is a constrained decoding procedure that uses these grammars to constrain the output of a neural language model, which selects fluent utterances. Our experiments show that this system outperforms both rule-based and learned approaches in human evaluations of fluency, relevance, and truthfulness.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.351.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Satisfying these constraints simultaneously isdifficult for the two predominant paradigms in language generation: neural language modeling and rule-based generation.\"\n\nThis evidence suggests that the paper mentions a limitation of LLMs (neural language modeling), but it is not the primary focus of the paper and is only briefly mentioned.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Satisfying these constraints simultaneously isdifficult for the two predominant paradigms in language generation: neural language modeling and rule-based generation.\"\n\nThis evidence suggests that the paper mentions a limitation of LLMs (neural language modeling), but it is not the primary focus of the paper and is only briefly mentioned."
    },
    {
        "title": "Enhancing Hierarchical Text Classification through Knowledge Graph Integration",
        "authors": [
            "Ye Liu",
            "Kai Zhang",
            "Zhenya Huang",
            "Kehang Wang",
            "Yanghai Zhang",
            "Qi Liu",
            "Enhong Chen"
        ],
        "published": "2023",
        "summary": "Hierarchical Text Classification (HTC) is an essential and challenging subtask of multi-label text classification with a taxonomic hierarchy. Recent advances in deep learning and pre-trained language models have led to significant breakthroughs in the HTC problem. However, despite their effectiveness, these methods are often restricted by a lack of domain knowledge, which leads them to make mistakes in a variety of situations. Generally, when manually classifying a specific document to the taxonomic hierarchy, experts make inference based on their prior knowledge and experience. For machines to achieve this capability, we propose a novel Knowledge-enabled Hierarchical Text Classification model (K-HTC), which incorporates knowledge graphs into HTC. Specifically, K-HTC innovatively integrates knowledge into both the text representation and hierarchical label learning process, addressing the knowledge limitations of traditional methods. Additionally, a novel knowledge-aware contrastive learning strategy is proposed to further exploit the information inherent in the data. Extensive experiments on two publicly available HTC datasets show the efficacy of our proposed method, and indicate the necessity of incorporating knowledge graphs in HTC tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.358.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, despite their effectiveness, these methods are often restricted by a lack of domain knowledge, which leads them to make mistakes in a variety of situations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, despite their effectiveness, these methods are often restricted by a lack of domain knowledge, which leads them to make mistakes in a variety of situations.\""
    },
    {
        "title": "An Exploration of Encoder-Decoder Approaches to Multi-Label Classification for Legal and Biomedical Text",
        "authors": [
            "Yova Kementchedjhieva",
            "Ilias Chalkidis"
        ],
        "published": "2023",
        "summary": "Standard methods for multi-label text classification largely rely on encoder-only pre-trained language models, whereas encoder-decoder models have proven more effective in other classification tasks. In this study, we compare four methods for multi-label classification, two based on an encoder only, and two based on an encoder-decoder. We carry out experiments on four datasets—two in the legal domain and two in the biomedical domain, each with two levels of label granularity— and always depart from the same pre-trained model, T5. Our results show that encoder-decoder methods outperform encoder-only methods, with a growing advantage on more complex datasets and labeling schemes of finer granularity. Using encoder-decoder models in a non-autoregressive fashion, in particular, yields the best performance overall, so we further study this approach through ablations to better understand its strengths.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.360.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Domain Incremental Lifelong Learning in an Open World",
        "authors": [
            "Yi Dai",
            "Hao Lang",
            "Yinhe Zheng",
            "Bowen Yu",
            "Fei Huang",
            "Yongbin Li"
        ],
        "published": "2023",
        "summary": "Lifelong learning (LL) is an important ability for NLP models to learn new tasks continuously. Architecture-based approaches are reported to be effective implementations for LL models. However, it is non-trivial to extend previous approaches to domain incremental LL scenarios since they either require access to task identities in the testing phase or cannot handle samples from unseen tasks. In this paper, we propose Diana: a dynamic architecture-based lifelong learning model that tries to learn a sequence of tasks with a prompt-enhanced language model. Four types of hierarchically organized prompts are used in Diana to capture knowledge from different granularities. Specifically, we dedicate task-level prompts to capture task-specific knowledge to retain high LL performances and maintain instance-level prompts to learn knowledge shared across input samples to improve the model’s generalization performance. Moreover, we dedicate separate prompts to explicitly model unseen tasks and introduce a set of prompt key vectors to facilitate knowledge sharing between tasks. Extensive experiments demonstrate that Diana outperforms state-of-the-art LL models, especially in handling unseen tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.361.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it is non-trivial to extend previous approaches to domain incremental LL scenarios since they either require access to task identities in the testing phase or cannot handle samples from unseen tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, it is non-trivial to extend previous approaches to domain incremental LL scenarios since they either require access to task identities in the testing phase or cannot handle samples from unseen tasks.\""
    },
    {
        "title": "Reasoning in Large Language Models Through Symbolic Math Word Problems",
        "authors": [
            "Vedant Gaur",
            "Nikunj Saunshi"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have revolutionized NLP by solving downstream tasks with little to no labeled data. Despite their versatile abilities, the larger question of their ability to reason remains ill-understood. This paper addresses reasoning in math word problems (MWPs) by studying symbolic versions of the numeric problems, since a symbolic expression is a “concise explanation” of the numeric answer. We create and use a symbolic version of the SVAMP dataset and find that GPT-3’s davinci-002 model also has good zero-shot accuracy on symbolic MWPs. To evaluate the faithfulness of the model’s reasoning, we go beyond accuracy and additionally evaluate the alignment between the final answer and the outputted reasoning, which correspond to numeric and symbolic answers respectively for MWPs. We explore a self-prompting approach to encourage the symbolic reasoning to align with the numeric answer, thus equipping the LLM with the ability to provide a concise and verifiable reasoning and making it more interpretable. Surprisingly, self-prompting also improves the symbolic accuracy to be higher than both the numeric and symbolic accuracies, thus providing an ensembling effect. The SVAMP-Sym dataset will be released for future research on symbolic math problems.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.364.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite their versatile abilities, the larger question of their ability to reason remains ill-understood.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Despite their versatile abilities, the larger question of their ability to reason remains ill-understood.\""
    },
    {
        "title": "Hybrid-Regressive Paradigm for Accurate and Speed-Robust Neural Machine Translation",
        "authors": [
            "Qiang Wang",
            "Xinhui Hu",
            "Ming Chen"
        ],
        "published": "2023",
        "summary": "This work empirically confirms that non-autoregressive translation (NAT) is less robust in decoding batch size and hardware settings than autoregressive translation (AT). To address this issue, we demonstrate that prompting a small number of AT predictions can significantly reduce the performance gap between AT and NAT through synthetic experiments. Following this line, we propose hybrid-regressive translation (HRT), a two-stage translation prototype that combines the strengths of AT and NAT. Specifically, HRT first generates discontinuous sequences via autoregression (e.g., make a prediction for every k tokens, k>1) and then fills in all previously skipped tokens at once in a non-autoregressive manner. Experiments on five translation tasks show that HRT achieves comparable translation quality with AT while having at least 1.5x faster inference regardless of batch size and device. Additionally, HRT successfully inherits the sound characteristics of AT in the deep-encoder-shallow-decoder architecture, allowing for further speedup without BLEU loss.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.367.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Commonsense Knowledge Transfer for Pre-trained Language Models",
        "authors": [
            "Wangchunshu Zhou",
            "Ronan Le Bras",
            "Yejin Choi"
        ],
        "published": "2023",
        "summary": "Despite serving as the foundation models for a wide range of NLP benchmarks, pre-trained language models have shown limited capabilities of acquiring implicit commonsense knowledge from self-supervision alone, compared to learning linguistic and factual knowledge that appear more explicitly in the surface patterns in text. In this work, we introduce commonsense knowledge transfer, a framework to transfer the commonsense knowledge stored in a neural commonsense knowledge model to a general-purpose pre-trained language model. It first exploits general texts to form queries for extracting commonsense knowledge from the neural commonsense knowledge model and then refines the language model with two self-supervised objectives: commonsense mask infilling and commonsense relation prediction, which align human language with the underlying commonsense knowledge. Empirical results show that our approach consistently improves the model’s performance on downstream tasks that require commonsense reasoning. Moreover, we find that the improvement is more significant in the few-shot setting. This suggests that our approach helps language models better transfer to downstream tasks without extensive supervision by injecting commonsense knowledge into their parameters.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.368.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite serving as the foundation models for a wide range of NLP benchmarks, pre-trained language models have shown limited capabilities of acquiring implicit commonsense knowledge from self-supervision alone, compared to learning linguistic and factual knowledge that appear more explicitly in the surface patterns in text.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite serving as the foundation models for a wide range of NLP benchmarks, pre-trained language models have shown limited capabilities of acquiring implicit commonsense knowledge from self-supervision alone, compared to learning linguistic and factual knowledge that appear more explicitly in the surface patterns in text.\""
    },
    {
        "title": "CoMave: Contrastive Pre-training with Multi-scale Masking for Attribute Value Extraction",
        "authors": [
            "Xinnan Guo",
            "Wentao Deng",
            "Yongrui Chen",
            "Yang Li",
            "Mengdi Zhou",
            "Guilin Qi",
            "Tianxing Wu",
            "Dong Yang",
            "Liubin Wang",
            "Yong Pan"
        ],
        "published": "2023",
        "summary": "Attribute Value Extraction (AVE) aims to automatically obtain attribute value pairs from product descriptions to aid e-commerce. Despite the progressive performance of existing approaches in e-commerce platforms, they still suffer from two challenges: 1) difficulty in identifying values at different scales simultaneously; 2) easy confusion by some highly similar fine-grained attributes. This paper proposes a pre-training technique for AVE to address these issues. In particular, we first improve the conventional token-level masking strategy, guiding the language model to understand multi-scale values by recovering spans at the phrase and sentence level. Second, we apply clustering to build a challenging negative set for each example and design a pre-training objective based on contrastive learning to force the model to discriminate similar attributes. Comprehensive experiments show that our solution provides a significant improvement over traditional pre-trained models in the AVE task, and achieves state-of-the-art on four benchmarks.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.373.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the progressive performance of existing approaches in e-commerce platforms, they still suffer from two challenges: 1) difficulty in identifying values at different scales simultaneously; 2) easy confusion by some highly similar fine-grained attributes.\"\n\nThis paper mentions two limitations of existing approaches, which can be related to LLMs, but does not go into detail and focuses on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite the progressive performance of existing approaches in e-commerce platforms, they still suffer from two challenges: 1) difficulty in identifying values at different scales simultaneously; 2) easy confusion by some highly similar fine-grained attributes.\"\n\nThis paper mentions two limitations of existing approaches, which can be related to LLMs, but does not go into detail and focuses on the proposed solution."
    },
    {
        "title": "Unlearning Bias in Language Models by Partitioning Gradients",
        "authors": [
            "Charles Yu",
            "Sullam Jeoung",
            "Anish Kasi",
            "Pengfei Yu",
            "Heng Ji"
        ],
        "published": "2023",
        "summary": "Recent research has shown that large-scale pretrained language models, specifically transformers, tend to exhibit issues relating to racism, sexism, religion bias, and toxicity in general. Unfortunately, these pretrained language models are used almost universally in downstream tasks, and natural language processing is often applied to make real-world predictions. Thus, debiasing these language models as early in development as possible is increasingly crucial for preventing unintentional harms caused by natural language systems. To this end, we propose a new technique called partitioned contrastive gradient unlearning (PCGU), a gray-box method for debiasing pretrained masked language models. PCGU aims to optimize only the weights that contribute most to a specific domain of bias, doing so by computing a first-order approximation based on the gradients of contrastive sentence pairs. Our experiments show that PCGU is both low-cost and seems particularly effective at pinpointing the sources of implicit social bias in large pretrained transformers. Although we train using PCGU in the gender-profession domain only, we find that doing so can also partially mitigate bias across other domains. All code for our implementation and experiments can be found at https://github.com/CharlesYu2000/PCGU-UnlearningBias.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.375.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent research has shown that large-scale pretrained language models, specifically transformers, tend to exhibit issues relating to racism, sexism, religion bias, and toxicity in general.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Recent research has shown that large-scale pretrained language models, specifically transformers, tend to exhibit issues relating to racism, sexism, religion bias, and toxicity in general.\""
    },
    {
        "title": "Meta-training with Demonstration Retrieval for Efficient Few-shot Learning",
        "authors": [
            "Aaron Mueller",
            "Kanika Narang",
            "Lambert Mathias",
            "Qifan Wang",
            "Hamed Firooz"
        ],
        "published": "2023",
        "summary": "Large language models show impressive results on few-shot NLP tasks. However, these models are memory and computation-intensive. Meta-training allows one to leverage smaller models for few-shot generalization in a domain-general and task-agnostic manner; however, these methods alone results in models that may not have sufficient parameterization or knowledge to adapt quickly to a large variety of tasks. To overcome this issue, we propose meta-training with demonstration retrieval, where we use a dense passage retriever to retrieve semantically similar labeled demonstrations to each example for more varied supervision. By separating external knowledge from model parameters, we can use meta-training to train parameter-efficient models that generalize well on a larger variety of tasks. We construct a meta-training set from UnifiedQA and CrossFit, and propose a demonstration bank based on UnifiedQA tasks. To our knowledge, our work is the first to combine retrieval with meta-training, to use DPR models to retrieve demonstrations, and to leverage demonstrations from many tasks simultaneously, rather than randomly sampling demonstrations from the training set of the target task. Our approach outperforms a variety of targeted parameter-efficient and retrieval-augmented few-shot methods on QA, NLI, and text classification tasks (including SQuAD, QNLI, and TREC). Our approach can be meta-trained and fine-tuned quickly on a single GPU.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.376.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"however, these methods alone results in models that may not have sufficient parameterization or knowledge to adapt quickly to a large variety of tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"however, these methods alone results in models that may not have sufficient parameterization or knowledge to adapt quickly to a large variety of tasks.\""
    },
    {
        "title": "VCSUM: A Versatile Chinese Meeting Summarization Dataset",
        "authors": [
            "Han Wu",
            "Mingjie Zhan",
            "Haochen Tan",
            "Zhaohui Hou",
            "Ding Liang",
            "Linqi Song"
        ],
        "published": "2023",
        "summary": "Compared to news and chat summarization, the development of meeting summarization is hugely decelerated by the limited data. To this end, we introduce a versatile Chinese meeting summarization dataset, dubbed VCSum, consisting of 239 real-life meetings, with a total duration of over 230 hours. We claim our dataset is versatile because we provide the annotations of topic segmentation, headlines, segmentation summaries, overall meeting summaries, and salient sentences for each meeting transcript. As such, the dataset can adapt to various summarization tasks or methods, including segmentation-based summarization, multi-granularity summarization and retrieval-then-generate summarization. Our analysis confirms the effectiveness and robustness of VCSum. We also provide a set of benchmark models regarding different downstream summarization tasks on VCSum to facilitate further research.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.377.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Reimagining Retrieval Augmented Language Models for Answering Queries",
        "authors": [
            "Wang-Chiew Tan",
            "Yuliang Li",
            "Pedro Rodriguez",
            "Richard James",
            "Xi Victoria Lin",
            "Alon Halevy",
            "Wen-tau Yih"
        ],
        "published": "2023",
        "summary": "We present a reality check on large language models and inspect the promise of retrieval-augmented language models in comparison. Such language models are semi-parametric, where models integrate model parameters and knowledge from external data sources to make their predictions, as opposed to the parametric nature of vanilla large language models. We give initial experimental findings that semi-parametric architectures can be enhanced with views, a query analyzer/planner, and provenance to make a significantly more powerful system for question answering in terms of accuracy and efficiency, and potentially for other NLP tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.382.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We present a reality check on large language models\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We present a reality check on large language models\""
    },
    {
        "title": "Numeric Magnitude Comparison Effects in Large Language Models",
        "authors": [
            "Raj Shah",
            "Vijay Marupudi",
            "Reba Koenen",
            "Khushi Bhardwaj",
            "Sashank Varma"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) do not differentially represent numbers, which are pervasive in text. In contrast, neuroscience research has identified distinct neural representations for numbers and words. In this work, we investigate how well popular LLMs capture the magnitudes of numbers (e.g., that 4<5) from a behavioral lens. Prior research on the representational capabilities of LLMs evaluates whether they show human-level performance, for instance, high overall accuracy on standard benchmarks. Here, we ask a different question, one inspired by cognitive science: How closely do the number representations of LLMscorrespond to those of human language users, who typically demonstrate the distance, size, and ratio effects? We depend on a linking hypothesis to map the similarities among the model embeddings of number words and digits to human response times. The results reveal surprisingly human-like representations across language models of different architectures, despite the absence of the neural circuitry that directly supports these representations in the human brain. This research shows the utility of understanding LLMs using behavioral benchmarks and points the way to future work on the number of representations of LLMs and their cognitive plausibility.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.383.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large Language Models (LLMs) do not differentially represent numbers, which are pervasive in text.\"\n\nThis evidence suggests that the paper mentions a limitation of LLMs in representing numbers, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large Language Models (LLMs) do not differentially represent numbers, which are pervasive in text.\"\n\nThis evidence suggests that the paper mentions a limitation of LLMs in representing numbers, but it is not the primary focus of the paper."
    },
    {
        "title": "Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks",
        "authors": [
            "Lukas Hauzenberger",
            "Shahed Masoudian",
            "Deepak Kumar",
            "Markus Schedl",
            "Navid Rekabsaz"
        ],
        "published": "2023",
        "summary": "Societal biases are reflected in large pre-trained language models and their fine-tuned versions on downstream tasks. Common in-processing bias mitigation approaches, such as adversarial training and mutual information removal, introduce additional optimization criteria, and update the model to reach a new debiased state. However, in practice, end-users and practitioners might prefer to switch back to the original model, or apply debiasing only on a specific subset of protected attributes. To enable this, we propose a novel modular bias mitigation approach, consisting of stand-alone highly sparse debiasing subnetworks, where each debiasing module can be integrated into the core model on-demand at inference time. Our approach draws from the concept of diff pruning, and proposes a novel training regime adaptable to various representation disentanglement optimizations. We conduct experiments on three classification tasks with gender, race, and age as protected attributes. The results show that our modular approach, while maintaining task performance, improves (or at least remains on-par with) the effectiveness of bias mitigation in comparison with baseline finetuning. Particularly on a two-attribute dataset, our approach with separately learned debiasing subnetworks shows effective utilization of either or both the subnetworks for selective bias mitigation.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.386.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Societal biases are reflected in large pre-trained language models and their fine-tuned versions on downstream tasks.\"\n\nThis rating is given because the paper mentions a limitation of LLMs (social biases) but does not explore it in depth, instead focusing on a proposed solution to mitigate this limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Societal biases are reflected in large pre-trained language models and their fine-tuned versions on downstream tasks.\"\n\nThis rating is given because the paper mentions a limitation of LLMs (social biases) but does not explore it in depth, instead focusing on a proposed solution to mitigate this limitation."
    },
    {
        "title": "DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models",
        "authors": [
            "Amr Keleg",
            "Walid Magdy"
        ],
        "published": "2023",
        "summary": "A few benchmarking datasets have been released to evaluate the factual knowledge of pretrained language models. These benchmarks (e.g., LAMA, and ParaRel) are mainly developed in English and later are translated to form new multilingual versions (e.g., mLAMA, and mParaRel). Results on these multilingual benchmarks suggest that using English prompts to recall the facts from multilingual models usually yields significantly better and more consistent performance than using non-English prompts. Our analysis shows that mLAMA is biased toward facts from Western countries, which might affect the fairness of probing models. We propose a new framework for curating factual triples from Wikidata that are culturally diverse. A new benchmark DLAMA-v1 is built of factual triples from three pairs of contrasting cultures having a total of 78,259 triples from 20 relation predicates. The three pairs comprise facts representing the (Arab and Western), (Asian and Western), and (South American and Western) countries respectively. Having a more balanced benchmark (DLAMA-v1) supports that mBERT performs better on Western facts than non-Western ones, while monolingual Arabic, English, and Korean models tend to perform better on their culturally proximate facts. Moreover, both monolingual and multilingual models tend to make a prediction that is culturally or geographically relevant to the correct label, even if the prediction is wrong.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.389.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our analysis shows that mLAMA is biased toward facts from Western countries, which might affect the fairness of probing models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Our analysis shows that mLAMA is biased toward facts from Western countries, which might affect the fairness of probing models.\""
    },
    {
        "title": "Evaluation of Question Generation Needs More References",
        "authors": [
            "Shinhyeok Oh",
            "Hyojun Go",
            "Hyeongdon Moon",
            "Yunsung Lee",
            "Myeongho Jeong",
            "Hyun Seung Lee",
            "Seungtaek Choi"
        ],
        "published": "2023",
        "summary": "Question generation (QG) is the task of generating a valid and fluent question based on a given context and the target answer. According to various purposes, even given the same context, instructors can ask questions about different concepts, and even the same concept can be written in different ways. However, the evaluation for QG usually depends on single reference-based similarity metrics, such as n-gram-based metric or learned metric, which is not sufficient to fully evaluate the potential of QG methods. To this end, we propose to paraphrase the reference question for a more robust QG evaluation. Using large language models such as GPT-3, we created semantically and syntactically diverse questions, then adopt the simple aggregation of the popular evaluation metrics as the final scores. Through our experiments, we found that using multiple (pseudo) references is more effective for QG evaluation while showing a higher correlation with human evaluations than evaluation with a single reference.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.396.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the evaluation for QG usually depends on single reference-based similarity metrics, such as n-gram-based metric or learned metric, which is not sufficient to fully evaluate the potential of QG methods.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the evaluation for QG usually depends on single reference-based similarity metrics, such as n-gram-based metric or learned metric, which is not sufficient to fully evaluate the potential of QG methods.\""
    },
    {
        "title": "A Multi-dimensional study on Bias in Vision-Language models",
        "authors": [
            "Gabriele Ruggeri",
            "Debora Nozza"
        ],
        "published": "2023",
        "summary": "In recent years, joint Vision-Language (VL) models have increased in popularity and capability. Very few studies have attempted to investigate bias in VL models, even though it is a well-known issue in both individual modalities. This paper presents the first multi-dimensional analysis of bias in English VL models, focusing on gender, ethnicity, and age as dimensions. When subjects are input as images, pre-trained VL models complete a neutral template with a hurtful word 5% of the time, with higher percentages for female and young subjects. Bias presence in downstream models has been tested on Visual Question Answering. We developed a novel bias metric called the Vision-Language Association Test based on questions designed to elicit biased associations between stereotypical concepts and targets. Our findings demonstrate that pre-trained VL models contain biases that are perpetuated in downstream tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.403.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"When subjects are input as images, pre-trained VL models complete a neutral template with a hurtful word 5% of the time, with higher percentages for female and young subjects. Bias presence in downstream models has been tested on Visual Question Answering.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"When subjects are input as images, pre-trained VL models complete a neutral template with a hurtful word 5% of the time, with higher percentages for female and young subjects. Bias presence in downstream models has been tested on Visual Question Answering.\""
    },
    {
        "title": "Guiding Dialogue Agents to Complex Semantic Targets by Dynamically Completing Knowledge Graph",
        "authors": [
            "Yue Tan",
            "Bo Wang",
            "Anqi Liu",
            "Dongming Zhao",
            "Kun Huang",
            "Ruifang He",
            "Yuexian Hou"
        ],
        "published": "2023",
        "summary": "In the target-oriented dialogue, the representation and achievement of targets are two interrelated essential issues. In current approaches, the target is typically supposed to be a single object represented as a word, which makes it relatively easy to achieve the target through dialogue with the help of a knowledge graph (KG). However, when the target has complex semantics, the existing knowledge graph is often incomplete in tracking complex semantic relations. This paper studies target-oriented dialog where the target is a topic sentence. We combine the methods of knowledge retrieval and relationship prediction to construct a context-related dynamic KG. On dynamic KG, we can track the implicit semantic paths in the speaker’s mind that may not exist in the existing KGs. In addition, we also designed a novel metric to evaluate the tracked path automatically. The experimental results show that our method can control the agent more logically and smoothly toward the complex target.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.407.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Chain of Thought Prompting Elicits Knowledge Augmentation",
        "authors": [
            "Dingjun Wu",
            "Jing Zhang",
            "Xinmei Huang"
        ],
        "published": "2023",
        "summary": "The knowledge-augmented deep learning paradigm refers to a paradigm in which domain knowledge is identified and integrated into deep models. Conventional methods typically employ task-specific approaches to gather external knowledge from various sources. In contrast, large language models are extensively pre-trained and can serve as a comprehensive source of external knowledge. In this paper, we propose CoT-KA, a Chain-of-Thought-based method that augments knowledge for deep learning. CoT-KA avoids the need for additional knowledge retrieval or knowledge reasoning models, as required in conventional augmentation methods. Our results demonstrate that CoT-KA outperforms both pure CoT-based methods and the non-augmented method across the majority of eleven publicly available benchmarks for various reasoning tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.408.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but \"large language models are extensively pre-trained and can serve as a comprehensive source of external knowledge\" implies their potential, but does not discuss limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but \"large language models are extensively pre-trained and can serve as a comprehensive source of external knowledge\" implies their potential, but does not discuss limitations."
    },
    {
        "title": "Werewolf Among Us: Multimodal Resources for Modeling Persuasion Behaviors in Social Deduction Games",
        "authors": [
            "Bolin Lai",
            "Hongxin Zhang",
            "Miao Liu",
            "Aryan Pariani",
            "Fiona Ryan",
            "Wenqi Jia",
            "Shirley Anugrah Hayati",
            "James Rehg",
            "Diyi Yang"
        ],
        "published": "2023",
        "summary": "Persuasion modeling is a key building block for conversational agents. Existing works in this direction are limited to analyzing textual dialogue corpus. We argue that visual signals also play an important role in understanding human persuasive behaviors. In this paper, we introduce the first multimodal dataset for modeling persuasion behaviors. Our dataset includes 199 dialogue transcriptions and videos captured in a multi-player social deduction game setting, 26,647 utterance level annotations of persuasion strategy, and game level annotations of deduction game outcomes. We provide extensive experiments to show how dialogue context and visual signals benefit persuasion strategy prediction. We also explore the generalization ability of language models for persuasion modeling and the role of persuasion strategies in predicting social deduction game outcomes. Our dataset can be found at https://persuasion-deductiongame. socialai-data.org. The codes and models are available at https://github.com/SALT-NLP/PersuationGames.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.411.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We also explore the generalization ability of language models for persuasion modeling\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We also explore the generalization ability of language models for persuasion modeling\""
    },
    {
        "title": "Towards Imperceptible Document Manipulations against Neural Ranking Models",
        "authors": [
            "Xuanang Chen",
            "Ben He",
            "Zheng Ye",
            "Le Sun",
            "Yingfei Sun"
        ],
        "published": "2023",
        "summary": "Adversarial attacks have gained traction in order to identify vulnerabilities in neural ranking models (NRMs), but current attack methods often introduce noticeable errors. Moreover, current methods rely heavily on using a well-imitated surrogate NRM to guarantee the attack effect, making them difficult to use in practice. This paper proposes a framework called Imperceptible DocumEnt Manipulation (IDEM) to produce adversarial documents that are less noticeable to both algorithms and humans. IDEM instructs a well-established generative language model like BART to generate error-free connection sentences, and employs a separate position-wise merging strategy to balance between relevance and coherence of the perturbed text. Evaluation results on the MS MARCO benchmark demonstrate that IDEM outperforms strong baselines while preserving fluency and correctness of the target documents. Furthermore, the separation of adversarial text generation from the surrogate NRM makes IDEM more robust and less affected by the quality of the surrogate NRM.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.416.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"current attack methods often introduce noticeable errors\", \"makes them difficult to use in practice\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"current attack methods often introduce noticeable errors\", \"makes them difficult to use in practice\""
    },
    {
        "title": "Ask an Expert: Leveraging Language Models to Improve Strategic Reasoning in Goal-Oriented Dialogue Models",
        "authors": [
            "Qiang Zhang",
            "Jason Naradowsky",
            "Yusuke Miyao"
        ],
        "published": "2023",
        "summary": "Existing dialogue models may encounter scenarios which are not well-represented in the training data, and as a result generate responses that are unnatural, inappropriate, or unhelpful. We propose the “Ask an Expert” framework in which the model is trained with access to an “expert” which it can consult at each turn. Advice is solicited via a structured dialogue with the expert, and the model is optimized to selectively utilize (or ignore) it given the context and dialogue history. In this work the expert takes the form of an LLM.We evaluate this framework in a mental health support domain, where the structure of the expert conversation is outlined by pre-specified prompts which reflect a reasoning strategy taught to practitioners in the field. Blenderbot models utilizing “Ask an Expert” show quality improvements across all expert sizes, including those with fewer parameters than the dialogue model itself. Our best model provides a ~10% improvement over baselines, approaching human-level scores on “engingingness” and “helpfulness” metrics.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.417.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicitly mentioned, but the use of an LLM as an \"expert\" in the \"Ask an Expert\" framework implies some limitations in the dialogue model itself, which the LLM is intended to address.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None explicitly mentioned, but the use of an LLM as an \"expert\" in the \"Ask an Expert\" framework implies some limitations in the dialogue model itself, which the LLM is intended to address."
    },
    {
        "title": "SciReviewGen: A Large-scale Dataset for Automatic Literature Review Generation",
        "authors": [
            "Tetsu Kasanishi",
            "Masaru Isonuma",
            "Junichiro Mori",
            "Ichiro Sakata"
        ],
        "published": "2023",
        "summary": "Automatic literature review generation is one of the most challenging tasks in natural language processing. Although large language models have tackled literature review generation, the absence of large-scale datasets has been a stumbling block to the progress. We release SciReviewGen, consisting of over 10,000 literature reviews and 690,000 papers cited in the reviews. Based on the dataset, we evaluate recent transformer-based summarization models on the literature review generation task, including Fusion-in-Decoder extended for literature review generation. Human evaluation results show that some machine-generated summaries are comparable to human-written reviews, while revealing the challenges of automatic literature review generation such as hallucinations and a lack of detailed information. Our dataset and code are available at [https://github.com/tetsu9923/SciReviewGen](https://github.com/tetsu9923/SciReviewGen).",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.418.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"while revealing the challenges of automatic literature review generation such as hallucinations and a lack of detailed information.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"while revealing the challenges of automatic literature review generation such as hallucinations and a lack of detailed information.\""
    },
    {
        "title": "Residual Prompt Tuning: improving prompt tuning with residual reparameterization",
        "authors": [
            "Anastasiia Razdaibiedina",
            "Yuning Mao",
            "Madian Khabsa",
            "Mike Lewis",
            "Rui Hou",
            "Jimmy Ba",
            "Amjad Almahairi"
        ],
        "published": "2023",
        "summary": "Prompt tuning is one of the successful approaches for parameter-efficient tuning of pre-trained language models. Despite being arguably the most parameter-efficient (tuned soft prompts constitute <0.1% of total parameters), it typically performs worse than other efficient tuning methods and is quite sensitive to hyper-parameters. In this work, we introduce Residual Prompt Tuning - a simple and efficient method that significantly improves the performance and stability of prompt tuning. We propose to reparameterize soft prompt embeddings using a shallow network with a residual connection. Our experiments show that Residual Prompt Tuning significantly outperforms prompt tuning across T5-Large, T5-Base and BERT-Base models. Notably, our method reaches +7 points improvement over prompt tuning on SuperGLUE benchmark with T5-Base model and allows to reduce the prompt length by 10 times without hurting performance. In addition, we show that our approach is robust to the choice of learning rate and prompt initialization, and is effective in few-shot settings.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.421.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite being arguably the most parameter-efficient (tuned soft prompts constitute <0.1% of total parameters), it typically performs worse than other efficient tuning methods and is quite sensitive to hyper-parameters.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite being arguably the most parameter-efficient (tuned soft prompts constitute <0.1% of total parameters), it typically performs worse than other efficient tuning methods and is quite sensitive to hyper-parameters.\""
    },
    {
        "title": "Attend, Select and Eliminate: Accelerating Multi-turn Response Selection with Dual-attention-based Content Elimination",
        "authors": [
            "Jianxin Liang",
            "Chang Liu",
            "Chongyang Tao",
            "Jiazhan Feng",
            "Dongyan Zhao"
        ],
        "published": "2023",
        "summary": "Although the incorporation of pre-trained language models (PLMs) significantly pushes the research frontier of multi-turn response selection, it brings a new issue of heavy computation costs. To alleviate this problem and make the PLM-based response selection model both effective and efficient, we propose an inference framework together with a post-training strategy that builds upon any pre-trained transformer-based response selection models to accelerate inference by progressively selecting and eliminating unimportant content under the guidance of context-response dual-attention. Specifically, at each transformer layer, we first identify the importance of each word based on context-to-response and response-to-context attention, then select a number of unimportant words to be eliminated following a retention configuration derived from evolutionary search while passing the rest of the representations into deeper layers. To mitigate the training-inference gap posed by content elimination, we introduce a post-training strategy where we use knowledge distillation to force the model with progressively eliminated content to mimic the predictions of the original model with no content elimination. Experiments on three benchmarks indicate that our method can effectively speeds-up SOTA models without much performance degradation and shows a better trade-off between speed and performance than previous methods.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.422.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although the incorporation of pre-trained language models (PLMs) significantly pushes the research frontier of multi-turn response selection, it brings a new issue of heavy computation costs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although the incorporation of pre-trained language models (PLMs) significantly pushes the research frontier of multi-turn response selection, it brings a new issue of heavy computation costs.\""
    },
    {
        "title": "Mitigating the Learning Bias towards Repetition by Self-Contrastive Training for Open-Ended Generation",
        "authors": [
            "Jian Guan",
            "Minlie Huang"
        ],
        "published": "2023",
        "summary": "Despite the huge progress in myriad generation tasks, pretrained language models (LMs) such as GPT2 still tend to generate repetitive texts with maximization-based decoding algorithms for open-ended generation. We attribute their overestimation of token-level repetition probabilities to the learning bias: LMs capture simple repetitive patterns faster with the MLE loss. We propose self-contrastive training to penalize the output of a premature checkpoint of the same model when it incorrectly predicts repetition, which is shown to mitigate repetition effectively while maintaining fluency on two datasets. Furthermore, we find that LMs use longer-range dependencies to predict repetitive tokens than non-repetitive ones, which may be the cause of sentence-level repetition loops.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.431.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the huge progress in myriad generation tasks, pretrained language models (LMs) such as GPT2 still tend to generate repetitive texts with maximization-based decoding algorithms for open-ended generation. We attribute their overestimation of token-level repetition probabilities to the learning bias: LMs capture simple repetitive patterns faster with the MLE loss.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Despite the huge progress in myriad generation tasks, pretrained language models (LMs) such as GPT2 still tend to generate repetitive texts with maximization-based decoding algorithms for open-ended generation. We attribute their overestimation of token-level repetition probabilities to the learning bias: LMs capture simple repetitive patterns faster with the MLE loss.\""
    },
    {
        "title": "Uncovering Hidden Consequences of Pre-training Objectives in Sequence-to-Sequence Models",
        "authors": [
            "Tannon Kew",
            "Rico Sennrich"
        ],
        "published": "2023",
        "summary": "Some variants of self-supervised denoising objectives for pre-training encoder-decoder language models have been reported to have a negligible impact on downstream performance. Yet the design of these pre-training objectives leads to behavioural differences that can be uncovered with specific manipulations. We reproduce a recently proposed zero-shot control method and find that it is only successful on a subset of models. To understand what causes the difference in its effectiveness, we perform a set of controlled experiments, varying only the pre-training objective, and find unexpected interactions between the pre-training method and downstream controllability of models after fine-tuning. Our results show that different pre-training objectives have consequences that may not be visible in standard downstream evaluation, but which should be taken into account when developing models with controllability in mind.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.438.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Some variants of self-supervised denoising objectives for pre-training encoder-decoder language models have been reported to have a negligible impact on downstream performance.... Our results show that different pre-training objectives have consequences that may not be visible in standard downstream evaluation, but which should be taken into account when developing models with controllability in mind.\"\n\nThis rating is chosen because the abstract mentions",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Some variants of self-supervised denoising objectives for pre-training encoder-decoder language models have been reported to have a negligible impact on downstream performance.... Our results show that different pre-training objectives have consequences that may not be visible in standard downstream evaluation, but which should be taken into account when developing models with controllability in mind.\"\n\nThis rating is chosen because the abstract mentions"
    },
    {
        "title": "Exploring Anisotropy and Outliers in Multilingual Language Models for Cross-Lingual Semantic Sentence Similarity",
        "authors": [
            "Katharina Hämmerl",
            "Alina Fastowski",
            "Jindřich Libovický",
            "Alexander Fraser"
        ],
        "published": "2023",
        "summary": "Previous work has shown that the representations output by contextual language models are more anisotropic than static type embeddings, and typically display outlier dimensions. This seems to be true for both monolingual and multilingual models, although much less work has been done on the multilingual context. Why these outliers occur and how they affect the representations is still an active area of research. We investigate outlier dimensions and their relationship to anisotropy in multiple pre-trained multilingual language models. We focus on cross-lingual semantic similarity tasks, as these are natural tasks for evaluating multilingual representations. Specifically, we examine sentence representations. Sentence transformers which are fine-tuned on parallel resources (that are not always available) perform better on this task, and we show that their representations are more isotropic. However, we aim to improve multilingual representations in general. We investigate how much of the performance difference can be made up by only transforming the embedding space without fine-tuning, and visualise the resulting spaces. We test different operations: Removing individual outlier dimensions, cluster-based isotropy enhancement, and ZCA whitening. We publish our code for reproducibility.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.439.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Why these outliers occur and how they affect the representations is still an active area of research.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Why these outliers occur and how they affect the representations is still an active area of research.\""
    },
    {
        "title": "Revisiting Sentence Union Generation as a Testbed for Text Consolidation",
        "authors": [
            "Eran Hirsch",
            "Valentina Pyatkin",
            "Ruben Wolhandler",
            "Avi Caciularu",
            "Asi Shefer",
            "Ido Dagan"
        ],
        "published": "2023",
        "summary": "Tasks involving text generation based on multiple input texts, such as multi-document summarization, long-form question answering and contemporary dialogue applications, challenge models for their ability to properly consolidate partly-overlapping multi-text information. However, these tasks entangle the consolidation phase with the often subjective and ill-defined content selection requirement, impeding proper assessment of models’ consolidation capabilities. In this paper, we suggest revisiting the sentence union generation task as an effective well-defined testbed for assessing text consolidation capabilities, decoupling the consolidation challenge from subjective content selection. To support research on this task, we present refined annotation methodology and tools for crowdsourcing sentence union, create the largest union dataset to date and provide an analysis of its rich coverage of various consolidation aspects. We then propose a comprehensive evaluation protocol for union generation, including both human and automatic evaluation. Finally, as baselines, we evaluate state-of-the-art language models on the task, along with a detailed analysis of their capacity to address multi-text consolidation challenges and their limitations.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.440.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Finally, as baselines, we evaluate state-of-the-art language models on the task, along with a detailed analysis of their capacity to address multi-text consolidation challenges and their limitations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Finally, as baselines, we evaluate state-of-the-art language models on the task, along with a detailed analysis of their capacity to address multi-text consolidation challenges and their limitations.\""
    },
    {
        "title": "Distilling Reasoning Capabilities into Smaller Language Models",
        "authors": [
            "Kumar Shridhar",
            "Alessandro Stolfo",
            "Mrinmaya Sachan"
        ],
        "published": "2023",
        "summary": "Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work. In this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models. In this work, we propose an alternative reasoning scheme, Socratic CoT that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps. We use Socratic CoT to train a combination of two small distilled models: a problem decomposer and a subproblem solver. In practice, given a new problem, the two distilled models work in sync to decompose and solve complex problems. On multiple reasoning datasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies boosts the performance of smaller models over 70% compared to the baselines. Finally, we investigate when Socratic CoT is an effective alternative to CoT, demonstrating cases where a much smaller model (GPT-2 large) can outperform a 10X larger model (GPT-3 6B). Our code is available: https://github.com/kumar-shridhar/Distiiling-LM.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.441.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work.\""
    },
    {
        "title": "Improving Language Model Integration for Neural Machine Translation",
        "authors": [
            "Christian Herold",
            "Yingbo Gao",
            "Mohammad Zeineldeen",
            "Hermann Ney"
        ],
        "published": "2023",
        "summary": "The integration of language models for neural machine translation has been extensively studied in the past. It has been shown that an external language model, trained on additional target-side monolingual data, can help improve translation quality. However, there has always been the assumption that the translation model also learns an implicit target-side language model during training, which interferes with the external language model at decoding time. Recently, some works on automatic speech recognition have demonstrated that, if the implicit language model is neutralized in decoding, further improvements can be gained when integrating an external language model. In this work, we transfer this concept to the task of machine translation and compare with the most prominent way of including additional monolingual data - namely back-translation. We find that accounting for the implicit language model significantly boosts the performance of language model fusion, although this approach is still outperformed by back-translation.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.444.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, there has always been the assumption that the translation model also learns an implicit target-side language model during training, which interferes with the external language model at decoding time.\"\n\nThis paper mentions a limitation of LLMs, but it is not the primary focus of the paper. The limitation mentioned is the interference between the implicit language model learned during training and the external language model at",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, there has always been the assumption that the translation model also learns an implicit target-side language model during training, which interferes with the external language model at decoding time.\"\n\nThis paper mentions a limitation of LLMs, but it is not the primary focus of the paper. The limitation mentioned is the interference between the implicit language model learned during training and the external language model at"
    },
    {
        "title": "Bridge the Gap Between CV and NLP! A Gradient-based Textual Adversarial Attack Framework",
        "authors": [
            "Lifan Yuan",
            "YiChi Zhang",
            "Yangyi Chen",
            "Wei Wei"
        ],
        "published": "2023",
        "summary": "Despite recent success on various tasks, deep learning techniques still perform poorly on adversarial examples with small perturbations. While optimization-based methods for adversarial attacks are well-explored in the field of computer vision, it is impractical to directly apply them in natural language processing due to the discrete nature of the text. To address the problem, we propose a unified framework to extend the existing optimization-based adversarial attack methods in the vision domain to craft textual adversarial samples. In this framework, continuously optimized perturbations are added to the embedding layer and amplified in the forward propagation process. Then the final perturbed latent representations are decoded with a masked language model head to obtain potential adversarial samples. In this paper, we instantiate our framework with an attack algorithm named Textual Projected Gradient Descent (T-PGD). We find our algorithm effective even using proxy gradient information. Therefore, we perform the more challenging transfer black-box attack and conduct comprehensive experiments to evaluate our attack algorithm with several models on three benchmark datasets. Experimental results demonstrate that our method achieves overall better performance and produces more fluent and grammatical adversarial samples compared to strong baseline methods. The code and data are available at https://github.com/Phantivia/T-PGD.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.446.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite recent success on various tasks, deep learning techniques still perform poorly on adversarial examples with small perturbations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite recent success on various tasks, deep learning techniques still perform poorly on adversarial examples with small perturbations.\""
    },
    {
        "title": "Prediction and Calibration: Complex Reasoning over Knowledge Graph with Bi-directional Directed Acyclic Graph Neural Network",
        "authors": [
            "Yao Xu",
            "Shizhu He",
            "Li Cai",
            "Kang Liu",
            "Jun Zhao"
        ],
        "published": "2023",
        "summary": "Answering complex logical queries is a challenging task for knowledge graph (KG) reasoning. Recently, query embedding (QE) has been proposed to encode queries and entities into the same vector space, and obtain answers based on numerical computation. However, such models obtain the node representations of a query only based on its predecessor nodes, which ignore the information contained in successor nodes. In this paper, we proposed a Bi-directional Directed Acyclic Graph neural network (BiDAG) that splits the reasoning process into prediction and calibration. The joint probability of all nodes is considered by applying a graph neural network (GNN) to the query graph in the calibration process. By the prediction in the first layer and the calibration in deep layers of GNN, BiDAG can outperform previous QE based methods on FB15k, FB15k-237, and NELL995.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.450.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Modeling Adversarial Attack on Pre-trained Language Models as Sequential Decision Making",
        "authors": [
            "Xuanjie Fang",
            "Sijie Cheng",
            "Yang Liu",
            "Wei Wang"
        ],
        "published": "2023",
        "summary": "Pre-trained language models (PLMs) have been widely used to underpin various downstream tasks. However, the adversarial attack task has found that PLMs are vulnerable to small perturbations. Mainstream methods adopt a detached two-stage framework to attack without considering the subsequent influence of substitution at each step. In this paper, we formally model the adversarial attack task on PLMs as a sequential decision-making problem, where the whole attack process is sequential with two decision-making problems, i.e., word finder and word substitution. Considering the attack process can only receive the final state without any direct intermediate signals, we propose to use reinforcement learning to find an appropriate sequential attack path to generate adversaries, named SDM-ATTACK. Our experimental results show that SDM-ATTACK achieves the highest attack success rate with a comparable modification rate and semantic similarity to attack fine-tuned BERT. Furthermore, our analyses demonstrate the generalization and transferability of SDM-ATTACK.Resources of this work will be released after this paper’s publication.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.461.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the adversarial attack task has found that PLMs are vulnerable to small perturbations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, the adversarial attack task has found that PLMs are vulnerable to small perturbations.\""
    },
    {
        "title": "Towards Robust Personalized Dialogue Generation via Order-Insensitive Representation Regularization",
        "authors": [
            "Liang Chen",
            "Hongru Wang",
            "Yang Deng",
            "Wai Chung Kwan",
            "Zezhong Wang",
            "Kam-Fai Wong"
        ],
        "published": "2023",
        "summary": "Generating persona consistent dialogue response is important for developing an intelligent conversational agent. Recent works typically fine-tune large-scale pre-trained models on this task by concatenating persona texts and dialogue history as a single input sequence to generate the target response. While simple and effective, our analysis shows that this popular practice is seriously affected by order sensitivity where different input orders of persona sentences significantly impact the quality and consistency of generated response, resulting in severe performance fluctuations (i.e., 29.4% on GPT2 and 83.2% on BART). To mitigate the order sensitivity problem, we propose a model-agnostic framework, ORder Insensitive Generation (ORIG), which enables dialogue models to learn robust representation under different persona orders and improve the consistency of response generation. Experiments on the Persona-Chat dataset justify the effectiveness and superiority of our method with two dominant pre-trained models (GPT2 and BART).",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.462.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"our analysis shows that this popular practice is seriously affected by order sensitivity where different input orders of persona sentences significantly impact the quality and consistency of generated response, resulting in severe performance fluctuations (i.e., 29.4% on GPT2 and 83.2% on BART).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"our analysis shows that this popular practice is seriously affected by order sensitivity where different input orders of persona sentences significantly impact the quality and consistency of generated response, resulting in severe performance fluctuations (i.e., 29.4% on GPT2 and 83.2% on BART).\""
    },
    {
        "title": "Cost-effective Distillation of Large Language Models",
        "authors": [
            "Sayantan Dasgupta",
            "Trevor Cohn",
            "Timothy Baldwin"
        ],
        "published": "2023",
        "summary": "Knowledge distillation (KD) involves training a small “student” model to replicate the strong performance of a high-capacity “teacher” model, enabling efficient deployment in resource-constrained settings. Top-performing methods tend to be task- or architecture-specific and lack generalizability. Several existing approaches require pretraining of the teacher on task-specific datasets, which can be costly for large and unstable for small datasets. Here we propose an approach for improving KD through a novel distillation loss agnostic to the task and model architecture. We successfully apply our method to the distillation of the BERT-base and achieve highly competitive results from the distilled student across a range of GLUE tasks, especially for tasks with smaller datasets.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.463.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Several existing approaches require pretraining of the teacher on task-specific datasets, which can be costly for large and unstable for small datasets.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Several existing approaches require pretraining of the teacher on task-specific datasets, which can be costly for large and unstable for small datasets.\""
    },
    {
        "title": "I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors",
        "authors": [
            "Tuhin Chakrabarty",
            "Arkadiy Saakyan",
            "Olivia Winn",
            "Artemis Panagopoulou",
            "Yue Yang",
            "Marianna Apidianaki",
            "Smaranda Muresan"
        ],
        "published": "2023",
        "summary": "Visual metaphors are powerful rhetorical devices used to persuade or communicate creative ideas through images. Similar to linguistic metaphors, they convey meaning implicitly through symbolism and juxtaposition of the symbols. We propose a new task of generating visual metaphors from linguistic metaphors. This is a challenging task for diffusion-based text-to-image models, such as DALL⋅E 2, since it requires the ability to model implicit meaning and compositionality. We propose to solve the task through the collaboration between Large Language Models (LLMs) and Diffusion Models: Instruct GPT-3 (davinci-002) with Chain-of-Thought prompting generates text that represents a visual elaboration of the linguistic metaphor containing the implicit meaning and relevant objects, which is then used as input to the diffusion-based text-to-image models. Using a human-AI collaboration framework, where humans interact both with the LLM and the top-performing diffusion model, we create a high-quality dataset containing 6,476 visual metaphors for 1,540 linguistic metaphors and their associated visual elaborations. Evaluation by professional illustrators shows the promise of LLM-Diffusion Model collaboration for this task.To evaluate the utility of our Human-AI collaboration framework and the quality of our dataset, we perform both an intrinsic human-based evaluation and an extrinsic evaluation using visual entailment as a downstream task.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.465.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This is a challenging task for diffusion-based text-to-image models, such as DALL-E 2, since it requires the ability to model implicit meaning and compositionality.\"\n\nNote: The paper mentions a limitation of diffusion-based text-to-image models, but not specifically of Large Language Models (LLMs). However, since the task is challenging for diffusion-based models and LLMs are",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"This is a challenging task for diffusion-based text-to-image models, such as DALL-E 2, since it requires the ability to model implicit meaning and compositionality.\"\n\nNote: The paper mentions a limitation of diffusion-based text-to-image models, but not specifically of Large Language Models (LLMs). However, since the task is challenging for diffusion-based models and LLMs are"
    },
    {
        "title": "Text Augmentation Using Dataset Reconstruction for Low-Resource Classification",
        "authors": [
            "Adir Rahamim",
            "Guy Uziel",
            "Esther Goldbraich",
            "Ateret Anaby Tavor"
        ],
        "published": "2023",
        "summary": "In the deployment of real-world text classification models, label scarcity is a common problem and as the number of classes increases, this problem becomes even more complex. An approach to addressing this problem is by applying text augmentation methods. One of the more prominent methods involves using the text-generation capabilities of language models. In this paper, we propose Text AUgmentation by Dataset Reconstruction (TAU-DR), a novel method of data augmentation for text classification. We conduct experiments on several multi-class datasets, showing that our approach improves the current state-of-the-art techniques for data augmentation.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.466.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"using the text-generation capabilities of language models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"using the text-generation capabilities of language models.\""
    },
    {
        "title": "Beyond Positive Scaling: How Negation Impacts Scaling Trends of Language Models",
        "authors": [
            "Yuhui Zhang",
            "Michihiro Yasunaga",
            "Zhengping Zhou",
            "Jeff Z. HaoChen",
            "James Zou",
            "Percy Liang",
            "Serena Yeung"
        ],
        "published": "2023",
        "summary": "Language models have been shown to exhibit positive scaling, where performance improves as models are scaled up in terms of size, compute, or data. In this work, we introduce NeQA, a dataset consisting of questions with negation in which language models do not exhibit straightforward positive scaling. We show that this task can exhibit inverse scaling, U-shaped scaling, or positive scaling, and the three scaling trends shift in this order as we use more powerful prompting methods or model families. We hypothesize that solving NeQA depends on two subtasks: question answering (task 1) and negation understanding (task 2). We find that task 1 has linear scaling, while task 2 has sigmoid-shaped scaling with an emergent transition point, and composing these two scaling trends yields the final scaling trend of NeQA. Our work reveals and provides a way to analyze the complex scaling trends of language models.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.472.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We show that this task can exhibit inverse scaling, U-shaped scaling, or positive scaling, and the three scaling trends shift in this order as we use more powerful prompting methods or model families.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We show that this task can exhibit inverse scaling, U-shaped scaling, or positive scaling, and the three scaling trends shift in this order as we use more powerful prompting methods or model families.\""
    },
    {
        "title": "Sequential Integrated Gradients: a simple but effective method for explaining language models",
        "authors": [
            "Joseph Enguehard"
        ],
        "published": "2023",
        "summary": "Several explanation methods such as Integrated Gradients (IG) can be characterised as path-based methods, as they rely on a straight line between the data and an uninformative baseline. However, when applied to language models, these methods produce a path for each word of a sentence simultaneously, which could lead to creating sentences from interpolated words either having no clear meaning, or having a significantly different meaning compared to the original sentence. In order to keep the meaning of these sentences as close as possible to the original one, we propose Sequential Integrated Gradients (SIG), which computes the importance of each word in a sentence by keeping fixed every other words, only creating interpolations between the baseline and the word of interest. Moreover, inspired by the training procedure of language models, we also propose to replace the baseline token “pad” with the trained token “mask”. While being a simple improvement over the original IG method, we show on various models and datasets that SIG proves to be a very effective method for explaining language models.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.477.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, when applied to language models, these methods produce a path for each word of a sentence simultaneously, which could lead to creating sentences from interpolated words either having no clear meaning, or having a significantly different meaning compared to the original sentence.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, when applied to language models, these methods produce a path for each word of a sentence simultaneously, which could lead to creating sentences from interpolated words either having no clear meaning, or having a significantly different meaning compared to the original sentence.\""
    },
    {
        "title": "DiffuDetox: A Mixed Diffusion Model for Text Detoxification",
        "authors": [
            "Griffin Floto",
            "Mohammad Mahdi Abdollah Pour",
            "Parsa Farinneya",
            "Zhenwei Tang",
            "Ali Pesaranghader",
            "Manasa Bharadwaj",
            "Scott Sanner"
        ],
        "published": "2023",
        "summary": "Text detoxification is a conditional text generation task aiming to remove offensive content from toxic text. It is highly useful for online forums and social media, where offensive content is frequently encountered. Intuitively, there are diverse ways to detoxify sentences while preserving their meanings, and we can select from detoxified sentences before displaying text to users. Conditional diffusion models are particularly suitable for this task given their demonstrated higher generative diversity than existing conditional text generation models based on language models. Nonetheless, text fluency declines when they are trained with insufficient data, which is the case for this task. In this work, we propose DiffuDetox, a mixed conditional and unconditional diffusion model for text detoxification. The conditional model takes toxic text as the condition and reduces its toxicity, yielding a diverse set of detoxified sentences. The unconditional model is trained to recover the input text, which allows the introduction of additional fluent text for training and thus ensures text fluency. Extensive experimental results and in-depth analysis demonstrate the effectiveness of our proposed DiffuDetox.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.478.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Conditional diffusion models are particularly suitable for this task given their demonstrated higher generative diversity than existing conditional text generation models based on language models. Nonetheless, text fluency declines when they are trained with insufficient data, which is the case for this task.\"\n\nThis abstract mentions a limitation of language models (text fluency decline when trained with insufficient data), but it is not the primary focus of",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Conditional diffusion models are particularly suitable for this task given their demonstrated higher generative diversity than existing conditional text generation models based on language models. Nonetheless, text fluency declines when they are trained with insufficient data, which is the case for this task.\"\n\nThis abstract mentions a limitation of language models (text fluency decline when trained with insufficient data), but it is not the primary focus of"
    },
    {
        "title": "Disentangling Reasoning Capabilities from Language Models with Compositional Reasoning Transformers",
        "authors": [
            "Wanjun Zhong",
            "Tingting Ma",
            "Jiahai Wang",
            "Jian Yin",
            "Tiejun Zhao",
            "Chin-Yew Lin",
            "Nan Duan"
        ],
        "published": "2023",
        "summary": "This paper presents ReasonFormer, a unified reasoning framework for mirroring the modular and compositional reasoning process of humans in complex decision-making. Inspired by dual-process theory in cognitive science, the representation module (automatic thinking) and reasoning modules (controlled thinking) are decoupled to capture different levels of cognition. Upon the top of the representation module, the pre-trained reasoning modules are modular and professional in specific and fundamental reasoning skills (e.g., logic, simple QA, etc). To mimic the controlled compositional thinking process, different reasoning modules are dynamically activated and composed in both parallel and cascaded manners to control what reasoning skills are activated and how deep the reasoning process will be reached to solve the current problems. The unified reasoning framework solves multiple tasks with a single model, and is trained and inferred in an end-to-end manner. Evaluated on 11 datasets requiring different reasoning skills and complexity, ReasonFormer demonstrates substantial performance boosts, revealing the compositional reasoning ability. Few-shot experiments exhibit better generalization ability by learning to compose pre-trained skills for new tasks with limited data, and decoupling the representation module and the reasoning modules. Further analysis shows the modularity of reasoning modules as different tasks activate distinct reasoning skills at different reasoning depths.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.480.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the title mentions \"Disentangling Reasoning Capabilities from Language Models\", implying that the paper may be addressing some limitations of LLMs, but the abstract does not provide any explicit evidence.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the title mentions \"Disentangling Reasoning Capabilities from Language Models\", implying that the paper may be addressing some limitations of LLMs, but the abstract does not provide any explicit evidence."
    },
    {
        "title": "Multilingual Sequence-to-Sequence Models for Hebrew NLP",
        "authors": [
            "Matan Eyal",
            "Hila Noga",
            "Roee Aharoni",
            "Idan Szpektor",
            "Reut Tsarfaty"
        ],
        "published": "2023",
        "summary": "Recent work attributes progress in NLP to large language models (LMs) with increased model size and large quantities of pretraining data. Despite this, current state-of-the-art LMs for Hebrew are both under-parameterized and under-trained compared to LMs in other languages. Additionally, previous work on pretrained Hebrew LMs focused on encoder-only models. While the encoder-only architecture is beneficial for classification tasks, it does not cater well for sub-word prediction tasks, such as Named Entity Recognition, when considering the morphologically rich nature of Hebrew. In this paper we argue that sequence-to-sequence generative architectures are more suitable for large LMs in morphologically rich languages (MRLs) such as Hebrew. We demonstrate this by casting tasks in the Hebrew NLP pipeline as text-to-text tasks, for which we can leverage powerful multilingual, pretrained sequence-to-sequence models as mT5, eliminating the need for a separate, specialized, morpheme-based, decoder. Using this approach, our experiments show substantial improvements over previously published results on all existing Hebrew NLP benchmarks. These results suggest that multilingual sequence-to-sequence models present a promising building block for NLP for MRLs.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.487.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite this, current state-of-the-art LMs for Hebrew are both under-parameterized and under-trained compared to LMs in other languages.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite this, current state-of-the-art LMs for Hebrew are both under-parameterized and under-trained compared to LMs in other languages.\""
    },
    {
        "title": "Multilingual Knowledge Graph Completion from Pretrained Language Models with Knowledge Constraints",
        "authors": [
            "Ran Song",
            "Shizhu He",
            "Shengxiang Gao",
            "Li Cai",
            "Kang Liu",
            "Zhengtao Yu",
            "Jun Zhao"
        ],
        "published": "2023",
        "summary": "Multilingual Knowledge Graph Completion (mKGC) aim at solving queries in different languages by reasoning a tail entity thus improving multilingual knowledge graphs. Previous studies leverage multilingual pretrained language models (PLMs) and the generative paradigm to achieve mKGC. Although multilingual pretrained language models contain extensive knowledge of different languages, its pretraining tasks cannot be directly aligned with the mKGC tasks. Moreover, the majority of KGs and PLMs currently available exhibit a pronounced English-centric bias. This makes it difficult for mKGC to achieve good results, particularly in the context of low-resource languages. To overcome previous problems, this paper introduces global and local knowledge constraints for mKGC. The former is used to constrain the reasoning of answer entities , while the latter is used to enhance the representation of query contexts. The proposed method makes the pretrained model better adapt to the mKGC task. Experimental results on public datasets demonstrate that our method outperforms the previous SOTA on Hits@1 and Hits@10 by an average of 12.32% and 16.03%, which indicates that our proposed method has significant enhancement on mKGC.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.488.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although multilingual pretrained language models contain extensive knowledge of different languages, its pretraining tasks cannot be directly aligned with the mKGC tasks. Moreover, the majority of KGs and PLMs currently available exhibit a pronounced English-centric bias.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although multilingual pretrained language models contain extensive knowledge of different languages, its pretraining tasks cannot be directly aligned with the mKGC tasks. Moreover, the majority of KGs and PLMs currently available exhibit a pronounced English-centric bias.\""
    },
    {
        "title": "Multi-Agent Language Learning: Symbolic Mapping",
        "authors": [
            "Yicheng Feng",
            "Zongqing Lu"
        ],
        "published": "2023",
        "summary": "The study of emergent communication has long been devoted to coax neural network agents to learn a language sharing similar properties with human language. In this paper, we try to find a ‘natural’ way to help agents learn a compositional and symmetric language in complex settings like dialog games. Inspired by the theory that human language was originated from simple interactions, we hypothesize that language may evolve from simple tasks to difficult tasks. We propose a curriculum learning method called task transfer, and propose a novel architecture called symbolic mapping. We find that task transfer distinctly helps language learning in difficult tasks, and symbolic mapping promotes the effect. Further, we explore vocabulary expansion, and show that with the help of symbolic mapping, agents can easily learn to use new symbols when the environment becomes more complex. All in all, we find that a process from simplicity to complexity can serve as a natural way to help multi-agent language learning, and the proposed symbolic mapping is effective for this process.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.491.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Scaling Laws for BERT in Low-Resource Settings",
        "authors": [
            "Gorka Urbizu",
            "Iñaki San Vicente",
            "Xabier Saralegi",
            "Rodrigo Agerri",
            "Aitor Soroa"
        ],
        "published": "2023",
        "summary": "Large language models are very resource intensive, both financially and environmentally, and require an amount of training data which is simply unobtainable for the majority of NLP practitioners. Previous work has researched the scaling laws of such models, but optimal ratios of model parameters, dataset size, and computation costs focused on the large scale. In contrast, we analyze the effect those variables have on the performance of language models in constrained settings, by building three lightweight BERT models (16M/51M/124M parameters) trained over a set of small corpora (5M/25M/125M words).We experiment on four languages of different linguistic characteristics (Basque, Spanish, Swahili and Finnish), and evaluate the models on MLM and several NLU tasks. We conclude that the power laws for parameters, data and compute for low-resource settings differ from the optimal scaling laws previously inferred, and data requirements should be higher. Our insights are consistent across all the languages we study, as well as across the MLM and downstream tasks. Furthermore, we experimentally establish when the cost of using a Transformer-based approach is worth taking, instead of favouring other computationally lighter solutions.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.492.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models are very resource intensive, both financially and environmentally, and require an amount of training data which is simply unobtainable for the majority of NLP practitioners.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Large language models are very resource intensive, both financially and environmentally, and require an amount of training data which is simply unobtainable for the majority of NLP practitioners.\""
    },
    {
        "title": "Pre-trained Language Model with Prompts for Temporal Knowledge Graph Completion",
        "authors": [
            "Wenjie Xu",
            "Ben Liu",
            "Miao Peng",
            "Xu Jia",
            "Min Peng"
        ],
        "published": "2023",
        "summary": "Temporal Knowledge graph completion (TKGC) is a crucial task that involves reasoning at known timestamps to complete the missing part of facts and has attracted more and more attention in recent years. Most existing methods focus on learning representations based on graph neural networks while inaccurately extracting information from timestamps and insufficiently utilizing the implied information in relations. To address these problems, we propose a novel TKGC model, namely Pre-trained Language Model with Prompts for TKGC (PPT). We convert a series of sampled quadruples into pre-trained language model inputs and convert intervals between timestamps into different prompts to make coherent sentences with implicit semantic information. We train our model with a masking strategy to convert TKGC task into a masked token prediction task, which can leverage the semantic information in pre-trained language models. Experiments on three benchmark datasets and extensive analysis demonstrate that our model has great competitiveness compared to other models with four metrics. Our model can effectively incorporate information from temporal knowledge graphs into the language models.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.493.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the title mentions \"Pre-trained Language Model\" which implies the involvement of LLMs, however, the abstract does not mention any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the title mentions \"Pre-trained Language Model\" which implies the involvement of LLMs, however, the abstract does not mention any limitations of LLMs."
    },
    {
        "title": "Impact of Adversarial Training on Robustness and Generalizability of Language Models",
        "authors": [
            "Enes Altinisik",
            "Hassan Sajjad",
            "Husrev Sencar",
            "Safa Messaoud",
            "Sanjay Chawla"
        ],
        "published": "2023",
        "summary": "Adversarial training is widely acknowledged as the most effective defense against adversarial attacks. However, it is also well established that achieving both robustness and generalization in adversarially trained models involves a trade-off. The goal of this work is to provide an in depth comparison of different approaches for adversarial training in language models. Specifically, we study the effect of pre-training data augmentation as well as training time input perturbations vs. embedding space perturbations on the robustness and generalization of transformer-based language models. Our findings suggest that better robustness can be achieved by pre-training data augmentation or by training with input space perturbation. However, training with embedding space perturbation significantly improves generalization. A linguistic correlation analysis of neurons of the learned models reveal that the improved generalization is due to ‘more specialized’ neurons. To the best of our knowledge, this is the first work to carry out a deep qualitative analysis of different methods of generating adversarial examples in adversarial training of language models.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.496.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it is also well established that achieving both robustness and generalization in adversarially trained models involves a trade-off.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, it is also well established that achieving both robustness and generalization in adversarially trained models involves a trade-off.\""
    },
    {
        "title": "Similarizing the Influence of Words with Contrastive Learning to Defend Word-level Adversarial Text Attack",
        "authors": [
            "Pengwei Zhan",
            "Jing Yang",
            "He Wang",
            "Chao Zheng",
            "Xiao Huang",
            "Liming Wang"
        ],
        "published": "2023",
        "summary": "Neural language models are vulnerable to word-level adversarial text attacks, which generate adversarial examples by directly substituting discrete input words. Previous search methods for word-level attacks assume that the information in the important words is more influential on prediction than unimportant words. In this paper, motivated by this assumption, we propose a self-supervised regularization method for Similarizing the Influence of Words with Contrastive Learning (SIWCon) that encourages the model to learn sentence representations in which words of varying importance have a more uniform influence on prediction. Experiments show that SIWCon is compatible with various training methods and effectively improves model robustness against various unforeseen adversarial attacks. The effectiveness of SIWCon is also intuitively shown through qualitative analysis and visualization of the loss landscape, sentence representation, and changes in model confidence.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.500.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Neural language models are vulnerable to word-level adversarial text attacks, which generate adversarial examples by directly substituting discrete input words.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Neural language models are vulnerable to word-level adversarial text attacks, which generate adversarial examples by directly substituting discrete input words.\""
    },
    {
        "title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes",
        "authors": [
            "Cheng-Yu Hsieh",
            "Chun-Liang Li",
            "Chih-kuan Yeh",
            "Hootan Nakhost",
            "Yasuhisa Fujii",
            "Alex Ratner",
            "Ranjay Krishna",
            "Chen-Yu Lee",
            "Tomas Pfister"
        ],
        "published": "2023",
        "summary": "Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for training small models within a multi-task framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to few-shot prompted LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our finetuned 770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80% of available data on a benchmark, whereas standard finetuning the same T5 model struggles to match even by using 100% of the dataset.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.507.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications.\""
    },
    {
        "title": "PreQuant: A Task-agnostic Quantization Approach for Pre-trained Language Models",
        "authors": [
            "Zhuocheng Gong",
            "Jiahao Liu",
            "Qifan Wang",
            "Yang Yang",
            "Jingang Wang",
            "Wei Wu",
            "Yunsen Xian",
            "Dongyan Zhao",
            "Rui Yan"
        ],
        "published": "2023",
        "summary": "While transformer-based pre-trained language models (PLMs) have dominated a number of NLP applications, these models are heavy to deploy and expensive to use. Therefore, effectively compressing large-scale PLMs becomes an increasingly important problem. Quantization, which represents high-precision tensors with low-bit fix-point format, is a viable solution. However, most existing quantization methods are task-specific, requiring customized training and quantization with a large number of trainable parameters on each individual task. Inspired by the observation that the over-parameterization nature of PLMs makes it possible to freeze most of the parameters during the fine-tuning stage, in this work, we propose a novel “quantize before fine-tuning” framework, PreQuant, that differs from both quantization-aware training and post-training quantization. {pasted macro ‘OUR’} is compatible with various quantization strategies, with outlier-aware parameter-efficient fine-tuning incorporated to correct the induced quantization error. We demonstrate the effectiveness of PreQuant on the GLUE benchmark using BERT, RoBERTa, and T5. We also provide an empirical investigation into the workflow of PreQuant, which sheds light on its efficacy.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.511.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While transformer-based pre-trained language models (PLMs) have dominated a number of NLP applications, these models are heavy to deploy and expensive to use.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While transformer-based pre-trained language models (PLMs) have dominated a number of NLP applications, these models are heavy to deploy and expensive to use.\""
    },
    {
        "title": "IDOL: Indicator-oriented Logic Pre-training for Logical Reasoning",
        "authors": [
            "Zihang Xu",
            "Ziqing Yang",
            "Yiming Cui",
            "Shijin Wang"
        ],
        "published": "2023",
        "summary": "In the field of machine reading comprehension (MRC), existing systems have surpassed the average performance of human beings in many tasks like SQuAD. However, there is still a long way to go when it comes to logical reasoning. Although some methods for it have been put forward, they either are designed in a quite complicated way or rely too much on external structures. In this paper, we proposed IDOL (InDicator-Oriented Logic Pre-training), an easy-to-understand but highly effective further pre-training task which logically strengthens the pre-trained models with the help of 6 types of logical indicators and a logically rich dataset LoGic Pre-training (LGP). IDOL achieves state-of-the-art performance on ReClor and LogiQA, the two most representative benchmarks in logical reasoning MRC, and is proven to be capable of generalizing to different pre-trained models and other types of MRC benchmarks like RACE and SQuAD 2.0 while keeping competitive general language understanding ability through testing on tasks in GLUE. Besides, at the beginning of the era of large language models, we take several of them like ChatGPT into comparison and find that IDOL still shows its advantage.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.513.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"at the beginning of the era of large language models, we take several of them like ChatGPT into comparison and find that IDOL still shows its advantage.\"\n\nThis abstract mentions LLMs but only briefly compares the proposed method to several LLMs, without explicitly discussing any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"at the beginning of the era of large language models, we take several of them like ChatGPT into comparison and find that IDOL still shows its advantage.\"\n\nThis abstract mentions LLMs but only briefly compares the proposed method to several LLMs, without explicitly discussing any limitations of LLMs."
    },
    {
        "title": "Forecasting Earnings Surprises from Conference Call Transcripts",
        "authors": [
            "Ross Koval",
            "Nicholas Andrews",
            "Xifeng Yan"
        ],
        "published": "2023",
        "summary": "There is a multitude of textual data relevant to the financial markets, spanning genres such as financial news, earnings conference calls, and social media posts. Earnings conference calls are one of the most important to information flow as they reflect a direct communication between company executives, financial analysts, and large shareholders. Since these calls contain content that is forward-looking in nature, they can be used to forecast the future performance of the company relative to market expectations. However, they typically contain over 5,000 words of text and large amounts of industry jargon. This length and domain-specific language present problems for many generic pretrained language models. In this work, we introduce a novel task of predicting earnings surprises from earnings call transcripts and contribute a new long document dataset that tests financial understanding with complex signals. We explore a variety of approaches for this long document classification task and establish some strong baselines. Furthermore, we demonstrate that it is possible to predict companies’ future earnings surprises from solely the text of their conference calls with reasonable accuracy. Finally, we probe the models through different interpretability methods and reveal some intuitive explanations of the linguistic features captured that go beyond traditional sentiment analysis.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.520.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This length and domain-specific language present problems for many generic pretrained language models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"This length and domain-specific language present problems for many generic pretrained language models.\""
    },
    {
        "title": "Reconstruction Probing",
        "authors": [
            "Najoung Kim",
            "Jatin Khilnani",
            "Alex Warstadt",
            "Abdelrahim Qaddoumi"
        ],
        "published": "2023",
        "summary": "We propose reconstruction probing, a new analysis method for contextualized representations based on reconstruction probabilities in masked language models (MLMs). This method relies on comparing the reconstruction probabilities of tokens in a given sequence when conditioned on the representation of a single token that has been fully contextualized and when conditioned on only the decontextualized lexical prior of the model. This comparison can be understood as quantifying the contribution of contextualization towards reconstruction—the difference in the reconstruction probabilities can only be attributed to the representational change of the single token induced by contextualization. We apply this analysis to three MLMs and find that contextualization boosts reconstructability of tokens that are close to the token being reconstructed in terms of linear and syntactic distance. Furthermore, we extend our analysis to finer-grained decomposition of contextualized representations, and we find that these boosts are largely attributable to static and positional embeddings at the input layer.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.523.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper does mention some limitations of MLMs in terms of reconstructability of tokens, however, the focus is on the analysis method rather than the limitations themselves.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None, but the paper does mention some limitations of MLMs in terms of reconstructability of tokens, however, the focus is on the analysis method rather than the limitations themselves."
    },
    {
        "title": "Multi-lingual and Multi-cultural Figurative Language Understanding",
        "authors": [
            "Anubha Kabra",
            "Emmy Liu",
            "Simran Khanuja",
            "Alham Fikri Aji",
            "Genta Winata",
            "Samuel Cahyawijaya",
            "Anuoluwapo Aremu",
            "Perez Ogayo",
            "Graham Neubig"
        ],
        "published": "2023",
        "summary": "Figurative language permeates human communication, but at the same time is relatively understudied in NLP. Datasets have been created in English to accelerate progress towards measuring and improving figurative language processing in language models (LMs). However, the use of figurative language is an expression of our cultural and societal experiences, making it difficult for these phrases to be universally applicable. In this work, we create a figurative language inference dataset, {pasted macro ‘DATASETNAME’}, for seven diverse languages associated with a variety of cultures: Hindi, Indonesian, Javanese, Kannada, Sundanese, Swahili and Yoruba. Our dataset reveals that each language relies on cultural and regional concepts for figurative expressions, with the highest overlap between languages originating from the same region. We assess multilingual LMs’ abilities to interpret figurative language in zero-shot and few-shot settings. All languages exhibit a significant deficiency compared to English, with variations in performance reflecting the availability of pre-training and fine-tuning data, emphasizing the need for LMs to be exposed to a broader range of linguistic and cultural variation during training. Data and code is released at https://anonymous.4open.science/r/Multilingual-Fig-QA-7B03/",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.525.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"All languages exhibit a significant deficiency compared to English, with variations in performance reflecting the availability of pre-training and fine-tuning data, emphasizing the need for LMs to be exposed to a broader range of linguistic and cultural variation during training.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"All languages exhibit a significant deficiency compared to English, with variations in performance reflecting the availability of pre-training and fine-tuning data, emphasizing the need for LMs to be exposed to a broader range of linguistic and cultural variation during training.\""
    },
    {
        "title": "Open-WikiTable : Dataset for Open Domain Question Answering with Complex Reasoning over Table",
        "authors": [
            "Sunjun Kweon",
            "Yeonsu Kwon",
            "Seonhee Cho",
            "Yohan Jo",
            "Edward Choi"
        ],
        "published": "2023",
        "summary": "Despite recent interest in open domain question answering (ODQA) over tables, many studies still rely on datasets that are not truly optimal for the task with respect to utilizing structural nature of table. These datasets assume answers reside as a single cell value and do not necessitate exploring over multiple cells such as aggregation, comparison, and sorting. Thus, we release Open-WikiTable, the first ODQA dataset that requires complex reasoning over tables. Open-WikiTable is built upon WikiSQL and WikiTableQuestions to be applicable in the open-domain setting. As each question is coupled with both textual answers and SQL queries, Open-WikiTable opens up a wide range of possibilities for future research, as both reader and parser methods can be applied. The dataset is publicly available.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.526.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs."
    },
    {
        "title": "What In-Context Learning “Learns” In-Context: Disentangling Task Recognition and Task Learning",
        "authors": [
            "Jane Pan",
            "Tianyu Gao",
            "Howard Chen",
            "Danqi Chen"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) exploit in-context learning (ICL) to solve tasks with only a few demonstrations, but its mechanisms are not yet well-understood. Some works suggest that LLMs only recall already learned concepts from pre-training, while others hint that ICL performs implicit learning over demonstrations. We characterize two ways through which ICL leverages demonstrations. Task recognition (TR) captures the extent to which LLMs can recognize a task through demonstrations – even without ground-truth labels – and apply their pre-trained priors, whereas task learning (TL) is the ability to capture new input-label mappings unseen in pre-training. Using a wide range of classification datasets and three LLM families (GPT-3, LLaMA and OPT), we design controlled experiments to disentangle the roles of TR and TL in ICL. We show that (1) models can achieve non-trivial performance with only TR, and TR does not further improve with larger models or more demonstrations; (2) LLMs acquire TL as the model scales, and TL’s performance consistently improves with more demonstrations in context. Our findings unravel two different forces behind ICL and we advocate for discriminating them in future ICL research due to their distinct nature.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.527.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"TR does not further improve with larger models or more demonstrations; (2) LLMs acquire TL as the model scales, and TL’s performance consistently improves with more demonstrations in context.\"\n\nThis rating is based on the fact that the paper discusses the limitations of in-context learning in LLMs, specifically the distinction between task recognition and task learning, and how they are affected by model",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"TR does not further improve with larger models or more demonstrations; (2) LLMs acquire TL as the model scales, and TL’s performance consistently improves with more demonstrations in context.\"\n\nThis rating is based on the fact that the paper discusses the limitations of in-context learning in LLMs, specifically the distinction between task recognition and task learning, and how they are affected by model"
    },
    {
        "title": "Cross-Lingual Retrieval Augmented Prompt for Low-Resource Languages",
        "authors": [
            "Ercong Nie",
            "Sheng Liang",
            "Helmut Schmid",
            "Hinrich Schütze"
        ],
        "published": "2023",
        "summary": "Multilingual Pretrained Language Models (MPLMs) perform strongly in cross-lingual transfer. We propose Prompts Augmented by Retrieval Crosslingually (PARC) to improve zero-shot performance on low-resource languages (LRLs) by augmenting the context with prompts consisting of semantically similar sentences retrieved from a high-resource language (HRL). PARC improves zero-shot performance on three downstream tasks (sentiment classification, topic categorization, natural language inference) with multilingual parallel test sets across 10 LRLs covering 6 language families in unlabeled (+5.1%) and labeled settings (+16.3%). PARC also outperforms finetuning by 3.7%. We find a significant positive correlation between cross-lingual transfer performance on one side, and the similarity between high- and low-resource languages as well as the amount of low-resource pretraining data on the other side. A robustness analysis suggests that PARC has the potential to achieve even stronger performance with more powerful MPLMs.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.528.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions the challenges of low-resource languages, which is a limitation of LLMs in certain domains, however it is not explicitly stated as a limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None, but the paper mentions the challenges of low-resource languages, which is a limitation of LLMs in certain domains, however it is not explicitly stated as a limitation."
    },
    {
        "title": "GRACE: Gradient-guided Controllable Retrieval for Augmenting Attribute-based Text Generation",
        "authors": [
            "Zhihua Wen",
            "Zhiliang Tian",
            "Zhen Huang",
            "Yuxin Yang",
            "Zexin Jian",
            "Changjian Wang",
            "Dongsheng Li"
        ],
        "published": "2023",
        "summary": "Attribute-based generation methods are of growing significance in controlling the generation of large pre-trained language models (PLMs). Existing studies control the generation by (1) finetuning the model with attributes or (2) guiding the inference processing toward control signals while freezing the PLM. However, finetuning approaches infuse domain bias into generation, making it hard to generate out-of-domain texts. Besides, many methods guide the inference in its word-by-word generation, pushing the word probability to the target attributes, resulting in less fluent sentences. We argue that distilling controlling information from natural texts can produce fluent sentences while maintaining high controllability. In this paper, we propose GRAdient-guided Controllable rEtrieval (GRACE), a retrieval-augmented generation framework to facilitate the generation of fluent sentences with high attribute relevance. GRACE memorizes the semantic and attribute information from unlabeled corpora and applies a controllable retrieval to obtain desired information. For the generation, we design techniques to eliminate the domain bias from the retrieval results and integrate it into the generation model. Additionally, we propose a gradient-guided generation scheme that iteratively steers generation toward higher attribute relevance. Experimental results and quantities of examples verify the effectiveness of our method.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.530.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, finetuning approaches infuse domain bias into generation, making it hard to generate out-of-domain texts. Besides, many methods guide the inference in its word-by-word generation, pushing the word probability to the target attributes, resulting in less fluent sentences.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, finetuning approaches infuse domain bias into generation, making it hard to generate out-of-domain texts. Besides, many methods guide the inference in its word-by-word generation, pushing the word probability to the target attributes, resulting in less fluent sentences.\""
    },
    {
        "title": "So many design choices: Improving and interpreting neural agent communication in signaling games",
        "authors": [
            "Timothée Bernard",
            "Timothee Mickus"
        ],
        "published": "2023",
        "summary": "Emergent language games are experimental protocols designed to model how communication may arise among a group of agents. In this paper, we focus on how to improve performances of neural agents playing a signaling game: a sender is exposed to an image and generates a sequence of symbols that is transmitted to a receiver, which uses it to distinguish between two images, one that is semantically related to the original image, and one that is not. We consider multiple design choices, such as pretraining the visual components of the agents, introducing regularization terms, how to sample training items from the dataset, and we study how these different choices impact the behavior and performances of the agents. To that end, we introduce a number of automated metrics to measure the properties of the emergent language. We find that some implementation choices are always beneficial, and that the information that is conveyed by the agents’ messages is shaped not only by the game, but also by the overall design of the agents as well as seemingly unrelated implementation choices.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.531.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Constructing Word-Context-Coupled Space Aligned with Associative Knowledge Relations for Interpretable Language Modeling",
        "authors": [
            "Fanyu Wang",
            "Zhenping Xie"
        ],
        "published": "2023",
        "summary": "As the foundation of current natural language processing methods, pre-trained language model has achieved excellent performance. However, the black-box structure of the deep neural network in pre-trained language models seriously limits the interpretability of the language modeling process. After revisiting the coupled requirement of deep neural representation and semantics logic of language modeling, a Word-Context-Coupled Space (W2CSpace) is proposed by introducing the alignment processing between uninterpretable neural representation and interpretable statistical logic. Moreover, a clustering process is also designed to connect the word- and context-level semantics. Specifically, an associative knowledge network (AKN), considered interpretable statistical logic, is introduced in the alignment process for word-level semantics. Furthermore, the context-relative distance is employed as the semantic feature for the downstream classifier, which is greatly different from the current uninterpretable semantic representations of pre-trained models. Our experiments for performance evaluation and interpretable analysis are executed on several types of datasets, including SIGHAN, Weibo, and ChnSenti. Wherein a novel evaluation strategy for the interpretability of machine learning models is first proposed. According to the experimental results, our language model can achieve better performance and highly credible interpretable ability compared to related state-of-the-art methods.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.532.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the black-box structure of the deep neural network in pre-trained language models seriously limits the interpretability of the language modeling process.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the black-box structure of the deep neural network in pre-trained language models seriously limits the interpretability of the language modeling process.\""
    },
    {
        "title": "Fixed Input Parameterization for Efficient Prompting",
        "authors": [
            "Eunbi Choi",
            "Yongrae Jo",
            "Joel Jang",
            "Joonwon Jang",
            "Minjoon Seo"
        ],
        "published": "2023",
        "summary": "Recent works have shown that attaching prompts to the input is effective at conditioning Language Models (LM) to perform specific tasks. However, prompts are always included in the input text during inference, even when they are fixed, thus incurring substantial computational and memory overhead. Also, there is currently no straightforward method of utilizing prompts that are longer than the maximum input length of the LMs without incurring additional costs during inference. We formally define Fixed Input Parameterization (FIP) problem that focuses on injecting the fixed prompt into the parameters of an LM to be an efficient alternative to attaching fixed prompts to the input. We show that in scenarios with long fixed prompts, FIP can be up to 280 times more efficient in terms of total FLOPs than previous approaches. We further explore methodologies for FIP and show promising results in persona-dependent conversation, semantic parsing, and zero-shot learning with task instructions. Through these explorations, we show that FIP can be a promising direction for conditioning language models, in scenarios with long and fixed prompts.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.533.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, prompts are always included in the input text during inference, even when they are fixed, thus incurring substantial computational and memory overhead.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, prompts are always included in the input text during inference, even when they are fixed, thus incurring substantial computational and memory overhead.\""
    },
    {
        "title": "Multi-Dimensional Evaluation of Text Summarization with In-Context Learning",
        "authors": [
            "Sameer Jain",
            "Vaishakh Keshava",
            "Swarnashree Mysore Sathyendra",
            "Patrick Fernandes",
            "Pengfei Liu",
            "Graham Neubig",
            "Chunting Zhou"
        ],
        "published": "2023",
        "summary": "Evaluation of natural language generation (NLG) is complex and multi-dimensional. Generated text can be evaluated for fluency, coherence, factuality, or any other dimensions of interest. Most frameworks that perform such multi-dimensional evaluation require training on large manually or synthetically generated datasets. In this paper, we study the efficacy of large language models as multi-dimensional evaluators using in-context learning, obviating the need for large training datasets. Our experiments show that in-context learning-based evaluators are competitive with learned evaluation frameworks for the task of text summarization, establishing state-of-the-art on dimensions such as relevance and factual consistency. We then analyze the effects of factors such as the selection and number of in-context examples on performance. Finally, we study the efficacy of in-context learning-based evaluators in evaluating zero-shot summaries written by large language models such as GPT-3.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.537.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We then analyze the effects of factors such as the selection and number of in-context examples on performance.\"\n\nThis paper mentions the limitations of in-context learning-based evaluators, but does not explicitly discuss the limitations of Large Language Models.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"We then analyze the effects of factors such as the selection and number of in-context examples on performance.\"\n\nThis paper mentions the limitations of in-context learning-based evaluators, but does not explicitly discuss the limitations of Large Language Models."
    },
    {
        "title": "Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models",
        "authors": [
            "Neal Lawton",
            "Anoop Kumar",
            "Govind Thattai",
            "Aram Galstyan",
            "Greg Ver Steeg"
        ],
        "published": "2023",
        "summary": "Parameter-efficient tuning (PET) methods fit pre-trained language models (PLMs) to downstream tasks by either computing a small compressed update for a subset of model parameters, or appending and fine-tuning a small number of new model parameters to the pre-trained network. Hand-designed PET architectures from the literature perform well in practice, but have the potential to be improved via automated neural architecture search (NAS). We propose an efficient NAS method for learning PET architectures via structured and unstructured pruning. We present experiments on GLUE demonstrating the effectiveness of our algorithm and discuss how PET architectural design choices affect performance in practice.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.539.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Parameter-efficient tuning (PET) methods fit pre-trained language models (PLMs) to downstream tasks by either computing a small compressed update for a subset of model parameters, or appending and fine-tuning a small number of new model parameters to the pre-trained network.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Parameter-efficient tuning (PET) methods fit pre-trained language models (PLMs) to downstream tasks by either computing a small compressed update for a subset of model parameters, or appending and fine-tuning a small number of new model parameters to the pre-trained network.\""
    },
    {
        "title": "Aligning Offline Metrics and Human Judgments of Value for Code Generation Models",
        "authors": [
            "Victor Dibia",
            "Adam Fourney",
            "Gagan Bansal",
            "Forough Poursabzi-Sangdeh",
            "Han Liu",
            "Saleema Amershi"
        ],
        "published": "2023",
        "summary": "Large language models have demonstrated great potential to assist programmers in generating code. For such human-AI pair programming scenarios, we empirically demonstrate that while generated code are most often evaluated in terms of their functional correctness (i.e., whether generations pass available unit tests), correctness does not fully capture (e.g., may underestimate) the productivity gains these models may provide. Through a user study with N=49 experienced programmers, we show that while correctness captures high-value generations, programmers still rate code that fails unit tests as valuable if it reduces the overall effort needed to complete a coding task. Finally, we propose a hybrid metric that combines functional correctness and syntactic similarity and show that it achieves a 14% stronger correlation with value and can therefore better represent real-world gains when evaluating and comparing models.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.540.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"while correctness does not fully capture (e.g., may underestimate) the productivity gains these models may provide.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"while correctness does not fully capture (e.g., may underestimate) the productivity gains these models may provide.\""
    },
    {
        "title": "Bi-level Finetuning with Task-dependent Similarity Structure for Low-resource Training",
        "authors": [
            "Sai Ashish Somayajula",
            "Lifeng Jin",
            "Linfeng Song",
            "Haitao Mi",
            "Dong Yu"
        ],
        "published": "2023",
        "summary": "Training a large language model in low-resource settings is challenging since they are susceptible to overfitting with limited generalization abilities. Previous work addresses this issue by approaches such as tunable parameters reduction or data augmentation. However, they either limit the trained models’ expressiveness or rely on task-independent knowledge. In this paper, we propose the Bi-level Finetuning with Task-dependent Similarity Structure framework where all parameters, including the embeddings for unseen tokens, are finetuned with task-dependent information from the training data only. In this framework, a task-dependent similarity structure is learned in a data-driven fashion, which in turn is used to compose soft embeddings from conventional embeddings to be used in training to update all parameters. In order to learn the similarity structure and model parameters, we propose a bi-level optimization algorithm with two stages—search and finetune—to ensure successful learning. Results of experiments on several classification datasets in low-resource scenarios demonstrate that models trained with our method outperform strong baselines. Ablation experiments further support the effectiveness of different components in our framework. Code is available at https://github.com/Sai-Ashish/BFTSS.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.544.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Training a large language model in low-resource settings is challenging since they are susceptible to overfitting with limited generalization abilities.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Training a large language model in low-resource settings is challenging since they are susceptible to overfitting with limited generalization abilities.\""
    },
    {
        "title": "Kanbun-LM: Reading and Translating Classical Chinese in Japanese Methods by Language Models",
        "authors": [
            "Hao Wang",
            "Hirofumi Shimizu",
            "Daisuke Kawahara"
        ],
        "published": "2023",
        "summary": "Recent studies in natural language processing (NLP) have focused on modern languages and achieved state-of-the-art results in many tasks. Meanwhile, little attention has been paid to ancient texts and related tasks. Classical Chinese first came to Japan approximately 2,000 years ago. It was gradually adapted to a Japanese form called Kanbun-Kundoku (Kanbun) in Japanese reading and translating methods, which has significantly impacted Japanese literature. However, compared to the rich resources of ancient texts in mainland China, Kanbun resources remain scarce in Japan.To solve this problem, we construct the first Classical-Chinese-to-Kanbun dataset in the world. Furthermore, we introduce two tasks, character reordering and machine translation, both of which play a significant role in Kanbun comprehension. We also test the current language models on these tasks and discuss the best evaluation method by comparing the results with human scores. We release our code and dataset on GitHub.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.545.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We also test the current language models on these tasks and discuss the best evaluation method by comparing the results with human scores.\"\n\nThis rating is given because the paper mentions testing current language models on certain tasks and comparing the results with human scores, which implies that the models may have limitations or weaknesses that are being evaluated. However, the discussion of these limitations is brief and not the primary focus",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We also test the current language models on these tasks and discuss the best evaluation method by comparing the results with human scores.\"\n\nThis rating is given because the paper mentions testing current language models on certain tasks and comparing the results with human scores, which implies that the models may have limitations or weaknesses that are being evaluated. However, the discussion of these limitations is brief and not the primary focus"
    },
    {
        "title": "Sentiment Analysis using the Relationship between Users and Products",
        "authors": [
            "Natthawut Kertkeidkachorn",
            "Kiyoaki Shirai"
        ],
        "published": "2023",
        "summary": "In product reviews, user and product aspects are useful in sentiment analysis. Nevertheless, previous studies mainly focus on modeling user and product aspects without considering the relationship between users and products. The relationship between users and products is typically helpful in estimating the bias of a user toward a product. In this paper, we, therefore, introduce the Graph Neural Network-based model with the pre-trained Language Model (GNNLM), where the relationship between users and products is incorporated. We conducted experiments on three well-known benchmarks for sentiment classification with the user and product information. The experimental results show that the relationship between users and products improves the performance of sentiment analysis. Furthermore, GNNLM achieves state-of-the-art results on yelp-2013 and yelp-2014 datasets.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.547.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but mentions the pre-trained Language Model (LM) as part of the proposed model GNNLM, without discussing its limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but mentions the pre-trained Language Model (LM) as part of the proposed model GNNLM, without discussing its limitations."
    },
    {
        "title": "Entropy-guided Vocabulary Augmentation of Multilingual Language Models for Low-resource Tasks",
        "authors": [
            "Arijit Nag",
            "Bidisha Samanta",
            "Animesh Mukherjee",
            "Niloy Ganguly",
            "Soumen Chakrabarti"
        ],
        "published": "2023",
        "summary": "Multilingual language models (MLLMs) like mBERTpromise to extend the benefits of NLP research to low-resource languages (LRLs). However, LRL words are under-represented in the wordpiece/subword vocabularies of MLLMs. This leads to many LRL words getting replaced by UNK, or concatenated from morphologically unrelated wordpieces, leading to low task accuracy. (Pre)-training MLLMs after including LRL documents is resource-intensive in terms of both human inputs and computational resources. In response, we propose EVALM (entropy-based vocabulary augmented language model), which uses a new task-cognizant measurement to detect the most vulnerable LRL words, whose wordpiece segmentations are undesirable. EVALM then provides reasonable initializations of their embeddings, followed by limited fine-tuning using the small LRL task corpus. Our experiments show significant performance improvements and also some surprising limits to such vocabulary augmentation strategies in various classification tasks for multiple diverse LRLs, as well as code-mixed texts. We will release the code and data to enable further research.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.548.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, LRL words are under-represented in the wordpiece/subword vocabularies of MLLMs. This leads to many LRL words getting replaced by UNK, or concatenated from morphologically unrelated wordpieces, leading to low task accuracy.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, LRL words are under-represented in the wordpiece/subword vocabularies of MLLMs. This leads to many LRL words getting replaced by UNK, or concatenated from morphologically unrelated wordpieces, leading to low task accuracy.\""
    },
    {
        "title": "Solving Cosine Similarity Underestimation between High Frequency Words by ℓ2 Norm Discounting",
        "authors": [
            "Saeth Wannasuphoprasit",
            "Yi Zhou",
            "Danushka Bollegala"
        ],
        "published": "2023",
        "summary": "Cosine similarity between two words, computed using their contextualised token embeddings obtained from masked language models (MLMs) such as BERT has shown to underestimate the actual similarity between those words CITATION.This similarity underestimation problem is particularly severe for high frequent words. Although this problem has been noted in prior work, no solution has been proposed thus far. We observe that the ℓ2 norm of contextualised embeddings of a word correlates with its log-frequency in the pretraining corpus.Consequently, the larger ℓ2 norms associated with the high frequent words reduce the cosine similarity values measured between them, thus underestimating the similarity scores.To solve this issue, we propose a method to discount the ℓ2 norm of a contextualised word embedding by the frequency of that word in a corpus when measuring the cosine similarities between words.We show that the so called stop words behave differently from the rest of the words, which require special consideration during their discounting process.Experimental results on a contextualised word similarity dataset show that our proposed discounting method accurately solves the similarity underestimation problem.An anonymized version of the source code of our proposed method is submitted to the reviewing system.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.550.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Cosine similarity between two words, computed using their contextualised token embeddings obtained from masked language models (MLMs) such as BERT has shown to underestimate the actual similarity between those words... This similarity underestimation problem is particularly severe for high frequent words.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Cosine similarity between two words, computed using their contextualised token embeddings obtained from masked language models (MLMs) such as BERT has shown to underestimate the actual similarity between those words... This similarity underestimation problem is particularly severe for high frequent words.\""
    },
    {
        "title": "Do Large Language Models Know What They Don’t Know?",
        "authors": [
            "Zhangyue Yin",
            "Qiushi Sun",
            "Qipeng Guo",
            "Jiawen Wu",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have a wealth of knowledge that allows them to excel in various Natural Language Processing (NLP) tasks. Current research focuses on enhancing their performance within their existing knowledge. Despite their vast knowledge, LLMs are still limited by the amount of information they can accommodate and comprehend. Therefore, the ability to understand their own limitations on the unknows, referred to as self-knowledge, is of paramount importance. This study aims to evaluate LLMs’ self-knowledge by assessing their ability to identify unanswerable or unknowable questions. We introduce an automated methodology to detect uncertainty in the responses of these models, providing a novel measure of their self-knowledge. We further introduce a unique dataset, SelfAware, consisting of unanswerable questions from five diverse categories and their answerable counterparts. Our extensive analysis, involving 20 LLMs including GPT-3, InstructGPT, and LLaMA, discovering an intrinsic capacity for self-knowledge within these models. Moreover, we demonstrate that in-context learning and instruction tuning can further enhance this self-knowledge. Despite this promising insight, our findings also highlight a considerable gap between the capabilities of these models and human proficiency in recognizing the limits of their knowledge.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.551.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "5",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite their vast knowledge, LLMs are still limited by the amount of information they can accommodate and comprehend.\"; \"our findings also highlight a considerable gap between the capabilities of these models and human proficiency in recognizing the limits of their knowledge.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 5\nEvidence: \"Despite their vast knowledge, LLMs are still limited by the amount of information they can accommodate and comprehend.\"; \"our findings also highlight a considerable gap between the capabilities of these models and human proficiency in recognizing the limits of their knowledge.\""
    },
    {
        "title": "AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities",
        "authors": [
            "Zhongzhi Chen",
            "Guang Liu",
            "Bo-Wen Zhang",
            "Qinghong Yang",
            "Ledell Wu"
        ],
        "published": "2023",
        "summary": "CLIP (Contrastive Language–Image Pretraining) is an English multimodal representation model learned from a massive amount of English text-image pairs and has achieved great success in various downstream tasks, including image classification, text-to-image retrieval, and image generation. When extending CLIP to other languages, the major problem is the lack of good-quality text-image pairs. In this work, we present AltCLIP, a simple and low-resource method to build a strong multilingual multimodal representation model. Instead of training a model from scratch on multilingual text-image pairs, we take the original CLIP model trained on English text-image pairs and alter its text encoder with a pre-trained multilingual text encoder (XLM-R). We then align text and image representations by a two-stage training schema consisting of teacher learning and contrastive learning. Our method utilizes the existence of rich parallel text data and pre-trained multilingual language models. We present extensive experimental evaluations to demonstrate the effectiveness of our proposed method. Our model sets new state-of-the-art zero-shot performances on a wide range of tasks in multilingual multimodal benchmarks, including ImageNet-CN/IT/JA/KO serials, Flicker30k-CN, COCO-CN, Multi30k, and XTD. Further, our model outperforms the original CLIP model on zero-shot cross-modal retrieval, Image Classification in the Wild (ICinW) tasks, and CLIP Benchmark. We plan to open-source our code, pre-trained model weights, and evaluation toolkits of multilingual multimodal tasks, to facilitate research on multilingual multimodal representation learning.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.552.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"When extending CLIP to other languages, the major problem is the lack of good-quality text-image pairs.\"\n\nThis paper discusses LLMs but does not explicitly mention any limitations of LLMs, except for the challenge of extending the model to other languages due to the lack of good-quality text-image pairs, which is a minor detail and not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"When extending CLIP to other languages, the major problem is the lack of good-quality text-image pairs.\"\n\nThis paper discusses LLMs but does not explicitly mention any limitations of LLMs, except for the challenge of extending the model to other languages due to the lack of good-quality text-image pairs, which is a minor detail and not the primary focus of the paper."
    },
    {
        "title": "Feature Interactions Reveal Linguistic Structure in Language Models",
        "authors": [
            "Jaap Jumelet",
            "Willem Zuidema"
        ],
        "published": "2023",
        "summary": "We study feature interactions in the context of feature attribution methods for post-hoc interpretability. In interpretability research, getting to grips with feature interactions is increasingly recognised as an important challenge, because interacting features are key to the success of neural networks. Feature interactions allow a model to build up hierarchical representations for its input, and might provide an ideal starting point for the investigation into linguistic structure in language models. However, uncovering the exact role that these interactions play is also difficult, and a diverse range of interaction attribution methods has been proposed. In this paper, we focus on the question which of these methods most faithfully reflects the inner workings of the target models. We work out a grey box methodology, in which we train models to perfection on a formal language classification task, using PCFGs. We show that under specific configurations, some methods are indeed able to uncover the grammatical rules acquired by a model. Based on these findings we extend our evaluation to a case study on language models, providing novel insights into the linguistic structure that these models have acquired.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.554.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, uncovering the exact role that these interactions play is also difficult\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, uncovering the exact role that these interactions play is also difficult\""
    },
    {
        "title": "MVP: Multi-task Supervised Pre-training for Natural Language Generation",
        "authors": [
            "Tianyi Tang",
            "Junyi Li",
            "Wayne Xin Zhao",
            "Ji-Rong Wen"
        ],
        "published": "2023",
        "summary": "Pre-trained language models (PLMs) have achieved remarkable success in natural language generation (NLG) tasks. Up to now, most NLG-oriented PLMs are pre-trained in an unsupervised manner using the large-scale general corpus. In the meanwhile, an increasing number of models pre-trained with labeled data (i.e. “supervised pre-training”) showcase superior performance compared to unsupervised pre-trained models. Motivated by the success of supervised pre-training, we propose Multi-task superVised Pre-training (MVP) for natural language generation. We collect a large-scale natural language generation corpus, MVPCorpus, from 77 datasets over 11 diverse NLG tasks. Then we unify these examples into a general text-to-text format to pre-train the text generation model MVP in a supervised manner. For each task, we further pre-train specific soft prompts to stimulate the model’s capacity to perform a specific task. Our MVP model can be seen as a practice that utilizes recent instruction tuning on relatively small PLMs. Extensive experiments have demonstrated the effectiveness and generality of our MVP model in a number of NLG tasks, which achieves state-of-the-art performance on 13 out of 17 datasets, outperforming BART by 9.3% and Flan-T5 by 5.8%.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.558.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations, but the motivation for the proposed method is to improve upon existing pre-trained language models (PLMs).",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations, but the motivation for the proposed method is to improve upon existing pre-trained language models (PLMs)."
    },
    {
        "title": "From Alignment to Entailment: A Unified Textual Entailment Framework for Entity Alignment",
        "authors": [
            "Yu Zhao",
            "Yike Wu",
            "Xiangrui Cai",
            "Ying Zhang",
            "Haiwei Zhang",
            "Xiaojie Yuan"
        ],
        "published": "2023",
        "summary": "Entity Alignment (EA) aims to find the equivalent entities between two Knowledge Graphs (KGs). Existing methods usually encode the triples of entities as embeddings and learn to align the embeddings, which prevents the direct interaction between the original information of the cross-KG entities. Moreover, they encode the relational triples and attribute triples of an entity in heterogeneous embedding spaces, which prevents them from helping each other. In this paper, we transform both triples into unified textual sequences, and model the EA task as a bi-directional textual entailment task between the sequences of cross-KG entities. Specifically, we feed the sequences of two entities simultaneously into a pre-trained language model (PLM) and propose two kinds of PLM-based entity aligners that model the entailment probability between sequences as the similarity between entities. Our approach captures the unified correlation pattern of two kinds of information between entities, and explicitly models the fine-grained interaction between original entity information. The experiments on five cross-lingual EA datasets show that our approach outperforms the state-of-the-art EA methods and enables the mutual enhancement of the heterogeneous information. Codes are available at https://github.com/OreOZhao/TEA.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.559.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing methods usually encode the triples of entities as embeddings and learn to align the embeddings, which prevents the direct interaction between the original information of the cross-KG entities.\"\n\nHowever, the limitation mentioned is not directly related to LLMs, but rather to existing methods in entity alignment. The paper does mention the use of a pre-trained language model (PLM) but does not discuss",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Existing methods usually encode the triples of entities as embeddings and learn to align the embeddings, which prevents the direct interaction between the original information of the cross-KG entities.\"\n\nHowever, the limitation mentioned is not directly related to LLMs, but rather to existing methods in entity alignment. The paper does mention the use of a pre-trained language model (PLM) but does not discuss"
    },
    {
        "title": "Defending against Insertion-based Textual Backdoor Attacks via Attribution",
        "authors": [
            "Jiazhao Li",
            "Zhuofeng Wu",
            "Wei Ping",
            "Chaowei Xiao",
            "V.G.Vinod Vydiswaran"
        ],
        "published": "2023",
        "summary": "Textual backdoor attack, as a novel attack model, has been shown to be effective in adding a backdoor to the model during training. Defending against such backdoor attacks has become urgent and important. In this paper, we propose AttDef, an efficient attribution-based pipeline to defend against two insertion-based poisoning attacks, BadNL and InSent. Specifically, we regard the tokens with larger attribution scores as potential triggers since larger attribution words contribute more to the false prediction results and therefore are more likely to be poison triggers. Additionally, we further utilize an external pre-trained language model to distinguish whether input is poisoned or not. We show that our proposed method can generalize sufficiently well in two common attack scenarios (poisoning training data and testing data), which consistently improves previous methods. For instance, AttDef can successfully mitigate both attacks with an average accuracy of 79.97% (56.59% up) and 48.34% (3.99% up) under pre-training and post-training attack defense respectively, achieving the new state-of-the-art performance on prediction recovery over four benchmark datasets.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.561.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Textual backdoor attack, as a novel attack model, has been shown to be effective in adding a backdoor to the model during training.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Textual backdoor attack, as a novel attack model, has been shown to be effective in adding a backdoor to the model during training.\""
    },
    {
        "title": "In-context Examples Selection for Machine Translation",
        "authors": [
            "Sweta Agrawal",
            "Chunting Zhou",
            "Mike Lewis",
            "Luke Zettlemoyer",
            "Marjan Ghazvininejad"
        ],
        "published": "2023",
        "summary": "Large-scale generative models show an impressive ability to perform a wide range of Natural Language Processing (NLP) tasks using in-context learning, where a few examples are used to describe a task to the model. For Machine Translation (MT), these examples are typically randomly sampled from the development dataset with a similar distribution as the evaluation set. However, it is unclear how the choice of these in context examples and their ordering impacts the output translation quality. In this work, we aim to understand the properties of good in-context examples for MT in both in-domain and out-of-domain settings. We show that the translation quality and the domain of the in-context examples matter and that 1-shot noisy unrelated examples can have a catastrophic impact on output quality. While concatenating multiple random examples reduces the effect of noise, a single good prompt optimized to maximize translation quality on the development dataset can elicit learned information from the pre-trained language model. Adding similar examples based on an n-gram overlap with the test source significantly and consistently improves the translation quality of the outputs, outperforming a strong kNN-MT baseline in 2 out of 4 out-of-domain datasets.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.564.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it is unclear how the choice of these in context examples and their ordering impacts the output translation quality.\"; \"1-shot noisy unrelated examples can have a catastrophic impact on output quality.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, it is unclear how the choice of these in context examples and their ordering impacts the output translation quality.\"; \"1-shot noisy unrelated examples can have a catastrophic impact on output quality.\""
    },
    {
        "title": "PropSegmEnt: A Large-Scale Corpus for Proposition-Level Segmentation and Entailment Recognition",
        "authors": [
            "Sihao Chen",
            "Senaka Buthpitiya",
            "Alex Fabrikant",
            "Dan Roth",
            "Tal Schuster"
        ],
        "published": "2023",
        "summary": "The widely studied task of Natural Language Inference (NLI) requires a system to recognize whether one piece of text is textually entailed by another, i.e. whether the entirety of its meaning can be inferred from the other. In current NLI datasets and models, textual entailment relations are typically defined on the sentence- or paragraph-level. However, even a simple sentence often contains multiple propositions, i.e. distinct units of meaning conveyed by the sentence. As these propositions can carry different truth values in the context of a given premise, we argue for the need to recognize the textual entailment relation of each proposition in a sentence individually. We propose PropSegmEnt, a corpus of over 45K propositions annotated by expert human raters. Our dataset structure resembles the tasks of (1) segmenting sentences within a document to the set of propositions, and (2) classifying the entailment relation of each proposition with respect to a different yet topically-aligned document, i.e. documents describing the same event or entity. We establish strong baselines for the segmentation and entailment tasks. Through case studies on summary hallucination detection and document-level NLI, we demonstrate that our conceptual framework is potentially useful for understanding and explaining the compositionality of NLI labels.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.565.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "CIF-PT: Bridging Speech and Text Representations for Spoken Language Understanding via Continuous Integrate-and-Fire Pre-Training",
        "authors": [
            "Linhao Dong",
            "Zhecheng An",
            "Peihao Wu",
            "Jun Zhang",
            "Lu Lu",
            "Ma Zejun"
        ],
        "published": "2023",
        "summary": "Speech or text representation generated by pre-trained models contains modal-specific information that could be combined for benefiting spoken language understanding (SLU) tasks. In this work, we propose a novel pre-training paradigm termed Continuous Integrate-and-Fire Pre-Training (CIF-PT). It relies on a simple but effective frame-to-token alignment: continuous integrate-and-fire (CIF) to bridge the representations between speech and text. It jointly performs speech-to-text training and language model distillation through CIF as the pre-training (PT). Evaluated on SLU benchmark SLURP dataset, CIF-PT outperforms the state-of-the-art model by 1.94% of accuracy and 2.71% of SLU-F1 on the tasks of intent classification and slot filling, respectively. We also observe the cross-modal representation extracted by CIF-PT obtains better performance than other neural interfaces for the tasks of SLU, including the dominant speech representation learned from self-supervised pre-training.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.566.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "What to Fuse and How to Fuse: Exploring Emotion and Personality Fusion Strategies for Explainable Mental Disorder Detection",
        "authors": [
            "Sourabh Zanwar",
            "Xiaofei Li",
            "Daniel Wiechmann",
            "Yu Qiao",
            "Elma Kerz"
        ],
        "published": "2023",
        "summary": "Mental health disorders (MHD) are increasingly prevalent worldwide and constitute one of the greatest challenges facing our healthcare systems and modern societies in general. In response to this societal challenge, there has been a surge in digital mental health research geared towards the development of new techniques for unobtrusive and efficient automatic detection of MHD. Within this area of research, natural language processing techniques are playing an increasingly important role, showing promising detection results from a variety of textual data. Recently, there has been a growing interest in improving mental illness detection from textual data by way of leveraging emotions: ‘Emotion fusion’ refers to the process of integrating emotion information with general textual information to obtain enhanced information for decision-making. However, while the available research has shown that MHD prediction can be improved through a variety of different fusion strategies, previous works have been confined to a particular fusion strategy applied to a specific dataset, and so is limited by the lack of meaningful comparability. In this work, we integrate and extend this research by conducting extensive experiments with three types of deep learning-based fusion strategies: (i) feature-level fusion, where a pre-trained masked language model for mental health detection (MentalRoBERTa) was infused with a comprehensive set of engineered features, (ii) model fusion, where the MentalRoBERTa model was infused with hidden representations of other language models and (iii) task fusion, where a multi-task framework was leveraged to learn the features for auxiliary tasks. In addition to exploring the role of different fusion strategies, we expand on previous work by broadening the information infusion to include a second domain related to mental health, namely personality. We evaluate algorithm performance on data from two benchmark datasets, encompassing five mental health conditions: attention deficit hyperactivity disorder, anxiety, bipolar disorder, depression and psychological stress.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.568.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"a pre-trained masked language model for mental health detection (MentalRoBERTa)\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"a pre-trained masked language model for mental health detection (MentalRoBERTa)\""
    },
    {
        "title": "Pruning Pre-trained Language Models with Principled Importance and Self-regularization",
        "authors": [
            "Siyu Ren",
            "Kenny Zhu"
        ],
        "published": "2023",
        "summary": "Iterative pruning is one of the most effective compression methods for pre-trained language models. We discovered that finding the optimal pruning decision is an equality-constrained 0-1 Integer Linear Programming problem. The solution to this optimization problem leads to a principled importance criterion which we use to rank parameters during iterative model pruning. To mitigate the poor generalization at high sparsity levels, we propose a self-regularization scheme where model prediction is regularized by the latest checkpoint with increasing sparsity throughout pruning. Our experiments on natural language understanding, question answering, named entity recognition, and data-to-text generation with various Transformer-based PLMs show the effectiveness of the approach at various sparsity levels.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.573.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"To mitigate the poor generalization at high sparsity levels\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"To mitigate the poor generalization at high sparsity levels\""
    },
    {
        "title": "The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code",
        "authors": [
            "Xiao Liu",
            "Da Yin",
            "Chen Zhang",
            "Yansong Feng",
            "Dongyan Zhao"
        ],
        "published": "2023",
        "summary": "Causal reasoning, the ability to identify cause-and-effect relationship, is crucial in human thinking. Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning. Given the fact that programming code may express causal relations more often and explicitly with conditional statements like “if“, we want to explore whether Code-LLMs acquire better causal reasoning abilities. Our experiments show that compared to text-only LLMs, Code-LLMs with code prompts are better causal reasoners. We further intervene on the prompts from different aspects, and discover that the key point is the programming structure. Code and data are available at https://github.com/xxxiaol/magic-if.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.574.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning.\""
    },
    {
        "title": "World Models for Math Story Problems",
        "authors": [
            "Andreas Opedal",
            "Niklas Stoehr",
            "Abulhair Saparov",
            "Mrinmaya Sachan"
        ],
        "published": "2023",
        "summary": "Solving math story problems is a complex task for students and NLP models alike, requiring them to understand the world as described in the story and reason over it to compute an answer. Recent years have seen impressive performance on automatically solving these problems with large pre-trained language models and innovative techniques to prompt them. However, it remains unclear if these models possess accurate representations of mathematical concepts. This leads to lack of interpretability and trustworthiness which impedes their usefulness in various applications. In this paper, we consolidate previous work on categorizing and representing math story problems and develop MathWorld, which is a graph-based semantic formalism specific for the domain of math story problems. With MathWorld, we can assign world models to math story problems which represent the situations and actions introduced in the text and their mathematical relationships. We combine math story problems from several existing datasets and annotate a corpus of 1,019 problems and 3,204 logical forms with MathWorld. Using this data, we demonstrate the following use cases of MathWorld: (1) prompting language models with synthetically generated question-answer pairs to probe their reasoning and world modeling abilities, and (2) generating new problems by using the world models as a design space.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.579.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it remains unclear if these models possess accurate representations of mathematical concepts. This leads to lack of interpretability and trustworthiness which impedes their usefulness in various applications.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, it remains unclear if these models possess accurate representations of mathematical concepts. This leads to lack of interpretability and trustworthiness which impedes their usefulness in various applications.\""
    },
    {
        "title": "Language Agnostic Multilingual Information Retrieval with Contrastive Learning",
        "authors": [
            "Xiyang Hu",
            "Xinchi Chen",
            "Peng Qi",
            "Deguang Kong",
            "Kunlun Liu",
            "William Yang Wang",
            "Zhiheng Huang"
        ],
        "published": "2023",
        "summary": "Multilingual information retrieval (IR) is challenging since annotated training data is costly to obtain in many languages. We present an effective method to train multilingual IR systems when only English IR training data and some parallel corpora between English and other languages are available. We leverage parallel and non-parallel corpora to improve the pretrained multilingual language models’ cross-lingual transfer ability. We design a semantic contrastive loss to align representations of parallel sentences that share the same semantics in different languages, and a new language contrastive loss to leverage parallel sentence pairs to remove language-specific information in sentence representations from non-parallel corpora. When trained on English IR data with these losses and evaluated zero-shot on non-English data, our model demonstrates significant improvement to prior work on retrieval performance, while it requires much less computational effort. We also demonstrate the value of our model for a practical setting when a parallel corpus is only available for a few languages, but a lack of parallel corpora resources persists for many other low-resource languages. Our model can work well even with a small number of parallel sentences, and be used as an add-on module to any backbones and other tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.581.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We leverage parallel and non-parallel corpora to improve the pretrained multilingual language models’ cross-lingual transfer ability.\"\n\nThis paper discusses LLMs but does not mention any explicit limitation of the models in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"We leverage parallel and non-parallel corpora to improve the pretrained multilingual language models’ cross-lingual transfer ability.\"\n\nThis paper discusses LLMs but does not mention any explicit limitation of the models in the abstract."
    },
    {
        "title": "Investigating Transformer-Guided Chaining for Interpretable Natural Logic Reasoning",
        "authors": [
            "Kanagasabai Rajaraman",
            "Saravanan Rajamanickam",
            "Wei Shi"
        ],
        "published": "2023",
        "summary": "Natural logic reasoning has received increasing attention lately, with several datasets and neural models proposed, though with limited success. More recently, a new class of works have emerged adopting a Neuro-Symbolic approach, called transformer guided chaining, whereby the idea is to iteratively perform 1-step neural inferences and chain together the results to generate a multi-step reasoning trace. Several works have adapted variants of this central idea and reported significantly high accuracies compared to vanilla LLM’s. In this paper, we perform a critical empirical investigation of the chaining approach on a multi-hop First-Order Logic (FOL) reasoning benchmark. In particular, we develop a reference implementation, called Chainformer, and conduct several experiments to analyze the accuracy, generalization, interpretability, and performance over FOLs. Our findings highlight key strengths and possible current limitations and suggest potential areas for future research in logic reasoning.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.588.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Several works have adapted variants of this central idea and reported significantly high accuracies compared to vanilla LLM’s\"; \"Our findings highlight key strengths and possible current limitations\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Several works have adapted variants of this central idea and reported significantly high accuracies compared to vanilla LLM’s\"; \"Our findings highlight key strengths and possible current limitations\""
    },
    {
        "title": "Zero-shot Visual Question Answering with Language Model Feedback",
        "authors": [
            "Yifan Du",
            "Junyi Li",
            "Tianyi Tang",
            "Wayne Xin Zhao",
            "Ji-Rong Wen"
        ],
        "published": "2023",
        "summary": "In this paper, we propose a novel language model guided captioning approach, LAMOC, for knowledge-based visual question answering (VQA). Our approach employs the generated captions by a captioning model as the context of an answer prediction model, which is a Pre-Trained Language model (PLM). As the major contribution, we leverage the guidance and feedback of the prediction model to improve the capability of the captioning model. In this way, the captioning model can become aware of the task goal and information need from the PLM. To develop our approach, we design two specific training stages, where the first stage adapts the captioning model to the prediction model (selecting more suitable caption propositions for training) and the second stage tunes the captioning model according to the task goal (learning from feedback of the PLM). Extensive experiments demonstrate the effectiveness of the proposed approach on the knowledge-based VQA task. Specifically, on the challenging A-OKVQA dataset, LAMOC outperforms several competitive zero-shot methods and even achieves comparable results to a fine-tuned VLP model. Our code is publicly available at https://github.com/RUCAIBox/LAMOC.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.590.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation of LLMs is mentioned in the abstract, but it does discuss the use of a pre-trained language model (PLM) and its guidance and feedback to improve the capability of the captioning model, implying that the PLM has limitations that need to be addressed.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation of LLMs is mentioned in the abstract, but it does discuss the use of a pre-trained language model (PLM) and its guidance and feedback to improve the capability of the captioning model, implying that the PLM has limitations that need to be addressed."
    },
    {
        "title": "Prompted Opinion Summarization with GPT-3.5",
        "authors": [
            "Adithya Bhaskar",
            "Alex Fabbri",
            "Greg Durrett"
        ],
        "published": "2023",
        "summary": "Large language models have shown impressive performance across a wide variety of tasks, including text summarization. In this paper, we show that this strong performance extends to opinion summarization. We explore several pipeline methods for applying GPT-3.5 to summarize a large collection of user reviews in aprompted fashion. To handle arbitrarily large numbers of user reviews, we explore recursive summarization as well as methods for selecting salient content to summarize through supervised clustering or extraction. On two datasets, an aspect-oriented summarization dataset of hotel reviews (SPACE) and a generic summarization dataset of Amazon and Yelp reviews (FewSum), we show that GPT-3.5 models achieve very strong performance in human evaluation. We argue that standard evaluation metrics do not reflect this, and introduce three new metrics targeting faithfulness, factuality, and genericity to contrast these different methods.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.591.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions that \"standard evaluation metrics do not reflect\" the performance of LLMs, implying a limitation in evaluating LLMs, however this is not the main focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions that \"standard evaluation metrics do not reflect\" the performance of LLMs, implying a limitation in evaluating LLMs, however this is not the main focus of the paper."
    },
    {
        "title": "GUMSum: Multi-Genre Data and Evaluation for English Abstractive Summarization",
        "authors": [
            "Yang Janet Liu",
            "Amir Zeldes"
        ],
        "published": "2023",
        "summary": "Automatic summarization with pre-trained language models has led to impressively fluent results, but is prone to ‘hallucinations’, low performance on non-news genres, and outputs which are not exactly summaries. Targeting ACL 2023’s ‘Reality Check’ theme, we present GUMSum, a small but carefully crafted dataset of English summaries in 12 written and spoken genres for evaluation of abstractive summarization. Summaries are highly constrained, focusing on substitutive potential, factuality, and faithfulness. We present guidelines and evaluate human agreement as well as subjective judgments on recent system outputs, comparing general-domain untuned approaches, a fine-tuned one, and a prompt-based approach, to human performance. Results show that while GPT3 achieves impressive scores, it still underperforms humans, with varying quality across genres. Human judgments reveal different types of errors in supervised, prompted, and human-generated summaries, shedding light on the challenges of producing a good summary.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.593.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"but is prone to ‘hallucinations’, low performance on non-news genres, and outputs which are not exactly summaries\"; \"Results show that while GPT3 achieves impressive scores, it still underperforms humans, with varying quality across genres.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"but is prone to ‘hallucinations’, low performance on non-news genres, and outputs which are not exactly summaries\"; \"Results show that while GPT3 achieves impressive scores, it still underperforms humans, with varying quality across genres.\""
    },
    {
        "title": "Teaching the Pre-trained Model to Generate Simple Texts for Text Simplification",
        "authors": [
            "Renliang Sun",
            "Wei Xu",
            "Xiaojun Wan"
        ],
        "published": "2023",
        "summary": "Randomly masking text spans in ordinary texts in the pre-training stage hardly allows models to acquire the ability to generate simple texts. It can hurt the performance of pre-trained models on text simplification tasks. In this paper, we propose a new continued pre-training strategy to teach the pre-trained model to generate simple texts. We continue pre-training BART, a representative model, to obtain SimpleBART. It consistently and significantly improves the results on lexical simplification, sentence simplification, and document-level simplification tasks over BART. At the end, we compare SimpleBART with several representative large language models (LLMs).",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.595.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Randomly masking text spans in ordinary texts in the pre-training stage hardly allows models to acquire the ability to generate simple texts. It can hurt the performance of pre-trained models on text simplification tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Randomly masking text spans in ordinary texts in the pre-training stage hardly allows models to acquire the ability to generate simple texts. It can hurt the performance of pre-trained models on text simplification tasks.\""
    },
    {
        "title": "Acquiring Frame Element Knowledge with Deep Metric Learning for Semantic Frame Induction",
        "authors": [
            "Kosuke Yamada",
            "Ryohei Sasano",
            "Koichi Takeda"
        ],
        "published": "2023",
        "summary": "The semantic frame induction tasks are defined as a clustering of words into the frames that they evoke, and a clustering of their arguments according to the frame element roles that they should fill. In this paper, we address the latter task of argument clustering, which aims to acquire frame element knowledge, and propose a method that applies deep metric learning. In this method, a pre-trained language model is fine-tuned to be suitable for distinguishing frame element roles through the use of frame-annotated data, and argument clustering is performed with embeddings obtained from the fine-tuned model. Experimental results on FrameNet demonstrate that our method achieves substantially better performance than existing methods.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.596.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"a pre-trained language model is fine-tuned to be suitable for distinguishing frame element roles through the use of frame-annotated data\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"a pre-trained language model is fine-tuned to be suitable for distinguishing frame element roles through the use of frame-annotated data\""
    },
    {
        "title": "Spontaneous gestures encoded by hand positions improve language models: An Information-Theoretic motivated study",
        "authors": [
            "Yang Xu",
            "Yang Cheng"
        ],
        "published": "2023",
        "summary": "The multi-modality nature of human communication has been utilized to enhance the performance of language modeling-related tasks. Driven by the development of large-scale end-to-end learning techniques and the availability of multi-modal data, it becomes possible to represent non-verbal communication behaviors through joint-learning, and directly study their interaction with verbal communication. However, there is still gaps in existing studies to better address the underlying mechanism of how non-verbal expression contributes to the overall communication purpose. Therefore, we explore two questions using mixed-modal language models trained against monologue video data: first, whether incorporating gesture representations can improve the language model’s performance (perplexity); second, whether spontaneous gestures demonstrate entropy rate constancy (ERC), which is an empirical pattern found in most verbal language data that supports the rational communication assumption from Information Theory. We have positive and interesting findings for both questions: speakers indeed use spontaneous gestures to convey “meaningful” information that enhances verbal communication, which can be captured with a simple spatial encoding scheme. More importantly, gestures are produced and organized rationally in a similar way as words, which optimizes the communication efficiency.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.600.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicitly mentioned, but the paper implies that existing studies have gaps in understanding how non-verbal expression contributes to overall communication purpose, which can be seen as a limitation in the field of multimodal language models.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None explicitly mentioned, but the paper implies that existing studies have gaps in understanding how non-verbal expression contributes to overall communication purpose, which can be seen as a limitation in the field of multimodal language models."
    },
    {
        "title": "Progressive Translation: Improving Domain Robustness of Neural Machine Translation with Intermediate Sequences",
        "authors": [
            "Chaojun Wang",
            "Yang Liu",
            "Wai Lam"
        ],
        "published": "2023",
        "summary": "Previous studies show that intermediate supervision signals benefit various Natural Language Processing tasks. However, it is not clear whether there exist intermediate signals that benefit Neural Machine Translation (NMT). Borrowing techniques from Statistical Machine Translation, we propose intermediate signals which are intermediate sequences from the “source-like” structure to the “target-like” structure. Such intermediate sequences introduce an inductive bias that reflects a domain-agnostic principle of translation, which reduces spurious correlations that are harmful to out-of-domain generalisation. Furthermore, we introduce a full-permutation multi-task learning to alleviate the spurious causal relations from intermediate sequences to the target, which results from exposure bias. The Minimum Bayes Risk decoding algorithm is used to pick the best candidate translation from all permutations to further improve the performance. Experiments show that the introduced intermediate signals can effectively improve the domain robustness of NMT and reduces the amount of hallucinations on out-of-domain translation. Further analysis shows that our methods are especially promising in low-resource scenarios.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.601.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Controlled Text Generation with Hidden Representation Transformations",
        "authors": [
            "Vaibhav Kumar",
            "Hana Koorehdavoudi",
            "Masud Moshtaghi",
            "Amita Misra",
            "Ankit Chadha",
            "Emilio Ferrara"
        ],
        "published": "2023",
        "summary": "We propose CHRT (Control HiddenRepresentation Transformation) – a con-trolled language generation framework thatsteers large language models to generatetext pertaining to certain attributes (such astoxicity). CHRT gains attribute control bymodifying the hidden representation of thebase model through learned transformations. We employ a contrastive-learning frameworkto learn these transformations that can becombined to gain multi-attribute control. Theeffectiveness of CHRT is experimentallyshown by comparing it with seven baselinesover three attributes. CHRT outperforms all thebaselines in the task of detoxification, positivesentiment steering, and text simplificationwhile minimizing the loss in linguistic qualities. Further, our approach has the lowest inferencelatency of only 0.01 seconds more than thebase model, making it the most suitable forhigh-performance production environments. We open-source our code and release two noveldatasets to further propel controlled languagegeneration research",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.602.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations, but the paper aims to address the lack of control in large language models through the proposed framework.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations, but the paper aims to address the lack of control in large language models through the proposed framework."
    },
    {
        "title": "Large Language Models Are Partially Primed in Pronoun Interpretation",
        "authors": [
            "Suet-Ying Lam",
            "Qingcheng Zeng",
            "Kexun Zhang",
            "Chenyu You",
            "Rob Voigt"
        ],
        "published": "2023",
        "summary": "While a large body of literature suggests that large language models (LLMs) acquire rich linguistic representations, little is known about whether they adapt to linguistic biases in a human-like way. The present study probes this question by asking whether LLMs display human-like referential biases using stimuli and procedures from real psycholinguistic experiments. Recent psycholinguistic studies suggest that humans adapt their referential biases with recent exposure to referential patterns; closely replicating three relevant psycholinguistic experiments from Johnson & Arnold (2022) in an in-context learning (ICL) framework, we found that InstructGPT adapts its pronominal interpretations in response to the frequency of referential patterns in the local discourse, though in a limited fashion: adaptation was only observed relative to syntactic but not semantic biases. By contrast, FLAN-UL2 fails to generate meaningful patterns. Our results provide further evidence that contemporary LLMs discourse representations are sensitive to syntactic patterns in the local context but less so to semantic patterns. Our data and code are available at https://github.com/zkx06111/llm_priming.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.605.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"though in a limited fashion: adaptation was only observed relative to syntactic but not semantic biases. By contrast, FLAN-UL2 fails to generate meaningful patterns. Our results provide further evidence that contemporary LLMs discourse representations are sensitive to syntactic patterns in the local context but less so to semantic patterns.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"though in a limited fashion: adaptation was only observed relative to syntactic but not semantic biases. By contrast, FLAN-UL2 fails to generate meaningful patterns. Our results provide further evidence that contemporary LLMs discourse representations are sensitive to syntactic patterns in the local context but less so to semantic patterns.\""
    },
    {
        "title": "ORCA: A Challenging Benchmark for Arabic Language Understanding",
        "authors": [
            "AbdelRahim Elmadany",
            "ElMoatez Billah Nagoudi",
            "Muhammad Abdul-Mageed"
        ],
        "published": "2023",
        "summary": "Due to the crucial role pretrained language models play in modern NLP, several benchmarks have been proposed to evaluate their performance. In spite of these efforts, no public benchmark of diverse nature currently exists for evaluating Arabic NLU. This makes it challenging to measure progress for both Arabic and multilingual language models. This challenge is compounded by the fact that any benchmark targeting Arabic needs to take into account the fact that Arabic is not a single language but rather a collection of languages and language varieties. In this work, we introduce a publicly available benchmark for Arabic language understanding evaluation dubbed ORCA. It is carefully constructed to cover diverse Arabic varieties and a wide range of challenging Arabic understanding tasks exploiting 60 different datasets (across seven NLU task clusters). To measure current progress in Arabic NLU, we use ORCA to offer a comprehensive comparison between 18 multilingual and Arabic language models. We also provide a public leaderboard with a unified single-number evaluation metric (ORCA score) to facilitate future research.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.609.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This challenge is compounded by the fact that any benchmark targeting Arabic needs to take into account the fact that Arabic is not a single language but rather a collection of languages and language varieties.\"\n\nThis evidence is brief and mentions a limitation of LLMs in the context of Arabic language understanding, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"This challenge is compounded by the fact that any benchmark targeting Arabic needs to take into account the fact that Arabic is not a single language but rather a collection of languages and language varieties.\"\n\nThis evidence is brief and mentions a limitation of LLMs in the context of Arabic language understanding, but it is not the primary focus of the paper."
    },
    {
        "title": "Are Intermediate Layers and Labels Really Necessary? A General Language Model Distillation Method",
        "authors": [
            "Shicheng Tan",
            "Weng Lam Tam",
            "Yuanchun Wang",
            "Wenwen Gong",
            "Shu Zhao",
            "Peng Zhang",
            "Jie Tang"
        ],
        "published": "2023",
        "summary": "The large scale of pre-trained language models poses a challenge for their deployment on various devices, with a growing emphasis on methods to compress these models, particularly knowledge distillation. However, current knowledge distillation methods rely on the model’s intermediate layer features and the golden labels (also called hard labels), which usually require aligned model architecture and enough labeled data respectively. Moreover, the parameters of vocabulary are usually neglected in existing methods. To address these problems, we propose a general language model distillation (GLMD) method that performs two-stage word prediction distillation and vocabulary compression, which is simple and surprisingly shows extremely strong performance. Specifically, GLMD supports more general application scenarios by eliminating the constraints of dimension and structure between models and the need for labeled datasets through the absence of intermediate layers and golden labels. Meanwhile, based on the long-tailed distribution of word frequencies in the data, GLMD designs a strategy of vocabulary compression through decreasing vocabulary size instead of dimensionality. Experimental results show that our method outperforms 25 state-of-the-art methods on the SuperGLUE benchmark, achieving an average score that surpasses the best method by 3%.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.614.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, current knowledge distillation methods rely on the model’s intermediate layer features and the golden labels (also called hard labels), which usually require aligned model architecture and enough labeled data respectively.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, current knowledge distillation methods rely on the model’s intermediate layer features and the golden labels (also called hard labels), which usually require aligned model architecture and enough labeled data respectively.\""
    },
    {
        "title": "Diable: Efficient Dialogue State Tracking as Operations on Tables",
        "authors": [
            "Pietro Lesci",
            "Yoshinari Fujinuma",
            "Momchil Hardalov",
            "Chao Shang",
            "Yassine Benajiba",
            "Lluis Marquez"
        ],
        "published": "2023",
        "summary": "Sequence-to-sequence state-of-the-art systems for dialogue state tracking (DST) use the full dialogue history as input, represent the current state as a list with all the slots, and generate the entire state from scratch at each dialogue turn. This approach is inefficient, especially when the number of slots is large and the conversation is long. We propose Diable, a new task formalisation that simplifies the design and implementation of efficient DST systems and allows one to easily plug and play large language models. We represent the dialogue state as a table and formalise DST as a table manipulation task. At each turn, the system updates the previous state by generating table operations based on the dialogue context. Extensive experimentation on the MultiWoz datasets demonstrates that Diable (i) outperforms strong efficient DST baselines, (ii) is 2.4x more time efficient than current state-of-the-art methods while retaining competitive Joint Goal Accuracy, and (iii) is robust to noisy data annotations due to the table operations approach.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.615.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"allows one to easily plug and play large language models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"allows one to easily plug and play large language models.\""
    },
    {
        "title": "Mapping Brains with Language Models: A Survey",
        "authors": [
            "Antonia Karamolegkou",
            "Mostafa Abdou",
            "Anders Søgaard"
        ],
        "published": "2023",
        "summary": "Over the years, many researchers have seemingly made the same observation: Brain and language model activations exhibit some structural similarities, enabling linear partial mappings between features extracted from neural recordings and computational language models. In an attempt to evaluate how much evidence has been accumulated for this observation, we survey over 30 studies spanning 10 datasets and 8 metrics. How much evidence has been accumulated, and what, if anything, is missing before we can draw conclusions? Our analysis of the evaluation methods used in the literature reveals that some of the metrics are less conservative. We also find that the accumulated evidence, for now, remains ambiguous, but correlations with model size and quality provide grounds for cautious optimism.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.618.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our analysis of the evaluation methods used in the literature reveals that some of the metrics are less conservative.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Our analysis of the evaluation methods used in the literature reveals that some of the metrics are less conservative.\""
    },
    {
        "title": "Figurative Language Processing: A Linguistically Informed Feature Analysis of the Behavior of Language Models and Humans",
        "authors": [
            "Hyewon Jang",
            "Qi Yu",
            "Diego Frassinelli"
        ],
        "published": "2023",
        "summary": "Recent years have witnessed a growing interest in investigating what Transformer-based language models (TLMs) actually learn from the training data. This is especially relevant for complex tasks such as the understanding of non-literal meaning. In this work, we probe the performance of three black-box TLMs and two intrinsically transparent white-box models on figurative language classification of sarcasm, similes, idioms, and metaphors. We conduct two studies on the classification results to provide insights into the inner workings of such models. With our first analysis on feature importance, we identify crucial differences in model behavior. With our second analysis using an online experiment with human participants, we inspect different linguistic characteristics of the four figurative language types.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.622.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent years have witnessed a growing interest in investigating what Transformer-based language models (TLMs) actually learn from the training data.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Recent years have witnessed a growing interest in investigating what Transformer-based language models (TLMs) actually learn from the training data.\""
    },
    {
        "title": "Making Pre-trained Language Models both Task-solvers and Self-calibrators",
        "authors": [
            "Yangyi Chen",
            "Xingyao Wang",
            "Heng Ji"
        ],
        "published": "2023",
        "summary": "Pre-trained language models (PLMs) serve as backbones for various real-world systems. For high-stake applications, it’s equally essential to have reasonable confidence estimations in predictions. While the vanilla confidence scores of PLMs can already be effectively utilized, PLMs consistently become overconfident in their wrong predictions, which is not desirable in practice. Previous work shows that introducing an extra calibration task can mitigate this issue. The basic idea involves acquiring additional data to train models in predicting the confidence of their initial predictions. However, it only demonstrates the feasibility of this kind of method, assuming that there are abundant extra available samples for the introduced calibration task. In this work, we consider the practical scenario that we need to effectively utilize training samples to make PLMs both task-solvers and self-calibrators. Three challenges are presented, including limited training samples, data imbalance, and distribution shifts. We first conduct pilot experiments to quantify various decisive factors in the calibration task. Based on the empirical analysis results, we propose a training algorithm LM-TOAST to tackle the challenges. Experimental results show that LM-TOAST can effectively utilize the training data to make PLMs have reasonable confidence estimations while maintaining the original task performance. Further, we consider three downstream applications, namely selective classification, adversarial defense, and model cascading, to show the practical usefulness of LM-TOAST.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.624.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"PLMs consistently become overconfident in their wrong predictions, which is not desirable in practice.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"PLMs consistently become overconfident in their wrong predictions, which is not desirable in practice.\""
    },
    {
        "title": "EmbedTextNet: Dimension Reduction with Weighted Reconstruction and Correlation Losses for Efficient Text Embedding",
        "authors": [
            "Dae Yon Hwang",
            "Bilal Taha",
            "Yaroslav Nechaev"
        ],
        "published": "2023",
        "summary": "The size of embeddings generated by large language models can negatively affect system latency and model size in certain downstream practical applications (e.g. KNN search). In this work, we propose EmbedTextNet, a light add-on network that can be appended to an arbitrary language model to generate a compact embedding without requiring any changes in its architecture or training procedure. Specifically, we use a correlation penalty added to the weighted reconstruction loss that better captures the informative features in the text embeddings, which improves the efficiency of the language models. We evaluated EmbedTextNet on three different downstream tasks: text similarity, language modelling, and text retrieval. Empirical results on diverse benchmark datasets demonstrate the effectiveness and superiority of EmbedTextNet compared to state-of-art methodologies in recent works, especially in extremely low dimensional embedding sizes. The developed code for reproducibility is included in the supplementary material.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.625.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The size of embeddings generated by large language models can negatively affect system latency and model size in certain downstream practical applications (e.g. KNN search).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The size of embeddings generated by large language models can negatively affect system latency and model size in certain downstream practical applications (e.g. KNN search).\""
    },
    {
        "title": "Local Temperature Beam Search: Avoid Neural Text DeGeneration via Enhanced Calibration",
        "authors": [
            "Dongkyu Lee",
            "Gyeonghun Kim",
            "Janghoon Han",
            "Taesuk Hong",
            "Yi-Reun Kim",
            "Stanley Jungkyu Choi",
            "Nevin L. Zhang"
        ],
        "published": "2023",
        "summary": "Previous studies have constantly observed that a language model repeats itself, creating repetitions in an output sequence. To cope with the issue, stochastic decoding schemes have been the de facto approaches; the strategies add randomness in inference, hence avoiding the “self-loop”. However, the remedy comes at the cost of sacrificing output quality due to the randomness involved. In this work, we introduce a deterministic decoding scheme, local temperature beam search. This inference algorithm is an embarrassingly simple variant of beam search, yet it reduces repetition, whose level is superior to that of a sampling-based decoding algorithm, while maintaining the level of coherence as in beam search. Our idea is rooted in the concept of model calibration; we view a repetition as a casualty from overconfidence in a model. Therefore, our work mitigates the miscalibration present in the course of inference with a post-calibration approach applied in beam-specific manner. Our inference scheme is validated on text completion tasks, in which the repetition problem is seen most clearly, and is exhaustively compared with existing inference schemes.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.628.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Previous studies have constantly observed that a language model repeats itself, creating repetitions in an output sequence.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Previous studies have constantly observed that a language model repeats itself, creating repetitions in an output sequence.\""
    },
    {
        "title": "Explanation Graph Generation via Generative Pre-training over Synthetic Graphs",
        "authors": [
            "Han Cui",
            "Shangzhan Li",
            "Yu Zhang",
            "Qi Shi"
        ],
        "published": "2023",
        "summary": "The generation of explanation graphs is a significant task that aims to produce explanation graphs in response to user input, revealing the internal reasoning process. This task is challenging due to the significant discrepancy be- tween unstructured user queries and structured explanation graphs. Current research commonly fine-tunes a text-based pre-trained language model on a small downstream dataset that is annotated with labeled graphs. However, due to the limited scale of available datasets, this approach may prove to be insufficient in bridging the gap between natural language text and structured graphs. In this paper, to alleviate the above limitations, we propose a novel pre-trained framework EG3P(for Explanation Graph Generation via Generative Pre-training over synthetic graphs) for the explanation graph generation task. Specifically, we first propose a text-to-graph generative task to pre-train the model with the goal of bridging the text-graph gap. Additionally, we propose an automatic corpus synthesis strategy for synthesizing a large scale of high-quality corpus, reducing the reliance on costly manual annotation methods. Experimental results on ExplaGraphs show the effectiveness of EG3P that our model surpasses all baseline systems with remarkable margins. Besides, further analysis demonstrates that EG3P is able to generate better explanation graphs on actual reasoning tasks such as CommonsenseQA and OpenbookQA.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.629.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, due to the limited scale of available datasets, this approach may prove to be insufficient in bridging the gap between natural language text and structured graphs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, due to the limited scale of available datasets, this approach may prove to be insufficient in bridging the gap between natural language text and structured graphs.\""
    },
    {
        "title": "FORK: A Bite-Sized Test Set for Probing Culinary Cultural Biases in Commonsense Reasoning Models",
        "authors": [
            "Shramay Palta",
            "Rachel Rudinger"
        ],
        "published": "2023",
        "summary": "It is common sense that one should prefer to eat a salad with a fork rather than with a chainsaw. However, for eating a bowl of rice, the choice between a fork and a pair of chopsticks is culturally relative. We introduce FORK, a small, manually-curated set of CommonsenseQA-style questions for probing cultural biases and assumptions present in commonsense reasoning systems, with a specific focus on food-related customs. We test several CommonsenseQA systems on FORK, and while we see high performance on questions about the US culture, the poor performance of these systems on questions about non-US cultures highlights systematic cultural assumptions aligned with US over non-US cultures.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.631.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the poor performance of these systems on questions about non-US cultures highlights systematic cultural assumptions aligned with US over non-US cultures.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"the poor performance of these systems on questions about non-US cultures highlights systematic cultural assumptions aligned with US over non-US cultures.\""
    },
    {
        "title": "FedPETuning: When Federated Learning Meets the Parameter-Efficient Tuning Methods of Pre-trained Language Models",
        "authors": [
            "Zhuo Zhang",
            "Yuanhang Yang",
            "Yong Dai",
            "Qifan Wang",
            "Yue Yu",
            "Lizhen Qu",
            "Zenglin Xu"
        ],
        "published": "2023",
        "summary": "With increasing concerns about data privacy, there is an increasing necessity of fine-tuning pre-trained language models (PLMs) for adapting to downstream tasks located in end-user devices or local clients without transmitting data to the central server. This urgent necessity therefore calls the research of investigating federated learning (FL) for PLMs. However, large PLMs bring the curse of prohibitive communication overhead and local model adaptation costs for the FL system. To this end, we investigate the parameter-efficient tuning (PETuning) of PLMs and develop a corresponding federated benchmark for four representative PETuning methods, dubbed FedPETuning. Specifically, FedPETuning provides the first holistic empirical study of representative PLMs tuning methods in FL, covering privacy attacks, performance comparisons, and resource-constrained analysis. Intensive experimental results have indicated that FedPETuning can efficiently defend against privacy attacks and maintains acceptable performance with reducing heavy resource consumption. The open-source code and data are available at https://github.com/SMILELab-FL/FedPETuning.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.632.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, large PLMs bring the curse of prohibitive communication overhead and local model adaptation costs for the FL system.\"\n\nThis abstract mentions a limitation of large pre-trained language models (PLMs), which are a type of LLMs, but only briefly and as a minor detail to motivate the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, large PLMs bring the curse of prohibitive communication overhead and local model adaptation costs for the FL system.\"\n\nThis abstract mentions a limitation of large pre-trained language models (PLMs), which are a type of LLMs, but only briefly and as a minor detail to motivate the proposed solution."
    },
    {
        "title": "SlowBERT: Slow-down Attacks on Input-adaptive Multi-exit BERT",
        "authors": [
            "Shengyao Zhang",
            "Xudong Pan",
            "Mi Zhang",
            "Min Yang"
        ],
        "published": "2023",
        "summary": "For pretrained language models such as Google’s BERT, recent research designs several input-adaptive inference mechanisms to improve the efficiency on cloud and edge devices. In this paper, we reveal a new attack surface on input-adaptive multi-exit BERT, where the adversary imperceptibly modifies the input texts to drastically increase the average inference cost. Our proposed slow-down attack called SlowBERT integrates a new rank-and-substitute adversarial text generation algorithm to efficiently search for the perturbation which maximally delays the exiting time. With no direct access to the model internals, we further devise a time-based approximation algorithm to infer the exit position as the loss oracle. Our extensive evaluation on two popular instances of multi-exit BERT for GLUE classification tasks validates the effectiveness of SlowBERT. In the worst case, SlowBERT increases the inference cost by 4.57×, which would strongly hurt the service quality of multi-exit BERT in practice, e.g., increasing the real-time cloud services’ response times for online users.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.634.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our proposed slow-down attack called SlowBERT integrates a new rank-and-substitute adversarial text generation algorithm to efficiently search for the perturbation which maximally delays the exiting time... In the worst case, SlowBERT increases the inference cost by 4.57×, which would strongly hurt the service quality of multi-exit BERT in practice, e.g., increasing the real-time",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Our proposed slow-down attack called SlowBERT integrates a new rank-and-substitute adversarial text generation algorithm to efficiently search for the perturbation which maximally delays the exiting time... In the worst case, SlowBERT increases the inference cost by 4.57×, which would strongly hurt the service quality of multi-exit BERT in practice, e.g., increasing the real-time"
    },
    {
        "title": "PREADD: Prefix-Adaptive Decoding for Controlled Text Generation",
        "authors": [
            "Jonathan Pei",
            "Kevin Yang",
            "Dan Klein"
        ],
        "published": "2023",
        "summary": "We propose Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation. Unlike existing methods that use auxiliary expert models to control for attributes, PREADD does not require an external model, instead relying on linearly combining output logits from multiple prompts. Specifically, PREADD contrasts the output logits generated using a raw prompt against those generated using a prefix-prepended prompt, enabling both positive and negative control with respect to any attribute encapsulated by the prefix. We evaluate PREADD on three tasks—toxic output mitigation, gender bias reduction, and sentiment control—and find that PREADD outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on our main metrics for each task.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.636.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "MTR: A Dataset Fusing Inductive, Deductive, and Defeasible Reasoning",
        "authors": [
            "Yitian Li",
            "Jidong Tian",
            "Caoyun Fan",
            "Wenqing Chen",
            "Hao He",
            "Yaohui Jin"
        ],
        "published": "2023",
        "summary": "A long-standing difficulty in AI is the introduction of human-like reasoning in machine reading comprehension. Since algorithmic models can already perform as well as humans on simple quality assurance tasks thanks to the development of deep learning techniques, more difficult reasoning datasets have been presented. However, these datasets mainly focus on a single type of reasoning. There are still significant gaps in the studies when compared to the complex reasoning used in daily life. In this work, we introduce a brand-new dataset, named MTR. There are two parts to it: the first combines deductive and inductive reasoning, and the second does the same with inductive and defeasible reasoning. It consists of more than 30k QA instances, inferring relations between characters in short stories. Results show that state-of-the-art neural models do noticeably worse than expected. Our empirical results highlight the gap in the models’ ability to handle sophisticated inference.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.640.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Results show that state-of-the-art neural models do noticeably worse than expected. Our empirical results highlight the gap in the models’ ability to handle sophisticated inference.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Results show that state-of-the-art neural models do noticeably worse than expected. Our empirical results highlight the gap in the models’ ability to handle sophisticated inference.\""
    },
    {
        "title": "NewsMet : A ‘do it all’ Dataset of Contemporary Metaphors in News Headlines",
        "authors": [
            "Rohan Joseph",
            "Timothy Liu",
            "Aik Beng Ng",
            "Simon See",
            "Sunny Rai"
        ],
        "published": "2023",
        "summary": "Metaphors are highly creative constructs of human language that grow old and eventually die. Popular datasets used for metaphor processing tasks were constructed from dated source texts. In this paper, we propose NewsMet, a large high-quality contemporary dataset of news headlines hand-annotated with metaphorical verbs. The dataset comprises headlines from various sources including political, satirical, reliable and fake. Our dataset serves the purpose of evaluation for the tasks of metaphor interpretation and generation. The experiments reveal several insights and limitations of using LLMs to automate metaphor processing tasks as frequently seen in the recent literature. The dataset is publicly available for research purposes https://github.com/AxleBlaze3/NewsMet_Metaphor_Dataset.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.641.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The experiments reveal several insights and limitations of using LLMs to automate metaphor processing tasks as frequently seen in the recent literature.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The experiments reveal several insights and limitations of using LLMs to automate metaphor processing tasks as frequently seen in the recent literature.\""
    },
    {
        "title": "StructSP: Efficient Fine-tuning of Task-Oriented Dialog System by Using Structure-aware Boosting and Grammar Constraints",
        "authors": [
            "Truong Do",
            "Phuong Nguyen",
            "Minh Nguyen"
        ],
        "published": "2023",
        "summary": "We have investigated methods utilizing hierarchical structure information representation in the semantic parsing task and have devised a method that reinforces the semantic awareness of a pre-trained language model via a two-step fine-tuning mechanism: hierarchical structure information strengthening and a final specific task. The model used is better than existing ones at learning the contextual representations of utterances embedded within its hierarchical semantic structure and thereby improves system performance. In addition, we created a mechanism using inductive grammar to dynamically prune the unpromising directions in the semantic structure parsing process. Finally, through experimentsOur code will be published when this paper is accepted. on the TOP and TOPv2 (low-resource setting) datasets, we achieved state-of-the-art (SOTA) performance, confirming the effectiveness of our proposed model.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.648.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit evidence of discussing LLM limitations, but mentions \"reinforces the semantic awareness of a pre-trained language model\" implying a pre-trained model may lack certain awareness.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit evidence of discussing LLM limitations, but mentions \"reinforces the semantic awareness of a pre-trained language model\" implying a pre-trained model may lack certain awareness."
    },
    {
        "title": "Tab-CoT: Zero-shot Tabular Chain of Thought",
        "authors": [
            "Jin Ziqi",
            "Wei Lu"
        ],
        "published": "2023",
        "summary": "The chain-of-though (CoT) prompting methods were successful in various natural language processing (NLP) tasks thanks to their ability to unveil the underlying complex reasoning processes. Such reasoning processes typically exhibit highly structured steps. Recent efforts also started investigating methods to encourage more structured reasoning procedures to be captured (cite least to most).In this work, we propose Tab-CoT, a novel tabular-format CoT prompting method, which allows the complex reasoning process to be explicitly modeled in a highly structured manner. Despite its simplicity, we show that our approach is capable of performing reasoning across multiple dimensions (i.e., both rows and columns).We demonstrate our approach’s strong zero-shot and few-shot capabilities through extensive experiments on a range of reasoning tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.651.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Augmenting Large Language Model Translators via Translation Memories",
        "authors": [
            "Yongyu Mu",
            "Abudurexiti Reheman",
            "Zhiquan Cao",
            "Yuchun Fan",
            "Bei Li",
            "Yinqiao Li",
            "Tong Xiao",
            "Chunliang Zhang",
            "Jingbo Zhu"
        ],
        "published": "2023",
        "summary": "Using translation memories (TMs) as prompts is a promising approach to in-context learning of machine translation models. In this work, we take a step towards prompting large language models (LLMs) with TMs and making them better translators. We find that the ability of LLMs to “understand” prompts is indeed helpful for making better use of TMs. Experiments show that the results of a pre-trained LLM translator can be greatly improved by using high-quality TM-based prompts. These results are even comparable to those of the state-of-the-art NMT systems which have access to large-scale in-domain bilingual data and are well tuned on the downstream tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.653.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the ability of LLMs to “understand” prompts is indeed helpful for making better use of TMs.\"\n\n(Note: The abstract mentions the ability of LLMs, but it does not discuss any limitations of LLMs. The focus is on improving the performance of LLMs using translation memories.)",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"the ability of LLMs to “understand” prompts is indeed helpful for making better use of TMs.\"\n\n(Note: The abstract mentions the ability of LLMs, but it does not discuss any limitations of LLMs. The focus is on improving the performance of LLMs using translation memories.)"
    },
    {
        "title": "PIP: Parse-Instructed Prefix for Syntactically Controlled Paraphrase Generation",
        "authors": [
            "Yixin Wan",
            "Kuan-Hao Huang",
            "Kai-Wei Chang"
        ],
        "published": "2023",
        "summary": "Syntactically controlled paraphrase generation requires language models to generate paraphrases for sentences according to specific syntactic structures. Existing fine-tuning methods on this task is costly, as all parameters of the model need to be updated during the training process. Inspired by recent studies on parameter-efficient learning, we propose Parse-Instructed Prefix (PIP), a novel adaptation of prefix-tuning to tune large pre-trained language models on syntactically controlled paraphrase generation task in a low-data setting with significantly less training cost. We introduce two methods to instruct a model’s encoder prefix to capture syntax-related knowledge: direct initiation (PIP-Direct) and indirect optimization (PIP-Indirect). Comparing to traditional fine-tuning methods for this task, PIP is a compute-efficient alternative with 10 times less learnable parameters. Comparing to existing prefix-tuning methods, PIP excels at capturing syntax control information, achieving significantly higher performance at the same level of learnable parameter count.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.659.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing fine-tuning methods on this task is costly, as all parameters of the model need to be updated during the training process.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Existing fine-tuning methods on this task is costly, as all parameters of the model need to be updated during the training process.\""
    },
    {
        "title": "DePlot: One-shot visual language reasoning by plot-to-table translation",
        "authors": [
            "Fangyu Liu",
            "Julian Eisenschlos",
            "Francesco Piccinno",
            "Syrine Krichene",
            "Chenxi Pang",
            "Kenton Lee",
            "Mandar Joshi",
            "Wenhu Chen",
            "Nigel Collier",
            "Yasemin Altun"
        ],
        "published": "2023",
        "summary": "Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. This paper presents the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The key in this method is a modality conversion module, named as DePlot, which translates the image of a plot or chart to a linearized table. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing unified task formats and metrics, and train DePlot end-to-end on this task. DePlot can then be used off-the-shelf together with LLMs in a plug-and-play fashion. Compared with a SOTA model finetuned on more than thousands of data points, DePlot+LLM with just one-shot prompting achieves a 29.4% improvement over finetuned SOTA on human-written queries from the task of chart QA.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.660.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries.\""
    },
    {
        "title": "Stochastic Bridges as Effective Regularizers for Parameter-Efficient Tuning",
        "authors": [
            "Weize Chen",
            "Xu Han",
            "Yankai Lin",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Jie Zhou"
        ],
        "published": "2023",
        "summary": "Parameter-efficient tuning methods (PETs) have achieved promising results in tuning large pre-trained language models (PLMs). By formalizing frozen PLMs and additional tunable parameters as systems and controls respectively, PETs can be theoretically grounded to optimal control and further viewed as optimizing the terminal cost and running cost in the optimal control literature. Despite the elegance of this theoretical grounding, in practice, existing PETs often ignore the running cost and only optimize the terminal cost, i.e., focus on optimizing the loss function of the output state, regardless of the running cost that depends on the intermediate states. Since it is non-trivial to directly model the intermediate states and design a running cost function, we propose to use latent stochastic bridges to regularize the intermediate states and use the regularization as the running cost of PETs. As the first work to propose regularized PETs that use stochastic bridges as the regularizers (running costs) for the intermediate states, we show the effectiveness and generality of this regularization across different tasks, PLMs and PETs. In view of the great potential and capacity, we believe more sophisticated regularizers can be designed for PETs and better performance can be achieved in the future.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.661.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the elegance of this theoretical grounding, in practice, existing PETs often ignore the running cost and only optimize the terminal cost, i.e., focus on optimizing the loss function of the output state, regardless of the running cost that depends on the intermediate states.\"\n\nThis abstract mentions a limitation of existing PETs for LLMs, but it is not the primary focus of the paper",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite the elegance of this theoretical grounding, in practice, existing PETs often ignore the running cost and only optimize the terminal cost, i.e., focus on optimizing the loss function of the output state, regardless of the running cost that depends on the intermediate states.\"\n\nThis abstract mentions a limitation of existing PETs for LLMs, but it is not the primary focus of the paper"
    },
    {
        "title": "How Well Do Large Language Models Perform on Faux Pas Tests?",
        "authors": [
            "Natalie Shapira",
            "Guy Zwirn",
            "Yoav Goldberg"
        ],
        "published": "2023",
        "summary": "Motivated by the question of the extent to which large language models “understand” social intelligence, we investigate the ability of such models to generate correct responses to questions involving descriptions of faux pas situations. The faux pas test is a test used in clinical psychology, which is known to be more challenging for children than individual tests of theory-of-mind or social intelligence. Our results demonstrate that, while the models seem to sometimes offer correct responses, they in fact struggle with this task, and that many of the seemingly correct responses can be attributed to over-interpretation by the human reader (“the ELIZA effect”). An additional phenomenon observed is the failure of most models to generate a correct response to presupposition questions. Finally, in an experiment in which the models are tasked with generating original faux pas stories, we find that while some models are capable of generating novel faux pas stories, the stories are all explicit, as the models are limited in their abilities to describe situations in an implicit manner.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.663.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "5",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our results demonstrate that, while the models seem to sometimes offer correct responses, they in fact struggle with this task, and that many of the seemingly correct responses can be attributed to over-interpretation by the human reader (“the ELIZA effect”); \"An additional phenomenon observed is the failure of most models to generate a correct response to presupposition questions\"; \"the stories are",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 5\nEvidence: \"Our results demonstrate that, while the models seem to sometimes offer correct responses, they in fact struggle with this task, and that many of the seemingly correct responses can be attributed to over-interpretation by the human reader (“the ELIZA effect”); \"An additional phenomenon observed is the failure of most models to generate a correct response to presupposition questions\"; \"the stories are"
    },
    {
        "title": "LMentry: A Language Model Benchmark of Elementary Language Tasks",
        "authors": [
            "Avia Efrat",
            "Or Honovich",
            "Omer Levy"
        ],
        "published": "2023",
        "summary": "As the performance of large language models rapidly improves, benchmarks are getting larger and more complex as well. We present LMentry, a benchmark that avoids this “arms race” by focusing on a compact set of tasks that are trivial to humans, e.g. writing a sentence containing a specific word, identifying which words in a list belong to a specific category, or choosing which of two words is longer.LMentry is specifically designed to provide quick and interpretable insights into the capabilities and robustness of large language models. Our experiments reveal a wide variety of failure cases that, while immediately obvious to humans, pose a considerable challenge for large language models, including OpenAI’s latest 175B-parameter instruction-tuned model, TextDavinci002.LMentry complements contemporary evaluation approaches of large language models, providing a quick, automatic, and easy-to-run “unit test”, without resorting to large benchmark suites of complex tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.666.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our experiments reveal a wide variety of failure cases that, while immediately obvious to humans, pose a considerable challenge for large language models, including OpenAI’s latest 175B-parameter instruction-tuned model, TextDavinci002.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Our experiments reveal a wide variety of failure cases that, while immediately obvious to humans, pose a considerable challenge for large language models, including OpenAI’s latest 175B-parameter instruction-tuned model, TextDavinci002.\""
    },
    {
        "title": "Differentiable Instruction Optimization for Cross-Task Generalization",
        "authors": [
            "Masaru Isonuma",
            "Junichiro Mori",
            "Ichiro Sakata"
        ],
        "published": "2023",
        "summary": "Instruction tuning has been attracting much attention to achieve generalization ability across a wide variety of tasks. Although various types of instructions have been manually created for instruction tuning, it is still unclear what kind of instruction is optimal to obtain cross-task generalization ability. This work presents instruction optimization, which optimizes training instructions with respect to generalization ability. Rather than manually tuning instructions, we introduce learnable instructions and optimize them with gradient descent by leveraging bilevel optimization. Experimental results show that the learned instruction enhances the diversity of instructions and improves the generalization ability compared to using only manually created instructions.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.667.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning",
        "authors": [
            "Zhanming Jie",
            "Wei Lu"
        ],
        "published": "2023",
        "summary": "Chain-of-thought (CoT) prompting with large language models has proven effective in numerous natural language process tasks, but designing prompts that generalize well to diverse problem types can be challenging CITATION, especially in the context of math word problem solving. Additionally, it is common to have a large amount of training data that have a better diversity coverage but CoT annotations are not available, which limits the use of supervised learning techniques. To address these issues, we investigate two approaches to leverage the training data in few-shot prompting scenario: dynamic program prompting and program distillation.Our approach is largely inspired by CITATION where they proposed to replace the CoT with the programs as the intermediate reasoning step. Such a prompting strategy allows us to accurately verify the answer correctness through program execution in MWP solving.Our dynamic program prompting involves annotating the training data by sampling correct programs from a large language model, while program distillation involves adapting a smaller model to the program-annotated training data.Our experiments on three standard MWP datasets demonstrate the effectiveness of these approaches, yielding significant improvements over previous baselines for prompting and fine-tuning.Our results suggest that leveraging a large amount of training data can improve the generalization ability of prompts and boost the performance of fine-tuned smaller models in MWP solving.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.668.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"designing prompts that generalize well to diverse problem types can be challenging, especially in the context of math word problem solving\", \"it is common to have a large amount of training data that have a better diversity coverage but CoT annotations are not available, which limits the use of supervised learning techniques.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"designing prompts that generalize well to diverse problem types can be challenging, especially in the context of math word problem solving\", \"it is common to have a large amount of training data that have a better diversity coverage but CoT annotations are not available, which limits the use of supervised learning techniques.\""
    },
    {
        "title": "Deep Span Representations for Named Entity Recognition",
        "authors": [
            "Enwei Zhu",
            "Yiyang Liu",
            "Jinpeng Li"
        ],
        "published": "2023",
        "summary": "Span-based models are one of the most straightforward methods for named entity recognition (NER). Existing span-based NER systems shallowly aggregate the token representations to span representations. However, this typically results in significant ineffectiveness for long entities, a coupling between the representations of overlapping spans, and ultimately a performance degradation. In this study, we propose DSpERT (Deep Span Encoder Representations from Transformers), which comprises a standard Transformer and a span Transformer. The latter uses low-layered span representations as queries, and aggregates the token representations as keys and values, layer by layer from bottom to top. Thus, DSpERT produces span representations of deep semantics. With weight initialization from pretrained language models, DSpERT achieves performance higher than or competitive with recent state-of-the-art systems on six NER benchmarks. Experimental results verify the importance of the depth for span representations, and show that DSpERT performs particularly well on long-span entities and nested structures. Further, the deep span representations are well structured and easily separable in the feature space.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.672.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Multi-armed bandits for resource efficient, online optimization of language model pre-training: the use case of dynamic masking",
        "authors": [
            "Inigo Urteaga",
            "Moulay Zaidane Draidia",
            "Tomer Lancewicki",
            "Shahram Khadivi"
        ],
        "published": "2023",
        "summary": "We design and evaluate a Bayesian optimization framework for resource efficient pre-training of Transformer-based language models (TLMs). TLM pre-training requires high computational resources and introduces many unresolved design choices, such as selecting its pre-training hyperparameters.We propose a multi-armed bandit framework for the sequential selection of pre-training hyperparameters, aimed at optimizing language model performance, in a resource efficient manner. We design a Thompson sampling algorithm, with a surrogate Gaussian process reward model of the Masked Language Model (MLM) pre-training objective, for its sequential minimization. Instead of MLM pre-training with fixed masking probabilities, the proposed Gaussian process-based Thompson sampling (GP-TS) accelerates pre-training by sequentially selecting masking hyperparameters that improve performance. We empirically demonstrate how GP-TS pre-trains language models efficiently, i.e., it achieves lower MLM loss in fewer epochs, across a variety of settings. In addition, GP-TS pre-trained TLMs attain competitive downstream performance, while avoiding expensive hyperparameter grid search. GP-TS provides an interactive framework for efficient and optimized TLM pre-training that, by circumventing costly hyperparameter selection, enables substantial computational savings.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.675.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"TLM pre-training requires high computational resources and introduces many unresolved design choices, such as selecting its pre-training hyperparameters.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"TLM pre-training requires high computational resources and introduces many unresolved design choices, such as selecting its pre-training hyperparameters.\""
    },
    {
        "title": "ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages",
        "authors": [
            "Yekun Chai",
            "Shuohuan Wang",
            "Chao Pang",
            "Yu Sun",
            "Hao Tian",
            "Hua Wu"
        ],
        "published": "2023",
        "summary": "Software engineers working with the same programming language (PL) may speak different natural languages (NLs) and vice versa, erecting huge barriers to communication and working efficiency. Recent studies have demonstrated the effectiveness of generative pre-training in computer programs, yet they are always English-centric. In this work, we step towards bridging the gap between multilingual NLs and multilingual PLs for large language models (LLMs). We release ERNIE-Code, a unified pre-trained language model for 116 NLs and 6 PLs. We employ two methods for universal cross-lingual pre-training: span-corruption language modeling that learns patterns from monolingual NL or PL; and pivot-based translation language modeling that relies on parallel data of many NLs and PLs. Extensive results show that ERNIE-Code outperforms previous multilingual LLMs for PL or NL across a wide range of end tasks of code intelligence, including multilingual code-to-text, text-to-code, code-to-code, and text-to-text generation. We further show its advantage of zero-shot prompting on multilingual code summarization and text-to-text translation. We release our code and pre-trained checkpoints.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.676.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent studies have demonstrated the effectiveness of generative pre-training in computer programs, yet they are always English-centric.\"\n\nThis paper is rated 1 because it mentions a limitation of existing pre-training methods (being English-centric) but does not elaborate on this limitation and instead focuses on the proposed solution, ERNIE-Code.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Recent studies have demonstrated the effectiveness of generative pre-training in computer programs, yet they are always English-centric.\"\n\nThis paper is rated 1 because it mentions a limitation of existing pre-training methods (being English-centric) but does not elaborate on this limitation and instead focuses on the proposed solution, ERNIE-Code."
    },
    {
        "title": "Understanding Programs by Exploiting (Fuzzing) Test Cases",
        "authors": [
            "Jianyu Zhao",
            "Yuyang Rong",
            "Yiwen Guo",
            "Yifeng He",
            "Hao Chen"
        ],
        "published": "2023",
        "summary": "Semantic understanding of programs has attracted great attention in the community. Inspired by recent successes of large language models (LLMs) in natural language understanding, tremendous progress has been made by treating programming language as another sort of natural language and training LLMs on corpora of program code. However, programs are essentially different from texts after all, in a sense that they are normally heavily structured and syntax-strict. In particular, programs and their basic units (i.e., functions and subroutines) are designed to demonstrate a variety of behaviors and/or provide possible outputs, given different inputs. The relationship between inputs and possible outputs/behaviors represents the functions/subroutines and profiles the program as a whole. Hence, we propose to incorporate such a relationship into learning, for achieving a deeper semantic understanding of programs. To obtain inputs that are representative enough to trigger the execution of most part of the code, we resort to fuzz testing and propose fuzz tuning to boost the performance of program understanding and code representation learning, given a pre-trained LLM. The effectiveness of the proposed method is verified on two program understanding tasks including code clone detection and code classification, and it outperforms current state-of-the-arts by large margins. Code is available at https://github.com/rabbitjy/FuzzTuning.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.678.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, programs are essentially different from texts after all, in a sense that they are normally heavily structured and syntax-strict.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, programs are essentially different from texts after all, in a sense that they are normally heavily structured and syntax-strict.\""
    },
    {
        "title": "Coherent or Not? Stressing a Neural Language Model for Discourse Coherence in Multiple Languages",
        "authors": [
            "Dominique Brunato",
            "Felice Dell’Orletta",
            "Irene Dini",
            "Andrea Amelio Ravelli"
        ],
        "published": "2023",
        "summary": "In this study, we investigate the capability of a Neural Language Model (NLM) to distinguish between coherent and incoherent text, where the latter has been artificially created to gradually undermine local coherence within text. While previous research on coherence assessment using NLMs has primarily focused on English, we extend our investigation to multiple languages. We employ a consistent evaluation framework to compare the performance of monolingual and multilingual models in both in-domain and out-domain settings. Additionally, we explore the model’s performance in a cross-language scenario.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.680.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While previous research on coherence assessment using NLMs has primarily focused on English, we extend our investigation to multiple languages.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While previous research on coherence assessment using NLMs has primarily focused on English, we extend our investigation to multiple languages.\""
    },
    {
        "title": "Masked Audio Text Encoders are Effective Multi-Modal Rescorers",
        "authors": [
            "Jinglun Cai",
            "Monica Sunkara",
            "Xilai Li",
            "Anshu Bhatia",
            "Xiao Pan",
            "Sravan Bodapati"
        ],
        "published": "2023",
        "summary": "Masked Language Models (MLMs) have proven to be effective for second-pass rescoring in Automatic Speech Recognition (ASR) systems. In this work, we propose Masked Audio Text Encoder (MATE), a multi-modal masked language model rescorer which incorporates acoustic representations into the input space of MLM. We adopt contrastive learning for effectively aligning the modalities by learning shared representations. We show that using a multi-modal rescorer is beneficial for domain generalization of the ASR system when target domain data is unavailable. MATE reduces word error rate (WER) by 4%-16% on in-domain, and 3%-7% on out-of-domain datasets, over the text-only baseline. Additionally, with very limited amount of training data (0.8 hours) MATE achieves a WER reduction of 8%-23% over the first-pass baseline.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.682.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "Pre-trained Personalized Review Summarization with Effective Salience Estimation",
        "authors": [
            "Hongyan Xu",
            "Hongtao Liu",
            "Zhepeng Lv",
            "Qing Yang",
            "Wenjun Wang"
        ],
        "published": "2023",
        "summary": "Personalized review summarization in recommender systems is a challenging task of generating condensed summaries for product reviews while preserving the salient content of reviews. Recently, Pretrained Language Models (PLMs) have become a new paradigm in text generation for the strong ability of natural language comprehension. However, it is nontrivial to apply PLMs in personalized review summarization directly since there are rich personalized information (e.g., user preferences and product characteristics) to be considered, which is crucial to the salience estimation of input review. In this paper, we propose a pre-trained personalized review summarization method, which aims to effectively incorporate the personalized information of users and products into the salience estimation of the input reviews. We design a personalized encoder that could identify the salient contents of the input sequence by jointly considering the semantic and personalized information respectively (i.e., ratings, user and product IDs, and linguistic features), yielding personalized representations for the input reviews and history summaries separately. Moreover, we design an interactive information selection mechanism that further identifies the salient contents of the input reviews and selects relative information from the history summaries. The results on real-world datasets show that our method performs better than the state-of-the-art baselines and could generate more readable summaries.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.684.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it is nontrivial to apply PLMs in personalized review summarization directly since there are rich personalized information... to be considered, which is crucial to the salience estimation of input review.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, it is nontrivial to apply PLMs in personalized review summarization directly since there are rich personalized information... to be considered, which is crucial to the salience estimation of input review.\""
    },
    {
        "title": "CaPE: Contrastive Parameter Ensembling for Reducing Hallucination in Abstractive Summarization",
        "authors": [
            "Prafulla Kumar Choubey",
            "Alex Fabbri",
            "Jesse Vig",
            "Chien-Sheng Wu",
            "Wenhao Liu",
            "Nazneen Rajani"
        ],
        "published": "2023",
        "summary": "Hallucination is a known issue for neural abstractive summarization models. Recent work suggests that the degree of hallucination may depend on factual errors in the training data. In this work, we propose a new method called Contrastive Parameter Ensembling (CaPE) to use training data more effectively, utilizing variations in noise in training samples to reduce hallucination. Starting with a base model fine-tuned on an entire dataset, we additionally train expert and anti-expert models on clean and noisy subsets of the data, respectively. We then adjust the parameters of the base model by adding (subtracting) the parameters of the expert (anti-expert), advancing the recent work on additive parameter ensembling approaches. Trained on a much smaller data subset, expert and anti-expert models only fractionally (<14%) increases the total training time. Further, CaPE uses parameter ensembling and does not increase the inference time. Experimental results show that CaPE improves performance across different automatic factual metrics and human evaluation, with a maximum improvement of 16.69% and 15.38% on summary-level dependency-arc entailment accuracy for the XSUM and CNN/DM datasets. The CaPE model performs comparably to the base model on metrics of informativeness such as ROUGE.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.685.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Learn to Not Link: Exploring NIL Prediction in Entity Linking",
        "authors": [
            "Fangwei Zhu",
            "Jifan Yu",
            "Hailong Jin",
            "Lei Hou",
            "Juanzi Li",
            "Zhifang Sui"
        ],
        "published": "2023",
        "summary": "Entity linking models have achieved significant success via utilizing pretrained language models to capture semantic features. However, the NIL prediction problem, which aims to identify mentions without a corresponding entity in the knowledge base, has received insufficient attention. We categorize mentions linking to NIL into Missing Entity and Non-Entity Phrase, and propose an entity linking dataset NEL that focuses on the NIL prediction problem.NEL takes ambiguous entities as seeds, collects relevant mention context in the Wikipedia corpus, and ensures the presence of mentions linking to NIL by human annotation and entity masking. We conduct a series of experiments with the widely used bi-encoder and cross-encoder entity linking models, results show that both types of NIL mentions in training data have a significant influence on the accuracy of NIL prediction. Our code and dataset can be accessed at https://github.com/solitaryzero/NIL_EL.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.690.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the NIL prediction problem, which aims to identify mentions without a corresponding entity in the knowledge base, has received insufficient attention.\"\n\nThis abstract mentions a limitation of entity linking models that utilize pre-trained language models, but does not explore it in depth.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the NIL prediction problem, which aims to identify mentions without a corresponding entity in the knowledge base, has received insufficient attention.\"\n\nThis abstract mentions a limitation of entity linking models that utilize pre-trained language models, but does not explore it in depth."
    },
    {
        "title": "Structured Pruning for Efficient Generative Pre-trained Language Models",
        "authors": [
            "Chaofan Tao",
            "Lu Hou",
            "Haoli Bai",
            "Jiansheng Wei",
            "Xin Jiang",
            "Qun Liu",
            "Ping Luo",
            "Ngai Wong"
        ],
        "published": "2023",
        "summary": "The increasing sizes of large generative Pre-trained Language Models (PLMs) hinder their deploymentin real-world applications. To obtain efficient PLMs, previous studies mostly focus on pruning the attention heads and feed-forward networks (FFNs) of the Transformer. Nevertheless, we find that in generative PLMs, the hidden dimension shared by many other modules (e.g., embedding layer and layer normalization) contains persistent outliers regardless of the network input. This study comprehensively investigates the structured pruning of generative PLMs with all the above compressible components. To identify redundant network structures, we assign learnable masks over compressible components followed by sparse training. Various sizes of PLMs can be flexibly extracted via different thresholds, and are then task-specifically fine-tuned for further improvement. Extensive experiments on language modeling, summarization and machine translation validate the effectiveness of the proposed method. For example, the pruned BART brings 1.51x/6.96x inference speedup on GPU/CPU with 67% size reduction, and can be further combined with quantization for more than 25× compression.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.692.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The increasing sizes of large generative Pre-trained Language Models (PLMs) hinder their deployment in real-world applications.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"The increasing sizes of large generative Pre-trained Language Models (PLMs) hinder their deployment in real-world applications.\""
    },
    {
        "title": "Contextualized Semantic Distance between Highly Overlapped Texts",
        "authors": [
            "Letian Peng",
            "Zuchao Li",
            "Hai Zhao"
        ],
        "published": "2023",
        "summary": "Overlapping frequently occurs in paired texts in natural language processing tasks like text editing and semantic similarity evaluation. Better evaluation of the semantic distance between the overlapped sentences benefits the language system’s understanding and guides the generation. Since conventional semantic metrics are based on word representations, they are vulnerable to the disturbance of overlapped components with similar representations. This paper aims to address the issue with a mask-and-predict strategy. We take the words in the longest common sequence (LCS) as neighboring words and use masked language modeling (MLM) from pre-trained language models (PLMs) to predict the distributions in their positions. Our metric, Neighboring Distribution Divergence (NDD), represents the semantic distance by calculating the divergence between distributions in the overlapped parts. Experiments on Semantic Textual Similarity show NDD to be more sensitive to various semantic differences, especially on highly overlapped paired texts. Based on the discovery, we further implement an unsupervised and training-free method for text compression, leading to a significant improvement on the previous perplexity-based method. The high compression rate controlling ability of our method even enables NDD to outperform the supervised state-of-the-art in domain adaption by a huge margin. Further experiments on syntax and semantics analyses verify the awareness of internal sentence structures, indicating the high potential of NDD for further studies.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.694.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Since conventional semantic metrics are based on word representations, they are vulnerable to the disturbance of overlapped components with similar representations.\"\n\nThis paper mentions a limitation of conventional semantic metrics based on word representations, which is related to pre-trained language models (PLMs), but it is a minor detail and not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Since conventional semantic metrics are based on word representations, they are vulnerable to the disturbance of overlapped components with similar representations.\"\n\nThis paper mentions a limitation of conventional semantic metrics based on word representations, which is related to pre-trained language models (PLMs), but it is a minor detail and not the primary focus of the paper."
    },
    {
        "title": "Enhancing Cross-lingual Prompting with Dual Prompt Augmentation",
        "authors": [
            "Meng Zhou",
            "Xin Li",
            "Yue Jiang",
            "Lidong Bing"
        ],
        "published": "2023",
        "summary": "Prompting shows promising results in few-shot scenarios. However, its strength for multilingual/cross-lingual problems has not been fully exploited. hao and Schütze (2021) made initial explorations in this direction by presenting that cross-lingual prompting outperforms cross-lingual finetuning. In this paper, we conduct an empirical exploration on the effect of each component in cross-lingual prompting and derive Universal Prompting, which helps alleviate the discrepancies between source-language training and target-language inference. Based on this, we propose DPA, a dual prompt augmentation framework, aiming at relieving the data scarcity issue in few-shot cross-lingual prompting. Notably, for XNLI, our method achieves 46.54% with only 16 English training examples per class, significantly better than 34.99% of fine-tuning. Our code is available at https://github.com/DAMO-NLP-SG/DPA.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.700.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Prompting shows promising results in few-shot scenarios. However, its strength for multilingual/cross-lingual problems has not been fully exploited.\"\n\nThis paper mentions a limitation of LLMs in cross-lingual scenarios, but it is a minor detail and not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Prompting shows promising results in few-shot scenarios. However, its strength for multilingual/cross-lingual problems has not been fully exploited.\"\n\nThis paper mentions a limitation of LLMs in cross-lingual scenarios, but it is a minor detail and not the primary focus of the paper."
    },
    {
        "title": "MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text Generation",
        "authors": [
            "Swarnadeep Saha",
            "Xinyan Yu",
            "Mohit Bansal",
            "Ramakanth Pasunuru",
            "Asli Celikyilmaz"
        ],
        "published": "2023",
        "summary": "Prompting large language models has enabled significant recent progress in multi-step reasoning over text. However, when applied to text generation from semi-structured data (e.g., graphs or tables), these methods typically suffer from low semantic coverage, hallucination, and logical inconsistency. We propose MURMUR a neuro-symbolic modular approach to text generation from semi-structured data with multi-step reasoning. MURMUR is a best-first search method that generates reasoning paths using: (1) neural and symbolic modules with specific linguistic and logical skills, (2) a grammar whose production rules define valid compositions of modules, and (3) value functions that assess the quality of each reasoning step. We conduct experiments on two diverse data-to-text generation tasks like WebNLG and LogicNLG. The tasks differ in their data representations (graphs and tables) and span multiple linguistic and logical skills. MURMUR obtains significant improvements over recent few-shot baselines like direct prompting and chain-of-thought prompting, while also achieving comparable performance to fine-tuned GPT-2 on out-of-domain data. Moreover, human evaluation shows that MURMUR generates highly faithful and correct reasoning paths that lead to 26% more logically consistent summaries on LogicNLG, compared to direct prompting.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.704.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, when applied to text generation from semi-structured data (e.g., graphs or tables), these methods typically suffer from low semantic coverage, hallucination, and logical inconsistency.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, when applied to text generation from semi-structured data (e.g., graphs or tables), these methods typically suffer from low semantic coverage, hallucination, and logical inconsistency.\""
    },
    {
        "title": "Revisit Few-shot Intent Classification with PLMs: Direct Fine-tuning vs. Continual Pre-training",
        "authors": [
            "Haode Zhang",
            "Haowen Liang",
            "Liming Zhan",
            "Albert Y.S. Lam",
            "Xiao-Ming Wu"
        ],
        "published": "2023",
        "summary": "We consider the task of few-shot intent detection, which involves training a deep learning model to classify utterances based on their underlying intents using only a small amount of labeled data. The current approach to address this problem is through continual pre-training, i.e., fine-tuning pre-trained language models (PLMs) on external resources (e.g., conversational corpora, public intent detection datasets, or natural language understanding datasets) before using them as utterance encoders for training an intent classifier. In this paper, we show that continual pre-training may not be essential, since the overfitting problem of PLMs on this task may not be as serious as expected. Specifically, we find that directly fine-tuning PLMs on only a handful of labeled examples already yields decent results compared to methods that employ continual pre-training, and the performance gap diminishes rapidly as the number of labeled data increases. To maximize the utilization of the limited available data, we propose a context augmentation method and leverage sequential self-distillation to boost performance. Comprehensive experiments on real-world benchmarks show that given only two or more labeled samples per class, direct fine-tuning outperforms many strong baselines that utilize external data sources for continual pre-training. The code can be found at https://github.com/hdzhang-code/DFTPlus.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.706.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the overfitting problem of PLMs on this task may not be as serious as expected.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the overfitting problem of PLMs on this task may not be as serious as expected.\""
    },
    {
        "title": "Improving Contrastive Learning of Sentence Embeddings from AI Feedback",
        "authors": [
            "Qinyuan Cheng",
            "Xiaogui Yang",
            "Tianxiang Sun",
            "Linyang Li",
            "Xipeng Qiu"
        ],
        "published": "2023",
        "summary": "Contrastive learning has become a popular approach in natural language processing, particularly for the learning of sentence embeddings.However, the discrete nature of natural language makes it difficult to ensure the quality of positive and negative sample pairs generated through data augmentation methods. Although supervised contrastive learning can produce more accurate sample pairs with human feedback labels, it still lacks fine-grained training signals. In this paper, we propose to improve Contrastive Learning of sentence embeddings from AI Feedback (CLAIF).Our method utilizes AI feedback from large pre-trained language models (LLMs) to construct sample pairs with fine-grained sample similarity scores to improve contrastive learning. Besides, we combine human feedback and AI feedback to provide better supervision signals for supervised contrastive learning of sentence embeddings.Experimental results show that our method achieves state-of-the-art performance on several semantic textual similarity (STS) and transfer learning tasks compared to other unsupervised and supervised contrastive learning methods.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.707.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although supervised contrastive learning can produce more accurate sample pairs with human feedback labels, it still lacks fine-grained training signals.\"\n\n(Note: The paper mentions LLMs, but does not discuss their limitations in detail, it only mentions the limitation of supervised contrastive learning which is not directly related to LLMs)",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Although supervised contrastive learning can produce more accurate sample pairs with human feedback labels, it still lacks fine-grained training signals.\"\n\n(Note: The paper mentions LLMs, but does not discuss their limitations in detail, it only mentions the limitation of supervised contrastive learning which is not directly related to LLMs)"
    },
    {
        "title": "Text Augmented Open Knowledge Graph Completion via Pre-Trained Language Models",
        "authors": [
            "Pengcheng Jiang",
            "Shivam Agarwal",
            "Bowen Jin",
            "Xuan Wang",
            "Jimeng Sun",
            "Jiawei Han"
        ],
        "published": "2023",
        "summary": "The mission of open knowledge graph (KG) completion is to draw new findings from known facts. Existing works that augment KG completion require either (1) factual triples to enlarge the graph reasoning space or (2) manually designed prompts to extract knowledge from a pre-trained language model (PLM), exhibiting limited performance and requiring expensive efforts from experts. To this end, we propose TagReal that automatically generates quality query prompts and retrieves support information from large text corpora to probe knowledge from PLM for KG completion. The results show that TagReal achieves state-of-the-art performance on two benchmark datasets. We find that TagReal has superb performance even with limited training data, outperforming existing embedding-based, graph-based, and PLM-based methods.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.709.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but \"pre-trained language model (PLM)\" implies the discussion of limitations is not present in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but \"pre-trained language model (PLM)\" implies the discussion of limitations is not present in the abstract."
    },
    {
        "title": "Discourse Analysis via Questions and Answers: Parsing Dependency Structures of Questions Under Discussion",
        "authors": [
            "Wei-Jen Ko",
            "Yating Wu",
            "Cutter Dalton",
            "Dananjay Srinivas",
            "Greg Durrett",
            "Junyi Jessy Li"
        ],
        "published": "2023",
        "summary": "Automatic discourse processing is bottlenecked by data: current discourse formalisms pose highly demanding annotation tasks involving large taxonomies of discourse relations, making them inaccessible to lay annotators. This work instead adopts the linguistic framework of Questions Under Discussion (QUD) for discourse analysis and seeks to derive QUD structures automatically. QUD views each sentence as an answer to a question triggered in prior context; thus, we characterize relationships between sentences as free-form questions, in contrast to exhaustive fine-grained taxonomies. We develop the first-of-its-kind QUD parser that derives a dependency structure of questions over full documents, trained using a large, crowdsourced question-answering dataset DCQA (Ko et al., 2022). Human evaluation results show that QUD dependency parsing is possible for language models trained with this crowdsourced, generalizable annotation scheme. We illustrate how our QUD structure is distinct from RST trees, and demonstrate the utility of QUD analysis in the context of document simplification. Our findings show that QUD parsing is an appealing alternative for automatic discourse processing.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.710.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Human evaluation results show that QUD dependency parsing is possible for language models trained with this crowdsourced, generalizable annotation scheme.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Human evaluation results show that QUD dependency parsing is possible for language models trained with this crowdsourced, generalizable annotation scheme.\""
    },
    {
        "title": "JECC: Commonsense Reasoning Tasks Derived from Interactive Fictions",
        "authors": [
            "Mo Yu",
            "Yi Gu",
            "Xiaoxiao Guo",
            "Yufei Feng",
            "Xiaodan Zhu",
            "Michael Greenspan",
            "Murray Campbell",
            "Chuang Gan"
        ],
        "published": "2023",
        "summary": "Commonsense reasoning simulates the human ability to make presumptions about our physical world, and it is an essential cornerstone in building general AI systems. We proposea new commonsense reasoning dataset based on human’s Interactive Fiction (IF) gameplaywalkthroughs as human players demonstrate plentiful and diverse commonsense reasoning. The new dataset provides a natural mixture of various reasoning types and requires multi-hopreasoning. Moreover, the IF game-based construction procedure requires much less humaninterventions than previous ones. Different from existing benchmarks, our dataset focuseson the assessment of functional commonsense knowledge rules rather than factual knowledge. Hence, in order to achieve higher performance on our tasks, models need to effectively uti-lize such functional knowledge to infer the outcomes of actions, rather than relying solely onmemorizing facts. Experiments show that the introduced dataset is challenging to previousmachine reading models as well as the new large language models with a significant 20%performance gap compared to human experts.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.713.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Experiments show that the introduced dataset is challenging to previous machine reading models as well as the new large language models with a significant 20% performance gap compared to human experts.\"\n\nThis rating is chosen because the abstract mentions a limitation of LLMs (performance gap compared to human experts) but does not explore it in depth. The primary focus of the paper is on introducing a new",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Experiments show that the introduced dataset is challenging to previous machine reading models as well as the new large language models with a significant 20% performance gap compared to human experts.\"\n\nThis rating is chosen because the abstract mentions a limitation of LLMs (performance gap compared to human experts) but does not explore it in depth. The primary focus of the paper is on introducing a new"
    },
    {
        "title": "A Study on Knowledge Distillation from Weak Teacher for Scaling Up Pre-trained Language Models",
        "authors": [
            "Hayeon Lee",
            "Rui Hou",
            "Jongpil Kim",
            "Davis Liang",
            "Sung Ju Hwang",
            "Alexander Min"
        ],
        "published": "2023",
        "summary": "Distillation from Weak Teacher (DWT) is a method of transferring knowledge from a smaller, weaker teacher model to a larger student model to improve its performance. Previous studies have shown that DWT can be effective in the vision domain and natural language processing (NLP) pre-training stage. Specifically, DWT shows promise in practical scenarios, such as enhancing new generation or larger models using pre-trained yet older or smaller models and lacking a resource budget. However, the optimal conditions for using DWT have yet to be fully investigated in NLP pre-training. Therefore, this study examines three key factors to optimize DWT, distinct from those used in the vision domain or traditional knowledge distillation. These factors are:(i) the impact of teacher model quality on DWT effectiveness, (ii) guidelines for adjusting the weighting value for DWT loss, and (iii) the impact of parameter remapping as a student model initialization technique for DWT.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.714.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"lacking a resource budget\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"lacking a resource budget\""
    },
    {
        "title": "SORTIE: Dependency-Aware Symbolic Reasoning for Logical Data-to-text Generation",
        "authors": [
            "Xueliang Zhao",
            "Tingchen Fu",
            "Lemao Liu",
            "Lingpeng Kong",
            "Shuming Shi",
            "Rui Yan"
        ],
        "published": "2023",
        "summary": "Logical data-to-text generation is a representative task in measuring the capabilities of both language generation and complex reasoning. Despite the introduction of reasoning skills in generation, existing works still rely on neural language models to output the final table description. However, due to the inefficacy of neural language models in complex reasoning, these methods inevitably have difficulty working out key entities in the description and might produce unfaithful descriptions. To alleviate these issues, we propose a dependency-aware symbolic reasoning framework that reasons out each entity in the table description with our designed table-compatible programming language. To figure out the dependency relationship among entities, we devise an entity scheduling mechanism to determine the order of programme synthesis such that the reasoning of an entity only relies on other “resolved” entities. Experiments on three datasets and three backbones show that ours outperforms previous methods not only in surface-level fidelity but also in logical fidelity. Notably, the proposed framework enhances GPT-2, BART and T5 with an absolute improvement of 5.7%~11.5% on SP-Acc.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.715.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, due to the inefficacy of neural language models in complex reasoning, these methods inevitably have difficulty working out key entities in the description and might produce unfaithful descriptions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, due to the inefficacy of neural language models in complex reasoning, these methods inevitably have difficulty working out key entities in the description and might produce unfaithful descriptions.\""
    },
    {
        "title": "Boosting Event Extraction with Denoised Structure-to-Text Augmentation",
        "authors": [
            "Bo Wang",
            "Heyan Huang",
            "Xiaochi Wei",
            "Ge Shi",
            "Xiao Liu",
            "Chong Feng",
            "Tong Zhou",
            "Shuaiqiang Wang",
            "Dawei Yin"
        ],
        "published": "2023",
        "summary": "Event extraction aims to recognize pre-defined event triggers and arguments from texts, which suffer from the lack of high-quality annotations. In most NLP applications, involving a large scale of synthetic training data is a practical and effective approach to alleviate the problem of data scarcity. However, when applying to the task of event extraction, recent data augmentation methods often neglect the problem of grammatical incorrectness, structure misalignment, and semantic drifting, leading to unsatisfactory performances. In order to solve these problems, we propose a denoised structure-to-text augmentation framework for event extraction (DAEE), which generates additional training data through the knowledge-based structure-to-text generation model and selects the effective subset from the generated data iteratively with a deep reinforcement learning agent. Experimental results on several datasets demonstrate that the proposed method generates more diverse text representations for event extraction and achieves comparable results with the state-of-the-art.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.716.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "A Simple, Yet Effective Approach to Finding Biases in Code Generation",
        "authors": [
            "Spyridon Mouselinos",
            "Mateusz Malinowski",
            "Henryk Michalewski"
        ],
        "published": "2023",
        "summary": "Recently, high-performing code generation systems based on large language models have surfaced. They are trained on massive corpora containing much more natural text than actual executable computer code. This work shows that current code generation systems exhibit undesired biases inherited from their large language model backbones, which can reduce the quality of the generated code under specific circumstances. To investigate the effect, we propose the “block of influence” concept, which enables a modular decomposition and analysis of the coding challenges. We introduce an automated intervention mechanism reminiscent of adversarial testing that exposes undesired biases through the failure modes of the models under test. Finally, we demonstrate how our framework can be used as a data transformation technique during fine-tuning, acting as a mitigation strategy for these biases.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.718.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"They are trained on massive corpora containing much more natural text than actual executable computer code. This work shows that current code generation systems exhibit undesired biases inherited from their large language model backbones, which can reduce the quality of the generated code under specific circumstances.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"They are trained on massive corpora containing much more natural text than actual executable computer code. This work shows that current code generation systems exhibit undesired biases inherited from their large language model backbones, which can reduce the quality of the generated code under specific circumstances.\""
    },
    {
        "title": "Membership Inference Attacks against Language Models via Neighbourhood Comparison",
        "authors": [
            "Justus Mattern",
            "Fatemehsadat Mireshghallah",
            "Zhijing Jin",
            "Bernhard Schoelkopf",
            "Mrinmaya Sachan",
            "Taylor Berg-Kirkpatrick"
        ],
        "published": "2023",
        "summary": "Membership Inference attacks (MIAs) aim to predict whether a data sample was present in the training data of a machine learning model or not, and are widely used for assessing the privacy risks of language models. Most existing attacks rely on the observation that models tend toassign higher probabilities to their training samples than non-training points. However, simple thresholding of the model score in isolation tends to lead to high false-positive rates as it does not account for the intrinsic complexity of a sample. Recent work has demonstrated that reference-based attacks which compare model scores to those obtained from a reference model trained on similar data can substantially improve the performance of MIAs.However, in order to train reference models, attacks of this kind make the strong and arguably unrealistic assumption that an adversary has access to samples closely resembling the original training data. Therefore, we investigate their performance in more realistic scenarios and find that they are highly fragile in relation to the data distribution used to train reference models. To investigate whether this fragility provides a layer of safety, we propose and evaluate neighbourhood attacks, which compare model scores for a given sample to scores of synthetically generated neighbour texts and therefore eliminate the need for access to the training data distribution. We show that, in addition to being competitive with reference-based attacks that have perfect knowledge about the training data distribution, our attack clearly outperforms existing reference-free attacks as well as reference-based attacks with imperfect knowledge, which demonstrates the need for a reevaluation of the threat model of adversarial attacks.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.719.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, simple thresholding of the model score in isolation tends to lead to high false-positive rates as it does not account for the intrinsic complexity of a sample\"; \"we find that they are highly fragile in relation to the data distribution used to train reference models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, simple thresholding of the model score in isolation tends to lead to high false-positive rates as it does not account for the intrinsic complexity of a sample\"; \"we find that they are highly fragile in relation to the data distribution used to train reference models.\""
    },
    {
        "title": "CFL: Causally Fair Language Models Through Token-level Attribute Controlled Generation",
        "authors": [
            "Rahul Madhavan",
            "Rishabh Garg",
            "Kahini Wadhawan",
            "Sameep Mehta"
        ],
        "published": "2023",
        "summary": "We propose a method to control the attributes of Language Models (LMs) for the text generation task using Causal Average Treatment Effect (ATE) scores and counterfactual augmentation. We explore this method, in the context of LM detoxification, and propose the Causally Fair Language (CFL) architecture for detoxifying pre-trained LMs in a plug-and-play manner. Our architecture is based on a Structural Causal Model (SCM) that is mathematically transparent and computationally efficient as compared with many existing detoxification techniques. We also propose several new metrics that aim to better understand the behaviour of LMs in the context of toxic text generation. Further, we achieve state of the art performance for toxic degeneration, which are computed using Real Toxicity Prompts. Our experiments show that CFL achieves such a detoxification without much impact on the model perplexity. We also show that CFL mitigates the unintended bias problem through experiments on the BOLD dataset.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.720.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We also show that CFL mitigates the unintended bias problem through experiments on the BOLD dataset.\"\n\nThis rating is based on the fact that the paper mentions a limitation of LLMs (unintended bias) but only briefly and as a minor detail, without exploring it in depth. The primary focus of the paper is on the proposed solution (CFL architecture) and its performance",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We also show that CFL mitigates the unintended bias problem through experiments on the BOLD dataset.\"\n\nThis rating is based on the fact that the paper mentions a limitation of LLMs (unintended bias) but only briefly and as a minor detail, without exploring it in depth. The primary focus of the paper is on the proposed solution (CFL architecture) and its performance"
    },
    {
        "title": "Recyclable Tuning for Continual Pre-training",
        "authors": [
            "Yujia Qin",
            "Cheng Qian",
            "Xu Han",
            "Yankai Lin",
            "Huadong Wang",
            "Ruobing Xie",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Jie Zhou"
        ],
        "published": "2023",
        "summary": "Continual pre-training is the paradigm where pre-trained language models (PLMs) continually acquire fresh knowledge from growing data and gradually get upgraded. Before an upgraded PLM is released, we may have tuned the original PLM for various tasks and stored the adapted weights. However, when tuning the upgraded PLM, these outdated adapted weights will typically be ignored and discarded, causing a potential waste of resources. We bring this issue to the forefront and contend that proper algorithms for recycling outdated adapted weights should be developed. To this end, we formulate the task of recyclable tuning for continual pre-training. In pilot studies, we find that after continual pre-training, the upgraded PLM remains compatible with the outdated adapted weights to some extent. Motivated by this finding, we analyze the connection between continually pre-trained PLMs from two novel aspects, i.e., mode connectivity, and functional similarity. Based on the corresponding findings, we propose both an initialization-based method and a distillation-based method for our task. We demonstrate their feasibility in improving the convergence and performance for tuning the upgraded PLM. We also show that both methods can be combined to achieve better performance.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.723.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, when tuning the upgraded PLM, these outdated adapted weights will typically be ignored and discarded, causing a potential waste of resources.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, when tuning the upgraded PLM, these outdated adapted weights will typically be ignored and discarded, causing a potential waste of resources.\""
    },
    {
        "title": "HyperPELT: Unified Parameter-Efficient Language Model Tuning for Both Language and Vision-and-Language Tasks",
        "authors": [
            "Zhengkun Zhang",
            "Wenya Guo",
            "Xiaojun Meng",
            "Yasheng Wang",
            "Yadao Wang",
            "Xin Jiang",
            "Qun Liu",
            "Zhenglu Yang"
        ],
        "published": "2023",
        "summary": "With the scale and capacity of pretrained models growing rapidly, parameter-efficient language model tuning has emerged as a popular paradigm for solving various NLP and Vision-and-Language (V&L) tasks. In this paper, we design a unified parameter-efficient multitask learning framework that works effectively on both NLP and V&L tasks. In particular, we use a shared hypernetwork that takes trainable hyper-embeddings and visual modality as input, and outputs weights for different modules in a pretrained language model, such as the parameters inserted into multi-head attention blocks (i.e., prefix-tuning) and feed-forward blocks (i.e., adapter-tuning.). Our proposed framework adds fewer trainable parameters in multi-task learning while achieving superior performances and transfer ability compared to state-of-the-art methods. Empirical results on the GLUE benchmark and multiple V&L tasks confirm the effectiveness of our framework.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.725.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "Generating Labeled Data for Relation Extraction: A Meta Learning Approach with Joint GPT-2 Training",
        "authors": [
            "Amir Pouran Ben Veyseh",
            "Franck Dernoncourt",
            "Bonan Min",
            "Thien Nguyen"
        ],
        "published": "2023",
        "summary": "Relation Extraction (RE) is the task of identifying semantic relation between real-world entities mentioned in text. Despite significant progress in RE research, a remaining challenge for RE concerns the lack of training data for data-hungry deep learning models. Cost of annotation and difficulty of the task are among hindrance to collect a large-scale RE dataset in different domains. To address this limitation, we propose a novel framework to automatically generate labeled data for RE. Our framework presents the pre-trained language model GPT-2 for data generation. In addition, to optimize the generated samples for an RE model, we introduce a meta learning approach to allow the GPT-2 model to be updated during the training process for RE. In particular, to leverage the feedback from the RE model to improve the data generation from GPT-2, we propose a novel reward function to update the GPT-2 model with REINFORCE, seeking to promote the similarity of the RE loss function’s gradients computed for generated data and a meta development set. We conduct extensive experiments on two benchmark datasets to produce state-of-the-art performance for RE.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.727.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Cost of annotation and difficulty of the task are among hindrance to collect a large-scale RE dataset in different domains.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Cost of annotation and difficulty of the task are among hindrance to collect a large-scale RE dataset in different domains.\""
    },
    {
        "title": "Disfluency Generation for More Robust Dialogue Systems",
        "authors": [
            "Benjamin Marie"
        ],
        "published": "2023",
        "summary": "Disfluencies in user utterances can trigger a chain of errors impacting all the modules of a dialogue system: natural language understanding, dialogue state tracking, and response generation. In this work, we first analyze existing dialogue datasets commonly used in research and show that they only contain a marginal number of disfluent utterances. Due to this relative absence of disfluencies in their training data, dialogue systems may then critically fail when exposed to disfluent utterances. Following this observation, we propose to augment existing datasets with disfluent user utterances by paraphrasing fluent utterances into disfluent ones. Relying on a pre-trained language model, our few-shot disfluent paraphraser guided by a disfluency classifier can generate useful disfluent utterances for training better dialogue systems. We report on improvements for both dialogue state tracking and response generation when the dialogue systems are trained on datasets augmented with our disfluent utterances.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.728.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Due to this relative absence of disfluencies in their training data, dialogue systems may then critically fail when exposed to disfluent utterances.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Due to this relative absence of disfluencies in their training data, dialogue systems may then critically fail when exposed to disfluent utterances.\""
    },
    {
        "title": "Dipping PLMs Sauce: Bridging Structure and Text for Effective Knowledge Graph Completion via Conditional Soft Prompting",
        "authors": [
            "Chen Chen",
            "Yufei Wang",
            "Aixin Sun",
            "Bing Li",
            "Kwok-Yan Lam"
        ],
        "published": "2023",
        "summary": "Knowledge Graph Completion (KGC) often requires both KG structural and textual information to be effective. Pre-trained Language Models (PLMs) have been used to learn the textual information, usually under the fine-tune paradigm for the KGC task. However, the fine-tuned PLMs often overwhelmingly focus on the textual information and overlook structural knowledge. To tackle this issue, this paper proposes CSProm-KG (Conditional Soft Prompts for KGC) which maintains a balance between structural information and textual knowledge. CSProm-KG only tunes the parameters of Conditional Soft Prompts that are generated by the entities and relations representations. We verify the effectiveness of CSProm-KG on three popular static KGC benchmarks WN18RR, FB15K-237 and Wikidata5M, and two temporal KGC benchmarks ICEWS14 and ICEWS05-15. CSProm-KG outperforms competitive baseline models and sets new state-of-the-art on these benchmarks. We conduct further analysis to show (i) the effectiveness of our proposed components, (ii) the efficiency of CSProm-KG, and (iii) the flexibility of CSProm-KG.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.729.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the fine-tuned PLMs often overwhelmingly focus on the textual information and overlook structural knowledge.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the fine-tuned PLMs often overwhelmingly focus on the textual information and overlook structural knowledge.\""
    },
    {
        "title": "FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference",
        "authors": [
            "Michiel de Jong",
            "Yury Zemlyanskiy",
            "Joshua Ainslie",
            "Nicholas FitzGerald",
            "Sumit Sanghai",
            "Fei Sha",
            "William Cohen"
        ],
        "published": "2023",
        "summary": "Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that sets the state-of-the-art on many knowledge-intensive NLP tasks. However, the architecture used for FiD was chosen by making minimal modifications to a standard T5 model, which our analysis shows to be highly suboptimal for a retrieval-augmented model. In particular, FiD allocates the bulk of FLOPs to the encoder, while the majority of inference time results from memory bandwidth constraints in the decoder. We propose two simple changes to the FiD architecture to alleviate memory bandwidth constraints, and speed up inference by 7x. This allows us to use a much larger decoder at modest cost. We denote FiD with the above modifications as FiDO, and show that it strongly improves performance over existing FiD models for a wide range of inference budgets. For example, FiDO-Large-XXL performs faster inference than FiD-Base and achieves better performance than FiD-Large.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.732.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the architecture used for FiD was chosen by making minimal modifications to a standard T5 model, which our analysis shows to be highly suboptimal for a retrieval-augmented model.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the architecture used for FiD was chosen by making minimal modifications to a standard T5 model, which our analysis shows to be highly suboptimal for a retrieval-augmented model.\""
    },
    {
        "title": "Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark",
        "authors": [
            "Jason Hoelscher-Obermaier",
            "Julia Persson",
            "Esben Kran",
            "Ioannis Konstas",
            "Fazl Barez"
        ],
        "published": "2023",
        "summary": "Recent model editing techniques promise to mitigate the problem of memorizing false or outdated associations during LLM training. However, we show that these techniques can introduce large unwanted side effects which are not detected by existing specificity benchmarks. We extend the existing CounterFact benchmark to include a dynamic component and dub our benchmark CounterFact+. Additionally, we extend the metrics used for measuring specificity by a principled KL divergence-based metric. We use this improved benchmark to evaluate recent model editing techniques and find that they suffer from low specificity. Our findings highlight the need for improved specificity benchmarks that identify and prevent unwanted side effects.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.733.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, we show that these techniques can introduce large unwanted side effects which are not detected by existing specificity benchmarks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, we show that these techniques can introduce large unwanted side effects which are not detected by existing specificity benchmarks.\""
    },
    {
        "title": "Structure-Aware Language Model Pretraining Improves Dense Retrieval on Structured Data",
        "authors": [
            "Xinze Li",
            "Zhenghao Liu",
            "Chenyan Xiong",
            "Shi Yu",
            "Yu Gu",
            "Zhiyuan Liu",
            "Ge Yu"
        ],
        "published": "2023",
        "summary": "This paper presents Structure Aware Dense Retrieval (SANTA) model, which encodes user queries and structured data in one universal embedding space for retrieving structured data. SANTA proposes two pretraining methods to make language models structure-aware and learn effective representations for structured data: 1) Structured Data Alignment, which utilizes the natural alignment relations between structured data and unstructured data for structure-aware pretraining. It contrastively trains language models to represent multi-modal text data and teaches models to distinguish matched structured data for unstructured texts. 2) Masked Entity Prediction, which designs an entity-oriented mask strategy and asks language models to fill in the masked entities. Our experiments show that SANTA achieves state-of-the-art on code search and product search and conducts convincing results in the zero-shot setting. SANTA learns tailored representations for multi-modal text data by aligning structured and unstructured data pairs and capturing structural semantics by masking and predicting entities in the structured data. All codes are available at https://github.com/OpenMatch/OpenMatch.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.734.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the title mentions \"Structure-Aware Language Model Pretraining\" implying that the paper addresses a limitation of LLMs in handling structured data, however, the abstract does not explicitly discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the title mentions \"Structure-Aware Language Model Pretraining\" implying that the paper addresses a limitation of LLMs in handling structured data, however, the abstract does not explicitly discuss any limitations of LLMs."
    },
    {
        "title": "AxomiyaBERTa: A Phonologically-aware Transformer Model for Assamese",
        "authors": [
            "Abhijnan Nath",
            "Sheikh Mannan",
            "Nikhil Krishnaswamy"
        ],
        "published": "2023",
        "summary": "Despite their successes in NLP, Transformer-based language models still require extensive computing resources and suffer in low-resource or low-compute settings. In this paper, we present AxomiyaBERTa, a novel BERT model for Assamese, a morphologically-rich low-resource language (LRL) of Eastern India. AxomiyaBERTa is trained only on the masked language modeling (MLM) task, without the typical additional next sentence prediction (NSP) objective, and our results show that in resource-scarce settings for very low-resource languages like Assamese, MLM alone can be successfully leveraged for a range of tasks. AxomiyaBERTa achieves SOTA on token-level tasks like Named Entity Recognition and also performs well on “longer-context” tasks like Cloze-style QA and Wiki Title Prediction, with the assistance of a novel embedding disperser and phonological signals respectively. Moreover, we show that AxomiyaBERTa can leverage phonological signals for even more challenging tasks, such as a novel cross-document coreference task on a translated version of the ECB+ corpus, where we present a new SOTA result for an LRL. Our source code and evaluation scripts may be found at https://github.com/csu-signal/axomiyaberta.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.739.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite their successes in NLP, Transformer-based language models still require extensive computing resources and suffer in low-resource or low-compute settings.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite their successes in NLP, Transformer-based language models still require extensive computing resources and suffer in low-resource or low-compute settings.\""
    },
    {
        "title": "An Exploratory Study on Model Compression for Text-to-SQL",
        "authors": [
            "Shuo Sun",
            "Yuze Gao",
            "Yuchen Zhang",
            "Jian Su",
            "Bin Chen",
            "Yingzhan Lin",
            "Shuqi Sun"
        ],
        "published": "2023",
        "summary": "Text-to-SQL translates user queries into SQL statements that can retrieve relevant answers from relational databases. Recent approaches to Text-to-SQL rely on pre-trained language models that are computationally expensive and technically challenging to deploy in real-world applications that require real-time or on-device processing capabilities. In this paper, we perform a focused study on the feasibility of applying recent model compression techniques to sketch-based and sequence-to-sequence Text-to-SQL models. Our results reveal that sketch-based Text-to-SQL models generally have higher inference efficiency and respond better to model compression than sequence-to-sequence models, making them ideal for real-world deployments, especially in use cases with simple SQL statements.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.740.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent approaches to Text-to-SQL rely on pre-trained language models that are computationally expensive and technically challenging to deploy in real-world applications that require real-time or on-device processing capabilities.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Recent approaches to Text-to-SQL rely on pre-trained language models that are computationally expensive and technically challenging to deploy in real-world applications that require real-time or on-device processing capabilities.\""
    },
    {
        "title": "KoRC: Knowledge Oriented Reading Comprehension Benchmark for Deep Text Understanding",
        "authors": [
            "Zijun Yao",
            "Yantao Liu",
            "Xin Lv",
            "Shulin Cao",
            "Jifan Yu",
            "Juanzi Li",
            "Lei Hou"
        ],
        "published": "2023",
        "summary": "Deep text understanding, which requires the connections between a given document and prior knowledge beyond its text, has been highlighted by many benchmarks in recent years. However, these benchmarks have encountered two major limitations. On the one hand, most of them require human annotation of knowledge, which leads to limited knowledge coverage. On the other hand, they usually use choices or spans in the texts as the answers, which results in narrow answer space. To overcome these limitations, we build a new challenging benchmark named KoRC in this paper. Compared with previous benchmarks, KoRC has two advantages, i.e., broad knowledge coverage and flexible answer format. Specifically, we utilize massive knowledge bases to guide annotators or large language models (LLMs) to construct knowledgable questions. Moreover, we use labels in knowledge bases rather than spans or choices as the final answers. We test state-of-the-art models on KoRC and the experimental results show that the strongest baseline only achieves 68.3% and 30.0% F1 measure in the IID and OOD test set, respectively. These results indicate that deep text understanding is still an unsolved challenge. We will release our dataset and baseline methods upon acceptance.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.743.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"These results indicate that deep text understanding is still an unsolved challenge.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"These results indicate that deep text understanding is still an unsolved challenge.\""
    },
    {
        "title": "DKAF: KB Arbitration for Learning Task-Oriented Dialog Systems with Dialog-KB Inconsistencies",
        "authors": [
            "Vishal Saley",
            "Rocktim Das",
            "Dinesh Raghu",
            "Mausam"
        ],
        "published": "2023",
        "summary": "Task-oriented dialog (TOD) agents often ground their responses on external knowledge bases (KBs). These KBs can be dynamic and may be updated frequently. Existing approaches for learning TOD agents assume the KB snapshot contemporary to each individual dialog is available during training. However, in real-world scenarios, only the latest KB snapshot is available during training and as a result, the train dialogs may contain facts conflicting with the latest KB. These dialog-KB inconsistencies in the training data may potentially confuse the TOD agent learning algorithm. In this work, we define the novel problem of learning a TOD agent with dialog-KB inconsistencies in the training data. We propose a Dialog-KB Arbitration Framework (DKAF) which reduces the dialog-KB inconsistencies by predicting the contemporary KB snapshot for each train dialog. These predicted KB snapshots are then used for training downstream TOD agents. As there are no existing datasets with dialog-KB inconsistencies, we systematically introduce inconsistencies in two publicly available dialog datasets. We show that TOD agents trained with DKAF perform better than existing baselines on both these datasets.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.744.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval",
        "authors": [
            "Yue Yu",
            "Yuchen Zhuang",
            "Rongzhi Zhang",
            "Yu Meng",
            "Jiaming Shen",
            "Chao Zhang"
        ],
        "published": "2023",
        "summary": "With the development of large language models (LLMs), zero-shot learning has attracted much attention for various NLP tasks. Different from prior works that generate training data with billion-scale natural language generation (NLG) models, we propose a retrieval-enhanced framework to create training data from a general-domain unlabeled corpus. To realize this, we first conduct contrastive pretraining to learn an unsupervised dense retriever for extracting the most relevant documents using class-descriptive verbalizers. We then further pro- pose two simple strategies, namely Verbalizer Augmentation with Demonstrations and Self- consistency Guided Filtering to improve the topic coverage of the dataset while removing noisy examples. Experiments on nine datasets demonstrate that ReGen achieves 4.3% gain over the strongest baselines and saves around 70% of the time when compared with baselines using large NLG models. Besides, REGEN can be naturally integrated with recently proposed large language models to boost performance.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.748.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicitly mentioned, but the paper proposes a new method to improve performance, implying that existing methods (potentially including LLMs) have limitations, such as being time-consuming or less effective.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None explicitly mentioned, but the paper proposes a new method to improve performance, implying that existing methods (potentially including LLMs) have limitations, such as being time-consuming or less effective."
    },
    {
        "title": "Race, Gender, and Age Biases in Biomedical Masked Language Models",
        "authors": [
            "Michelle Kim",
            "Junghwan Kim",
            "Kristen Johnson"
        ],
        "published": "2023",
        "summary": "Biases cause discrepancies in healthcare services. Race, gender, and age of a patient affect interactions with physicians and the medical treatments one receives. These biases in clinical practices can be amplified following the release of pre-trained language models trained on biomedical corpora. To bring awareness to such repercussions, we examine social biases present in the biomedical masked language models. We curate prompts based on evidence-based practice and compare generated diagnoses based on biases. For a case study, we measure bias in diagnosing coronary artery disease and using cardiovascular procedures based on bias. Our study demonstrates that biomedical models are less biased than BERT in gender, while the opposite is true for race and age.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.749.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"These biases in clinical practices can be amplified following the release of pre-trained language models trained on biomedical corpora... Our study demonstrates that biomedical models are less biased than BERT in gender, while the opposite is true for race and age.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"These biases in clinical practices can be amplified following the release of pre-trained language models trained on biomedical corpora... Our study demonstrates that biomedical models are less biased than BERT in gender, while the opposite is true for race and age.\""
    },
    {
        "title": "HELP ME THINK: A Simple Prompting Strategy for Non-experts to Create Customized Content with Models",
        "authors": [
            "Swaroop Mishra",
            "Elnaz Nouri"
        ],
        "published": "2023",
        "summary": "Controlling the text generated by language models and customizing the content has been a long-standing challenge. Existing prompting techniques proposed in pursuit of providing control are task-specific and lack generality; this provides overwhelming choices for non-expert users to find a suitable method for their task. The effort associated with those techniques, such as in writing examples, explanations, instructions, etc. further limits their adoption among non-expert users. In this paper, we propose a simple prompting strategy Help Me Think where we encourage largelanguage models (such as GPT3 and ChatGPT) to help non-expert users by asking a set of relevant questions and leveraging user answers to execute the task. We demonstrate the efficacy of our technique Help Me Think on a variety of tasks. Specifically, we focus on tasks that are hard for average humans and require significant thinking to perform. We hope our work will encourage the development of unconventional ways to harness the power of large language models.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.751.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing prompting techniques proposed in pursuit of providing control are task-specific and lack generality; this provides overwhelming choices for non-expert users to find a suitable method for their task.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing prompting techniques proposed in pursuit of providing control are task-specific and lack generality; this provides overwhelming choices for non-expert users to find a suitable method for their task.\""
    },
    {
        "title": "Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language Model Control",
        "authors": [
            "Xiang Fan",
            "Yiwei Lyu",
            "Paul Pu Liang",
            "Ruslan Salakhutdinov",
            "Louis-Philippe Morency"
        ],
        "published": "2023",
        "summary": "Pretrained language models have demonstrated extraordinary capabilities in language generation. However, real-world tasks often require controlling the distribution of generated text in order to mitigate bias, promote fairness, and achieve personalization. Existing techniques for controlling the distribution of generated text only work with quantified distributions, which require pre-defined categories, proportions of the distribution, or an existing corpus following the desired distributions. However, many important distributions, such as personal preferences, are unquantified. In this work, we tackle the problem of generating text following arbitrary distributions (quantified and unquantified) by proposing NANO, a few-shot human-in-the-loop training algorithm that continuously learns from human feedback. NANO achieves state-of-the-art results on single topic/attribute as well as quantified distribution control compared to previous works. We also show that NANO is able to learn unquantified distributions, achieves personalization, and captures differences between different individuals’ personal preferences with high sample efficiency.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.758.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, real-world tasks often require controlling the distribution of generated text in order to mitigate bias, promote fairness, and achieve personalization.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, real-world tasks often require controlling the distribution of generated text in order to mitigate bias, promote fairness, and achieve personalization.\""
    },
    {
        "title": "Explanation Regeneration via Information Bottleneck",
        "authors": [
            "Qintong Li",
            "Zhiyong Wu",
            "Lingpeng Kong",
            "Wei Bi"
        ],
        "published": "2023",
        "summary": "Explaining the black-box predictions of NLP models naturally and accurately is an important open problem in natural language generation. These free-text explanations are expected to contain sufficient and carefully-selected evidence to form supportive arguments for predictions. Thanks to the superior generative capacity of large pretrained language models (PLM), recent work built on prompt engineering enables explanations generated without specific training. However, explanations generated through single-pass prompting often lack sufficiency and conciseness, due to the prompt complexity and hallucination issues. To discard the dross and take the essence of current PLM’s results, we propose to produce sufficient and concise explanations via the information bottleneck (EIB) theory. EIB regenerates explanations by polishing the single-pass output of PLM but retaining the information that supports the contents being explained by balancing two information bottleneck objectives. Experiments on two different tasks verify the effectiveness of EIB through automatic evaluation and thoroughly-conducted human evaluation.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.765.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, explanations generated through single-pass prompting often lack sufficiency and conciseness, due to the prompt complexity and hallucination issues.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, explanations generated through single-pass prompting often lack sufficiency and conciseness, due to the prompt complexity and hallucination issues.\""
    },
    {
        "title": "Neural Networks Against (and For) Self-Training: Classification with Small Labeled and Large Unlabeled Sets",
        "authors": [
            "Payam Karisani"
        ],
        "published": "2023",
        "summary": "We propose a semi-supervised text classifier based on self-training using one positive and one negative property of neural networks. One of the weaknesses of self-training is the semantic drift problem, where noisy pseudo-labels accumulate over iterations and consequently the error rate soars. In order to tackle this challenge, we reshape the role of pseudo-labels and create a hierarchical order of information. In addition, a crucial step in self-training is to use the classifier confidence prediction to select the best candidate pseudo-labels. This step cannot be efficiently done by neural networks, because it is known that their output is poorly calibrated. To overcome this challenge, we propose a hybrid metric to replace the plain confidence measurement. Our metric takes into account the prediction uncertainty via a subsampling technique. We evaluate our model in a set of five standard benchmarks, and show that it significantly outperforms a set of ten diverse baseline models. Furthermore, we show that the improvement achieved by our model is additive to language model pretraining, which is a widely used technique for using unlabeled documents.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.769.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"it is known that their output is poorly calibrated.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"it is known that their output is poorly calibrated.\""
    },
    {
        "title": "Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training",
        "authors": [
            "Jing Huang",
            "Zhengxuan Wu",
            "Kyle Mahowald",
            "Christopher Potts"
        ],
        "published": "2023",
        "summary": "Language tasks involving character-level manipulations (e.g., spelling corrections, arithmetic operations, word games) are challenging for models operating on subword units. To address this, we develop a causal intervention framework to learn robust and interpretable character representations inside subword-based language models. Our method treats each character as a typed variable in a causal model and learns such causal structures by adapting the interchange intervention training method of Geiger et al. (2021). We additionally introduce a suite of character-level tasks that systematically vary in their dependence on meaning and sequence-level context. While character-level models still perform best on purely form-based tasks like string reversal, our method outperforms character-level models on more complex tasks that blend form, meaning, and context, such as spelling correction in context and word search games. Compared with standard subword-based models, our approach also significantly improves robustness on unseen token sequences and leads to human-interpretable internal representations of characters.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.770.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Language tasks involving character-level manipulations (e.g., spelling corrections, arithmetic operations, word games) are challenging for models operating on subword units.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Language tasks involving character-level manipulations (e.g., spelling corrections, arithmetic operations, word games) are challenging for models operating on subword units.\""
    },
    {
        "title": "Language acquisition: do children and language models follow similar learning stages?",
        "authors": [
            "Linnea Evanson",
            "Yair Lakretz",
            "Jean Rémi King"
        ],
        "published": "2023",
        "summary": "During language acquisition, children follow a typical sequence of learning stages, whereby they first learn to categorize phonemes before they develop their lexicon and eventually master increasingly complex syntactic structures. However, the computational principles that lead to this learning trajectory remain largely unknown. To investigate this, we here compare the learning trajectories of deep language models to those of human children. Specifically, we test whether, during its training, GPT-2 exhibits stages of language acquisition comparable to those observed in children aged between 18 months and 6 years. For this, we train 48 GPT-2 models from scratch and evaluate their syntactic and semantic abilities at each training step, using 96 probes curated from the BLiMP, Zorro and BIG-Bench benchmarks. We then compare these evaluations with the behavior of 54 children during language production. Our analyses reveal three main findings. First, similarly to children, the language models tend to learn linguistic skills in a systematic order. Second, this learning scheme is parallel: the language tasks that are learned last improve from the very first training steps. Third, some – but not all – learning stages are shared between children and these language models. Overall, these results shed new light on the principles of language acquisition, and highlight important divergences in how humans and modern algorithms learn to process natural language.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.773.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Third, some – but not all – learning stages are shared between children and these language models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Third, some – but not all – learning stages are shared between children and these language models.\""
    },
    {
        "title": "The Role of Output Vocabulary in T2T LMs for SPARQL Semantic Parsing",
        "authors": [
            "Debayan Banerjee",
            "Pranav Nair",
            "Ricardo Usbeck",
            "Chris Biemann"
        ],
        "published": "2023",
        "summary": "In this work, we analyse the role of output vocabulary for text-to-text (T2T) models on the task of SPARQL semantic parsing. We perform experiments within the the context of knowledge graph question answering (KGQA), where the task is to convert questions in natural language to the SPARQL query language. We observe that the query vocabulary is distinct from human vocabulary. Language Models (LMs) are pre-dominantly trained for human language tasks, and hence, if the query vocabulary is replaced with a vocabulary more attuned to the LM tokenizer, the performance of models may improve. We carry out carefully selected vocabulary substitutions on the queries and find absolute gains in the range of 17% on the GrailQA dataset.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.774.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Language Models (LMs) are pre-dominantly trained for human language tasks, and hence, if the query vocabulary is replaced with a vocabulary more attuned to the LM tokenizer, the performance of models may improve.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Language Models (LMs) are pre-dominantly trained for human language tasks, and hence, if the query vocabulary is replaced with a vocabulary more attuned to the LM tokenizer, the performance of models may improve.\""
    },
    {
        "title": "Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation",
        "authors": [
            "Marius Mosbach",
            "Tiago Pimentel",
            "Shauli Ravfogel",
            "Dietrich Klakow",
            "Yanai Elazar"
        ],
        "published": "2023",
        "summary": "Few-shot fine-tuning and in-context learning are two alternative strategies for task adaptation of pre-trained language models. Recently, in-context learning has gained popularity over fine-tuning due to its simplicity and improved out-of-domain generalization, and because extensive evidence shows that fine-tuned models pick up on spurious correlations. Unfortunately, previous comparisons of the two approaches were done using models of different sizes. This raises the question of whether the observed weaker out-of-domain generalization of fine-tuned models is an inherent property of fine-tuning or a limitation of the experimental setup. In this paper, we compare the generalization of few-shot fine-tuning and in-context learning to challenge datasets, while controlling for the models used, the number of examples, and the number of parameters, ranging from 125M to 30B. Our results show that fine-tuned language models can in fact generalize well out-of-domain. We find that both approaches generalize similarly; they exhibit large variation and depend on properties such as model size and the number of examples, highlighting that robust task adaptation remains a challenge.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.779.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Unfortunately, previous comparisons of the two approaches were done using models of different sizes. This raises the question of whether the observed weaker out-of-domain generalization of fine-tuned models is an inherent property of fine-tuning or a limitation of the experimental setup.\"; \"highlighting that robust task adaptation remains a challenge.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Unfortunately, previous comparisons of the two approaches were done using models of different sizes. This raises the question of whether the observed weaker out-of-domain generalization of fine-tuned models is an inherent property of fine-tuning or a limitation of the experimental setup.\"; \"highlighting that robust task adaptation remains a challenge.\""
    },
    {
        "title": "HuaSLIM: Human Attention Motivated Shortcut Learning Identification and Mitigation for Large Language models",
        "authors": [
            "Yuqi Ren",
            "Deyi Xiong"
        ],
        "published": "2023",
        "summary": "Large language models have made remarkable progress on a variety of NLP tasks. However, it has been found that they tend to rely on shortcut features that spuriously correlate with labels for prediction, which weakens their generalization on out-of-distribution samples. In this paper, we propose a human attention guided approach to identifying and mitigating shortcut learning, which encourages the LLM-based target model to learn relevant features. We define an attention-based measurement to capture both model and data bias and identify shortcut tokens by exploring both human and neural attention. In a self-distillation framework, we mitigate shortcut learning by dynamically adjusting the distillation temperature according to the detected shortcut tokens and estimated shortcut degree. Additionally, we utilize human attention as a supervisory signal to constrain large language models to pay more attention to relevant tokens. Experimental results on multiple NLP tasks show that our proposed method can effectively identify shortcut tokens, and significantly improve the robustness of large language models on OOD samples, while not undermining the performance on IID data.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.781.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it has been found that they tend to rely on shortcut features that spuriously correlate with labels for prediction, which weakens their generalization on out-of-distribution samples.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, it has been found that they tend to rely on shortcut features that spuriously correlate with labels for prediction, which weakens their generalization on out-of-distribution samples.\""
    },
    {
        "title": "PMI-Align: Word Alignment With Point-Wise Mutual Information Without Requiring Parallel Training Data",
        "authors": [
            "Fatemeh Azadi",
            "Heshaam Faili",
            "Mohammad Javad Dousti"
        ],
        "published": "2023",
        "summary": "Word alignment has many applications including cross-lingual annotation projection, bilingual lexicon extraction, and the evaluation or analysis of translation outputs. Recent studies show that using contextualized embeddings from pre-trained multilingual language models could give us high quality word alignments without the need of parallel training data. In this work, we propose PMI-Align which computes and uses the point-wise mutual information between source and target tokens to extract word alignments, instead of the cosine similarity or dot product which is mostly used in recent approaches. Our experiments show that our proposed PMI-Align approach could outperform the rival methods on five out of six language pairs. Although our approach requires no parallel training data, we show that this method could also benefit the approaches using parallel data to fine-tune pre-trained language models on word alignments. Our code and data are publicly available.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.782.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "TEPrompt: Task Enlightenment Prompt Learning for Implicit Discourse Relation Recognition",
        "authors": [
            "Wei Xiang",
            "Chao Liang",
            "Bang Wang"
        ],
        "published": "2023",
        "summary": "Implicit Discourse Relation Recognition (IDRR) aims at classifying the relation sense between two arguments without an explicit connective. Recently, the ConnPrompt (Xiang et al., 2022) has leveraged the powerful prompt learning for IDRR based on the fusion of multi-prompt decisions from three different yet much similar connective prediction templates. Instead of multi-prompt ensembling, we propose to design auxiliary tasks with enlightened prompt learning for the IDRR task. Although an auxiliary task is not used to directly output final prediction, we argue that during the joint training some of its learned features can be useful to boost the main task. In light of such motivations, we propose a task enlightenment prompt learning model, called TEPrompt, to fuse learned features from three related tasks for IDRR. In particular, the TEPrompt contains three tasks, viz., Discourse Relation Recognition (DRR), Sense Semantics Classification (SSC) and Annotated Connective Prediction (ACP), each with a unique prompt template and an answer space. In the training phase, we jointly train three prompt learning tasks with shared argument representation. In the testing phase, we only take the DRR output with fused features as the final IDRR decision. Experiments with the same conditions have shown that the proposed TEPrompt outperforms the ConnPrompt. This can be attributed to the promoted decision features and language models benefited from joint-training of auxiliary tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.785.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"and language models benefited from joint-training of auxiliary tasks.\"\n\nThis evidence is not explicitly discussing limitations of LLMs, but rather mentions the benefits of joint-training of auxiliary tasks for LLMs, which implies that LLMs may not perform well without such training.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"and language models benefited from joint-training of auxiliary tasks.\"\n\nThis evidence is not explicitly discussing limitations of LLMs, but rather mentions the benefits of joint-training of auxiliary tasks for LLMs, which implies that LLMs may not perform well without such training."
    },
    {
        "title": "Distractor Generation based on Text2Text Language Models with Pseudo Kullback-Leibler Divergence Regulation",
        "authors": [
            "Hui-Juan Wang",
            "Kai-Yu Hsieh",
            "Han-Cheng Yu",
            "Jui-Ching Tsou",
            "Yu An Shih",
            "Chen-Hua Huang",
            "Yao-Chung Fan"
        ],
        "published": "2023",
        "summary": "In this paper, we address the task of cloze-style multiple choice question (MCQs) distractor generation. Our study is featured by the following designs. First, we propose to formulate the cloze distractor generation as a Text2Text task. Second, we propose pseudo Kullback-Leibler Divergence for regulating the generation to consider the item discrimination index in education evaluation. Third, we explore the candidate augmentation strategy and multi-tasking training with cloze-related tasks to further boost the generation performance. Through experiments with benchmarking datasets, our best perfomring model advances the state-of-the-art result from 10.81 to 22.00 (p@1 score).",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.790.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the title mentions \"Text2Text Language Models\", indicating the involvement of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the title mentions \"Text2Text Language Models\", indicating the involvement of LLMs."
    },
    {
        "title": "CausalDialogue: Modeling Utterance-level Causality in Conversations",
        "authors": [
            "Yi-Lin Tuan",
            "Alon Albalak",
            "Wenda Xu",
            "Michael Saxon",
            "Connor Pryor",
            "Lise Getoor",
            "William Yang Wang"
        ],
        "published": "2023",
        "summary": "Despite their widespread adoption, neural conversation models have yet to exhibit natural chat capabilities with humans. In this research, we examine user utterances as causes and generated responses as effects, recognizing that changes in a cause should produce a different effect. To further explore this concept, we have compiled and expanded upon a new dataset called CausalDialogue through crowd-sourcing. This dataset includes multiple cause-effect pairs within a directed acyclic graph (DAG) structure. Our analysis reveals that traditional loss functions struggle to effectively incorporate the DAG structure, leading us to propose a causality-enhanced method called Exponential Maximum Average Treatment Effect (ExMATE) to enhance the impact of causality at the utterance level in training neural conversation models. To evaluate the needs of considering causality in dialogue generation, we built a comprehensive benchmark on CausalDialogue dataset using different models, inference, and training methods. Through experiments, we find that a causality-inspired loss like ExMATE can improve the diversity and agility of conventional loss function and there is still room for improvement to reach human-level quality on this new dataset.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.792.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite their widespread adoption, neural conversation models have yet to exhibit natural chat capabilities with humans.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite their widespread adoption, neural conversation models have yet to exhibit natural chat capabilities with humans.\""
    },
    {
        "title": "Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses",
        "authors": [
            "Liyan Tang",
            "Yifan Peng",
            "Yanshan Wang",
            "Ying Ding",
            "Greg Durrett",
            "Justin Rousseau"
        ],
        "published": "2023",
        "summary": "A human decision-maker benefits the most from an AI assistant that corrects for their biases. For problems such as generating interpretation of a radiology report given findings, a system predicting only highly likely outcomes may be less useful, where such outcomes are already obvious to the user. To alleviate biases in human decision-making, it is worth considering a broad differential diagnosis, going beyond the most likely options. We introduce a new task, “less likely brainstorming,” that asks a model to generate outputs that humans think are relevant but less likely to happen. We explore the task in two settings: a brain MRI interpretation generation setting and an everyday commonsense reasoning setting. We found that a baseline approach of training with less likely hypotheses as targets generates outputs that humans evaluate as either likely or irrelevant nearly half of the time; standard MLE training is not effective. To tackle this problem, we propose a controlled text generation method that uses a novel contrastive learning strategy to encourage models to differentiate between generating likely and less likely outputs according to humans. We compare our method with several state-of-the-art controlled text generation models via automatic and human evaluations and show that our models’ capability of generating less likely outputs is improved.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.794.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We found that a baseline approach of training with less likely hypotheses as targets generates outputs that humans evaluate as either likely or irrelevant nearly half of the time; standard MLE training is not effective.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We found that a baseline approach of training with less likely hypotheses as targets generates outputs that humans evaluate as either likely or irrelevant nearly half of the time; standard MLE training is not effective.\""
    },
    {
        "title": "Language Modeling with Latent Situations",
        "authors": [
            "Belinda Z. Li",
            "Maxwell Nye",
            "Jacob Andreas"
        ],
        "published": "2023",
        "summary": "Language models (LMs) often generate incoherent outputs: they refer to events and entity states that are incompatible with the state of the world described in inputs. We introduce SITUATIONSUPERVISION, a family of approaches for improving coherence in LMs by training them to construct and condition on explicit representations of entities and their states. SITUATIONSUPERVISION has two components: an *auxiliary situation modeling* task that trains models to predict entity state representations in context, and a *latent state inference* procedure that imputes these states from partially annotated training data. SITUATIONSUPERVISION can be applied via fine-tuning (by supervising LMs to encode state variables in their hidden representations) and prompting (by inducing LMs to interleave textual descriptions of entity states with output text). In both cases, it requires only a small number of state annotations to produce substantial coherence improvements (up to an 16% reduction in errors), showing that standard LMs can be efficiently adapted to explicitly model language and aspects of its meaning.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.795.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Language models (LMs) often generate incoherent outputs: they refer to events and entity states that are incompatible with the state of the world described in inputs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Language models (LMs) often generate incoherent outputs: they refer to events and entity states that are incompatible with the state of the world described in inputs.\""
    },
    {
        "title": "Revisiting the Architectures like Pointer Networks to Efficiently Improve the Next Word Distribution, Summarization Factuality, and Beyond",
        "authors": [
            "Haw-Shiuan Chang",
            "Zonghai Yao",
            "Alolika Gon",
            "Hong Yu",
            "Andrew McCallum"
        ],
        "published": "2023",
        "summary": "Is the output softmax layer, which is adopted by most language models (LMs), always the best way to compute the next word probability? Given so many attention layers in a modern transformer-based LM, are the pointer networks redundant nowadays? In this study, we discover that the answers to both questions are no. This is because the softmax bottleneck sometimes prevents the LMs from predicting the desired distribution and the pointer networks can be used to break the bottleneck efficiently. Based on the finding, we propose several softmax alternatives by simplifying the pointer networks and accelerating the word-by-word rerankers. In GPT-2, our proposals are significantly better and more efficient than mixture of softmax, a state-of-the-art softmax alternative. In summarization experiments, without very significantly decreasing its training/testing speed, our best method based on T5-Small improves factCC score by 2 points in CNN/DM and XSUM dataset, and improves MAUVE scores by 30% in BookSum paragraph-level dataset.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.805.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the softmax bottleneck sometimes prevents the LMs from predicting the desired distribution\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the softmax bottleneck sometimes prevents the LMs from predicting the desired distribution\""
    },
    {
        "title": "GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-Distribution Generalization Perspective",
        "authors": [
            "Linyi Yang",
            "Shuibai Zhang",
            "Libo Qin",
            "Yafu Li",
            "Yidong Wang",
            "Hanmeng Liu",
            "Jindong Wang",
            "Xing Xie",
            "Yue Zhang"
        ],
        "published": "2023",
        "summary": "Pre-trained language models (PLMs) are known to improve the generalization performance of natural language understanding models by leveraging large amounts of data during the pre-training phase. However, the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods. This paper presents the first attempt at creating a unified benchmark named GLUE-X for evaluating OOD robustness in NLP models, highlighting the importance of OOD robustness and providing insights on how to measure the robustness of a model and how to improve it. The benchmark includes 13 publicly available datasets for OOD testing, and evaluations are conducted on 8 classic NLP tasks over 21 popularly used PLMs. Our findings confirm the need for improved OOD accuracy in NLP tasks, as significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.806.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods.\"; \"Our findings confirm the need for improved OOD accuracy in NLP tasks, as significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods.\"; \"Our findings confirm the need for improved OOD accuracy in NLP tasks, as significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy.\""
    },
    {
        "title": "Causal Intervention for Mitigating Name Bias in Machine Reading Comprehension",
        "authors": [
            "Jiazheng Zhu",
            "Shaojuan Wu",
            "Xiaowang Zhang",
            "Yuexian Hou",
            "Zhiyong Feng"
        ],
        "published": "2023",
        "summary": "Machine Reading Comprehension (MRC) is to answer questions based on a given passage, which has made great achievements using pre-trained Language Models (LMs). We study the robustness of MRC models to names which is flexible and repeatability. MRC models based on LMs may overuse the name information to make predictions, which causes the representation of names to be non-interchangeable, called name bias. In this paper, we propose a novel Causal Interventional paradigm for MRC (CI4MRC) to mitigate name bias. Specifically, we uncover that the pre-trained knowledge concerning names is indeed a confounder by analyzing the causalities among the pre-trained knowledge, context representation and answers based on a Structural Causal Model (SCM). We develop effective CI4MRC algorithmic implementations to constrain the confounder based on the neuron-wise and token-wise adjustments. Experiments demonstrate that our proposed CI4MRC effectively mitigates the name bias and achieves competitive performance on the original SQuAD. Moreover, our method is general to various pre-trained LMs and performs robustly on the adversarial datasets.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.812.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"MRC models based on LMs may overuse the name information to make predictions, which causes the representation of names to be non-interchangeable, called name bias.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"MRC models based on LMs may overuse the name information to make predictions, which causes the representation of names to be non-interchangeable, called name bias.\""
    },
    {
        "title": "Triplet-Free Knowledge-Guided Response Generation",
        "authors": [
            "Dongming Li",
            "Jianfeng Liu",
            "Baoyuan Wang"
        ],
        "published": "2023",
        "summary": "Generating vivid and informative responses (e.g., comments for social posts and utterances for dialogues) is challenging without giving relevant knowledge. Prior works focus on constructing the ”latent” knowledge first and then learning how to ”ground” it based on pseudo (context, knowledge, response) triplets. However, the retrieval between real responses and their latent knowledge is difficult in nature. In this paper, instead of focusing on how to ground knowledge given the responses, we take a different perspective to optimize the final responses for given guided knowledge directly. This allows us to re-formulate the entire problem in a simplified yet more scalable way. Specifically, we pretrain a response language model (LM) to measure the relevance and consistency between any context and response, then use search engines to collect the top-ranked passages to serve as the guiding knowledge without explicitly optimizing the ‘‘best” latent knowledge that corresponds to a given response. The final response generation model is trained through reinforcement learning by taking both the response LM prior and knowledge-injection rate as rewards. For better evaluations, we construct a new Chinese benchmark, ”IceKC”, using fresh multimodal online social posts. Both automatic evaluations and human evaluations show our zero-resource approach performs significantly better than prior works.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.815.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the retrieval between real responses and their latent knowledge is difficult in nature.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the retrieval between real responses and their latent knowledge is difficult in nature.\""
    },
    {
        "title": "Selecting Better Samples from Pre-trained LLMs: A Case Study on Question Generation",
        "authors": [
            "Xingdi Yuan",
            "Tong Wang",
            "Yen-Hsiang Wang",
            "Emery Fine",
            "Rania Abdelghani",
            "Hélène Sauzéon",
            "Pierre-Yves Oudeyer"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have in recent years demonstrated impressive prowess in natural language generation. A common practice to improve generation diversity is to sample multiple outputs from the model. However, partly due to the inaccessibility of LLMs, there lacks a simple and robust way of selecting the best output from these stochastic samples. As a case study framed in the context of question generation, we propose two prompt-based approaches, namely round-trip and prompt-based score, to selecting high-quality questions from a set of LLM-generated candidates. Our method works without the need to modify the underlying model, nor does it rely on human-annotated references — both of which are realistic constraints for real-world deployment of LLMs. With automatic as well as human evaluations, we empirically demonstrate that our approach can effectively select questions of higher qualities than greedy generation.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.820.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"partly due to the inaccessibility of LLMs, there lacks a simple and robust way of selecting the best output from these stochastic samples.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"partly due to the inaccessibility of LLMs, there lacks a simple and robust way of selecting the best output from these stochastic samples.\""
    },
    {
        "title": "Theory of Mind in Freely-Told Children’s Narratives: A Classification Approach",
        "authors": [
            "Bram van Dijk",
            "Marco Spruit",
            "Max van Duijn"
        ],
        "published": "2023",
        "summary": "Children are the focal point for studying the link between language and Theory of Mind (ToM) competence. Language and ToM are often studied with younger children and standardized tests, but as both are social competences, data and methods with higher ecological validity are critical. We leverage a corpus of 442 freely-told stories by Dutch children aged 4-12, recorded in their everyday classroom environments, to study language and ToM with NLP-tools. We labelled stories according to the mental depth of story characters children create, as a proxy for their ToM competence ‘in action’, and built a classifier with features encoding linguistic competences identified in existing work as predictive of ToM.We obtain good and fairly robust results (F1-macro = .71), relative to the complexity of the task for humans. Our results are explainable in that we link specific linguistic features such as lexical complexity and sentential complementation, that are relatively independent of children’s ages, to higher levels of character depth. This confirms and extends earlier work, as our study includes older children and socially embedded data from a different domain. Overall, our results support the idea that language and ToM are strongly interlinked, and that in narratives the former can scaffold the latter.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.822.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Better Language Models of Code through Self-Improvement",
        "authors": [
            "Hung To",
            "Nghi Bui",
            "Jin L.C. Guo",
            "Tien Nguyen"
        ],
        "published": "2023",
        "summary": "Pre-trained language models for code (PLMCs) have gained attention in recent research. These models are pre-trained on large-scale datasets using multi-modal objectives. However, fine-tuning them requires extensive supervision and is limited by the size of the dataset provided. We aim to improve this issue by proposing a data augmentation framework using knowledge distillation. Our framework utilizes knowledge gained during the pre-training and fine-tuning stage to augment training data, which is then used for the next step. We incorporate this framework into the state-of-the-art language models, such as CodeT5, CodeBERT, and UnixCoder. The results show that our framework significantly improves PLMCs’ performance in sequence-generation tasks, such as code summarization and code generation in the CodeXGLUE benchmark.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.823.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, fine-tuning them requires extensive supervision and is limited by the size of the dataset provided.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, fine-tuning them requires extensive supervision and is limited by the size of the dataset provided.\""
    },
    {
        "title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them",
        "authors": [
            "Mirac Suzgun",
            "Nathan Scales",
            "Nathanael Schärli",
            "Sebastian Gehrmann",
            "Yi Tay",
            "Hyung Won Chung",
            "Aakanksha Chowdhery",
            "Quoc Le",
            "Ed Chi",
            "Denny Zhou",
            "Jason Wei"
        ],
        "published": "2023",
        "summary": "BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models? In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the tasks for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.824.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models\""
    },
    {
        "title": "Probing Graph Decomposition for Argument Pair Extraction",
        "authors": [
            "Yang Sun",
            "Bin Liang",
            "Jianzhu Bao",
            "Yice Zhang",
            "Geng Tu",
            "Min Yang",
            "Ruifeng Xu"
        ],
        "published": "2023",
        "summary": "Argument pair extraction (APE) aims to extract interactive argument pairs from two passages within a discussion. The key challenge of APE is to effectively capture the complex context-aware interactive relations of arguments between the two passages. In this paper, we elicit relational semantic knowledge from large-scale pre-trained language models (PLMs) via a probing technique. The induced sentence-level relational probing graph can help capture rich explicit interactive relations between argument pairs effectively. Since the relevance score of a sentence pair within a passage is generally larger than that of the sentence pair from different passages, each sentence would prefer to propagate information within the same passage and under-explore the interactive relations between two passages. To tackle this issue, we propose a graph decomposition method to decompose the probing graph into four sub-graphs from intra- and inter-passage perspectives, where the intra-passage graphs can help detect argument spans within each passage and the inter-passage graphs can help identify the argument pairs between the review and rebuttal passages. Experimental results on two benchmark datasets show that our method achieves substantial improvements over strong baselines for APE.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.827.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Since the relevance score of a sentence pair within a passage is generally larger than that of the sentence pair from different passages, each sentence would prefer to propagate information within the same passage and under-explore the interactive relations between two passages.\"\n\nThis rating is given because the paper mentions a limitation of large-scale pre-trained language models (PLMs) in capturing interactive relations between argument pairs, but",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Since the relevance score of a sentence pair within a passage is generally larger than that of the sentence pair from different passages, each sentence would prefer to propagate information within the same passage and under-explore the interactive relations between two passages.\"\n\nThis rating is given because the paper mentions a limitation of large-scale pre-trained language models (PLMs) in capturing interactive relations between argument pairs, but"
    },
    {
        "title": "Towards Parameter-Efficient Integration of Pre-Trained Language Models In Temporal Video Grounding",
        "authors": [
            "Erica K. Shimomoto",
            "Edison Marrese-Taylor",
            "Hiroya Takamura",
            "Ichiro Kobayashi",
            "Hideki Nakayama",
            "Yusuke Miyao"
        ],
        "published": "2023",
        "summary": "This paper explores the task of Temporal Video Grounding (TVG) where, given an untrimmed video and a query sentence, the goal is to recognize and determine temporal boundaries of action instances in the video described by natural language queries. Recent works tackled this task by improving query inputs with large pre-trained language models (PLM), at the cost of more expensive training. However, the effects of this integration are unclear, as these works also propose improvements in the visual inputs. Therefore, this paper studies the role of query sentence representation with PLMs in TVG and assesses the applicability of parameter-efficient training with NLP adapters. We couple popular PLMs with a selection of existing approaches and test different adapters to reduce the impact of the additional parameters. Our results on three challenging datasets show that, with the same visual inputs, TVG models greatly benefited from the PLM integration and fine-tuning, stressing the importance of the text query representation in this task. Furthermore, adapters were an effective alternative to full fine-tuning, even though they are not tailored to our task, allowing PLM integration in larger TVG models and delivering results comparable to SOTA models. Finally, our results shed light on which adapters work best in different scenarios.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.829.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the effects of this integration are unclear, as these works also propose improvements in the visual inputs.\"\n\nThis abstract mentions Large Language Models (referred to as PLMs) but does not discuss any limitations of LLMs in detail. The evidence provided is more related to the unclear effects of integrating PLMs rather than a limitation of the models themselves.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, the effects of this integration are unclear, as these works also propose improvements in the visual inputs.\"\n\nThis abstract mentions Large Language Models (referred to as PLMs) but does not discuss any limitations of LLMs in detail. The evidence provided is more related to the unclear effects of integrating PLMs rather than a limitation of the models themselves."
    },
    {
        "title": "CoRRPUS: Code-based Structured Prompting for Neurosymbolic Story Understanding",
        "authors": [
            "Yijiang River Dong",
            "Lara J. Martin",
            "Chris Callison-Burch"
        ],
        "published": "2023",
        "summary": "Story generation and understanding—as with all NLG/NLU tasks—has seen a surge in neurosymbolic work. Researchers have recognized that, while large language models (LLMs) have tremendous utility, they can be augmented with symbolic means to be even better and to make up for many flaws that neural networks have. However, symbolic methods are extremely costly in terms of the amount of time and expertise needed to create them. In this work, we capitalize on state-of-the-art Code-LLMs, such as Codex, to bootstrap the use of symbolic methods for tracking the state of stories and aiding in story understanding. We show that our CoRRPUS system and abstracted prompting procedures can beat current state-of-the-art structured LLM techniques on pre-existing story understanding tasks (bAbI Task 2 and Re³) with minimal hand engineering. This work highlights the usefulness of code-based symbolic representations for enabling LLMs to better perform story reasoning tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.832.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"while large language models (LLMs) have tremendous utility, they can be augmented with symbolic means to be even better and to make up for many flaws that neural networks have.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"while large language models (LLMs) have tremendous utility, they can be augmented with symbolic means to be even better and to make up for many flaws that neural networks have.\""
    },
    {
        "title": "Abstract then Play: A Skill-centric Reinforcement Learning Framework for Text-based Games",
        "authors": [
            "Anjie Zhu",
            "Peng-Fei Zhang",
            "Yi Zhang",
            "Zi Huang",
            "Jie Shao"
        ],
        "published": "2023",
        "summary": "Text-based games present an exciting test-bed for reinforcement learning algorithms in the natural language environment. In these adventure games, an agent must learn to interact with the environment through text in order to accomplish tasks, facing large and combinational action space as well as partial observability issues. However, existing solutions fail to decompose the task and abstract the action autonomously, which either pre-specify the subtasks or pre-train on the human gameplay dataset. In this work, we introduce a novel skill-centric reinforcement learning framework, which is capable of abstracting the action in an end-to-end manner. To learn a more disentangled skill, we focus on the informativeness and distinguishability of the skill in accordance with the information bottleneck principle. Specifically, we introduce a discriminator to enable the skill to reflect the trajectory and push their representations onto the unit hypersphere to distribute uniformly. Moreover, a self-predictive mechanism is employed to learn inverse and forward dynamics, and a self-recovery mechanism is leveraged to refine the action representation, thus resulting in a more comprehensive perception of dynamics and more effective representations of textual state and action. Empirical experiments are carried out on the Jericho environment and the results validate the superiority against state-of-the-art baselines.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.836.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs or their limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs or their limitations."
    },
    {
        "title": "PruMUX: Augmenting Data Multiplexing with Model Compression",
        "authors": [
            "Yushan Su",
            "Vishvak Murahari",
            "Karthik Narasimhan",
            "Kai Li"
        ],
        "published": "2023",
        "summary": "As language models increase in size by the day, methods for efficient inference are critical to leveraging their capabilities for various applications. Prior work has investigated techniques like model pruning, knowledge distillation, and data multiplexing to increase model throughput without sacrificing accuracy. In this paper, we combine two such methods – structured pruning and data multiplexing – to compound the speedup gains obtained by either method. Our approach, PruMUX, obtains up to 7.5-29.5X throughput improvement over BERT-base model with accuracy threshold from 80% to 74%. We further study various combinations of parameters (such as sparsity and multiplexing factor) in the two techniques to provide a comprehensive analysis of the tradeoff between accuracy and throughput in the resulting models. We then propose Auto-PruMUX, a meta-level model that can predict the high-performance parameters for pruning and multiplexing given a desired accuracy loss budget, providing a practical method to leverage the combination effectively.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.841.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"As language models increase in size by the day, methods for efficient inference are critical to leveraging their capabilities for various applications.\"\n\nNote: Although the paper discusses the limitations of large language models in terms of size and inference efficiency, it does not elaborate on this limitation in detail and instead focuses on proposing a solution to address this issue.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"As language models increase in size by the day, methods for efficient inference are critical to leveraging their capabilities for various applications.\"\n\nNote: Although the paper discusses the limitations of large language models in terms of size and inference efficiency, it does not elaborate on this limitation in detail and instead focuses on proposing a solution to address this issue."
    },
    {
        "title": "Are Layout-Infused Language Models Robust to Layout Distribution Shifts? A Case Study with Scientific Documents",
        "authors": [
            "Catherine Chen",
            "Zejiang Shen",
            "Dan Klein",
            "Gabriel Stanovsky",
            "Doug Downey",
            "Kyle Lo"
        ],
        "published": "2023",
        "summary": "Recent work has shown that infusing layout features into language models (LMs) improves processing of visually-rich documents such as scientific papers. Layout-infused LMs are often evaluated on documents with familiar layout features (e.g., papers from the same publisher), but in practice models encounter documents with unfamiliar distributions of layout features, such as new combinations of text sizes and styles, or new spatial configurations of textual elements. In this work we test whether layout-infused LMs are robust to layout distribution shifts. As a case study we use the task of scientific document structure recovery, segmenting a scientific paper into its structural categories (e.g., “title”, “caption”, “reference”). To emulate distribution shifts that occur in practice we re-partition the GROTOAP2 dataset. We find that under layout distribution shifts model performance degrades by up to 20 F1. Simple training strategies, such as increasing training diversity, can reduce this degradation by over 35% relative F1; however, models fail to reach in-distribution performance in any tested out-of-distribution conditions. This work highlights the need to consider layout distribution shifts during model evaluation, and presents a methodology for conducting such evaluations.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.844.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that under layout distribution shifts model performance degrades by up to 20 F1. Simple training strategies, such as increasing training diversity, can reduce this degradation by over 35% relative F1; however, models fail to reach in-distribution performance in any tested out-of-distribution conditions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We find that under layout distribution shifts model performance degrades by up to 20 F1. Simple training strategies, such as increasing training diversity, can reduce this degradation by over 35% relative F1; however, models fail to reach in-distribution performance in any tested out-of-distribution conditions.\""
    },
    {
        "title": "Discovering Language Model Behaviors with Model-Written Evaluations",
        "authors": [
            "Ethan Perez",
            "Sam Ringer",
            "Kamile Lukosiute",
            "Karina Nguyen",
            "Edwin Chen",
            "Scott Heiner",
            "Craig Pettit",
            "Catherine Olsson",
            "Sandipan Kundu",
            "Saurav Kadavath",
            "Andy Jones",
            "Anna Chen",
            "Benjamin Mann",
            "Brian Israel",
            "Bryan Seethor",
            "Cameron McKinnon",
            "Christopher Olah",
            "Da Yan",
            "Daniela Amodei",
            "Dario Amodei",
            "Dawn Drain",
            "Dustin Li",
            "Eli Tran-Johnson",
            "Guro Khundadze",
            "Jackson Kernion",
            "James Landis",
            "Jamie Kerr",
            "Jared Mueller",
            "Jeeyoon Hyun",
            "Joshua Landau",
            "Kamal Ndousse",
            "Landon Goldberg",
            "Liane Lovitt",
            "Martin Lucas",
            "Michael Sellitto",
            "Miranda Zhang",
            "Neerav Kingsland",
            "Nelson Elhage",
            "Nicholas Joseph",
            "Noemi Mercado",
            "Nova DasSarma",
            "Oliver Rausch",
            "Robin Larson",
            "Sam McCandlish",
            "Scott Johnston",
            "Shauna Kravec",
            "Sheer El Showk",
            "Tamera Lanham",
            "Timothy Telleen-Lawton",
            "Tom Brown",
            "Tom Henighan",
            "Tristan Hume",
            "Yuntao Bai",
            "Zac Hatfield-Dodds",
            "Jack Clark",
            "Samuel R. Bowman",
            "Amanda Askell",
            "Roger Grosse",
            "Danny Hernandez",
            "Deep Ganguli",
            "Evan Hubinger",
            "Nicholas Schiefer",
            "Jared Kaplan"
        ],
        "published": "2023",
        "summary": "As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user’s preferred answer (“sycophancy”) and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.847.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user’s preferred answer (“sycophancy”) and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user’s preferred answer (“sycophancy”) and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF"
    },
    {
        "title": "GeoDRL: A Self-Learning Framework for Geometry Problem Solving using Reinforcement Learning in Deductive Reasoning",
        "authors": [
            "Shuai Peng",
            "Di Fu",
            "Yijun Liang",
            "Liangcai Gao",
            "Zhi Tang"
        ],
        "published": "2023",
        "summary": "Ensuring both interpretability and correctness is a great challenge in automated geometry problem solving (GPS), and the scarcity of labeled data hinders learning mathematical reasoning from samples. Therefore, we present GeoDRL, a self-learning geometry problem solving framework that integrates logic graph deduction and Deep Reinforcement Learning (DRL) to optimize geometry reasoning as a Markov Decision Process. GeoDRL employs a Graph Neural Network on a Geometry Logic Graph, updating the problem state using a symbolic system. Incorporating DRL into deductive reasoning enables GeoDRL to achieve unsupervised self-learning while maintaining correctness. GeoDRL, through unsupervised learning, exhibits enhanced accuracy in the Geometry3K dataset, improving by 11.1% over previous SOTA methods, and simultaneously boosts efficiency and interpretability.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.850.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Uncertainty-Aware Unlikelihood Learning Improves Generative Aspect Sentiment Quad Prediction",
        "authors": [
            "Mengting Hu",
            "Yinhao Bai",
            "Yike Wu",
            "Zhen Zhang",
            "Liqi Zhang",
            "Hang Gao",
            "Shiwan Zhao",
            "Minlie Huang"
        ],
        "published": "2023",
        "summary": "Recently, aspect sentiment quad prediction has received widespread attention in the field of aspect-based sentiment analysis. Existing studies extract quadruplets via pre-trained generative language models to paraphrase the original sentence into a templated target sequence. However, previous works only focus on what to generate but ignore what not to generate. We argue that considering the negative samples also leads to potential benefits. In this work, we propose a template-agnostic method to control the token-level generation, which boosts original learning and reduces mistakes simultaneously. Specifically, we introduce Monte Carlo dropout to understand the built-in uncertainty of pre-trained language models, acquiring the noises and errors. We further propose marginalized unlikelihood learning to suppress the uncertainty-aware mistake tokens. Finally, we introduce minimization entropy to balance the effects of marginalized unlikelihood learning. Extensive experiments on four public datasets demonstrate the effectiveness of our approach on various generation templates.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.851.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"acquiring the noises and errors\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"acquiring the noises and errors\""
    },
    {
        "title": "Adversarial Knowledge Stimulated Contrastive Prompting for Few-shot Language Learners",
        "authors": [
            "Kai Zheng",
            "Qingfeng Sun",
            "Yaming Yang",
            "Tengchao Lv",
            "Yeyong Pi",
            "Changlin Zhao",
            "Fei Xu",
            "Qi Zhang"
        ],
        "published": "2023",
        "summary": "Prompt-based fine-tuning has boosted the performance of Pre-trained Language Models(PLMs) on few-shot Natural Language Understanding (NLU) tasks by employing task-specific prompts. Yet, PLMsare unfamiliar with prompt-style expressionsduring pre-training, which limits the few-shotlearning performance on downstream tasks. It would be desirable if the models can stimulate prompting knowledge while adaptation to specific NLU tasks. We present the Adversarial Knowledge Stimulated Contrastive Prompting (AKSCP) framework, leading to better few-shot NLU tasks for language models by implicitly stimulate knowledge from pretrained language model. In AKSCP, a novel paradigm Cloze-driven prompt is proposed for joint prompt tuning across word cloze task and prompt-based learning, forcing PLMs to stimulate prompting knowledge. We further design an Adversarial Contrastive learning method to improve the generalization ability of PLM for different downstream tasks. Experiments over a variety of NLU tasks show that AKSCP consistently outperforms state-of-the-arts for prompt-based fine-tuning.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.852.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Yet, PLMs are unfamiliar with prompt-style expressions during pre-training, which limits the few-shot learning performance on downstream tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Yet, PLMs are unfamiliar with prompt-style expressions during pre-training, which limits the few-shot learning performance on downstream tasks.\""
    },
    {
        "title": "Making Pre-trained Language Models Better Learn Few-Shot Spoken Language Understanding in More Practical Scenarios",
        "authors": [
            "Yufan Wang",
            "Jie Mei",
            "Bowei Zou",
            "Rui Fan",
            "Tingting He",
            "Ai Ti Aw"
        ],
        "published": "2023",
        "summary": "Most previous few-shot Spoken Language Understanding (SLU) models typically need to be trained on a set of data-rich source domains and adapt to the target domain with a few examples. In this paper, we explore a more practical scenario for few-shot SLU, in which we only assume access to a pre-trained language model and a few labeled examples without any other source domain data. We concentrate on understanding how far the few-shot SLU could be pushed in this setting. To this end, we develop a prompt-based intent detection model in few-shot settings, which leverages the BERT original pre-training next sentence prediction task and the prompt template to detect the user’s intent. For slot filling, we propose an approach of reconstructing slot labels, which reduces the training complexity by reducing the number of slot labels in few-shot settings. To evaluate the few-shot SLU for a more practical scenario, we present two benchmarks, FewShotATIS and FewShotSNIPS. And a dynamic sampling strategy is designed to construct the two datasets according to the learning difficulty of each intent and slot. Experiments on FewShotATIS and FewShotSNIPS demonstrate that our proposed model achieves state-of-the-art performance.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.853.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper focuses on improving the performance of pre-trained language models in few-shot spoken language understanding scenarios, implying that existing models may have limitations in this area.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper focuses on improving the performance of pre-trained language models in few-shot spoken language understanding scenarios, implying that existing models may have limitations in this area."
    },
    {
        "title": "Second Language Acquisition of Neural Language Models",
        "authors": [
            "Miyu Oba",
            "Tatsuki Kuribayashi",
            "Hiroki Ouchi",
            "Taro Watanabe"
        ],
        "published": "2023",
        "summary": "With the success of neural language models (LMs), their language acquisition has gained much attention. This work sheds light on the second language (L2) acquisition of LMs, while previous work has typically explored their first language (L1) acquisition. Specifically, we trained bilingual LMs with a scenario similar to human L2 acquisition and analyzed their cross-lingual transfer from linguistic perspectives. Our exploratory experiments demonstrated that the L1 pretraining accelerated their linguistic generalization in L2, and language transfer configurations (e.g., the L1 choice, and presence of parallel texts) substantially affected their generalizations. These clarify their (non-)human-like L2 acquisition in particular aspects.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.856.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"language transfer configurations (e.g., the L1 choice, and presence of parallel texts) substantially affected their generalizations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"language transfer configurations (e.g., the L1 choice, and presence of parallel texts) substantially affected their generalizations.\""
    },
    {
        "title": "Exploring the Effectiveness of Prompt Engineering for Legal Reasoning Tasks",
        "authors": [
            "Fangyi Yu",
            "Lee Quartey",
            "Frank Schilder"
        ],
        "published": "2023",
        "summary": "The use of large language models (LLMs) for zero- or few-shot prompting in natural language processing has given rise to a new research area known as prompt engineering. Recent studies have demonstrated that Chain-of-Thought (CoT) prompts can lead to significant improvements in tasks such as arithmetic and common-sense reasoning. This paper explores the use of such approaches in legal reasoning tasks by conducting experiments on the COLIEE entailment task, which is based on the Japanese Bar exam. We evaluate zero-shot/few-shot and fine-tuning approaches with and without explanations, as well as various prompting strategies. Our results indicate that while CoT prompting and fine-tuning with explanations can improve performance, the best results are achieved with prompts derived from specific legal reasoning techniques, such as IRAC (Issue, Rule, Application, Conclusion). In addition, we observe that few-shot learning where the demonstrations are derived from clustering past training data consistently yields high performance on the COLIEE entailment task for both the years of the data that we tested. Through our experiments, we improve the previous best result on the 2021 COLIEE task from 0.7037 to 0.8025 and surpass the best system from 2022 with an accuracy of 0.789.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.858.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations, but the paper focuses on improving performance in a specific task, implying that there is room for improvement.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: No explicit mention of limitations, but the paper focuses on improving performance in a specific task, implying that there is room for improvement."
    },
    {
        "title": "SkillQG: Learning to Generate Question for Reading Comprehension Assessment",
        "authors": [
            "Xiaoqiang Wang",
            "Bang Liu",
            "Siliang Tang",
            "Lingfei Wu"
        ],
        "published": "2023",
        "summary": "We present SkillQG: a question generation framework with controllable comprehension types for assessing and improving machine reading comprehension models. Existing question generation systems widely differentiate questions by literal information such as question words and answer types to generate semantically relevant questions for a given context. However, they rarely consider the comprehension nature of questions, i.e., the different comprehension capabilities embodied by different questions. In comparison, our SkillQG is able to tailor a fine-grained assessment and improvement to the capabilities of questions answering models built on it. Specifically, we first frame the comprehension type of questions based on a hierarchical skill-based schema. We then formulate SkillQG as a skill-conditioned question generator. Furthermore, to improve the controllability of generation, we augment the input text with skill-specific question focus and knowledge, which are constructed by iteratively prompting the pre-trained language models. Empirical results demonstrate that SkillQG outperforms baselines in terms of quality, relevance, and skill-controllability while showing a promising performance boost in downstream question answering task.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.870.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"improve the controllability of generation, we augment the input text with skill-specific question focus and knowledge, which are constructed by iteratively prompting the pre-trained language models.\"\n\n(Note: Although LLMs are mentioned indirectly through \"pre-trained language models\", the abstract does not explicitly discuss limitations of LLMs, hence the rating of 1)",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"improve the controllability of generation, we augment the input text with skill-specific question focus and knowledge, which are constructed by iteratively prompting the pre-trained language models.\"\n\n(Note: Although LLMs are mentioned indirectly through \"pre-trained language models\", the abstract does not explicitly discuss limitations of LLMs, hence the rating of 1)"
    },
    {
        "title": "Improving Long Dialogue Summarization with Semantic Graph Representation",
        "authors": [
            "Yilun Hua",
            "Zhaoyuan Deng",
            "Kathleen McKeown"
        ],
        "published": "2023",
        "summary": "Although Large Language Models (LLMs) are successful in abstractive summarization of short dialogues, summarization of long dialogues remains challenging. To address this challenge, we propose a novel algorithm that processes complete dialogues comprising thousands of tokens into topic-segment-level Abstract Meaning Representation (AMR) graphs, which explicitly capture the dialogue structure, highlight salient semantics, and preserve high-level information. We also develop a new text-graph attention to leverage both graph semantics and a pretrained LLM that exploits the text. Finally, we propose an AMR node selection loss used jointly with conventional cross-entropy loss, to create additional training signals that facilitate graph feature encoding and content selection. Experiments show that our system outperforms the state-of-the-art models on multiple long dialogue summarization datasets, especially in low-resource settings, and generalizes well to out-of-domain data.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.871.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although Large Language Models (LLMs) are successful in abstractive summarization of short dialogues, summarization of long dialogues remains challenging.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although Large Language Models (LLMs) are successful in abstractive summarization of short dialogues, summarization of long dialogues remains challenging.\""
    },
    {
        "title": "EfficientVLM: Fast and Accurate Vision-Language Models via Knowledge Distillation and Modal-adaptive Pruning",
        "authors": [
            "Tiannan Wang",
            "Wangchunshu Zhou",
            "Yan Zeng",
            "Xinsong Zhang"
        ],
        "published": "2023",
        "summary": "Pre-trained vision-language models (VLMs) have achieved impressive results in a range of vision-language tasks. However, popular VLMs usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and deployment in real-world applications due to space, memory, and latency constraints. In this work, we introduce a distilling then pruning framework to compress large vision-language models into smaller, faster, and more accurate ones. We first shrink the size ofa pre-trained large VLM and apply knowledge distillation in the vision-language pre-training stage to obtain a task-agnostic compact VLM. Then we propose a modal-adaptive pruning algorithm to automatically infer the importance of vision and language modalities for different downstream tasks and adaptively remove redundant structures and neurons in different encoders with controllable target sparsity. We apply our framework to train EfficientVLM, a fast and accurate vision-language model consisting of 6 vision layers, 3 text layers, and 3 cross-modal fusion layers, accounting for only 93 million parameters in total, which is 44.3% of the teacher model. EfficientVLM retains 98.4% performance of the teacher model and accelerates its inference speed by 2.2×. EfficientVLM achieves a large absolute improvement over previous SoTA efficient VLMs of similar sizes by a large margin on various vision-language tasks, including VQAv2 (+4.9%), NLVR2 (+5.6%), ITR (R@1 on TR +17.2%, on IR + 15.6% ) and COCO caption generation (CIDEr +6.5), demonstrating a large potential on training lightweight VLMs.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.873.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, popular VLMs usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and deployment in real-world applications due to space, memory, and latency constraints.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, popular VLMs usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and deployment in real-world applications due to space, memory, and latency constraints.\""
    },
    {
        "title": "Robustness of Learning from Task Instructions",
        "authors": [
            "Jiasheng Gu",
            "Hongyu Zhao",
            "Hanzi Xu",
            "Liangyu Nie",
            "Hongyuan Mei",
            "Wenpeng Yin"
        ],
        "published": "2023",
        "summary": "Traditional supervised learning mostly works on individual tasks and requires training on a large set of task-specific examples. This paradigm seriously hinders the development of task generalization since preparing a task-specific example set is costly. To build a system that can quickly and easily generalize to new tasks, task instructions have been adopted as an emerging trend of supervision recently. These instructions give the model the definition of the task and allow the model to output the appropriate answer based on the instructions and inputs. However, task instructions are often expressed in different forms, which can be interpreted from two threads: first, some instructions are short sentences and are pretrained language model (PLM) oriented, such as prompts, while other instructions are paragraphs and are human-oriented, such as those in Amazon MTurk; second, different end-users very likely explain the same task with instructions of different textual expressions. A robust system for task generalization should be able to handle any new tasks regardless of the variability of instructions. However, the system robustness in dealing with instruction-driven task generalization is still unexplored. This work investigates the system robustness when the instructions of new tasks are (i) manipulated, (ii) paraphrased, or (iii) from different levels of conciseness. To our knowledge, this is the first work that systematically studies how robust a PLM is when it is supervised by instructions with different factors of variability.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.875.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the system robustness in dealing with instruction-driven task generalization is still unexplored.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the system robustness in dealing with instruction-driven task generalization is still unexplored.\""
    },
    {
        "title": "Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence",
        "authors": [
            "Haoran Li",
            "Mingshi Xu",
            "Yangqiu Song"
        ],
        "published": "2023",
        "summary": "Sentence-level representations are beneficial for various natural language processing tasks. It is commonly believed that vector representations can capture rich linguistic properties. Currently, large language models (LMs) achieve state-of-the-art performance on sentence embedding. However, some recent works suggest that vector representations from LMs can cause information leakage. In this work, we further investigate the information leakage issue and propose a generative embedding inversion attack (GEIA) that aims to reconstruct input sequences based only on their sentence embeddings. Given the black-box access to a language model, we treat sentence embeddings as initial tokens’ representations and train or fine-tune a powerful decoder model to decode the whole sequences directly. We conduct extensive experiments to demonstrate that our generative inversion attack outperforms previous embedding inversion attacks in classification metrics and generates coherent and contextually similar sentences as the original inputs.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.881.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, some recent works suggest that vector representations from LMs can cause information leakage.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, some recent works suggest that vector representations from LMs can cause information leakage.\""
    },
    {
        "title": "Cross-Lingual Knowledge Distillation for Answer Sentence Selection in Low-Resource Languages",
        "authors": [
            "Shivanshu Gupta",
            "Yoshitomo Matsubara",
            "Ankit Chadha",
            "Alessandro Moschitti"
        ],
        "published": "2023",
        "summary": "While impressive performance has been achieved on the task of Answer Sentence Selection (AS2) for English, the same does not hold for languages that lack large labeled datasets. In this work, we propose Cross-Lingual Knowledge Distillation (CLKD) from a strong English AS2 teacher as a method to train AS2 models for low-resource languages in the tasks without the need of labeled data for the target language. To evaluate our method, we introduce 1) Xtr-WikiQA, a translation-based WikiQA dataset for 9 additional languages, and 2) TyDi-AS2, a multilingual AS2 dataset with over 70K questions spanning 8 typologically diverse languages. We conduct extensive experiments on Xtr-WikiQA and TyDi-AS2 with multiple teachers, diverse monolingual and multilingual pretrained language models (PLMs) as students, and both monolingual and multilingual training. The results demonstrate that CLKD either outperforms or rivals even supervised fine-tuning with the same amount of labeled data and a combination of machine translation and the teacher model. Our method can potentially enable stronger AS2 models for low-resource languages, while TyDi-AS2 can serve as the largest multilingual AS2 dataset for further studies in the research community.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.885.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While impressive performance has been achieved on the task of Answer Sentence Selection (AS2) for English, the same does not hold for languages that lack large labeled datasets.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs in passing, specifically that they do not perform well on low-resource languages, but does not elaborate on this limitation further.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"While impressive performance has been achieved on the task of Answer Sentence Selection (AS2) for English, the same does not hold for languages that lack large labeled datasets.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs in passing, specifically that they do not perform well on low-resource languages, but does not elaborate on this limitation further."
    },
    {
        "title": "Check-COVID: Fact-Checking COVID-19 News Claims with Scientific Evidence",
        "authors": [
            "Gengyu Wang",
            "Kate Harwood",
            "Lawrence Chillrud",
            "Amith Ananthram",
            "Melanie Subbiah",
            "Kathleen McKeown"
        ],
        "published": "2023",
        "summary": "We present a new fact-checking benchmark, Check-COVID, that requires systems to verify claims about COVID-19 from news using evidence from scientific articles. This approach to fact-checking is particularly challenging as it requires checking internet text written in everyday language against evidence from journal articles written in formal academic language. Check-COVID contains 1, 504 expert-annotated news claims about the coronavirus paired with sentence-level evidence from scientific journal articles and veracity labels. It includes both extracted (journalist-written) and composed (annotator-written) claims. Experiments using both a fact-checking specific system and GPT-3.5, which respectively achieve F1 scores of 76.99 and 69.90 on this task, reveal the difficulty of automatically fact-checking both claim types and the importance of in-domain data for good performance. Our data and models are released publicly at https://github.com/posuer/Check-COVID.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.888.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Experiments using both a fact-checking specific system and GPT-3.5, which respectively achieve F1 scores of 76.99 and 69.90 on this task, reveal the difficulty of automatically fact-checking both claim types and the importance of in-domain data for good performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Experiments using both a fact-checking specific system and GPT-3.5, which respectively achieve F1 scores of 76.99 and 69.90 on this task, reveal the difficulty of automatically fact-checking both claim types and the importance of in-domain data for good performance.\""
    },
    {
        "title": "Early Exit with Disentangled Representation and Equiangular Tight Frame",
        "authors": [
            "Yixin Ji",
            "Jikai Wang",
            "Juntao Li",
            "Qiang Chen",
            "Wenliang Chen",
            "Min Zhang"
        ],
        "published": "2023",
        "summary": "Dynamic early exit has demonstrated great potential in coping with the sharply increasing number of pre-trained language model parameters, which can achieve a good trade-off between performance and efficiency. The existing early exit paradigm relies on training parametrical internal classifiers at each intermediate layer to complete specific tasks. Based on the predictions of these internal classifiers, different methods are designed to decide when to exit. Under this circumstance, each intermediate layer takes on both generic language representation learning and task-specific feature extraction, which makes each intermediate layer struggle to balance two types of backward loss signals during training. To break this dilemma, we propose an adapter method to decouple the two distinct types of representation and further introduce a non-parametric simplex equiangular tight frame classifier (ETF) for improvement. Extensive experiments on monolingual and multilingual tasks demonstrate that our method gains significant improvements over strong PLM backbones and early exit methods.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.889.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"coping with the sharply increasing number of pre-trained language model parameters\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"coping with the sharply increasing number of pre-trained language model parameters\""
    },
    {
        "title": "Tokenization with Factorized Subword Encoding",
        "authors": [
            "David Samuel",
            "Lilja Øvrelid"
        ],
        "published": "2023",
        "summary": "In recent years, language models have become increasingly larger and more complex. However, the input representations for these models continue to rely on simple and greedy subword tokenization methods. In this paper, we propose a novel tokenization method that factorizes subwords onto discrete triplets using a VQ-VAE model. The effectiveness of the proposed tokenization method, referred to as the Factorizer, is evaluated on language modeling and morpho-syntactic tasks for 7 diverse languages. Results indicate that this method is more appropriate and robust for morphological tasks than the commonly used byte-pair encoding (BPE) tokenization algorithm.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.890.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the input representations for these models continue to rely on simple and greedy subword tokenization methods.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the input representations for these models continue to rely on simple and greedy subword tokenization methods.\""
    },
    {
        "title": "Rarely a problem? Language models exhibit inverse scaling in their predictions following few-type quantifiers",
        "authors": [
            "James Michaelov",
            "Benjamin Bergen"
        ],
        "published": "2023",
        "summary": "How well do language models deal with quantification? In this study, we focus on ‘few’-type quantifiers, as in ‘few children like toys’, which might pose a particular challenge for language models because the sentence components with out the quantifier are likely to co-occur, and ‘few’-type quantifiers are rare. We present 960 English sentence stimuli from two human neurolinguistic experiments to 22 autoregressive transformer models of differing sizes. Not only do all the models perform poorly on ‘few’-type quantifiers, but overall the larger the model, the worse its performance. This inverse scaling is consistent with previous work suggesting that larger models increasingly reflect online rather than offline human processing, and we argue that the decreasing performance of larger models may challenge uses of language models as the basis for natural language systems.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.891.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Not only do all the models perform poorly on ‘few’-type quantifiers, but overall the larger the model, the worse its performance. This inverse scaling is consistent with previous work suggesting that larger models increasingly reflect online rather than offline human processing, and we argue that the decreasing performance of larger models may challenge uses of language models as the basis for natural language systems.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Not only do all the models perform poorly on ‘few’-type quantifiers, but overall the larger the model, the worse its performance. This inverse scaling is consistent with previous work suggesting that larger models increasingly reflect online rather than offline human processing, and we argue that the decreasing performance of larger models may challenge uses of language models as the basis for natural language systems.\""
    },
    {
        "title": "Images in Language Space: Exploring the Suitability of Large Language Models for Vision & Language Tasks",
        "authors": [
            "Sherzod Hakimov",
            "David Schlangen"
        ],
        "published": "2023",
        "summary": "Large language models have demonstrated robust performance on various language tasks using zero-shot or few-shot learning paradigms. While being actively researched, multimodal models that can additionally handle images as input have yet to catch up in size and generality with language-only models. In this work, we ask whether language-only models can be utilised for tasks that require visual input – but also, as we argue, often require a strong reasoning component. Similar to some recent related work, we make visual information accessible to the language model using separate verbalisation models. Specifically, we investigate the performance of open-source, open-access language models against GPT-3 on five vision-language tasks when given textually-encoded visual information. Our results suggest that language models are effective for solving vision-language tasks even with limited samples. This approach also enhances the interpretability of a model’s output by providing a means of tracing the output back through the verbalised image content.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.894.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While being actively researched, multimodal models that can additionally handle images as input have yet to catch up in size and generality with language-only models.\"\n\n(Note: Although the abstract mentions a limitation of multimodal models, it is not a limitation of LLMs specifically, but rather a comparison between LLMs and multimodal models. The abstract does not discuss any limitations of L",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"While being actively researched, multimodal models that can additionally handle images as input have yet to catch up in size and generality with language-only models.\"\n\n(Note: Although the abstract mentions a limitation of multimodal models, it is not a limitation of LLMs specifically, but rather a comparison between LLMs and multimodal models. The abstract does not discuss any limitations of L"
    },
    {
        "title": "An Empirical Comparison of LM-based Question and Answer Generation Methods",
        "authors": [
            "Asahi Ushio",
            "Fernando Alva-Manchego",
            "Jose Camacho-Collados"
        ],
        "published": "2023",
        "summary": "Question and answer generation (QAG) consists of generating a set of question-answer pairs given a context (e.g. a paragraph). This task has a variety of applications, such as data augmentation for question answering (QA) models, information retrieval and education. In this paper, we establish baselines with three different QAG methodologies that leverage sequence-to-sequence language model (LM) fine-tuning. Experiments show that an end-to-end QAG model, which is computationally light at both training and inference times, is generally robust and outperforms other more convoluted approaches. However, there are differences depending on the underlying generative LM. Finally, our analysis shows that QA models fine-tuned solely on generated question-answer pairs can be competitive when compared to supervised QA models trained on human-labeled data.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.899.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, there are differences depending on the underlying generative LM.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, there are differences depending on the underlying generative LM.\""
    },
    {
        "title": "Decouple knowledge from paramters for plug-and-play language modeling",
        "authors": [
            "Xin Cheng",
            "Yankai Lin",
            "Xiuying Chen",
            "Dongyan Zhao",
            "Rui Yan"
        ],
        "published": "2023",
        "summary": "Pre-trained language models (PLM) have made impressive results in a wide range of NLP tasks and it has been revealed that one of the key factors to their success is the parameters of these models implicitly learn various types of knowledge in the pre-training corpus. However, encoding knowledge implicitly in the model parameters has two fundamental drawbacks. First, the knowledge is neither editable nor scalable once the model is trained, which is especially problematic in that knowledge is consistently evolving. Second, it lacks interpretability and prevents us from understanding what kind of knowledge PLM needs to solve a certain task. In this paper, we introduce {pasted macro ‘MODEL’}, a pre-training model with differentiable plug-in memory (DPM). The key intuition behind is to decouple the knowledge storage from model parameters with an editable and scalable key-value memory and leverage knowledge in an explainable manner by knowledge retrieval in the {pasted macro ‘MEMORY’}. We conduct extensive experiments under various settings to justify this design choice. In domain adaptation setting, {pasted macro ‘MODEL’} could be easily adapted to different domains with pluggable in-domain memory—obtaining 3.95 F1 improvements across four domains, without any in-domain training. {pasted macro ‘MODEL’} could also keep absorbing new knowledge after pre-training is done by knowledge updating operation in the {pasted macro ‘MEMORY’} without re-training. Finally, we show that by incorporating training samples into {pasted macro ‘MEMORY’} with knowledge prompting, {pasted macro ‘MODEL’} could further be improved by the instruction of in-task knowledge.",
        "pdf_link": "https://aclanthology.org/2023.findings-acl.901.pdf",
        "source": "acl2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, encoding knowledge implicitly in the model parameters has two fundamental drawbacks. First, the knowledge is neither editable nor scalable once the model is trained, which is especially problematic in that knowledge is consistently evolving. Second, it lacks interpretability and prevents us from understanding what kind of knowledge PLM needs to solve a certain task.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, encoding knowledge implicitly in the model parameters has two fundamental drawbacks. First, the knowledge is neither editable nor scalable once the model is trained, which is especially problematic in that knowledge is consistently evolving. Second, it lacks interpretability and prevents us from understanding what kind of knowledge PLM needs to solve a certain task.\""
    }
]