[
    {
        "title": "AdapLeR: Speeding up Inference by Adaptive Length Reduction",
        "authors": [
            "Ali Modarressi",
            "Hosein Mohebbi",
            "Mohammad Taher Pilehvar"
        ],
        "published": "2022",
        "summary": "Pre-trained language models have shown stellar performance in various downstream tasks. But, this usually comes at the cost of high latency and computation, hindering their usage in resource-limited settings. In this work, we propose a novel approach for reducing the computational cost of BERT with minimal loss in downstream performance. Our method dynamically eliminates less contributing tokens through layers, resulting in shorter lengths and consequently lower computational cost. To determine the importance of each token representation, we train a Contribution Predictor for each layer using a gradient-based saliency method. Our experiments on several diverse classification tasks show speedups up to 22x during inference time without much sacrifice in performance. We also validate the quality of the selected tokens in our method using human annotations in the ERASER benchmark. In comparison to other widely used strategies for selecting important tokens, such as saliency and attention, our proposed method has a significantly lower false positive rate in generating rationales. Our code is freely available at https://github.com/amodaresi/AdapLeR.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.1.pdf",
        "keywords": [
            "pre trained language models",
            "adaptive length reduction"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"But, this usually comes at the cost of high latency and computation, hindering their usage in resource-limited settings.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"But, this usually comes at the cost of high latency and computation, hindering their usage in resource-limited settings.\""
    },
    {
        "title": "Rare Tokens Degenerate All Tokens: Improving Neural Text Generation via Adaptive Gradient Gating for Rare Token Embeddings",
        "authors": [
            "Sangwon Yu",
            "Jongyoon Song",
            "Heeseung Kim",
            "Seongmin Lee",
            "Woo-Jong Ryu",
            "Sungroh Yoon"
        ],
        "published": "2022",
        "summary": "Recent studies have determined that the learned token embeddings of large-scale neural language models are degenerated to be anisotropic with a narrow-cone shape. This phenomenon, called the representation degeneration problem, facilitates an increase in the overall similarity between token embeddings that negatively affect the performance of the models. Although the existing methods that address the degeneration problem based on observations of the phenomenon triggered by the problem improves the performance of the text generation, the training dynamics of token embeddings behind the degeneration problem are still not explored. In this study, we analyze the training dynamics of the token embeddings focusing on rare token embedding. We demonstrate that the specific part of the gradient for rare token embeddings is the key cause of the degeneration problem for all tokens during training stage. Based on the analysis, we propose a novel method called, adaptive gradient gating(AGG). AGG addresses the degeneration problem by gating the specific part of the gradient for rare token embeddings. Experimental results from language modeling, word similarity, and machine translation tasks quantitatively and qualitatively verify the effectiveness of AGG.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.3.pdf",
        "keywords": [
            "rare",
            "text generation",
            "rare token",
            "neural text generation",
            "rare token embeddings",
            "gating",
            "adaptive gradient gating"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent studies have determined that the learned token embeddings of large-scale neural language models are degenerated to be anisotropic with a narrow-cone shape. This phenomenon, called the representation degeneration problem, facilitates an increase in the overall similarity between token embeddings that negatively affect the performance of the models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Recent studies have determined that the learned token embeddings of large-scale neural language models are degenerated to be anisotropic with a narrow-cone shape. This phenomenon, called the representation degeneration problem, facilitates an increase in the overall similarity between token embeddings that negatively affect the performance of the models.\""
    },
    {
        "title": "AlephBERT: Language Model Pre-training and Evaluation from Sub-Word to Sentence Level",
        "authors": [
            "Amit Seker",
            "Elron Bandel",
            "Dan Bareket",
            "Idan Brusilovsky",
            "Refael Greenfeld",
            "Reut Tsarfaty"
        ],
        "published": "2022",
        "summary": "Large Pre-trained Language Models (PLMs) have become ubiquitous in the development of language understanding technology and lie at the heart of many artificial intelligence advances. While advances reported for English using PLMs are unprecedented, reported advances using PLMs for Hebrew are few and far between. The problem is twofold. First, so far, Hebrew resources for training large language models are not of the same magnitude as their English counterparts. Second, most benchmarks available to evaluate progress in Hebrew NLP require morphological boundaries which are not available in the output of standard PLMs. In this work we remedy both aspects. We present AlephBERT, a large PLM for Modern Hebrew, trained on larger vocabulary and a larger dataset than any Hebrew PLM before. Moreover, we introduce a novel neural architecture that recovers the morphological segments encoded in contextualized embedding vectors. Based on this new morphological component we offer an evaluation suite consisting of multiple tasks and benchmarks that cover sentence-level, word-level and sub-word level analyses. On all tasks, AlephBERT obtains state-of-the-art results beyond contemporary Hebrew baselines. We make our AlephBERT model, the morphological extraction model, and the Hebrew evaluation suite publicly available, for evaluating future Hebrew PLMs.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.4.pdf",
        "keywords": [
            "hebrew",
            "alephbert",
            "language model",
            "benchmarks",
            "pre trained language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"First, so far, Hebrew resources for training large language models are not of the same magnitude as their English counterparts. Second, most benchmarks available to evaluate progress in Hebrew NLP require morphological boundaries which are not available in the output of standard PLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"First, so far, Hebrew resources for training large language models are not of the same magnitude as their English counterparts. Second, most benchmarks available to evaluate progress in Hebrew NLP require morphological boundaries which are not available in the output of standard PLMs.\""
    },
    {
        "title": "An Unsupervised Multiple-Task and Multiple-Teacher Model for Cross-lingual Named Entity Recognition",
        "authors": [
            "Zhuoran Li",
            "Chunming Hu",
            "Xiaohui Guo",
            "Junfan Chen",
            "Wenyi Qin",
            "Richong Zhang"
        ],
        "published": "2022",
        "summary": "Cross-lingual named entity recognition task is one of the critical problems for evaluating the potential transfer learning techniques on low resource languages. Knowledge distillation using pre-trained multilingual language models between source and target languages have shown their superiority in transfer. However, existing cross-lingual distillation models merely consider the potential transferability between two identical single tasks across both domains. Other possible auxiliary tasks to improve the learning performance have not been fully investigated. In this study, based on the knowledge distillation framework and multi-task learning, we introduce the similarity metric model as an auxiliary task to improve the cross-lingual NER performance on the target domain. Specifically, an entity recognizer and a similarity evaluator are first trained in parallel as two teachers from the source domain. Then, two tasks in the student model are supervised by these teachers simultaneously. Empirical studies on the three datasets across 7 different languages confirm the effectiveness of the proposed model.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.14.pdf",
        "keywords": [
            "knowledge distillation",
            "cross lingual distillation",
            "named entity recognition",
            "multiple teacher model",
            "student model",
            "multi task learning",
            "unsupervised multiple task"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing cross-lingual distillation models merely consider the potential transferability between two identical single tasks across both domains.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, existing cross-lingual distillation models merely consider the potential transferability between two identical single tasks across both domains.\""
    },
    {
        "title": "Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm",
        "authors": [
            "Shaoyi Huang",
            "Dongkuan Xu",
            "Ian Yen",
            "Yijue Wang",
            "Sung-En Chang",
            "Bingbing Li",
            "Shiyang Chen",
            "Mimi Xie",
            "Sanguthevar Rajasekaran",
            "Hang Liu",
            "Caiwen Ding"
        ],
        "published": "2022",
        "summary": "Conventional wisdom in pruning Transformer-based language models is that pruning reduces the model expressiveness and thus is more likely to underfit rather than overfit. However, under the trending pretrain-and-finetune paradigm, we postulate a counter-traditional hypothesis, that is: pruning increases the risk of overfitting when performed at the fine-tuning phase. In this paper, we aim to address the overfitting problem and improve pruning performance via progressive knowledge distillation with error-bound properties. We show for the first time that reducing the risk of overfitting can help the effectiveness of pruning under the pretrain-and-finetune paradigm. Ablation studies and experiments on the GLUE benchmark show that our method outperforms the leading competitors across different tasks.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.16.pdf",
        "keywords": [
            "pruning",
            "progressive knowledge distillation",
            "progressive distillation",
            "bound"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, under the trending pretrain-and-finetune paradigm, we postulate a counter-traditional hypothesis, that is: pruning increases the risk of overfitting when performed at the fine-tuning phase.\"\n\nThis abstract mentions a limitation of LLMs, specifically the risk of overfitting when pruning is performed at the fine-tuning phase, but does not explore it",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, under the trending pretrain-and-finetune paradigm, we postulate a counter-traditional hypothesis, that is: pruning increases the risk of overfitting when performed at the fine-tuning phase.\"\n\nThis abstract mentions a limitation of LLMs, specifically the risk of overfitting when pruning is performed at the fine-tuning phase, but does not explore it"
    },
    {
        "title": "Overlap-based Vocabulary Generation Improves Cross-lingual Transfer Among Related Languages",
        "authors": [
            "Vaidehi Patil",
            "Partha Talukdar",
            "Sunita Sarawagi"
        ],
        "published": "2022",
        "summary": "Pre-trained multilingual language models such as mBERT and XLM-R have demonstrated great potential for zero-shot cross-lingual transfer to low web-resource languages (LRL). However, due to limited model capacity, the large difference in the sizes of available monolingual corpora between high web-resource languages (HRL) and LRLs does not provide enough scope of co-embedding the LRL with the HRL, thereby affecting the downstream task performance of LRLs. In this paper, we argue that relatedness among languages in a language family along the dimension of lexical overlap may be leveraged to overcome some of the corpora limitations of LRLs. We propose Overlap BPE (OBPE), a simple yet effective modification to the BPE vocabulary generation algorithm which enhances overlap across related languages. Through extensive experiments on multiple NLP tasks and datasets, we observe that OBPE generates a vocabulary that increases the representation of LRLs via tokens shared with HRLs. This results in improved zero-shot transfer from related HRLs to LRLs without reducing HRL representation and accuracy. Unlike previous studies that dismissed the importance of token-overlap, we show that in the low-resource related language setting, token overlap matters. Synthetically reducing the overlap to zero can cause as much as a four-fold drop in zero-shot transfer accuracy.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.18.pdf",
        "keywords": [
            "vocabulary generation",
            "overlap",
            "multilingual language models",
            "shot transfer"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, due to limited model capacity, the large difference in the sizes of available monolingual corpora between high web-resource languages (HRL) and LRLs does not provide enough scope of co-embedding the LRL with the HRL, thereby affecting the downstream task performance of LRLs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, due to limited model capacity, the large difference in the sizes of available monolingual corpora between high web-resource languages (HRL) and LRLs does not provide enough scope of co-embedding the LRL with the HRL, thereby affecting the downstream task performance of LRLs.\""
    },
    {
        "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling",
        "authors": [
            "Zhengxiao Du",
            "Yujie Qian",
            "Xiao Liu",
            "Ming Ding",
            "Jiezhong Qiu",
            "Zhilin Yang",
            "Jie Tang"
        ],
        "published": "2022",
        "summary": "There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25× parameters of BERT Large , demonstrating its generalizability to different downstream tasks.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.26.pdf",
        "keywords": [
            "blank",
            "general language model",
            "general language model pretraining",
            "pretraining",
            "blank infilling",
            "generation",
            "blank filling pretraining",
            "natural language",
            "autoencoding",
            "unconditional generation"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation.\""
    },
    {
        "title": "Mix and Match: Learning-free Controllable Text Generationusing Energy Language Models",
        "authors": [
            "Fatemehsadat Mireshghallah",
            "Kartik Goyal",
            "Taylor Berg-Kirkpatrick"
        ],
        "published": "2022",
        "summary": "Recent work on controlled text generation has either required attribute-based fine-tuning of the base language model (LM), or has restricted the parameterization of the attribute discriminator to be compatible with the base autoregressive LM. In this work, we propose Mix and Match LM, a global score-based alternative for controllable text generation that combines arbitrary pre-trained black-box models for achieving the desired attributes in the generated text without involving any fine-tuning or structural assumptions about the black-box models. We interpret the task of controllable generation as drawing samples from an energy-based model whose energy values are a linear combination of scores from black-box models that are separately responsible for fluency, the control attribute, and faithfulness to any conditioning context. We use a Metropolis-Hastings sampling scheme to sample from this energy-based model using bidirectional context and global attribute features. We validate the effectiveness of our approach on various controlled generation and style-based text revision tasks by outperforming recently proposed methods that involve extra training, fine-tuning, or restrictive assumptions over the form of models.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.31.pdf",
        "keywords": [
            "sample",
            "text generation",
            "energy language models",
            "controlled text generation",
            "base language model"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent work on controlled text generation has either required attribute-based fine-tuning of the base language model (LM), or has restricted the parameterization of the attribute discriminator to be compatible with the base autoregressive LM.\"\n\nThis evidence indicates that the paper discusses limitations of existing approaches to controlled text generation using LLMs, but it does not explicitly discuss limitations of LLMs themselves.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Recent work on controlled text generation has either required attribute-based fine-tuning of the base language model (LM), or has restricted the parameterization of the attribute discriminator to be compatible with the base autoregressive LM.\"\n\nThis evidence indicates that the paper discusses limitations of existing approaches to controlled text generation using LLMs, but it does not explicitly discuss limitations of LLMs themselves."
    },
    {
        "title": "KaFSP: Knowledge-Aware Fuzzy Semantic Parsing for Conversational Question Answering over a Large-Scale Knowledge Base",
        "authors": [
            "Junzhuo Li",
            "Deyi Xiong"
        ],
        "published": "2022",
        "summary": "In this paper, we study two issues of semantic parsing approaches to conversational question answering over a large-scale knowledge base: (1) The actions defined in grammar are not sufficient to handle uncertain reasoning common in real-world scenarios. (2) Knowledge base information is not well exploited and incorporated into semantic parsing. To mitigate the two issues, we propose a knowledge-aware fuzzy semantic parsing framework (KaFSP). It defines fuzzy comparison operations in the grammar system for uncertain reasoning based on the fuzzy set theory. In order to enhance the interaction between semantic parsing and knowledge base, we incorporate entity triples from the knowledge base into a knowledge-aware entity disambiguation module. Additionally, we propose a multi-label classification framework to not only capture correlations between entity types and relations but also detect knowledge base information relevant to the current utterance. Both enhancements are based on pre-trained language models. Experiments on a large-scale conversational question answering benchmark demonstrate that the proposed KaFSP achieves significant improvements over previous state-of-the-art models, setting new SOTA results on 8 out of 10 question types, gaining improvements of over 10% F1 or accuracy on 3 question types, and improving overall F1 from 83.01% to 85.33%. The source code of KaFSP is available at https://github.com/tjunlp-lab/KaFSP.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.35.pdf",
        "keywords": [
            "conversational question answering",
            "semantic parsing",
            "knowledge base",
            "knowledge aware",
            "grammar"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Based on pre-trained language models.\"\n\nThis paper talks about LLMs but does not explicitly mention any limitations of LLMs. The mention of \"pre-trained language models\" is the only evidence of LLMs in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Based on pre-trained language models.\"\n\nThis paper talks about LLMs but does not explicitly mention any limitations of LLMs. The mention of \"pre-trained language models\" is the only evidence of LLMs in the abstract."
    },
    {
        "title": "Perceiving the World: Question-guided Reinforcement Learning for Text-based Games",
        "authors": [
            "Yunqiu Xu",
            "Meng Fang",
            "Ling Chen",
            "Yali Du",
            "Joey Zhou",
            "Chengqi Zhang"
        ],
        "published": "2022",
        "summary": "Text-based games provide an interactive way to study natural language processing. While deep reinforcement learning has shown effectiveness in developing the game playing agent, the low sample efficiency and the large action space remain to be the two major challenges that hinder the DRL from being applied in the real world. In this paper, we address the challenges by introducing world-perceiving modules, which automatically decompose tasks and prune actions by answering questions about the environment. We then propose a two-phase training framework to decouple language learning from reinforcement learning, which further improves the sample efficiency. The experimental results show that the proposed method significantly improves the performance and sample efficiency. Besides, it shows robustness against compound error and limited pre-training data.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.41.pdf",
        "keywords": [
            "text based games",
            "reinforcement learning",
            "language learning",
            "sample efficiency",
            "world"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Answer-level Calibration for Free-form Multiple Choice Question Answering",
        "authors": [
            "Sawan Kumar"
        ],
        "published": "2022",
        "summary": "Pre-trained language models have recently shown that training on large corpora using the language modeling objective enables few-shot and zero-shot capabilities on a variety of NLP tasks, including commonsense reasoning tasks. This is achieved using text interactions with the model, usually by posing the task as a natural language text completion problem. While using language model probabilities to obtain task specific scores has been generally useful, it often requires task-specific heuristics such as length normalization, or probability calibration. In this work, we consider the question answering format, where we need to choose from a set of (free-form) textual choices of unspecified lengths given a context. We present ALC (Answer-Level Calibration), where our main suggestion is to model context-independent biases in terms of the probability of a choice without the associated context and to subsequently remove it using an unsupervised estimate of similarity with the full context. We show that our unsupervised answer-level calibration consistently improves over or is competitive with baselines using standard evaluation metrics on a variety of tasks including commonsense reasoning tasks. Further, we show that popular datasets potentially favor models biased towards easy cues which are available independent of the context. We analyze such biases using an associated F1-score. Our analysis indicates that answer-level calibration is able to remove such biases and leads to a more robust measure of model capability.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.49.pdf",
        "keywords": [
            "answer level calibration",
            "question answering",
            "multiple choice question answering",
            "commonsense reasoning",
            "interactions"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While using language model probabilities to obtain task specific scores has been generally useful, it often requires task-specific heuristics such as length normalization, or probability calibration.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While using language model probabilities to obtain task specific scores has been generally useful, it often requires task-specific heuristics such as length normalization, or probability calibration.\""
    },
    {
        "title": "Meta-learning via Language Model In-context Tuning",
        "authors": [
            "Yanda Chen",
            "Ruiqi Zhong",
            "Sheng Zha",
            "George Karypis",
            "He He"
        ],
        "published": "2022",
        "summary": "The goal of meta-learning is to learn to adapt to a new task with only a few labeled examples. Inspired by the recent progress in large language models, we propose in-context tuning (ICT), which recasts task adaptation and prediction as a simple sequence prediction problem: to form the input sequence, we concatenate the task instruction, labeled in-context examples, and the target input to predict; to meta-train the model to learn from in-context examples, we fine-tune a pre-trained language model (LM) to predict the target label given the input sequence on a collection of tasks.We benchmark our method on two collections of text classification tasks: LAMA and BinaryClfs. Compared to MAML which adapts the model through gradient descent, our method leverages the inductive bias of pre-trained LMs to perform pattern matching, and outperforms MAML by an absolute 6% average AUC-ROC score on BinaryClfs, gaining more advantage with increasing model size. Compared to non-fine-tuned in-context learning (i.e. prompting a raw LM), in-context tuning meta-trains the model to learn from in-context examples. On BinaryClfs, ICT improves the average AUC-ROC score by an absolute 10%, and reduces the variance due to example ordering by 6x and example choices by 2x.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.53.pdf",
        "keywords": [
            "task adaptation",
            "context tuning",
            "meta learning",
            "model"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation is mentioned, but the paper discusses improving upon existing methods (MAML and non-fine-tuned in-context learning) that utilize LLMs, implying that those methods have limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation is mentioned, but the paper discusses improving upon existing methods (MAML and non-fine-tuned in-context learning) that utilize LLMs, implying that those methods have limitations."
    },
    {
        "title": "Prompt-Based Rule Discovery and Boosting for Interactive Weakly-Supervised Learning",
        "authors": [
            "Rongzhi Zhang",
            "Yue Yu",
            "Pranav Shetty",
            "Le Song",
            "Chao Zhang"
        ],
        "published": "2022",
        "summary": "Weakly-supervised learning (WSL) has shown promising results in addressing label scarcity on many NLP tasks, but manually designing a comprehensive, high-quality labeling rule set is tedious and difficult. We study interactive weakly-supervised learning—the problem of iteratively and automatically discovering novel labeling rules from data to improve the WSL model. Our proposed model, named PRBoost, achieves this goal via iterative prompt-based rule discovery and model boosting. It uses boosting to identify large-error instances and discovers candidate rules from them by prompting pre-trained LMs with rule templates. The candidate rules are judged by human experts, and the accepted rules are used to generate complementary weak labels and strengthen the current model. Experiments on four tasks show PRBoost outperforms state-of-the-art WSL baselines up to 7.1%, and bridges the gaps with fully supervised models.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.55.pdf",
        "keywords": [
            "boosting",
            "rule discovery",
            "weakly supervised learning",
            "supervised learning",
            "model boosting"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"uses boosting to identify large-error instances and discovers candidate rules from them by prompting pre-trained LMs with rule templates.\"\n\nThis paper mentions LLMs but does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"uses boosting to identify large-error instances and discovers candidate rules from them by prompting pre-trained LMs with rule templates.\"\n\nThis paper mentions LLMs but does not discuss any limitations of LLMs."
    },
    {
        "title": "An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels",
        "authors": [
            "Taylor Sorensen",
            "Joshua Robinson",
            "Christopher Rytting",
            "Alexander Shaw",
            "Kyle Rogers",
            "Alexia Delorey",
            "Mahmoud Khalil",
            "Nancy Fulda",
            "David Wingate"
        ],
        "published": "2022",
        "summary": "Pre-trained language models derive substantial linguistic and factual knowledge from the massive corpora on which they are trained, and prompt engineering seeks to align these models to specific tasks. Unfortunately, existing prompt engineering methods require significant amounts of labeled data, access to model parameters, or both. We introduce a new method for selecting prompt templates without labeled examples and without direct access to the model. Specifically, over a set of candidate templates, we choose the template that maximizes the mutual information between the input and the corresponding model output. Across 8 datasets representing 7 distinct NLP tasks, we show that when a template has high mutual information, it also has high accuracy on the task. On the largest model, selecting prompts with our method gets 90% of the way from the average prompt accuracy to the best prompt accuracy and requires no ground truth labels.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.60.pdf",
        "keywords": [
            "mutual information",
            "ground truth labels",
            "trained language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Unfortunately, existing prompt engineering methods require significant amounts of labeled data, access to model parameters, or both.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Unfortunately, existing prompt engineering methods require significant amounts of labeled data, access to model parameters, or both.\""
    },
    {
        "title": "Language-agnostic BERT Sentence Embedding",
        "authors": [
            "Fangxiaoyu Feng",
            "Yinfei Yang",
            "Daniel Cer",
            "Naveen Arivazhagan",
            "Wei Wang"
        ],
        "published": "2022",
        "summary": "While BERT is an effective method for learning monolingual sentence embeddings for semantic similarity and embedding based transfer learning BERT based cross-lingual sentence embeddings have yet to be explored. We systematically investigate methods for learning multilingual sentence embeddings by combining the best methods for learning monolingual and cross-lingual representations including: masked language modeling (MLM), translation language modeling (TLM), dual encoder translation ranking, and additive margin softmax. We show that introducing a pre-trained multilingual language model dramatically reduces the amount of parallel training data required to achieve good performance by 80%. Composing the best of these methods produces a model that achieves 83.7% bi-text retrieval accuracy over 112 languages on Tatoeba, well above the 65.5% achieved by LASER, while still performing competitively on monolingual transfer learning benchmarks. Parallel data mined from CommonCrawl using our best model is shown to train competitive NMT models for en-zh and en-de. We publicly release our best multilingual sentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.62.pdf",
        "keywords": [
            "sentence embeddings",
            "language model",
            "bert sentence embedding",
            "transfer learning"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining",
        "authors": [
            "Hui Su",
            "Weiwei Shi",
            "Xiaoyu Shen",
            "Zhou Xiao",
            "Tuo Ji",
            "Jiarui Fang",
            "Jie Zhou"
        ],
        "published": "2022",
        "summary": "Large-scale pretrained language models have achieved SOTA results on NLP tasks. However, they have been shown vulnerable to adversarial attacks especially for logographic languages like Chinese. In this work, we propose RoCBert: a pretrained Chinese Bert that is robust to various forms of adversarial attacks like word perturbation, synonyms, typos, etc. It is pretrained with the contrastive learning objective which maximizes the label consistency under different synthesized adversarial examples. The model takes as input multimodal information including the semantic, phonetic and visual features. We show all these features areimportant to the model robustness since the attack can be performed in all the three forms. Across 5 Chinese NLU tasks, RoCBert outperforms strong baselines under three blackbox adversarial algorithms without sacrificing the performance on clean testset. It also performs the best in the toxic content detection task under human-made attacks.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.65.pdf",
        "keywords": [
            "rocbert",
            "multimodal contrastive pretraining",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, they have been shown vulnerable to adversarial attacks especially for logographic languages like Chinese.\"\n\nThis paper mentions a limitation of LLMs (vulnerability to adversarial attacks), but it is not the primary focus of the paper, which is on proposing a robust model.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, they have been shown vulnerable to adversarial attacks especially for logographic languages like Chinese.\"\n\nThis paper mentions a limitation of LLMs (vulnerability to adversarial attacks), but it is not the primary focus of the paper, which is on proposing a robust model."
    },
    {
        "title": "Premise-based Multimodal Reasoning: Conditional Inference on Joint Textual and Visual Clues",
        "authors": [
            "Qingxiu Dong",
            "Ziwei Qin",
            "Heming Xia",
            "Tian Feng",
            "Shoujie Tong",
            "Haoran Meng",
            "Lin Xu",
            "Zhongyu Wei",
            "Weidong Zhan",
            "Baobao Chang",
            "Sujian Li",
            "Tianyu Liu",
            "Zhifang Sui"
        ],
        "published": "2022",
        "summary": "It is a common practice for recent works in vision language cross-modal reasoning to adopt a binary or multi-choice classification formulation taking as input a set of source image(s) and textual query. In this work, we take a sober look at such an “unconditional” formulation in the sense that no prior knowledge is specified with respect to the source image(s). Inspired by the designs of both visual commonsense reasoning and natural language inference tasks, we propose a new task termed “Premise-based Multi-modal Reasoning” (PMR) where a textual premise is the background presumption on each source image. The PMR dataset contains 15,360 manually annotated samples which are created by a multi-phase crowd-sourcing process. With selected high-quality movie screenshots and human-curated premise templates from 6 pre-defined categories, we ask crowd-source workers to write one true hypothesis and three distractors (4 choices) given the premise and image through a cross-check procedure.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.66.pdf",
        "keywords": [
            "visual commonsense reasoning",
            "multimodal reasoning",
            "multi modal reasoning",
            "natural language inference",
            "premise templates",
            "cross modal reasoning",
            "conditional inference",
            "textual premise"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs."
    },
    {
        "title": "Modeling Multi-hop Question Answering as Single Sequence Prediction",
        "authors": [
            "Semih Yavuz",
            "Kazuma Hashimoto",
            "Yingbo Zhou",
            "Nitish Shirish Keskar",
            "Caiming Xiong"
        ],
        "published": "2022",
        "summary": "Fusion-in-decoder (Fid) (Izacard and Grave, 2020) is a generative question answering (QA) model that leverages passage retrieval with a pre-trained transformer and pushed the state of the art on single-hop QA. However, the complexity of multi-hop QA hinders the effectiveness of the generative QA approach. In this work, we propose a simple generative approach (PathFid) that extends the task beyond just answer generation by explicitly modeling the reasoning process to resolve the answer for multi-hop questions. By linearizing the hierarchical reasoning path of supporting passages, their key sentences, and finally the factoid answer, we cast the problem as a single sequence prediction task. To facilitate complex reasoning with multiple clues, we further extend the unified flat representation of multiple input documents by encoding cross-passage interactions. Our extensive experiments demonstrate that PathFid leads to strong performance gains on two multi-hop QA datasets: HotpotQA and IIRC. Besides the performance gains, PathFid is more interpretable, which in turn yields answers that are more faithfully grounded to the supporting passages and facts compared to the baseline Fid model.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.69.pdf",
        "keywords": [
            "single sequence prediction",
            "multi hop question answering",
            "multi hop qa",
            "passages",
            "generative",
            "reasoning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the complexity of multi-hop QA hinders the effectiveness of the generative QA approach.\"\n\nThis rating is given because the abstract mentions a limitation of the generative QA approach (which is based on a pre-trained transformer, a type of LLM) in handling multi-hop QA, but does not elaborate on this limitation and focuses on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the complexity of multi-hop QA hinders the effectiveness of the generative QA approach.\"\n\nThis rating is given because the abstract mentions a limitation of the generative QA approach (which is based on a pre-trained transformer, a type of LLM) in handling multi-hop QA, but does not elaborate on this limitation and focuses on the proposed solution."
    },
    {
        "title": "Multi-Granularity Structural Knowledge Distillation for Language Model Compression",
        "authors": [
            "Chang Liu",
            "Chongyang Tao",
            "Jiazhan Feng",
            "Dongyan Zhao"
        ],
        "published": "2022",
        "summary": "Transferring the knowledge to a small model through distillation has raised great interest in recent years. Prevailing methods transfer the knowledge derived from mono-granularity language units (e.g., token-level or sample-level), which is not enough to represent the rich semantics of a text and may lose some vital knowledge. Besides, these methods form the knowledge as individual representations or their simple dependencies, neglecting abundant structural relations among intermediate representations. To overcome the problems, we present a novel knowledge distillation framework that gathers intermediate representations from multiple semantic granularities (e.g., tokens, spans and samples) and forms the knowledge as more sophisticated structural relations specified as the pair-wise interactions and the triplet-wise geometric angles based on multi-granularity representations. Moreover, we propose distilling the well-organized multi-granularity structural knowledge to the student hierarchically across layers. Experimental results on GLUE benchmark demonstrate that our method outperforms advanced distillation methods.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.71.pdf",
        "keywords": [
            "knowledge distillation",
            "language model compression",
            "structural knowledge distillation",
            "hierarchically",
            "sample"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Prevailing methods transfer the knowledge derived from mono-granularity language units (e.g., token-level or sample-level), which is not enough to represent the rich semantics of a text and may lose some vital knowledge.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Prevailing methods transfer the knowledge derived from mono-granularity language units (e.g., token-level or sample-level), which is not enough to represent the rich semantics of a text and may lose some vital knowledge.\""
    },
    {
        "title": "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts",
        "authors": [
            "Yue Guo",
            "Yi Yang",
            "Ahmed Abbasi"
        ],
        "published": "2022",
        "summary": "Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. Different from previous debiasing work that uses external corpora to fine-tune the pretrained models, we instead directly probe the biases encoded in pretrained models through prompts. Specifically, we propose a variant of the beam search method to automatically search for biased prompts such that the cloze-style completions are the most different with respect to different demographic groups. Given the identified biased prompts, we then propose a distribution alignment loss to mitigate the biases. Experiment results on standard datasets and metrics show that our proposed Auto-Debias approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT. Moreover, the improvement in fairness does not decrease the language models’ understanding abilities, as shown using the GLUE benchmark.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.72.pdf",
        "keywords": [
            "language models",
            "language",
            "biased prompts",
            "prompts",
            "auto debias",
            "debiasing",
            "automated biased prompts",
            "debiasing masked language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Human-like biases and undesired social stereotypes exist in large pretrained language models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Human-like biases and undesired social stereotypes exist in large pretrained language models.\""
    },
    {
        "title": "Where to Go for the Holidays: Towards Mixed-Type Dialogs for Clarification of User Goals",
        "authors": [
            "Zeming Liu",
            "Jun Xu",
            "Zeyang Lei",
            "Haifeng Wang",
            "Zheng-Yu Niu",
            "Hua Wu"
        ],
        "published": "2022",
        "summary": "Most dialog systems posit that users have figured out clear and specific goals before starting an interaction. For example, users have determined the departure, the destination, and the travel time for booking a flight. However, in many scenarios, limited by experience and knowledge, users may know what they need, but still struggle to figure out clear and specific goals by determining all the necessary slots. In this paper, we identify this challenge, and make a step forward by collecting a new human-to-human mixed-type dialog corpus. It contains 5k dialog sessions and 168k utterances for 4 dialog types and 5 domains. Within each session, an agent first provides user-goal-related knowledge to help figure out clear and specific goals, and then help achieve them. Furthermore, we propose a mixed-type dialog model with a novel Prompt-based continual learning mechanism. Specifically, the mechanism enables the model to continually strengthen its ability on any specific type by utilizing existing dialog corpora effectively.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.73.pdf",
        "keywords": [
            "dialogs",
            "mixed type dialog corpus",
            "mixed type dialog",
            "user goals",
            "holidays"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Explanation Graph Generation via Pre-trained Language Models: An Empirical Study with Contrastive Learning",
        "authors": [
            "Swarnadeep Saha",
            "Prateek Yadav",
            "Mohit Bansal"
        ],
        "published": "2022",
        "summary": "Pre-trained sequence-to-sequence language models have led to widespread success in many natural language generation tasks. However, there has been relatively less work on analyzing their ability to generate structured outputs such as graphs. Unlike natural language, graphs have distinct structural and semantic properties in the context of a downstream NLP task, e.g., generating a graph that is connected and acyclic can be attributed to its structural constraints, while the semantics of a graph can refer to how meaningfully an edge represents the relation between two node concepts. In this work, we study pre-trained language models that generate explanation graphs in an end-to-end manner and analyze their ability to learn the structural constraints and semantics of such graphs. We first show that with limited supervision, pre-trained language models often generate graphs that either violate these constraints or are semantically incoherent. Since curating large amount of human-annotated graphs is expensive and tedious, we propose simple yet effective ways of graph perturbations via node and edge edit operations that lead to structurally and semantically positive and negative graphs. Next, we leverage these graphs in different contrastive learning models with Max-Margin and InfoNCE losses. Our methods lead to significant improvements in both structural and semantic accuracy of explanation graphs and also generalize to other similar graph generation tasks. Lastly, we show that human errors are the best negatives for contrastive learning and also that automatically generating more such human-like negative graphs can lead to further improvements.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.85.pdf",
        "keywords": [
            "graph generation",
            "contrastive learning models",
            "language",
            "explanation graph generation",
            "pre trained language models",
            "empirical study"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"we first show that with limited supervision, pre-trained language models often generate graphs that either violate these constraints or are semantically incoherent.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"we first show that with limited supervision, pre-trained language models often generate graphs that either violate these constraints or are semantically incoherent.\""
    },
    {
        "title": "Tracing Origins: Coreference-aware Machine Reading Comprehension",
        "authors": [
            "Baorong Huang",
            "Zhuosheng Zhang",
            "Hai Zhao"
        ],
        "published": "2022",
        "summary": "Machine reading comprehension is a heavily-studied research and test field for evaluating new pre-trained language models (PrLMs) and fine-tuning strategies, and recent studies have enriched the pre-trained language models with syntactic, semantic and other linguistic information to improve the performance of the models. In this paper, we imitate the human reading process in connecting the anaphoric expressions and explicitly leverage the coreference information of the entities to enhance the word embeddings from the pre-trained language model, in order to highlight the coreference mentions of the entities that must be identified for coreference-intensive question answering in QUOREF, a relatively new dataset that is specifically designed to evaluate the coreference-related performance of a model. We use two strategies to fine-tune a pre-trained language model, namely, placing an additional encoder layer after a pre-trained language model to focus on the coreference mentions or constructing a relational graph convolutional network to model the coreference relations. We demonstrate that the explicit incorporation of coreference information in the fine-tuning stage performs better than the incorporation of the coreference information in pre-training a language model.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.91.pdf",
        "keywords": [
            "language",
            "human reading process",
            "machine reading comprehension",
            "pre trained language models",
            "language model",
            "anaphoric"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"enriched the pre-trained language models with syntactic, semantic and other linguistic information to improve the performance of the models\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"enriched the pre-trained language models with syntactic, semantic and other linguistic information to improve the performance of the models\""
    },
    {
        "title": "FrugalScore: Learning Cheaper, Lighter and Faster Evaluation Metrics for Automatic Text Generation",
        "authors": [
            "Moussa Kamal Eddine",
            "Guokan Shang",
            "Antoine Tixier",
            "Michalis Vazirgiannis"
        ],
        "published": "2022",
        "summary": "Fast and reliable evaluation metrics are key to R&D progress. While traditional natural language generation metrics are fast, they are not very reliable. Conversely, new metrics based on large pretrained language models are much more reliable, but require significant computational resources. In this paper, we propose FrugalScore, an approach to learn a fixed, low cost version of any expensive NLG metric, while retaining most of its original performance. Experiments with BERTScore and MoverScore on summarization and translation show that FrugalScore is on par with the original metrics (and sometimes better), while having several orders of magnitude less parameters and running several times faster. On average over all learned metrics, tasks, and variants, FrugalScore retains 96.8% of the performance, runs 24 times faster, and has 35 times less parameters than the original metrics. We make our trained metrics publicly available, to benefit the entire NLP community and in particular researchers and practitioners with limited resources.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.93.pdf",
        "keywords": [
            "frugalscore",
            "summarization",
            "automatic text generation",
            "metrics",
            "language generation",
            "learn",
            "language models",
            "translation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Conversely, new metrics based on large pretrained language models are much more reliable, but require significant computational resources.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Conversely, new metrics based on large pretrained language models are much more reliable, but require significant computational resources.\""
    },
    {
        "title": "Better Language Model with Hypernym Class Prediction",
        "authors": [
            "He Bai",
            "Tong Wang",
            "Alessandro Sordoni",
            "Peng Shi"
        ],
        "published": "2022",
        "summary": "Class-based language models (LMs) have been long devised to address context sparsity in n-gram LMs. In this study, we revisit this approach in the context of neural LMs. We hypothesize that class-based prediction leads to an implicit context aggregation for similar words and thus can improve generalization for rare words. We map words that have a common WordNet hypernym to the same class and train large neural LMs by gradually annealing from predicting the class to token prediction during training. Empirically, this curriculum learning strategy consistently improves perplexity over various large, highly-performant state-of-the-art Transformer-based models on two datasets, WikiText-103 and ARXIV. Our analysis shows that the performance improvement is achieved without sacrificing performance on rare words. Finally, we document other attempts that failed to yield empirical gains, and discuss future directions for the adoption of class-based LMs on a larger scale.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.96.pdf",
        "keywords": [
            "hypernym class prediction",
            "class based prediction",
            "wordnet hypernym",
            "class based language models",
            "language model",
            "token prediction",
            "context aggregation",
            "neural lms"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Class-based language models (LMs) have been long devised to address context sparsity in n-gram LMs.\"\n\nThis paper mentions a limitation of LMs (context sparsity) but does not elaborate on it in detail.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Class-based language models (LMs) have been long devised to address context sparsity in n-gram LMs.\"\n\nThis paper mentions a limitation of LMs (context sparsity) but does not elaborate on it in detail."
    },
    {
        "title": "Structured Pruning Learns Compact and Accurate Models",
        "authors": [
            "Mengzhou Xia",
            "Zexuan Zhong",
            "Danqi Chen"
        ],
        "published": "2022",
        "summary": "The growing size of neural language models has led to increased attention in model compression. The two predominant approaches are pruning, which gradually removes weights from a pre-trained model, and distillation, which trains a smaller compact model to match a larger one. Pruning methods can significantly reduce the model size but hardly achieve large speedups as distillation. However, distillation methods require large amounts of unlabeled data and are expensive to train. In this work, we propose a task-specific structured pruning method CoFi (Coarse- and Fine-grained Pruning), which delivers highly parallelizable subnetworks and matches the distillation methods in both accuracy and latency, without resorting to any unlabeled data. Our key insight is to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads and hidden units) modules, which controls the pruning decision of each parameter with masks of different granularity. We also devise a layerwise distillation strategy to transfer knowledge from unpruned to pruned models during optimization. Our experiments on GLUE and SQuAD datasets show that CoFi yields models with over 10X speedups with a small accuracy drop, showing its effectiveness and efficiency compared to previous pruning and distillation approaches.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.107.pdf",
        "keywords": [
            "pruning",
            "structured pruning",
            "distillation",
            "compact",
            "speedups",
            "model compression"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Differentiable Multi-Agent Actor-Critic for Multi-Step Radiology Report Summarization",
        "authors": [
            "Sanjeev Kumar Karn",
            "Ning Liu",
            "Hinrich Schuetze",
            "Oladimeji Farri"
        ],
        "published": "2022",
        "summary": "The IMPRESSIONS section of a radiology report about an imaging study is a summary of the radiologist’s reasoning and conclusions, and it also aids the referring physician in confirming or excluding certain diagnoses. A cascade of tasks are required to automatically generate an abstractive summary of the typical information-rich radiology report. These tasks include acquisition of salient content from the report and generation of a concise, easily consumable IMPRESSIONS section. Prior research on radiology report summarization has focused on single-step end-to-end models – which subsume the task of salient content acquisition. To fully explore the cascade structure and explainability of radiology report summarization, we introduce two innovations. First, we design a two-step approach: extractive summarization followed by abstractive summarization. Second, we additionally break down the extractive part into two independent tasks: extraction of salient (1) sentences and (2) keywords. Experiments on English radiology reports from two clinical sites show our novel approach leads to a more precise summary compared to single-step and to two-step-with-single-extractive-process baselines with an overall improvement in F1 score of 3-4%.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.109.pdf",
        "keywords": [
            "extractive summarization",
            "multi step radiology",
            "actor critic"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "SummN: A Multi-Stage Summarization Framework for Long Input Dialogues and Documents",
        "authors": [
            "Yusen Zhang",
            "Ansong Ni",
            "Ziming Mao",
            "Chen Henry Wu",
            "Chenguang Zhu",
            "Budhaditya Deb",
            "Ahmed Awadallah",
            "Dragomir Radev",
            "Rui Zhang"
        ],
        "published": "2022",
        "summary": "Text summarization helps readers capture salient information from documents, news, interviews, and meetings. However, most state-of-the-art pretrained language models (LM) are unable to efficiently process long text for many summarization tasks. In this paper, we propose SummN, a simple, flexible, and effective multi-stage framework for input texts that are longer than the maximum context length of typical pretrained LMs. SummN first splits the data samples and generates a coarse summary in multiple stages and then produces the final fine-grained summary based on it. Our framework can process input text of arbitrary length by adjusting the number of stages while keeping the LM input size fixed. Moreover, it can deal with both single-source documents and dialogues, and it can be used on top of different backbone abstractive summarization models. To the best of our knowledge, SummN is the first multi-stage split-then-summarize framework for long input summarization. Our experiments demonstrate that SummN outperforms previous state-of-the-art methods by improving ROUGE scores on three long meeting summarization datasets AMI, ICSI, and QMSum, two long TV series datasets from SummScreen, and a long document summarization dataset GovReport. Our data and code are available at https://github.com/psunlpgroup/Summ-N.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.112.pdf",
        "keywords": [
            "summarization",
            "dialogues",
            "language models",
            "document summarization",
            "text summarization",
            "multi stage split"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"most state-of-the-art pretrained language models (LM) are unable to efficiently process long text for many summarization tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"most state-of-the-art pretrained language models (LM) are unable to efficiently process long text for many summarization tasks.\""
    },
    {
        "title": "Domain Knowledge Transferring for Pre-trained Language Model via Calibrated Activation Boundary Distillation",
        "authors": [
            "Dongha Choi",
            "HongSeok Choi",
            "Hyunju Lee"
        ],
        "published": "2022",
        "summary": "Since the development and wide use of pretrained language models (PLMs), several approaches have been applied to boost their performance on downstream tasks in specific domains, such as biomedical or scientific domains. Additional pre-training with in-domain texts is the most common approach for providing domain-specific knowledge to PLMs. However, these pre-training methods require considerable in-domain data and training resources and a longer training time. Moreover, the training must be re-performed whenever a new PLM emerges. In this study, we propose a domain knowledge transferring (DoKTra) framework for PLMs without additional in-domain pretraining. Specifically, we extract the domain knowledge from an existing in-domain pretrained language model and transfer it to other PLMs by applying knowledge distillation. In particular, we employ activation boundary distillation, which focuses on the activation of hidden neurons. We also apply an entropy regularization term in both teacher training and distillation to encourage the model to generate reliable output probabilities, and thus aid the distillation. By applying the proposed DoKTra framework to downstream tasks in the biomedical, clinical, and financial domains, our student models can retain a high percentage of teacher performance and even outperform the teachers in certain tasks. Our code is available at https://github.com/DMCB-GIST/DoKTra.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.116.pdf",
        "keywords": [
            "knowledge distillation",
            "activation boundary distillation",
            "calibrated activation boundary distillation",
            "pretrained language models",
            "domain knowledge transferring",
            "pre trained language model"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these pre-training methods require considerable in-domain data and training resources and a longer training time. Moreover, the training must be re-performed whenever a new PLM emerges.\"\n\nThis abstract mentions limitations of pre-trained language models (e.g., requiring considerable in-domain data and training resources, and the need for re-training with new models), but these limitations are not the primary focus",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these pre-training methods require considerable in-domain data and training resources and a longer training time. Moreover, the training must be re-performed whenever a new PLM emerges.\"\n\nThis abstract mentions limitations of pre-trained language models (e.g., requiring considerable in-domain data and training resources, and the need for re-training with new models), but these limitations are not the primary focus"
    },
    {
        "title": "Leveraging Similar Users for Personalized Language Modeling with Limited Data",
        "authors": [
            "Charles Welch",
            "Chenxi Gu",
            "Jonathan K. Kummerfeld",
            "Veronica Perez-Rosas",
            "Rada Mihalcea"
        ],
        "published": "2022",
        "summary": "Personalized language models are designed and trained to capture language patterns specific to individual users. This makes them more accurate at predicting what a user will write. However, when a new user joins a platform and not enough text is available, it is harder to build effective personalized language models. We propose a solution for this problem, using a model trained on users that are similar to a new user. In this paper, we explore strategies for finding the similarity between new users and existing ones and methods for using the data from existing users who are a good match. We further explore the trade-off between available data for new users and how well their language can be modeled.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.122.pdf",
        "keywords": [
            "personalized",
            "personalized language modeling"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, when a new user joins a platform and not enough text is available, it is harder to build effective personalized language models.\"\n\nThis evidence briefly mentions a limitation of personalized language models, specifically the challenge of building effective models with limited data for new users.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, when a new user joins a platform and not enough text is available, it is harder to build effective personalized language models.\"\n\nThis evidence briefly mentions a limitation of personalized language models, specifically the challenge of building effective models with limited data for new users."
    },
    {
        "title": "Probing Structured Pruning on Multilingual Pre-trained Models: Settings, Algorithms, and Efficiency",
        "authors": [
            "Yanyang Li",
            "Fuli Luo",
            "Runxin Xu",
            "Songfang Huang",
            "Fei Huang",
            "Liwei Wang"
        ],
        "published": "2022",
        "summary": "Structured pruning has been extensively studied on monolingual pre-trained language models and is yet to be fully evaluated on their multilingual counterparts. This work investigates three aspects of structured pruning on multilingual pre-trained language models: settings, algorithms, and efficiency. Experiments on nine downstream tasks show several counter-intuitive phenomena: for settings, individually pruning for each language does not induce a better result; for algorithms, the simplest method performs the best; for efficiency, a fast model does not imply that it is also small. To facilitate the comparison on all sparsity levels, we present Dynamic Sparsification, a simple approach that allows training the model once and adapting to different model sizes at inference. We hope this work fills the gap in the study of structured pruning on multilingual pre-trained models and sheds light on future research.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.130.pdf",
        "keywords": [
            "pruning",
            "language models",
            "pre trained language models",
            "pre trained models",
            "probing",
            "efficiency"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"for settings, individually pruning for each language does not induce a better result; for algorithms, the simplest method performs the best; for efficiency, a fast model does not imply that it is also small.\"\n\nThis abstract mentions some counter-intuitive phenomena related to structured pruning on multilingual pre-trained language models, which can be seen as limitations of LLMs, but they are not the",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"for settings, individually pruning for each language does not induce a better result; for algorithms, the simplest method performs the best; for efficiency, a fast model does not imply that it is also small.\"\n\nThis abstract mentions some counter-intuitive phenomena related to structured pruning on multilingual pre-trained language models, which can be seen as limitations of LLMs, but they are not the"
    },
    {
        "title": "GPT-D: Inducing Dementia-related Linguistic Anomalies by Deliberate Degradation of Artificial Neural Language Models",
        "authors": [
            "Changye Li",
            "David Knopman",
            "Weizhe Xu",
            "Trevor Cohen",
            "Serguei Pakhomov"
        ],
        "published": "2022",
        "summary": "Deep learning (DL) techniques involving fine-tuning large numbers of model parameters have delivered impressive performance on the task of discriminating between language produced by cognitively healthy individuals, and those with Alzheimer’s disease (AD). However, questions remain about their ability to generalize beyond the small reference sets that are publicly available for research. As an alternative to fitting model parameters directly, we propose a novel method by which a Transformer DL model (GPT-2) pre-trained on general English text is paired with an artificially degraded version of itself (GPT-D), to compute the ratio between these two models’ perplexities on language from cognitively healthy and impaired individuals. This technique approaches state-of-the-art performance on text data from a widely used “Cookie Theft” picture description task, and unlike established alternatives also generalizes well to spontaneous conversations. Furthermore, GPT-D generates text with characteristics known to be associated with AD, demonstrating the induction of dementia-related linguistic anomalies. Our study is a step toward better understanding of the relationships between the inner workings of generative neural language models, the language that they produce, and the deleterious effects of dementia on human speech and language characteristics.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.131.pdf",
        "keywords": [
            "deep learning",
            "dementia",
            "linguistic",
            "linguistic anomalies",
            "neural language models",
            "artificial neural language models",
            "transformer dl model"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, questions remain about their ability to generalize beyond the small reference sets that are publicly available for research.\"\n\nThis paper mentions a limitation of LLMs (generalization) in passing, but does not explore it in depth.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, questions remain about their ability to generalize beyond the small reference sets that are publicly available for research.\"\n\nThis paper mentions a limitation of LLMs (generalization) in passing, but does not explore it in depth."
    },
    {
        "title": "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models",
        "authors": [
            "Nicholas Meade",
            "Elinor Poole-Dayan",
            "Siva Reddy"
        ],
        "published": "2022",
        "summary": "Recent work has shown pre-trained language models capture social biases from the large amounts of text they are trained on. This has attracted attention to developing techniques that mitigate such biases. In this work, we perform an empirical survey of five recently proposed bias mitigation techniques: Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace Projection, Self-Debias, and SentenceDebias. We quantify the effectiveness of each technique using three intrinsic bias benchmarks while also measuring the impact of these techniques on a model’s language modeling ability, as well as its performance on downstream NLU tasks. We experimentally find that: (1) Self-Debias is the strongest debiasing technique, obtaining improved scores on all bias benchmarks; (2) Current debiasing techniques perform less consistently when mitigating non-gender biases; And (3) improvements on bias benchmarks such as StereoSet and CrowS-Pairs by using debiasing strategies are often accompanied by a decrease in language modeling ability, making it difficult to determine whether the bias mitigation was effective.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.132.pdf",
        "keywords": [
            "debiasing",
            "pre trained language models",
            "language modeling",
            "self debias",
            "empirical survey",
            "debiasing techniques",
            "bias mitigation",
            "bias mitigation techniques",
            "bias benchmarks",
            "counterfactual data augmentation",
            "s language modeling",
            "effectiveness"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent work has shown pre-trained language models capture social biases from the large amounts of text they are trained on.\"; \"improvements on bias benchmarks such as StereoSet and CrowS-Pairs by using debiasing strategies are often accompanied by a decrease in language modeling ability, making it difficult to determine whether the bias mitigation was effective.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Recent work has shown pre-trained language models capture social biases from the large amounts of text they are trained on.\"; \"improvements on bias benchmarks such as StereoSet and CrowS-Pairs by using debiasing strategies are often accompanied by a decrease in language modeling ability, making it difficult to determine whether the bias mitigation was effective.\""
    },
    {
        "title": "Exploring and Adapting Chinese GPT to Pinyin Input Method",
        "authors": [
            "Minghuan Tan",
            "Yong Dai",
            "Duyu Tang",
            "Zhangyin Feng",
            "Guoping Huang",
            "Jing Jiang",
            "Jiwei Li",
            "Shuming Shi"
        ],
        "published": "2022",
        "summary": "While GPT has become the de-facto method for text generation tasks, its application to pinyin input method remains unexplored. In this work, we make the first exploration to leverage Chinese GPT for pinyin input method. We find that a frozen GPT achieves state-of-the-art performance on perfect pinyin. However, the performance drops dramatically when the input includes abbreviated pinyin.A reason is that an abbreviated pinyin can be mapped to many perfect pinyin, which links to even larger number of Chinese characters. We mitigate this issue with two strategies,including enriching the context with pinyin and optimizing the training process to help distinguish homophones. To further facilitate the evaluation of pinyin input method, we create a dataset consisting of 270K instances from fifteen domains. Results show that our approach improves the performance on abbreviated pinyin across all domains. Model analysis demonstrates that both strategiescontribute to the performance boost.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.133.pdf",
        "keywords": [
            "pinyin",
            "pinyin input",
            "pinyin input method"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the performance drops dramatically when the input includes abbreviated pinyin.A reason is that an abbreviated pinyin can be mapped to many perfect pinyin, which links to even larger number of Chinese characters.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the performance drops dramatically when the input includes abbreviated pinyin.A reason is that an abbreviated pinyin can be mapped to many perfect pinyin, which links to even larger number of Chinese characters.\""
    },
    {
        "title": "Enhancing Cross-lingual Natural Language Inference by Prompt-learning from Cross-lingual Templates",
        "authors": [
            "Kunxun Qi",
            "Hai Wan",
            "Jianfeng Du",
            "Haolan Chen"
        ],
        "published": "2022",
        "summary": "Cross-lingual natural language inference (XNLI) is a fundamental task in cross-lingual natural language understanding. Recently this task is commonly addressed by pre-trained cross-lingual language models. Existing methods usually enhance pre-trained language models with additional data, such as annotated parallel corpora. These additional data, however, are rare in practice, especially for low-resource languages. Inspired by recent promising results achieved by prompt-learning, this paper proposes a novel prompt-learning based framework for enhancing XNLI. It reformulates the XNLI problem to a masked language modeling problem by constructing cloze-style questions through cross-lingual templates. To enforce correspondence between different languages, the framework augments a new question for every question using a sampled template in another language and then introduces a consistency loss to make the answer probability distribution obtained from the new question as similar as possible with the corresponding distribution obtained from the original question. Experimental results on two benchmark datasets demonstrate that XNLI models enhanced by our proposed framework significantly outperform original ones under both the full-shot and few-shot cross-lingual transfer settings.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.134.pdf",
        "keywords": [
            "natural language inference",
            "templates"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing methods usually enhance pre-trained language models with additional data, such as annotated parallel corpora. These additional data, however, are rare in practice, especially for low-resource languages.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of existing methods for enhancing pre-trained language models (the rarity of additional data, especially for low-resource languages), but does not elaborate on",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing methods usually enhance pre-trained language models with additional data, such as annotated parallel corpora. These additional data, however, are rare in practice, especially for low-resource languages.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of existing methods for enhancing pre-trained language models (the rarity of additional data, especially for low-resource languages), but does not elaborate on"
    },
    {
        "title": "Enhancing Chinese Pre-trained Language Model via Heterogeneous Linguistics Graph",
        "authors": [
            "Yanzeng Li",
            "Jiangxia Cao",
            "Xin Cong",
            "Zhenyu Zhang",
            "Bowen Yu",
            "Hongsong Zhu",
            "Tingwen Liu"
        ],
        "published": "2022",
        "summary": "Chinese pre-trained language models usually exploit contextual character information to learn representations, while ignoring the linguistics knowledge, e.g., word and sentence information. Hence, we propose a task-free enhancement module termed as Heterogeneous Linguistics Graph (HLG) to enhance Chinese pre-trained language models by integrating linguistics knowledge. Specifically, we construct a hierarchical heterogeneous graph to model the characteristics linguistics structure of Chinese language, and conduct a graph-based method to summarize and concretize information on different granularities of Chinese linguistics hierarchies. Experimental results demonstrate our model has the ability to improve the performance of vanilla BERT, BERTwwm and ERNIE 1.0 on 6 natural language processing tasks with 10 benchmark datasets. Further, the detailed experimental analyses have proven that this kind of modelization achieves more improvements compared with previous strong baseline MWA. Meanwhile, our model introduces far fewer parameters (about half of MWA) and the training/inference speed is about 7x faster than MWA.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.140.pdf",
        "keywords": [
            "linguistics",
            "heterogeneous linguistics graph",
            "pre trained language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Chinese pre-trained language models usually exploit contextual character information to learn representations, while ignoring the linguistics knowledge, e.g., word and sentence information.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Chinese pre-trained language models usually exploit contextual character information to learn representations, while ignoring the linguistics knowledge, e.g., word and sentence information.\""
    },
    {
        "title": "Metaphors in Pre-Trained Language Models: Probing and Generalization Across Datasets and Languages",
        "authors": [
            "Ehsan Aghazadeh",
            "Mohsen Fayyaz",
            "Yadollah Yaghoobzadeh"
        ],
        "published": "2022",
        "summary": "Human languages are full of metaphorical expressions. Metaphors help people understand the world by connecting new concepts and domains to more familiar ones. Large pre-trained language models (PLMs) are therefore assumed to encode metaphorical knowledge useful for NLP systems. In this paper, we investigate this hypothesis for PLMs, by probing metaphoricity information in their encodings, and by measuring the cross-lingual and cross-dataset generalization of this information. We present studies in multiple metaphor detection datasets and in four languages (i.e., English, Spanish, Russian, and Farsi). Our extensive experiments suggest that contextual representations in PLMs do encode metaphorical knowledge, and mostly in their middle layers. The knowledge is transferable between languages and datasets, especially when the annotation is consistent across training and testing sets. Our findings give helpful insights for both cognitive and NLP scientists.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.144.pdf",
        "keywords": [
            "languages",
            "metaphorical knowledge",
            "metaphors",
            "pre trained language models",
            "multiple metaphor detection",
            "metaphorical expressions"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper assumes that PLMs encode metaphorical knowledge and investigates this hypothesis, implying that the paper does not discuss limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper assumes that PLMs encode metaphorical knowledge and investigates this hypothesis, implying that the paper does not discuss limitations of LLMs."
    },
    {
        "title": "bert2BERT: Towards Reusable Pretrained Language Models",
        "authors": [
            "Cheng Chen",
            "Yichun Yin",
            "Lifeng Shang",
            "Xin Jiang",
            "Yujia Qin",
            "Fengyu Wang",
            "Zhi Wang",
            "Xiao Chen",
            "Zhiyuan Liu",
            "Qun Liu"
        ],
        "published": "2022",
        "summary": "In recent years, researchers tend to pre-train ever-larger language models to explore the upper limit of deep models. However, large language model pre-training costs intensive computational resources, and most of the models are trained from scratch without reusing the existing pre-trained models, which is wasteful. In this paper, we propose bert2BERT, which can effectively transfer the knowledge of an existing smaller pre-trained model to a large model through parameter initialization and significantly improve the pre-training efficiency of the large model. Specifically, we extend the previous function-preserving method proposed in computer vision on the Transformer-based language model, and further improve it by proposing a novel method, advanced knowledge for large model’s initialization. In addition, a two-stage learning method is proposed to further accelerate the pre-training. We conduct extensive experiments on representative PLMs (e.g., BERT and GPT) and demonstrate that (1) our method can save a significant amount of training cost compared with baselines including learning from scratch, StackBERT and MSLT; (2) our method is generic and applicable to different types of pre-trained models. In particular, bert2BERT saves about 45% and 47% computational cost of pre-training BERT BASE and GPT BASE by reusing the models of almost their half sizes.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.151.pdf",
        "keywords": [
            "language models",
            "bert2bert"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"large language model pre-training costs intensive computational resources, and most of the models are trained from scratch without reusing the existing pre-trained models, which is wasteful.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"large language model pre-training costs intensive computational resources, and most of the models are trained from scratch without reusing the existing pre-trained models, which is wasteful.\""
    },
    {
        "title": "Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment Analysis",
        "authors": [
            "Yan Ling",
            "Jianfei Yu",
            "Rui Xia"
        ],
        "published": "2022",
        "summary": "As an important task in sentiment analysis, Multimodal Aspect-Based Sentiment Analysis (MABSA) has attracted increasing attention inrecent years. However, previous approaches either (i) use separately pre-trained visual and textual models, which ignore the crossmodalalignment or (ii) use vision-language models pre-trained with general pre-training tasks, which are inadequate to identify fine-grainedaspects, opinions, and their alignments across modalities. To tackle these limitations, we propose a task-specific Vision-LanguagePre-training framework for MABSA (VLP-MABSA), which is a unified multimodal encoder-decoder architecture for all the pretrainingand downstream tasks. We further design three types of task-specific pre-training tasks from the language, vision, and multimodalmodalities, respectively. Experimental results show that our approach generally outperforms the state-of-the-art approaches on three MABSA subtasks. Further analysis demonstrates the effectiveness of each pre-training task. The source code is publicly released at https://github.com/NUSTM/VLP-MABSA.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.152.pdf",
        "keywords": [
            "sentiment analysis",
            "multimodal"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, previous approaches either (i) use separately pre-trained visual and textual models, which ignore the cross-modal alignment or (ii) use vision-language models pre-trained with general pre-training tasks, which are inadequate to identify fine-grained aspects, opinions, and their alignments across modalities.\"\n\nNote: The abstract mentions limitations of previous approaches, which include separately pre-trained visual and textual",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, previous approaches either (i) use separately pre-trained visual and textual models, which ignore the cross-modal alignment or (ii) use vision-language models pre-trained with general pre-training tasks, which are inadequate to identify fine-grained aspects, opinions, and their alignments across modalities.\"\n\nNote: The abstract mentions limitations of previous approaches, which include separately pre-trained visual and textual"
    },
    {
        "title": "“You might think about slightly revising the title”: Identifying Hedges in Peer-tutoring Interactions",
        "authors": [
            "Yann Raphalen",
            "Chloé Clavel",
            "Justine Cassell"
        ],
        "published": "2022",
        "summary": "Hedges have an important role in the management of rapport. In peer-tutoring, they are notably used by tutors in dyads experiencing low rapport to tone down the impact of instructions and negative feedback. Pursuing the objective of building a tutoring agent that manages rapport with teenagers in order to improve learning, we used a multimodal peer-tutoring dataset to construct a computational framework for identifying hedges. We compared approaches relying on pre-trained resources with others that integrate insights from the social science literature. Our best performance involved a hybrid approach that outperforms the existing baseline while being easier to interpret. We employ a model explainability tool to explore the features that characterize hedges in peer-tutoring conversations, and we identify some novel features, and the benefits of a such a hybrid model approach.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.153.pdf",
        "keywords": [
            "hedges"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs."
    },
    {
        "title": "Robust Lottery Tickets for Pre-trained Language Models",
        "authors": [
            "Rui Zheng",
            "Bao Rong",
            "Yuhao Zhou",
            "Di Liang",
            "Sirui Wang",
            "Wei Wu",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2022",
        "summary": "Recent works on Lottery Ticket Hypothesis have shown that pre-trained language models (PLMs) contain smaller matching subnetworks(winning tickets) which are capable of reaching accuracy comparable to the original models. However, these tickets are proved to be notrobust to adversarial examples, and even worse than their PLM counterparts. To address this problem, we propose a novel method based on learning binary weight masks to identify robust tickets hidden in the original PLMs. Since the loss is not differentiable for the binary mask, we assign the hard concrete distribution to the masks and encourage their sparsity using a smoothing approximation of L0 regularization. Furthermore, we design an adversarial loss objective to guide the search for robust tickets and ensure that the tickets perform well bothin accuracy and robustness. Experimental results show the significant improvement of the proposed method over previous work on adversarial robustness evaluation.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.157.pdf",
        "keywords": [
            "tickets",
            "pre trained language models",
            "lottery ticket",
            "robust tickets",
            "robust lottery tickets",
            "winning tickets"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these tickets are proved to be not robust to adversarial examples, and even worse than their PLM counterparts.\"\n\nThis abstract mentions a limitation of pre-trained language models (specifically, their vulnerability to adversarial examples), but it is not the primary focus of the paper and is used to motivate the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these tickets are proved to be not robust to adversarial examples, and even worse than their PLM counterparts.\"\n\nThis abstract mentions a limitation of pre-trained language models (specifically, their vulnerability to adversarial examples), but it is not the primary focus of the paper and is used to motivate the proposed solution."
    },
    {
        "title": "Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification",
        "authors": [
            "Shengding Hu",
            "Ning Ding",
            "Huadong Wang",
            "Zhiyuan Liu",
            "Jingang Wang",
            "Juanzi Li",
            "Wei Wu",
            "Maosong Sun"
        ],
        "published": "2022",
        "summary": "Tuning pre-trained language models (PLMs) with task-specific prompts has been a promising approach for text classification. Particularly, previous studies suggest that prompt-tuning has remarkable superiority in the low-data scenario over the generic fine-tuning methods with extra classifiers. The core idea of prompt-tuning is to insert text pieces, i.e., template, to the input and transform a classification problem into a masked language modeling problem, where a crucial step is to construct a projection, i.e., verbalizer, between a label space and a label word space. A verbalizer is usually handcrafted or searched by gradient descent, which may lack coverage and bring considerable bias and high variances to the results. In this work, we focus on incorporating external knowledge into the verbalizer, forming a knowledgeable prompttuning (KPT), to improve and stabilize prompttuning. Specifically, we expand the label word space of the verbalizer using external knowledge bases (KBs) and refine the expanded label word space with the PLM itself before predicting with the expanded label word space. Extensive experiments on zero and few-shot text classification tasks demonstrate the effectiveness of knowledgeable prompt-tuning.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.158.pdf",
        "keywords": [
            "tuning",
            "prompt tuning",
            "knowledgeable prompt tuning",
            "knowledge",
            "verbalizer",
            "fine tuning",
            "searched",
            "trained language",
            "text classification",
            "knowledgeable prompttuning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"A verbalizer is usually handcrafted or searched by gradient descent, which may lack coverage and bring considerable bias and high variances to the results.\"\n\nThis abstract mentions a limitation of the prompt-tuning approach for pre-trained language models, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"A verbalizer is usually handcrafted or searched by gradient descent, which may lack coverage and bring considerable bias and high variances to the results.\"\n\nThis abstract mentions a limitation of the prompt-tuning approach for pre-trained language models, but it is not the primary focus of the paper."
    },
    {
        "title": "Cross-Lingual Contrastive Learning for Fine-Grained Entity Typing for Low-Resource Languages",
        "authors": [
            "Xu Han",
            "Yuqi Luo",
            "Weize Chen",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Zhou Botong",
            "Hao Fei",
            "Suncong Zheng"
        ],
        "published": "2022",
        "summary": "Fine-grained entity typing (FGET) aims to classify named entity mentions into fine-grained entity types, which is meaningful for entity-related NLP tasks. For FGET, a key challenge is the low-resource problem — the complex entity type hierarchy makes it difficult to manually label data. Especially for those languages other than English, human-labeled data is extremely scarce. In this paper, we propose a cross-lingual contrastive learning framework to learn FGET models for low-resource languages. Specifically, we use multi-lingual pre-trained language models (PLMs) as the backbone to transfer the typing knowledge from high-resource languages (such as English) to low-resource languages (such as Chinese). Furthermore, we introduce entity-pair-oriented heuristic rules as well as machine translation to obtain cross-lingual distantly-supervised data, and apply cross-lingual contrastive learning on the distantly-supervised data to enhance the backbone PLMs. Experimental results show that by applying our framework, we can easily learn effective FGET models for low-resource languages, even without any language-specific human-labeled data. Our code is also available at https://github.com/thunlp/CrossET.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.159.pdf",
        "keywords": [
            "entity typing",
            "low resource languages",
            "high resource languages",
            "fine grained entity typing",
            "fine",
            "trained language models"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"For FGET, a key challenge is the low-resource problem — the complex entity type hierarchy makes it difficult to manually label data. Especially for those languages other than English, human-labeled data is extremely scarce.\"\n\nThis rating is given because the paper mentions a limitation of LLMs, specifically the low-resource problem, but it is not the primary focus of the paper. The paper",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"For FGET, a key challenge is the low-resource problem — the complex entity type hierarchy makes it difficult to manually label data. Especially for those languages other than English, human-labeled data is extremely scarce.\"\n\nThis rating is given because the paper mentions a limitation of LLMs, specifically the low-resource problem, but it is not the primary focus of the paper. The paper"
    },
    {
        "title": "PLANET: Dynamic Content Planning in Autoregressive Transformers for Long-form Text Generation",
        "authors": [
            "Zhe Hu",
            "Hou Pong Chan",
            "Jiachen Liu",
            "Xinyan Xiao",
            "Hua Wu",
            "Lifu Huang"
        ],
        "published": "2022",
        "summary": "Despite recent progress of pre-trained language models on generating fluent text, existing methods still suffer from incoherence problems in long-form text generation tasks that require proper content control and planning to form a coherent high-level logical flow. In this work, we propose PLANET, a novel generation framework leveraging autoregressive self-attention mechanism to conduct content planning and surface realization dynamically. To guide the generation of output sentences, our framework enriches the Transformer decoder with latent representations to maintain sentence-level semantic plans grounded by bag-of-words. Moreover, we introduce a new coherence-based contrastive learning objective to further improve the coherence of output. Extensive experiments are conducted on two challenging long-form text generation tasks including counterargument generation and opinion article generation. Both automatic and human evaluations show that our method significantly outperforms strong baselines and generates more coherent texts with richer contents.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.163.pdf",
        "keywords": [
            "text generation",
            "content planning",
            "coherence",
            "dynamic content planning",
            "surface realization",
            "semantic plans",
            "generation framework",
            "language",
            "transformers"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite recent progress of pre-trained language models on generating fluent text, existing methods still suffer from incoherence problems in long-form text generation tasks that require proper content control and planning to form a coherent high-level logical flow.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite recent progress of pre-trained language models on generating fluent text, existing methods still suffer from incoherence problems in long-form text generation tasks that require proper content control and planning to form a coherent high-level logical flow.\""
    },
    {
        "title": "CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation",
        "authors": [
            "Pei Ke",
            "Hao Zhou",
            "Yankai Lin",
            "Peng Li",
            "Jie Zhou",
            "Xiaoyan Zhu",
            "Minlie Huang"
        ],
        "published": "2022",
        "summary": "Existing reference-free metrics have obvious limitations for evaluating controlled text generation models. Unsupervised metrics can only provide a task-agnostic evaluation result which correlates weakly with human judgments, whereas supervised ones may overfit task-specific data with poor generalization ability to other datasets. In this paper, we propose an unsupervised reference-free metric called CTRLEval, which evaluates controlled text generation from different aspects by formulating each aspect into multiple text infilling tasks. On top of these tasks, the metric assembles the generation probabilities from a pre-trained language model without any model training. Experimental results show that our metric has higher correlations with human judgments than other baselines, while obtaining better generalization of evaluating generated texts from different models and with different qualities.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.164.pdf",
        "keywords": [
            "text generation",
            "reference free metric",
            "controlled text generation",
            "unsupervised reference free metric",
            "evaluating generated"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"whereas supervised ones may overfit task-specific data with poor generalization ability to other datasets.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"whereas supervised ones may overfit task-specific data with poor generalization ability to other datasets.\""
    },
    {
        "title": "Are Prompt-based Models Clueless?",
        "authors": [
            "Pride Kavumba",
            "Ryo Takahashi",
            "Yusuke Oda"
        ],
        "published": "2022",
        "summary": "Finetuning large pre-trained language models with a task-specific head has advanced the state-of-the-art on many natural language understanding benchmarks. However, models with a task-specific head require a lot of training data, making them susceptible to learning and exploiting dataset-specific superficial cues that do not generalize to other datasets. Prompting has reduced the data requirement by reusing the language model head and formatting the task input to match the pre-training objective. Therefore, it is expected that few-shot prompt-based models do not exploit superficial cues. This paper presents an empirical examination of whether few-shot prompt-based models also exploit superficial cues. Analyzing few-shot prompt-based models on MNLI, SNLI, HANS, and COPA has revealed that prompt-based models also exploit superficial cues. While the models perform well on instances with superficial cues, they often underperform or only marginally outperform random accuracy on instances without superficial cues.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.166.pdf",
        "keywords": [],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, models with a task-specific head require a lot of training data, making them susceptible to learning and exploiting dataset-specific superficial cues that do not generalize to other datasets... Analyzing few-shot prompt-based models on MNLI, SNLI, HANS, and COPA has revealed that prompt-based models also exploit superficial cues. While the models perform well on instances with superficial cues,",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, models with a task-specific head require a lot of training data, making them susceptible to learning and exploiting dataset-specific superficial cues that do not generalize to other datasets... Analyzing few-shot prompt-based models on MNLI, SNLI, HANS, and COPA has revealed that prompt-based models also exploit superficial cues. While the models perform well on instances with superficial cues,"
    },
    {
        "title": "Things not Written in Text: Exploring Spatial Commonsense from Visual Signals",
        "authors": [
            "Xiao Liu",
            "Da Yin",
            "Yansong Feng",
            "Dongyan Zhao"
        ],
        "published": "2022",
        "summary": "Spatial commonsense, the knowledge about spatial position and relationship between objects (like the relative size of a lion and a girl, and the position of a boy relative to a bicycle when cycling), is an important part of commonsense knowledge. Although pretrained language models (PLMs) succeed in many NLP tasks, they are shown to be ineffective in spatial commonsense reasoning. Starting from the observation that images are more likely to exhibit spatial commonsense than texts, we explore whether models with visual signals learn more spatial commonsense than text-based PLMs. We propose a spatial commonsense benchmark that focuses on the relative scales of objects, and the positional relationship between people and objects under different actions. We probe PLMs and models with visual signals, including vision-language pretrained models and image synthesis models, on this benchmark, and find that image synthesis models are more capable of learning accurate and consistent spatial knowledge than other models. The spatial knowledge from image synthesis models also helps in natural language understanding tasks that require spatial commonsense.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.168.pdf",
        "keywords": [
            "commonsense",
            "spatial commonsense",
            "image synthesis",
            "spatial knowledge",
            "visual signals",
            "language"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although pretrained language models (PLMs) succeed in many NLP tasks, they are shown to be ineffective in spatial commonsense reasoning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Although pretrained language models (PLMs) succeed in many NLP tasks, they are shown to be ineffective in spatial commonsense reasoning.\""
    },
    {
        "title": "Conditional Bilingual Mutual Information Based Adaptive Training for Neural Machine Translation",
        "authors": [
            "Songming Zhang",
            "Yijin Liu",
            "Fandong Meng",
            "Yufeng Chen",
            "Jinan Xu",
            "Jian Liu",
            "Jie Zhou"
        ],
        "published": "2022",
        "summary": "Token-level adaptive training approaches can alleviate the token imbalance problem and thus improve neural machine translation, through re-weighting the losses of different target tokens based on specific statistical metrics (e.g., token frequency or mutual information). Given that standard translation models make predictions on the condition of previous target contexts, we argue that the above statistical metrics ignore target context information and may assign inappropriate weights to target tokens. While one possible solution is to directly take target contexts into these statistical metrics, the target-context-aware statistical computing is extremely expensive, and the corresponding storage overhead is unrealistic. To solve the above issues, we propose a target-context-aware metric, named conditional bilingual mutual information (CBMI), which makes it feasible to supplement target context information for statistical metrics. Particularly, our CBMI can be formalized as the log quotient of the translation model probability and language model probability by decomposing the conditional joint distribution. Thus CBMI can be efficiently calculated during model training without any pre-specific statistical calculations and large storage overhead. Furthermore, we propose an effective adaptive training approach based on both the token- and sentence-level CBMI. Experimental results on WMT14 English-German and WMT19 Chinese-English tasks show our approach can significantly outperform the Transformer baseline and other related methods.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.169.pdf",
        "keywords": [
            "neural machine translation",
            "mutual information",
            "adaptive training",
            "conditional bilingual mutual information",
            "token level adaptive training"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Adversarial Soft Prompt Tuning for Cross-Domain Sentiment Analysis",
        "authors": [
            "Hui Wu",
            "Xiaodong Shi"
        ],
        "published": "2022",
        "summary": "Cross-domain sentiment analysis has achieved promising results with the help of pre-trained language models. As GPT-3 appears, prompt tuning has been widely explored to enable better semantic modeling in many natural language processing tasks. However, directly using a fixed predefined template for cross-domain research cannot model different distributions of the \\operatorname{[MASK]} token in different domains, thus making underuse of the prompt tuning technique. In this paper, we propose a novel Adversarial Soft Prompt Tuning method (AdSPT) to better model cross-domain sentiment analysis. On the one hand, AdSPT adopts separate soft prompts instead of hard templates to learn different vectors for different domains, thus alleviating the domain discrepancy of the \\operatorname{[MASK]} token in the masked language modeling task. On the other hand, AdSPT uses a novel domain adversarial training strategy to learn domain-invariant representations between each source domain and the target domain. Experiments on a publicly available sentiment analysis dataset show that our model achieves the new state-of-the-art results for both single-source domain adaptation and multi-source domain adaptation.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.174.pdf",
        "keywords": [
            "sentiment analysis",
            "cross domain sentiment analysis",
            "tuning",
            "adversarial soft prompt tuning",
            "multi source domain adaptation",
            "adversarial training"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, directly using a fixed predefined template for cross-domain research cannot model different distributions of the \\operatorname{[MASK]} token in different domains, thus making underuse of the prompt tuning technique.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, directly using a fixed predefined template for cross-domain research cannot model different distributions of the \\operatorname{[MASK]} token in different domains, thus making underuse of the prompt tuning technique.\""
    },
    {
        "title": "ExtEnD: Extractive Entity Disambiguation",
        "authors": [
            "Edoardo Barba",
            "Luigi Procopio",
            "Roberto Navigli"
        ],
        "published": "2022",
        "summary": "Local models for Entity Disambiguation (ED) have today become extremely powerful, in most part thanks to the advent of large pre-trained language models. However, despite their significant performance achievements, most of these approaches frame ED through classification formulations that have intrinsic limitations, both computationally and from a modeling perspective. In contrast with this trend, here we propose ExtEnD, a novel local formulation for ED where we frame this task as a text extraction problem, and present two Transformer-based architectures that implement it. Based on experiments in and out of domain, and training over two different data regimes, we find our approach surpasses all its competitors in terms of both data efficiency and raw performance. ExtEnD outperforms its alternatives by as few as 6 F1 points on the more constrained of the two data regimes and, when moving to the other higher-resourced regime, sets a new state of the art on 4 out of 4 benchmarks under consideration, with average improvements of 0.7 F1 points overall and 1.1 F1 points out of domain. In addition, to gain better insights from our results, we also perform a fine-grained evaluation of our performances on different classes of label frequency, along with an ablation study of our architectural choices and an error analysis. We release our code and models for research purposes at https://github.com/SapienzaNLP/extend.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.177.pdf",
        "keywords": [
            "entity disambiguation",
            "extractive entity disambiguation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"most of these approaches frame ED through classification formulations that have intrinsic limitations, both computationally and from a modeling perspective.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"most of these approaches frame ED through classification formulations that have intrinsic limitations, both computationally and from a modeling perspective.\""
    },
    {
        "title": "Text-to-Table: A New Way of Information Extraction",
        "authors": [
            "Xueqing Wu",
            "Jiacheng Zhang",
            "Hang Li"
        ],
        "published": "2022",
        "summary": "We study a new problem setting of information extraction (IE), referred to as text-to-table. In text-to-table, given a text, one creates a table or several tables expressing the main content of the text, while the model is learned from text-table pair data. The problem setting differs from those of the existing methods for IE. First, the extraction can be carried out from long texts to large tables with complex structures. Second, the extraction is entirely data-driven, and there is no need to explicitly define the schemas. As far as we know, there has been no previous work that studies the problem. In this work, we formalize text-to-table as a sequence-to-sequence (seq2seq) problem. We first employ a seq2seq model fine-tuned from a pre-trained language model to perform the task. We also develop a new method within the seq2seq approach, exploiting two additional techniques in table generation: table constraint and table relation embeddings. We consider text-to-table as an inverse problem of the well-studied table-to-text, and make use of four existing table-to-text datasets in our experiments on text-to-table. Experimental results show that the vanilla seq2seq model can outperform the baseline methods of using relation extraction and named entity extraction. The results also show that our method can further boost the performances of the vanilla seq2seq model. We further discuss the main challenges of the proposed task. The code and data are available at https://github.com/shirley-wu/text_to_table.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.180.pdf",
        "keywords": [
            "relation extraction",
            "named entity extraction",
            "text",
            "text to table",
            "table",
            "information extraction",
            "table generation",
            "table relation embeddings",
            "table constraint"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We further discuss the main challenges of the proposed task.\"\n\nThis abstract mentions a limitation of the proposed task, which is related to LLMs, but does not elaborate on it and focuses more on the solution and results.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We further discuss the main challenges of the proposed task.\"\n\nThis abstract mentions a limitation of the proposed task, which is related to LLMs, but does not elaborate on it and focuses more on the solution and results."
    },
    {
        "title": "MSCTD: A Multimodal Sentiment Chat Translation Dataset",
        "authors": [
            "Yunlong Liang",
            "Fandong Meng",
            "Jinan Xu",
            "Yufeng Chen",
            "Jie Zhou"
        ],
        "published": "2022",
        "summary": "Multimodal machine translation and textual chat translation have received considerable attention in recent years. Although the conversation in its natural form is usually multimodal, there still lacks work on multimodal machine translation in conversations. In this work, we introduce a new task named Multimodal Chat Translation (MCT), aiming to generate more accurate translations with the help of the associated dialogue history and visual context. To this end, we firstly construct a Multimodal Sentiment Chat Translation Dataset (MSCTD) containing 142,871 English-Chinese utterance pairs in 14,762 bilingual dialogues. Each utterance pair, corresponding to the visual context that reflects the current conversational scene, is annotated with a sentiment label. Then, we benchmark the task by establishing multiple baseline systems that incorporate multimodal and sentiment features for MCT. Preliminary experiments on two language directions (English-Chinese) verify the potential of contextual and multimodal information fusion and the positive impact of sentiment on the MCT task. Additionally, we provide a new benchmark on multimodal dialogue sentiment analysis with the constructed MSCTD. Our work can facilitate researches on both multimodal chat translation and multimodal dialogue sentiment analysis.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.186.pdf",
        "keywords": [
            "multimodal chat translation",
            "multimodal sentiment chat translation",
            "multimodal dialogue sentiment analysis",
            "multimodal",
            "multimodal machine translation"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Good Examples Make A Faster Learner: Simple Demonstration-based Learning for Low-resource NER",
        "authors": [
            "Dong-Ho Lee",
            "Akshen Kadakia",
            "Kangmin Tan",
            "Mahak Agarwal",
            "Xinyu Feng",
            "Takashi Shibuya",
            "Ryosuke Mitani",
            "Toshiyuki Sekiya",
            "Jay Pujara",
            "Xiang Ren"
        ],
        "published": "2022",
        "summary": "Recent advances in prompt-based learning have shown strong results on few-shot text classification by using cloze-style templates. Similar attempts have been made on named entity recognition (NER) which manually design templates to predict entity types for every text span in a sentence. However, such methods may suffer from error propagation induced by entity span detection, high cost due to enumeration of all possible text spans, and omission of inter-dependencies among token labels in a sentence. Here we present a simple demonstration-based learning method for NER, which lets the input be prefaced by task demonstrations for in-context learning. We perform a systematic study on demonstration strategy regarding what to include (entity examples, with or without surrounding context), how to select the examples, and what templates to use. Results on in-domain learning and domain adaptation show that the model’s performance in low-resource settings can be largely improved with a suitable demonstration strategy (e.g., a 4-17% improvement on 25 train instances). We also find that good demonstration can save many labeled examples and consistency in demonstration contributes to better performance.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.192.pdf",
        "keywords": [
            "named entity recognition",
            "demonstration",
            "domain adaptation",
            "domain learning",
            "text classification",
            "task demonstrations",
            "context learning"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Contextual Representation Learning beyond Masked Language Modeling",
        "authors": [
            "Zhiyi Fu",
            "Wangchunshu Zhou",
            "Jingjing Xu",
            "Hao Zhou",
            "Lei Li"
        ],
        "published": "2022",
        "summary": "Currently, masked language modeling (e.g., BERT) is the prime choice to learn contextualized representations. Due to the pervasiveness, it naturally raises an interesting question: how do masked language models (MLMs) learn contextual representations? In this work, we analyze the learning dynamics of MLMs and find that it adopts sampled embeddings as anchors to estimate and inject contextual semantics to representations, which limits the efficiency and effectiveness of MLMs. To address these problems, we propose TACO, a simple yet effective representation learning approach to directly model global semantics. To be specific, TACO extracts and aligns contextual semantics hidden in contextualized representations to encourage models to attend global semantics when generating contextualized representations. Experiments on the GLUE benchmark show that TACO achieves up to 5x speedup and up to 1.2 points average improvement over MLM.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.193.pdf",
        "keywords": [
            "contextualized representations",
            "contextual semantics",
            "contextual representation learning",
            "masked",
            "masked language"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"it adopts sampled embeddings as anchors to estimate and inject contextual semantics to representations, which limits the efficiency and effectiveness of MLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"it adopts sampled embeddings as anchors to estimate and inject contextual semantics to representations, which limits the efficiency and effectiveness of MLMs.\""
    },
    {
        "title": "Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-Modal Knowledge Transfer",
        "authors": [
            "Woojeong Jin",
            "Dong-Ho Lee",
            "Chenguang Zhu",
            "Jay Pujara",
            "Xiang Ren"
        ],
        "published": "2022",
        "summary": "Pre-trained language models are still far from human performance in tasks that need understanding of properties (e.g. appearance, measurable quantity) and affordances of everyday objects in the real world since the text lacks such information due to reporting bias. In this work, we study whether integrating visual knowledge into a language model can fill the gap. We investigate two types of knowledge transfer: (1) text knowledge transfer using image captions that may contain enriched visual knowledge and (2) cross-modal knowledge transfer using both images and captions with vision-language training objectives.On 5 downstream tasks that may need visual knowledge to solve the problem, we perform extensive empirical comparisons over the presented objectives.Our experiments show that visual knowledge transfer can improve performance in both low-resource and fully supervised settings.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.196.pdf",
        "keywords": [
            "knowledge transfer",
            "text knowledge transfer",
            "visual knowledge transfer",
            "modal knowledge transfer",
            "cross modal knowledge transfer"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Pre-trained language models are still far from human performance in tasks that need understanding of properties (e.g. appearance, measurable quantity) and affordances of everyday objects in the real world since the text lacks such information due to reporting bias.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Pre-trained language models are still far from human performance in tasks that need understanding of properties (e.g. appearance, measurable quantity) and affordances of everyday objects in the real world since the text lacks such information due to reporting bias.\""
    },
    {
        "title": "A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models",
        "authors": [
            "Woojeong Jin",
            "Yu Cheng",
            "Yelong Shen",
            "Weizhu Chen",
            "Xiang Ren"
        ],
        "published": "2022",
        "summary": "Large pre-trained vision-language (VL) models can learn a new task with a handful of examples and generalize to a new task without fine-tuning. However, these VL models are hard to deploy for real-world applications due to their impractically huge sizes and slow inference speed. To solve this limitation, we study prompt-based low-resource learning of VL tasks with our proposed method, FewVLM, relatively smaller than recent few-shot learners. For FewVLM, we pre-train a sequence-to-sequence transformer model with prefix language modeling (PrefixLM) and masked language modeling (MaskedLM).Furthermore, we analyze the effect of diverse prompts for few-shot tasks. Experimental results on VQA show that FewVLM with prompt-based learning outperforms Frozen which is 31x larger than FewVLM by 18.2% point and achieves comparable results to a 246x larger model, PICa.In our analysis, we observe that (1) prompts significantly affect zero-shot performance but marginally affect few-shot performance, (2) models with noisy prompts learn as quickly as hand-crafted prompts given larger training data, and (3) MaskedLM helps VQA tasks while PrefixLM boosts captioning performance. Our code is publicly available at https://github.com/woojeongjin/FewVLM",
        "pdf_link": "https://aclanthology.org/2022.acl-long.197.pdf",
        "keywords": [
            "resource",
            "language modeling"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these VL models are hard to deploy for real-world applications due to their impractically huge sizes and slow inference speed.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these VL models are hard to deploy for real-world applications due to their impractically huge sizes and slow inference speed.\""
    },
    {
        "title": "Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval",
        "authors": [
            "Luyu Gao",
            "Jamie Callan"
        ],
        "published": "2022",
        "summary": "Recent research demonstrates the effectiveness of using fine-tuned language models (LM) for dense retrieval. However, dense retrievers are hard to train, typically requiring heavily engineered fine-tuning pipelines to realize their full potential. In this paper, we identify and address two underlying problems of dense retrievers: i) fragility to training data noise and ii) requiring large batches to robustly learn the embedding space. We use the recently proposed Condenser pre-training architecture, which learns to condense information into the dense vector through LM pre-training. On top of it, we propose coCondenser, which adds an unsupervised corpus-level contrastive loss to warm up the passage embedding space. Experiments on MS-MARCO, Natural Question, and Trivia QA datasets show that coCondenser removes the need for heavy data engineering such as augmentation, synthesis, or filtering, and the need for large batch training. It shows comparable performance to RocketQA, a state-of-the-art, heavily engineered system, using simple small batch fine-tuning.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.203.pdf",
        "keywords": [
            "unsupervised",
            "fine tuned language models",
            "dense passage retrieval",
            "unsupervised corpus",
            "dense retrieval"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, dense retrievers are hard to train, typically requiring heavily engineered fine-tuning pipelines to realize their full potential.\"\n\nThis paper mentions a limitation of dense retrievers (which are often based on LLMs) in passing, but does not explore it in detail.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, dense retrievers are hard to train, typically requiring heavily engineered fine-tuning pipelines to realize their full potential.\"\n\nThis paper mentions a limitation of dense retrievers (which are often based on LLMs) in passing, but does not explore it in detail."
    },
    {
        "title": "Multimodal Dialogue Response Generation",
        "authors": [
            "Qingfeng Sun",
            "Yujing Wang",
            "Can Xu",
            "Kai Zheng",
            "Yaming Yang",
            "Huang Hu",
            "Fei Xu",
            "Jessica Zhang",
            "Xiubo Geng",
            "Daxin Jiang"
        ],
        "published": "2022",
        "summary": "Responsing with image has been recognized as an important capability for an intelligent conversational agent. Yet existing works only focus on exploring the multimodal dialogue models which depend on retrieval-based methods, but neglecting generation methods. To fill in the gaps, we first present a new task: multimodal dialogue response generation (MDRG) - given the dialogue history, one model needs to generate a text sequence or an image as response. Learning such a MDRG model often requires multimodal dialogues containing both texts and images which are difficult to obtain. Motivated by the challenge in practice, we consider MDRG under a natural assumption that only limited training examples are available. In such a low-resource setting, we devise a novel conversational agent, Divter, in order to isolate parameters that depend on multimodal dialogues from the entire generation model. By this means, the major part of the model can be learned from a large number of text-only dialogues and text-image pairs respectively, then the whole parameters can be well fitted using the limited training examples. Extensive experiments demonstrate our method achieves state-of-the-art results in both automatic and human evaluation, and can generate informative text and high-resolution image responses.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.204.pdf",
        "keywords": [
            "multimodal",
            "agent",
            "multimodal dialogues",
            "multimodal dialogue response generation",
            "conversational agent"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Learning such a MDRG model often requires multimodal dialogues containing both texts and images which are difficult to obtain.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Learning such a MDRG model often requires multimodal dialogues containing both texts and images which are difficult to obtain.\""
    },
    {
        "title": "Confidence Based Bidirectional Global Context Aware Training Framework for Neural Machine Translation",
        "authors": [
            "Chulun Zhou",
            "Fandong Meng",
            "Jie Zhou",
            "Min Zhang",
            "Hongji Wang",
            "Jinsong Su"
        ],
        "published": "2022",
        "summary": "Most dominant neural machine translation (NMT) models are restricted to make predictions only according to the local context of preceding words in a left-to-right manner. Although many previous studies try to incorporate global information into NMT models, there still exist limitations on how to effectively exploit bidirectional global context. In this paper, we propose a Confidence Based Bidirectional Global Context Aware (CBBGCA) training framework for NMT, where the NMT model is jointly trained with an auxiliary conditional masked language model (CMLM). The training consists of two stages: (1) multi-task joint training; (2) confidence based knowledge distillation. At the first stage, by sharing encoder parameters, the NMT model is additionally supervised by the signal from the CMLM decoder that contains bidirectional global contexts. Moreover, at the second stage, using the CMLM as teacher, we further pertinently incorporate bidirectional global context to the NMT model on its unconfidently-predicted target words via knowledge distillation. Experimental results show that our proposed CBBGCA training framework significantly improves the NMT model by +1.02, +1.30 and +0.57 BLEU scores on three large-scale translation datasets, namely WMT’14 English-to-German, WMT’19 Chinese-to-English and WMT’14 English-to-French, respectively.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.206.pdf",
        "keywords": [
            "global context aware",
            "global context",
            "neural machine translation",
            "knowledge distillation",
            "confidence",
            "global information",
            "joint training"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Contrastive Visual Semantic Pretraining Magnifies the Semantics of Natural Language Representations",
        "authors": [
            "Robert Wolfe",
            "Aylin Caliskan"
        ],
        "published": "2022",
        "summary": "We examine the effects of contrastive visual semantic pretraining by comparing the geometry and semantic properties of contextualized English language representations formed by GPT-2 and CLIP, a zero-shot multimodal image classifier which adapts the GPT-2 architecture to encode image captions. We find that contrastive visual semantic pretraining significantly mitigates the anisotropy found in contextualized word embeddings from GPT-2, such that the intra-layer self-similarity (mean pairwise cosine similarity) of CLIP word embeddings is under .25 in all layers, compared to greater than .95 in the top layer of GPT-2. CLIP word embeddings outperform GPT-2 on word-level semantic intrinsic evaluation tasks, and achieve a new corpus-based state of the art for the RG65 evaluation, at .88. CLIP also forms fine-grained semantic representations of sentences, and obtains Spearman’s 𝜌 = .73 on the SemEval-2017 Semantic Textual Similarity Benchmark with no fine-tuning, compared to no greater than 𝜌 = .45 in any layer of GPT-2. Finally, intra-layer self-similarity of CLIP sentence embeddings decreases as the layer index increases, finishing at .25 in the top layer, while the self-similarity of GPT-2 sentence embeddings formed using the EOS token increases layer-over-layer and never falls below .97. Our results indicate that high anisotropy is not an inevitable consequence of contextualization, and that visual semantic pretraining is beneficial not only for ordering visual representations, but also for encoding useful semantic representations of language, both on the word level and the sentence level.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.217.pdf",
        "keywords": [
            "self similarity",
            "pretraining",
            "clip word embeddings"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"high anisotropy is not an inevitable consequence of contextualization\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"high anisotropy is not an inevitable consequence of contextualization\""
    },
    {
        "title": "Achieving Conversational Goals with Unsupervised Post-hoc Knowledge Injection",
        "authors": [
            "Bodhisattwa Prasad Majumder",
            "Harsh Jhamtani",
            "Taylor Berg-Kirkpatrick",
            "Julian McAuley"
        ],
        "published": "2022",
        "summary": "A limitation of current neural dialog models is that they tend to suffer from a lack of specificity and informativeness in generated responses, primarily due to dependence on training data that covers a limited variety of scenarios and conveys limited knowledge. One way to alleviate this issue is to extract relevant knowledge from external sources at decoding time and incorporate it into the dialog response. In this paper, we propose a post-hoc knowledge-injection technique where we first retrieve a diverse set of relevant knowledge snippets conditioned on both the dialog history and an initial response from an existing dialog model. We construct multiple candidate responses, individually injecting each retrieved snippet into the initial response using a gradient-based decoding method, and then select the final response with an unsupervised ranking step. Our experiments in goal-oriented and knowledge-grounded dialog settings demonstrate that human annotators judge the outputs from the proposed method to be more engaging and informative compared to responses from prior dialog systems. We further show that knowledge-augmentation promotes success in achieving conversational goals in both experimental settings.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.224.pdf",
        "keywords": [
            "knowledge injection",
            "knowledge augmentation",
            "hoc knowledge injection"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"A limitation of current neural dialog models is that they tend to suffer from a lack of specificity and informativeness in generated responses, primarily due to dependence on training data that covers a limited variety of scenarios and conveys limited knowledge.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"A limitation of current neural dialog models is that they tend to suffer from a lack of specificity and informativeness in generated responses, primarily due to dependence on training data that covers a limited variety of scenarios and conveys limited knowledge.\""
    },
    {
        "title": "Generated Knowledge Prompting for Commonsense Reasoning",
        "authors": [
            "Jiacheng Liu",
            "Alisa Liu",
            "Ximing Lu",
            "Sean Welleck",
            "Peter West",
            "Ronan Le Bras",
            "Yejin Choi",
            "Hannaneh Hajishirzi"
        ],
        "published": "2022",
        "summary": "It remains an open question whether incorporating external knowledge benefits commonsense reasoning while maintaining the flexibility of pretrained sequence models. To investigate this question, we develop generated knowledge prompting, which consists of generating knowledge from a language model, then providing the knowledge as additional input when answering a question. Our method does not require task-specific supervision for knowledge integration, or access to a structured knowledge base, yet it improves performance of large-scale, state-of-the-art models on four commonsense reasoning tasks, achieving state-of-the-art results on numerical commonsense (NumerSense), general commonsense (CommonsenseQA 2.0), and scientific commonsense (QASC) benchmarks. Generated knowledge prompting highlights large-scale language models as flexible sources of external knowledge for improving commonsense reasoning. Our code is available at github.com/liujch1998/GKP",
        "pdf_link": "https://aclanthology.org/2022.acl-long.225.pdf",
        "keywords": [
            "commonsense",
            "knowledge prompting",
            "commonsense reasoning",
            "generated knowledge prompting"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "Life after BERT: What do Other Muppets Understand about Language?",
        "authors": [
            "Vladislav Lialin",
            "Kevin Zhao",
            "Namrata Shivagunde",
            "Anna Rumshisky"
        ],
        "published": "2022",
        "summary": "Existing pre-trained transformer analysis works usually focus only on one or two model families at a time, overlooking the variability of the architecture and pre-training objectives. In our work, we utilize the oLMpics bench- mark and psycholinguistic probing datasets for a diverse set of 29 models including T5, BART, and ALBERT. Additionally, we adapt the oLMpics zero-shot setup for autoregres- sive models and evaluate GPT networks of different sizes. Our findings show that none of these models can resolve compositional questions in a zero-shot fashion, suggesting that this skill is not learnable using existing pre-training objectives. Furthermore, we find that global model decisions such as architecture, directionality, size of the dataset, and pre-training objective are not predictive of a model’s linguistic capabilities.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.227.pdf",
        "keywords": [],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our findings show that none of these models can resolve compositional questions in a zero-shot fashion, suggesting that this skill is not learnable using existing pre-training objectives.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Our findings show that none of these models can resolve compositional questions in a zero-shot fashion, suggesting that this skill is not learnable using existing pre-training objectives.\""
    },
    {
        "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
        "authors": [
            "Stephanie Lin",
            "Jacob Hilton",
            "Owain Evans"
        ],
        "published": "2022",
        "summary": "We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.229.pdf",
        "keywords": [
            "truthful",
            "human falsehoods",
            "imitation"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution.\""
    },
    {
        "title": "Adaptive Testing and Debugging of NLP Models",
        "authors": [
            "Marco Tulio Ribeiro",
            "Scott Lundberg"
        ],
        "published": "2022",
        "summary": "Current approaches to testing and debugging NLP models rely on highly variable human creativity and extensive labor, or only work for a very restrictive class of bugs. We present AdaTest, a process which uses large scale language models (LMs) in partnership with human feedback to automatically write unit tests highlighting bugs in a target model. Such bugs are then addressed through an iterative text-fix-retest loop, inspired by traditional software development. In experiments with expert and non-expert users and commercial / research models for 8 different tasks, AdaTest makes users 5-10x more effective at finding bugs than current approaches, and helps users effectively fix bugs without adding new bugs.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.230.pdf",
        "keywords": [
            "debugging",
            "language models",
            "adaptive testing and debugging"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but \"large scale language models (LMs)\" is mentioned as part of the solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but \"large scale language models (LMs)\" is mentioned as part of the solution."
    },
    {
        "title": "ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection",
        "authors": [
            "Thomas Hartvigsen",
            "Saadia Gabriel",
            "Hamid Palangi",
            "Maarten Sap",
            "Dipankar Ray",
            "Ece Kamar"
        ],
        "published": "2022",
        "summary": "Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate. Such over-reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language. To help mitigate these issues, we create ToxiGen, a new large-scale and machine-generated dataset of 274k toxic and benign statements about 13 minority groups. We develop a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method to generate subtly toxic and benign text with a massive pretrained language model. Controlling machine generation in this way allows ToxiGen to cover implicitly toxic text at a larger scale, and about more demographic groups, than previous resources of human-written text. We conduct a human evaluation on a challenging subset of ToxiGen and find that annotators struggle to distinguish machine-generated text from human-written language. We also find that 94.5% of toxic examples are labeled as hate speech by human annotators. Using three publicly-available datasets, we show that finetuning a toxicity classifier on our data improves its performance on human-written data substantially. We also demonstrate that ToxiGen can be used to fight machine-generated toxicity as finetuning improves the classifier significantly on our evaluation subset.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.234.pdf",
        "keywords": [
            "machine generated",
            "implicit hate speech detection",
            "toxic language detection"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate.\"\n\nThis evidence suggests that the paper discusses a limitation of LLMs in the context of toxic language detection, but it is not a major focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate.\"\n\nThis evidence suggests that the paper discusses a limitation of LLMs in the context of toxic language detection, but it is not a major focus of the paper."
    },
    {
        "title": "Hallucinated but Factual! Inspecting the Factuality of Hallucinations in Abstractive Summarization",
        "authors": [
            "Meng Cao",
            "Yue Dong",
            "Jackie Cheung"
        ],
        "published": "2022",
        "summary": "State-of-the-art abstractive summarization systems often generate hallucinations; i.e., content that is not directly inferable from the source text. Despite being assumed to be incorrect, we find that much hallucinated content is actually consistent with world knowledge, which we call factual hallucinations. Including these factual hallucinations in a summary can be beneficial because they provide useful background information. In this work, we propose a novel detection approach that separates factual from non-factual hallucinations of entities. Our method is based on an entity’s prior and posterior probabilities according to pre-trained and finetuned masked language models, respectively. Empirical results suggest that our method vastly outperforms two baselines in both accuracy and F1 scores and has a strong correlation with human judgments on factuality classification tasks. Furthermore, we use our method as a reward signal to train a summarization system using an off-line reinforcement learning (RL) algorithm that can significantly improve the factuality of generated summaries while maintaining the level of abstractiveness.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.236.pdf",
        "keywords": [
            "factuality",
            "summarization",
            "posterior probabilities",
            "reinforcement learning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions \"masked language models\" as part of the proposed method, which is related to LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None, but the paper mentions \"masked language models\" as part of the proposed method, which is related to LLMs."
    },
    {
        "title": "Image Retrieval from Contextual Descriptions",
        "authors": [
            "Benno Krojer",
            "Vaibhav Adlakha",
            "Vibhav Vineet",
            "Yash Goyal",
            "Edoardo Ponti",
            "Siva Reddy"
        ],
        "published": "2022",
        "summary": "The ability to integrate context, including perceptual and temporal cues, plays a pivotal role in grounding the meaning of a linguistic utterance. In order to measure to what extent current vision-and-language models master this ability, we devise a new multimodal challenge, Image Retrieval from Contextual Descriptions (ImageCoDe). In particular, models are tasked with retrieving the correct image from a set of 10 minimally contrastive candidates based on a contextual description. As such, each description contains only the details that help distinguish between images. Because of this, descriptions tend to be complex in terms of syntax and discourse and require drawing pragmatic inferences. Images are sourced from both static pictures and video frames. We benchmark several state-of-the-art models, including both cross-encoders such as ViLBERT and bi-encoders such as CLIP, on ImageCoDe.Our results reveal that these models dramatically lag behind human performance: the best variant achieves an accuracy of 20.9 on video frames and 59.4 on static pictures, compared with 90.8 in humans. Furthermore, we experiment with new model variants that are better equipped to incorporate visual and temporal context into their representations, which achieve modest gains. Our hope is that ImageCoDE will foster progress in grounded language understanding by encouraging models to focus on fine-grained visual differences.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.241.pdf",
        "keywords": [
            "image retrieval",
            "contextual descriptions",
            "multimodal",
            "language models",
            "grounded language",
            "context"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our results reveal that these models dramatically lag behind human performance: the best variant achieves an accuracy of 20.9 on video frames and 59.4 on static pictures, compared with 90.8 in humans.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Our results reveal that these models dramatically lag behind human performance: the best variant achieves an accuracy of 20.9 on video frames and 59.4 on static pictures, compared with 90.8 in humans.\""
    },
    {
        "title": "Multilingual Molecular Representation Learning via Contrastive Pre-training",
        "authors": [
            "Zhihui Guo",
            "Pramod Sharma",
            "Andy Martinez",
            "Liang Du",
            "Robin Abraham"
        ],
        "published": "2022",
        "summary": "Molecular representation learning plays an essential role in cheminformatics. Recently, language model-based approaches have gained popularity as an alternative to traditional expert-designed features to encode molecules. However, these approaches only utilize a single molecular language for representation learning. Motivated by the fact that a given molecule can be described using different languages such as Simplified Molecular Line Entry System (SMILES), The International Union of Pure and Applied Chemistry (IUPAC), and The IUPAC International Chemical Identifier (InChI), we propose a multilingual molecular embedding generation approach called MM-Deacon (multilingual molecular domain embedding analysis via contrastive learning). MM-Deacon is pre-trained using SMILES and IUPAC as two different languages on large-scale molecules. We evaluated the robustness of our method on seven molecular property prediction tasks from MoleculeNet benchmark, zero-shot cross-lingual retrieval, and a drug-drug interaction prediction task.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.242.pdf",
        "keywords": [
            "molecular",
            "molecular representation",
            "pre training",
            "mm deacon"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these approaches only utilize a single molecular language for representation learning.\"\n\nThis paper mentions a limitation of LLM-based approaches, but only briefly and in passing, to motivate the proposed method.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, these approaches only utilize a single molecular language for representation learning.\"\n\nThis paper mentions a limitation of LLM-based approaches, but only briefly and in passing, to motivate the proposed method."
    },
    {
        "title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions",
        "authors": [
            "Swaroop Mishra",
            "Daniel Khashabi",
            "Chitta Baral",
            "Hannaneh Hajishirzi"
        ],
        "published": "2022",
        "summary": "Humans (e.g., crowdworkers) have a remarkable ability in solving different tasks, by simply reading textual instructions that define them and looking at a few examples. Despite the success of the conventional supervised learning on individual datasets, such models often struggle with generalization across tasks (e.g., a question-answering system cannot solve classification tasks). A long-standing challenge in AI is to build a model that learns a new task by understanding the human-readable instructions that define it. To study this, we introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their human-authored instructions, and 193k task instances (input-output pairs). The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema. Using this meta-dataset, we measure cross-task generalization by training models on seen tasks and measuring generalization to the remaining unseen ones. We adopt generative pre-trained language models to encode task-specific instructions along with input and generate task output. Our results indicate that models benefit from instructions when evaluated in terms of generalization to unseen tasks (19% better for models utilizing instructions). These models, however, are far behind an estimated performance upperbound indicating significant room for more progress in this direction.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.244.pdf",
        "keywords": [
            "cross task generalization",
            "crowdsourcing instructions",
            "natural instructions"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"These models, however, are far behind an estimated performance upperbound indicating significant room for more progress in this direction.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"These models, however, are far behind an estimated performance upperbound indicating significant room for more progress in this direction.\""
    },
    {
        "title": "Imputing Out-of-Vocabulary Embeddings with LOVE Makes LanguageModels Robust with Little Cost",
        "authors": [
            "Lihu Chen",
            "Gael Varoquaux",
            "Fabian Suchanek"
        ],
        "published": "2022",
        "summary": "State-of-the-art NLP systems represent inputs with word embeddings, but these are brittle when faced with Out-of-Vocabulary (OOV) words. To address this issue, we follow the principle of mimick-like models to generate vectors for unseen words, by learning the behavior of pre-trained embeddings using only the surface form of words. We present a simple contrastive learning framework, LOVE, which extends the word representation of an existing pre-trained language model (such as BERT) and makes it robust to OOV with few additional parameters. Extensive evaluations demonstrate that our lightweight model achieves similar or even better performances than prior competitors, both on original datasets and on corrupted variants. Moreover, it can be used in a plug-and-play fashion with FastText and BERT, where it significantly improves their robustness.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.245.pdf",
        "keywords": [
            "love makes languagemodels"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"State-of-the-art NLP systems represent inputs with word embeddings, but these are brittle when faced with Out-of-Vocabulary (OOV) words.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"State-of-the-art NLP systems represent inputs with word embeddings, but these are brittle when faced with Out-of-Vocabulary (OOV) words.\""
    },
    {
        "title": "NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks",
        "authors": [
            "Swaroop Mishra",
            "Arindam Mitra",
            "Neeraj Varshney",
            "Bhavdeep Sachdeva",
            "Peter Clark",
            "Chitta Baral",
            "Ashwin Kalyan"
        ],
        "published": "2022",
        "summary": "Given the ubiquitous nature of numbers in text, reasoning with numbers to perform simple calculations is an important skill of AI systems. While many datasets and models have been developed to this end, state-of-the-art AI systems are brittle; failing to perform the underlying mathematical reasoning when they appear in a slightly different scenario. Drawing inspiration from GLUE that was proposed in the context of natural language understanding, we propose NumGLUE, a multi-task benchmark that evaluates the performance of AI systems on eight different tasks, that at their core require simple arithmetic understanding. We show that this benchmark is far from being solved with neural models including state-of-the-art large-scale language models performing significantly worse than humans (lower by 46.4 %). Further, NumGLUE promotes sharing knowledge across tasks, especially those with limited training data as evidenced by the superior performance (average gain of 3.4 % on each task) when a model is jointly trained on all the tasks as opposed to task-specific modeling. Finally, we hope that NumGLUE will encourage systems that perform robust and general arithmetic reasoning within language, a first step towards being able to perform more complex mathematical reasoning.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.246.pdf",
        "keywords": [
            "arithmetic reasoning",
            "mathematical reasoning",
            "complex mathematical reasoning",
            "natural language"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"state-of-the-art large-scale language models performing significantly worse than humans (lower by 46.4 %)\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"state-of-the-art large-scale language models performing significantly worse than humans (lower by 46.4 %)\""
    },
    {
        "title": "Upstream Mitigation Is Not All You Need: Testing the Bias Transfer Hypothesis in Pre-Trained Language Models",
        "authors": [
            "Ryan Steed",
            "Swetasudha Panda",
            "Ari Kobren",
            "Michael Wick"
        ],
        "published": "2022",
        "summary": "A few large, homogenous, pre-trained models undergird many machine learning systems — and often, these models contain harmful stereotypes learned from the internet. We investigate the bias transfer hypothesis: the theory that social biases (such as stereotypes) internalized by large language models during pre-training transfer into harmful task-specific behavior after fine-tuning. For two classification tasks, we find that reducing intrinsic bias with controlled interventions before fine-tuning does little to mitigate the classifier’s discriminatory behavior after fine-tuning. Regression analysis suggests that downstream disparities are better explained by biases in the fine-tuning dataset. Still, pre-training plays a role: simple alterations to co-occurrence rates in the fine-tuning dataset are ineffective when the model has been pre-trained. Our results encourage practitioners to focus more on dataset quality and context-specific harms.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.247.pdf",
        "keywords": [
            "bias",
            "bias transfer hypothesis",
            "classification",
            "language models",
            "pre trained language models",
            "regression"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"A few large, homogenous, pre-trained models undergird many machine learning systems — and often, these models contain harmful stereotypes learned from the internet.\"; \"For two classification tasks, we find that reducing intrinsic bias with controlled interventions before fine-tuning does little to mitigate the classifier’s discriminatory behavior after fine-tuning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"A few large, homogenous, pre-trained models undergird many machine learning systems — and often, these models contain harmful stereotypes learned from the internet.\"; \"For two classification tasks, we find that reducing intrinsic bias with controlled interventions before fine-tuning does little to mitigate the classifier’s discriminatory behavior after fine-tuning.\""
    },
    {
        "title": "ConditionalQA: A Complex Reading Comprehension Dataset with Conditional Answers",
        "authors": [
            "Haitian Sun",
            "William Cohen",
            "Ruslan Salakhutdinov"
        ],
        "published": "2022",
        "summary": "We describe a Question Answering (QA) dataset that contains complex questions with conditional answers, i.e. the answers are only applicable when certain conditions apply. We call this dataset ConditionalQA. In addition to conditional answers, the dataset also features:(1) long context documents with information that is related in logically complex ways;(2) multi-hop questions that require compositional logical reasoning;(3) a combination of extractive questions, yes/no questions, questions with multiple answers, and not-answerable questions;(4) questions asked without knowing the answers. We show that ConditionalQA is challenging for many of the existing QA models, especially in selecting answer conditions. We believe that this dataset will motivate further research in answering complex questions over long documents.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.253.pdf",
        "keywords": [
            "conditional answers",
            "question answering",
            "complex reading comprehension"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Prompt-free and Efficient Few-shot Learning with Language Models",
        "authors": [
            "Rabeeh Karimi Mahabadi",
            "Luke Zettlemoyer",
            "James Henderson",
            "Lambert Mathias",
            "Marzieh Saeidi",
            "Veselin Stoyanov",
            "Majid Yazdani"
        ],
        "published": "2022",
        "summary": "Current methods for few-shot fine-tuning of pretrained masked language models (PLMs) require carefully engineered prompts and verbalizers for each new task to convert examples into a cloze-format that the PLM can score. In this work, we propose Perfect, a simple and efficient method for few-shot fine-tuning of PLMs without relying on any such handcrafting, which is highly effective given as few as 32 data points. Perfect makes two key design choices: First, we show that manually engineered task prompts can be replaced with task-specific adapters that enable sample-efficient fine-tuning and reduce memory and storage costs by roughly factors of 5 and 100, respectively. Second, instead of using handcrafted verbalizers, we learn new multi-token label embeddings during fine-tuning, which are not tied to the model vocabulary and which allow us to avoid complex auto-regressive decoding. These embeddings are not only learnable from limited data but also enable nearly 100x faster training and inference. Experiments on a wide range of few shot NLP tasks demonstrate that Perfect, while being simple and efficient, also outperforms existing state-of-the-art few-shot learning methods. Our code is publicly available at https://github.com/rabeehk/perfect.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.254.pdf",
        "keywords": [
            "few shot learning",
            "masked language models",
            "language models",
            "shot fine tuning",
            "fine tuning",
            "engineered"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Current methods for few-shot fine-tuning of pretrained masked language models (PLMs) require carefully engineered prompts and verbalizers for each new task to convert examples into a cloze-format that the PLM can score.\"\n\nThis paper discusses LLMs, but the limitation mentioned is brief and not explored in depth, and the primary focus is on the proposed solution, Perfect.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Current methods for few-shot fine-tuning of pretrained masked language models (PLMs) require carefully engineered prompts and verbalizers for each new task to convert examples into a cloze-format that the PLM can score.\"\n\nThis paper discusses LLMs, but the limitation mentioned is brief and not explored in depth, and the primary focus is on the proposed solution, Perfect."
    },
    {
        "title": "An Investigation of the (In)effectiveness of Counterfactually Augmented Data",
        "authors": [
            "Nitish Joshi",
            "He He"
        ],
        "published": "2022",
        "summary": "While pretrained language models achieve excellent performance on natural language understanding benchmarks, they tend to rely on spurious correlations and generalize poorly to out-of-distribution (OOD) data. Recent work has explored using counterfactually-augmented data (CAD)—data generated by minimally perturbing examples to flip the ground-truth label—to identify robust features that are invariant under distribution shift. However, empirical results using CAD during training for OOD generalization have been mixed. To explain this discrepancy, through a toy theoretical example and empirical analysis on two crowdsourced CAD datasets, we show that: (a) while features perturbed in CAD are indeed robust features, it may prevent the model from learning unperturbed robust features; and (b) CAD may exacerbate existing spurious correlations in the data. Our results thus show that the lack of perturbation diversity limits CAD’s effectiveness on OOD generalization, calling for innovative crowdsourcing procedures to elicit diverse perturbation of examples.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.256.pdf",
        "keywords": [
            "crowdsourcing",
            "cad"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While pretrained language models achieve excellent performance on natural language understanding benchmarks, they tend to rely on spurious correlations and generalize poorly to out-of-distribution (OOD) data.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"While pretrained language models achieve excellent performance on natural language understanding benchmarks, they tend to rely on spurious correlations and generalize poorly to out-of-distribution (OOD) data.\""
    },
    {
        "title": "The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems",
        "authors": [
            "Caleb Ziems",
            "Jane Yu",
            "Yi-Chia Wang",
            "Alon Halevy",
            "Diyi Yang"
        ],
        "published": "2022",
        "summary": "Conversational agents have come increasingly closer to human competence in open-domain dialogue settings; however, such models can reflect insensitive, hurtful, or entirely incoherent viewpoints that erode a user’s trust in the moral integrity of the system. Moral deviations are difficult to mitigate because moral judgments are not universal, and there may be multiple competing judgments that apply to a situation simultaneously. In this work, we introduce a new resource, not to authoritatively resolve moral ambiguities, but instead to facilitate systematic understanding of the intuitions, values and moral judgments reflected in the utterances of dialogue systems. The Moral Integrity Corpus, MIC, is such a resource, which captures the moral assumptions of 38k prompt-reply pairs, using 99k distinct Rules of Thumb (RoTs). Each RoT reflects a particular moral conviction that can explain why a chatbot’s reply may appear acceptable or problematic. We further organize RoTs with a set of 9 moral and social attributes and benchmark performance for attribute classification. Most importantly, we show that current neural language models can automatically generate new RoTs that reasonably describe previously unseen interactions, but they still struggle with certain scenarios. Our findings suggest that MIC will be a useful resource for understanding and language models’ implicit moral assumptions and flexibly benchmarking the integrity of conversational agents. To download the data, see https://github.com/GT-SALT/mic",
        "pdf_link": "https://aclanthology.org/2022.acl-long.261.pdf",
        "keywords": [
            "moral",
            "moral integrity",
            "moral ambiguities",
            "ethical dialogue systems",
            "moral integrity corpus",
            "moral judgments",
            "moral deviations"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Most importantly, we show that current neural language models can automatically generate new RoTs that reasonably describe previously unseen interactions, but they still struggle with certain scenarios.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Most importantly, we show that current neural language models can automatically generate new RoTs that reasonably describe previously unseen interactions, but they still struggle with certain scenarios.\""
    },
    {
        "title": "The Trade-offs of Domain Adaptation for Neural Language Models",
        "authors": [
            "David Grangier",
            "Dan Iter"
        ],
        "published": "2022",
        "summary": "This work connects language model adaptation with concepts of machine learning theory. We consider a training setup with a large out-of-domain set and a small in-domain set. We derive how the benefit of training a model on either set depends on the size of the sets and the distance between their underlying distributions. We analyze how out-of-domain pre-training before in-domain fine-tuning achieves better generalization than either solution independently. Finally, we present how adaptation techniques based on data selection, such as importance sampling, intelligent data selection and influence functions, can be presented in a common framework which highlights their similarity and also their subtle differences.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.264.pdf",
        "keywords": [
            "domain adaptation",
            "language model adaptation",
            "neural language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We consider a training setup with a large out-of-domain set and a small in-domain set.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We consider a training setup with a large out-of-domain set and a small in-domain set.\""
    },
    {
        "title": "Neural Pipeline for Zero-Shot Data-to-Text Generation",
        "authors": [
            "Zdeněk Kasner",
            "Ondrej Dusek"
        ],
        "published": "2022",
        "summary": "In data-to-text (D2T) generation, training on in-domain data leads to overfitting to the data representation and repeating training data noise. We examine how to avoid finetuning pretrained language models (PLMs) on D2T generation datasets while still taking advantage of surface realization capabilities of PLMs. Inspired by pipeline approaches, we propose to generate text by transforming single-item descriptions with a sequence of modules trained on general-domain text-based operations: ordering, aggregation, and paragraph compression. We train PLMs for performing these operations on a synthetic corpus WikiFluent which we build from English Wikipedia. Our experiments on two major triple-to-text datasets—WebNLG and E2E—show that our approach enables D2T generation from RDF triples in zero-shot settings.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.271.pdf",
        "keywords": [
            "pipeline",
            "neural pipeline",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"In data-to-text (D2T) generation, training on in-domain data leads to overfitting to the data representation and repeating training data noise.\"\n\nThis evidence suggests that the paper mentions a limitation of pre-trained language models (PLMs) in data-to-text generation, specifically overfitting to the data representation and repeating training data noise. However, the discussion of this limitation",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"In data-to-text (D2T) generation, training on in-domain data leads to overfitting to the data representation and repeating training data noise.\"\n\nThis evidence suggests that the paper mentions a limitation of pre-trained language models (PLMs) in data-to-text generation, specifically overfitting to the data representation and repeating training data noise. However, the discussion of this limitation"
    },
    {
        "title": "ePiC: Employing Proverbs in Context as a Benchmark for Abstract Language Understanding",
        "authors": [
            "Sayan Ghosh",
            "Shashank Srivastava"
        ],
        "published": "2022",
        "summary": "While large language models have shown exciting progress on several NLP benchmarks, evaluating their ability for complex analogical reasoning remains under-explored. Here, we introduce a high-quality crowdsourced dataset of narratives for employing proverbs in context as a benchmark for abstract language understanding. The dataset provides fine-grained annotation of aligned spans between proverbs and narratives, and contains minimal lexical overlaps between narratives and proverbs, ensuring that models need to go beyond surface-level reasoning to succeed. We explore three tasks: (1) proverb recommendation and alignment prediction, (2) narrative generation for a given proverb and topic, and (3) identifying narratives with similar motifs. Our experiments show that neural language models struggle on these tasks compared to humans, and these tasks pose multiple learning challenges.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.276.pdf",
        "keywords": [
            "narratives",
            "analogical reasoning",
            "prediction",
            "language models",
            "neural language",
            "abstract language",
            "benchmark"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While large language models have shown exciting progress on several NLP benchmarks, evaluating their ability for complex analogical reasoning remains under-explored.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"While large language models have shown exciting progress on several NLP benchmarks, evaluating their ability for complex analogical reasoning remains under-explored.\""
    },
    {
        "title": "Chart-to-Text: A Large-Scale Benchmark for Chart Summarization",
        "authors": [
            "Shankar Kantharaj",
            "Rixie Tiffany Leong",
            "Xiang Lin",
            "Ahmed Masry",
            "Megh Thakkar",
            "Enamul Hoque",
            "Shafiq Joty"
        ],
        "published": "2022",
        "summary": "Charts are commonly used for exploring data and communicating insights. Generating natural language summaries from charts can be very helpful for people in inferring key insights that would otherwise require a lot of cognitive and perceptual efforts. We present Chart-to-text, a large-scale benchmark with two datasets and a total of 44,096 charts covering a wide range of topics and chart types. We explain the dataset construction process and analyze the datasets. We also introduce a number of state-of-the-art neural models as baselines that utilize image captioning and data-to-text generation techniques to tackle two problem variations: one assumes the underlying data table of the chart is available while the other needs to extract data from chart images. Our analysis with automatic and human evaluation shows that while our best models usually generate fluent summaries and yield reasonable BLEU scores, they also suffer from hallucinations and factual errors as well as difficulties in correctly explaining complex patterns and trends in charts.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.277.pdf",
        "keywords": [
            "chart summarization",
            "chart"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our analysis with automatic and human evaluation shows that while our best models usually generate fluent summaries and yield reasonable BLEU scores, they also suffer from hallucinations and factual errors as well as difficulties in correctly explaining complex patterns and trends in charts.\"\n\nThis rating is chosen because the paper discusses limitations of Large Language Models (LMs) in the context of chart summarization, specifically mentioning hallucinations",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Our analysis with automatic and human evaluation shows that while our best models usually generate fluent summaries and yield reasonable BLEU scores, they also suffer from hallucinations and factual errors as well as difficulties in correctly explaining complex patterns and trends in charts.\"\n\nThis rating is chosen because the paper discusses limitations of Large Language Models (LMs) in the context of chart summarization, specifically mentioning hallucinations"
    },
    {
        "title": "PromDA: Prompt-based Data Augmentation for Low-Resource NLU Tasks",
        "authors": [
            "Yufei Wang",
            "Can Xu",
            "Qingfeng Sun",
            "Huang Hu",
            "Chongyang Tao",
            "Xiubo Geng",
            "Daxin Jiang"
        ],
        "published": "2022",
        "summary": "This paper focuses on the Data Augmentation for low-resource Natural Language Understanding (NLU) tasks. We propose Prompt-based Data Augmentation model (PromDA) which only trains small-scale Soft Prompt (i.e., a set of trainable vectors) in the frozen Pre-trained Language Models (PLMs). This avoids human effort in collecting unlabeled in-domain data and maintains the quality of generated synthetic data. In addition, PromDA generates synthetic data via two different views and filters out the low-quality data using NLU models. Experiments on four benchmarks show that synthetic data produced by PromDA successfully boost up the performance of NLU models which consistently outperform several competitive baseline models, including a state-of-the-art semi-supervised model using unlabeled in-domain data. The synthetic data from PromDA are also complementary with unlabeled in-domain data. The NLU models can be further improved when they are combined for training.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.292.pdf",
        "keywords": [
            "data augmentation",
            "data"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This avoids human effort in collecting unlabeled in-domain data\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"This avoids human effort in collecting unlabeled in-domain data\""
    },
    {
        "title": "RST Discourse Parsing with Second-Stage EDU-Level Pre-training",
        "authors": [
            "Nan Yu",
            "Meishan Zhang",
            "Guohong Fu",
            "Min Zhang"
        ],
        "published": "2022",
        "summary": "Pre-trained language models (PLMs) have shown great potentials in natural language processing (NLP) including rhetorical structure theory (RST) discourse parsing. Current PLMs are obtained by sentence-level pre-training, which is different from the basic processing unit, i.e. element discourse unit (EDU).To this end, we propose a second-stage EDU-level pre-training approach in this work, which presents two novel tasks to learn effective EDU representations continually based on well pre-trained language models. Concretely, the two tasks are (1) next EDU prediction (NEP) and (2) discourse marker prediction (DMP).We take a state-of-the-art transition-based neural parser as baseline, and adopt it with a light bi-gram EDU modification to effectively explore the EDU-level pre-trained EDU representation. Experimental results on a benckmark dataset show that our method is highly effective,leading a 2.1-point improvement in F1-score. All codes and pre-trained models will be released publicly to facilitate future studies.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.294.pdf",
        "keywords": [
            "edu prediction",
            "edu level",
            "edu level pre training",
            "pre trained language models",
            "discourse marker prediction",
            "language models",
            "rhetorical structure theory",
            "neural parser",
            "representation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Current PLMs are obtained by sentence-level pre-training, which is different from the basic processing unit, i.e. element discourse unit (EDU).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Current PLMs are obtained by sentence-level pre-training, which is different from the basic processing unit, i.e. element discourse unit (EDU).\""
    },
    {
        "title": "SimKGC: Simple Contrastive Knowledge Graph Completion with Pre-trained Language Models",
        "authors": [
            "Liang Wang",
            "Wei Zhao",
            "Zhuoyu Wei",
            "Jingming Liu"
        ],
        "published": "2022",
        "summary": "Knowledge graph completion (KGC) aims to reason over known facts and infer the missing links. Text-based methods such as KGBERT (Yao et al., 2019) learn entity representations from natural language descriptions, and have the potential for inductive KGC. However, the performance of text-based methods still largely lag behind graph embedding-based methods like TransE (Bordes et al., 2013) and RotatE (Sun et al., 2019b). In this paper, we identify that the key issue is efficient contrastive learning. To improve the learning efficiency, we introduce three types of negatives: in-batch negatives, pre-batch negatives, and self-negatives which act as a simple form of hard negatives. Combined with InfoNCE loss, our proposed model SimKGC can substantially outperform embedding-based methods on several benchmark datasets. In terms of mean reciprocal rank (MRR), we advance the state-of-the-art by +19% on WN18RR, +6.8% on the Wikidata5M transductive setting, and +22% on the Wikidata5M inductive setting. Thorough analyses are conducted to gain insights into each component. Our code is available at https://github.com/intfloat/SimKGC .",
        "pdf_link": "https://aclanthology.org/2022.acl-long.295.pdf",
        "keywords": [
            "knowledge graph completion",
            "graph embedding",
            "text based methods",
            "self negatives",
            "language models",
            "infonce"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the paper does not mention any limitations of LLMs explicitly, but it does mention that \"the performance of text-based methods still largely lag behind graph embedding-based methods\" which could be seen as a limitation of text-based methods that use pre-trained language models.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the paper does not mention any limitations of LLMs explicitly, but it does mention that \"the performance of text-based methods still largely lag behind graph embedding-based methods\" which could be seen as a limitation of text-based methods that use pre-trained language models."
    },
    {
        "title": "Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze?",
        "authors": [
            "Oliver Eberle",
            "Stephanie Brandl",
            "Jonas Pilot",
            "Anders Søgaard"
        ],
        "published": "2022",
        "summary": "Learned self-attention functions in state-of-the-art NLP models often correlate with human attention. We investigate whether self-attention in large-scale pre-trained language models is as predictive of human eye fixation patterns during task-reading as classical cognitive models of human attention. We compare attention functions across two task-specific reading datasets for sentiment analysis and relation extraction. We find the predictiveness of large-scale pre-trained self-attention for human attention depends on ‘what is in the tail’, e.g., the syntactic nature of rare contexts. Further, we observe that task-specific fine-tuning does not increase the correlation with human task-specific reading. Through an input reduction experiment we give complementary insights on the sparsity and fidelity trade-off, showing that lower-entropy attention vectors are more faithful.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.296.pdf",
        "keywords": [
            "self attention",
            "human attention",
            "attention patterns",
            "sentiment analysis",
            "relation extraction",
            "transformer models",
            "language models",
            "cognitive models",
            "entropy",
            "eye"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find the predictiveness of large-scale pre-trained self-attention for human attention depends on ‘what is in the tail’, e.g., the syntactic nature of rare contexts.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We find the predictiveness of large-scale pre-trained self-attention for human attention depends on ‘what is in the tail’, e.g., the syntactic nature of rare contexts.\""
    },
    {
        "title": "Scheduled Multi-task Learning for Neural Chat Translation",
        "authors": [
            "Yunlong Liang",
            "Fandong Meng",
            "Jinan Xu",
            "Yufeng Chen",
            "Jie Zhou"
        ],
        "published": "2022",
        "summary": "Neural Chat Translation (NCT) aims to translate conversational text into different languages. Existing methods mainly focus on modeling the bilingual dialogue characteristics (e.g., coherence) to improve chat translation via multi-task learning on small-scale chat translation data. Although the NCT models have achieved impressive success, it is still far from satisfactory due to insufficient chat translation data and simple joint training manners. To address the above issues, we propose a scheduled multi-task learning framework for NCT. Specifically, we devise a three-stage training framework to incorporate the large-scale in-domain chat translation data into training by adding a second pre-training stage between the original pre-training and fine-tuning stages. Further, we investigate where and how to schedule the dialogue-related auxiliary tasks in multiple training stages to effectively enhance the main chat translation task. Extensive experiments on four language directions (English-Chinese and English-German) verify the effectiveness and superiority of the proposed approach. Additionally, we will make the large-scale in-domain paired bilingual dialogue dataset publicly available for the research community.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.300.pdf",
        "keywords": [
            "scheduled multi task learning",
            "chat translation",
            "neural chat translation"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "FairLex: A Multilingual Benchmark for Evaluating Fairness in Legal Text Processing",
        "authors": [
            "Ilias Chalkidis",
            "Tommaso Pasini",
            "Sheng Zhang",
            "Letizia Tomada",
            "Sebastian Schwemer",
            "Anders Søgaard"
        ],
        "published": "2022",
        "summary": "We present a benchmark suite of four datasets for evaluating the fairness of pre-trained language models and the techniques used to fine-tune them for downstream tasks. Our benchmarks cover four jurisdictions (European Council, USA, Switzerland, and China), five languages (English, German, French, Italian and Chinese) and fairness across five attributes (gender, age, region, language, and legal area). In our experiments, we evaluate pre-trained language models using several group-robust fine-tuning techniques and show that performance group disparities are vibrant in many cases, while none of these techniques guarantee fairness, nor consistently mitigate group disparities. Furthermore, we provide a quantitative and qualitative analysis of our results, highlighting open challenges in the development of robustness methods in legal NLP.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.301.pdf",
        "keywords": [
            "fairness",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"In our experiments, we evaluate pre-trained language models using several group-robust fine-tuning techniques and show that performance group disparities are vibrant in many cases, while none of these techniques guarantee fairness, nor consistently mitigate group disparities.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"In our experiments, we evaluate pre-trained language models using several group-robust fine-tuning techniques and show that performance group disparities are vibrant in many cases, while none of these techniques guarantee fairness, nor consistently mitigate group disparities.\""
    },
    {
        "title": "Leveraging Unimodal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition",
        "authors": [
            "Xichen Pan",
            "Peiyu Chen",
            "Yichen Gong",
            "Helong Zhou",
            "Xinbing Wang",
            "Zhouhan Lin"
        ],
        "published": "2022",
        "summary": "Training Transformer-based models demands a large amount of data, while obtaining aligned and labelled data in multimodality is rather cost-demanding, especially for audio-visual speech recognition (AVSR). Thus it makes a lot of sense to make use of unlabelled unimodal data. On the other side, although the effectiveness of large-scale self-supervised learning is well established in both audio and visual modalities, how to integrate those pre-trained models into a multimodal scenario remains underexplored. In this work, we successfully leverage unimodal self-supervised learning to promote the multimodal AVSR. In particular, audio and visual front-ends are trained on large-scale unimodal datasets, then we integrate components of both front-ends into a larger multimodal framework which learns to recognize parallel audio-visual data into characters through a combination of CTC and seq2seq decoding. We show that both components inherited from unimodal self-supervised learning cooperate well, resulting in that the multimodal framework yields competitive results through fine-tuning. Our model is experimentally validated on both word-level and sentence-level tasks. Especially, even without an external language model, our proposed model raises the state-of-the-art performances on the widely accepted Lip Reading Sentences 2 (LRS2) dataset by a large margin, with a relative improvement of 30%.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.308.pdf",
        "keywords": [
            "unimodal self supervised",
            "audio visual speech recognition",
            "unimodal self supervised learning",
            "self supervised learning",
            "multimodal"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "SummaReranker: A Multi-Task Mixture-of-Experts Re-ranking Framework for Abstractive Summarization",
        "authors": [
            "Mathieu Ravaut",
            "Shafiq Joty",
            "Nancy Chen"
        ],
        "published": "2022",
        "summary": "Sequence-to-sequence neural networks have recently achieved great success in abstractive summarization, especially through fine-tuning large pre-trained language models on the downstream dataset. These models are typically decoded with beam search to generate a unique summary. However, the search space is very large, and with the exposure bias, such decoding is not optimal. In this paper, we show that it is possible to directly train a second-stage model performing re-ranking on a set of summary candidates. Our mixture-of-experts SummaReranker learns to select a better candidate and consistently improves the performance of the base model. With a base PEGASUS, we push ROUGE scores by 5.44% on CNN- DailyMail (47.16 ROUGE-1), 1.31% on XSum (48.12 ROUGE-1) and 9.34% on Reddit TIFU (29.83 ROUGE-1), reaching a new state-of-the-art. Our code and checkpoints will be available at https://github.com/ntunlp/SummaReranker.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.309.pdf",
        "keywords": [
            "summarization",
            "abstractive summarization",
            "sequence neural networks",
            "beam search"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the search space is very large, and with the exposure bias, such decoding is not optimal.\"\n\nThis abstract mentions a limitation of large pre-trained language models in the context of abstractive summarization, specifically the suboptimal decoding due to the large search space and exposure bias. However, the limitation is not explored in depth and is only used to motivate the proposed solution",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the search space is very large, and with the exposure bias, such decoding is not optimal.\"\n\nThis abstract mentions a limitation of large pre-trained language models in the context of abstractive summarization, specifically the suboptimal decoding due to the large search space and exposure bias. However, the limitation is not explored in depth and is only used to motivate the proposed solution"
    },
    {
        "title": "SRL4E – Semantic Role Labeling for Emotions: A Unified Evaluation Framework",
        "authors": [
            "Cesare Campagnano",
            "Simone Conia",
            "Roberto Navigli"
        ],
        "published": "2022",
        "summary": "In the field of sentiment analysis, several studies have highlighted that a single sentence may express multiple, sometimes contrasting, sentiments and emotions, each with its own experiencer, target and/or cause. To this end, over the past few years researchers have started to collect and annotate data manually, in order to investigate the capabilities of automatic systems not only to distinguish between emotions, but also to capture their semantic constituents. However, currently available gold datasets are heterogeneous in size, domain, format, splits, emotion categories and role labels, making comparisons across different works difficult and hampering progress in the area. In this paper, we tackle this issue and present a unified evaluation framework focused on Semantic Role Labeling for Emotions (SRL4E), in which we unify several datasets tagged with emotions and semantic roles by using a common labeling scheme. We use SRL4E as a benchmark to evaluate how modern pretrained language models perform and analyze where we currently stand in this task, hoping to provide the tools to facilitate studies in this complex area.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.314.pdf",
        "keywords": [
            "emotions",
            "sentiment analysis",
            "semantic roles",
            "semantic role labeling",
            "srl4e"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"analyze where we currently stand in this task\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"analyze where we currently stand in this task\""
    },
    {
        "title": "Probing for Predicate Argument Structures in Pretrained Language Models",
        "authors": [
            "Simone Conia",
            "Roberto Navigli"
        ],
        "published": "2022",
        "summary": "Thanks to the effectiveness and wide availability of modern pretrained language models (PLMs), recently proposed approaches have achieved remarkable results in dependency- and span-based, multilingual and cross-lingual Semantic Role Labeling (SRL). These results have prompted researchers to investigate the inner workings of modern PLMs with the aim of understanding how, where, and to what extent they encode information about SRL. In this paper, we follow this line of research and probe for predicate argument structures in PLMs. Our study shows that PLMs do encode semantic structures directly into the contextualized representation of a predicate, and also provides insights into the correlation between predicate senses and their structures, the degree of transferability between nominal and verbal structures, and how such structures are encoded across languages. Finally, we look at the practical implications of such insights and demonstrate the benefits of embedding predicate argument structure information into an SRL model.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.316.pdf",
        "keywords": [
            "predicate argument structures",
            "embedding predicate argument structure",
            "probing"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the paper discusses the inner workings and capabilities of PLMs, but does not mention any limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the paper discusses the inner workings and capabilities of PLMs, but does not mention any limitations."
    },
    {
        "title": "Multilingual Generative Language Models for Zero-Shot Cross-Lingual Event Argument Extraction",
        "authors": [
            "Kuan-Hao Huang",
            "I-Hung Hsu",
            "Prem Natarajan",
            "Kai-Wei Chang",
            "Nanyun Peng"
        ],
        "published": "2022",
        "summary": "We present a study on leveraging multilingual pre-trained generative language models for zero-shot cross-lingual event argument extraction (EAE). By formulating EAE as a language generation task, our method effectively encodes event structures and captures the dependencies between arguments. We design language-agnostic templates to represent the event argument structures, which are compatible with any language, hence facilitating the cross-lingual transfer. Our proposed model finetunes multilingual pre-trained generative language models to generate sentences that fill in the language-agnostic template with arguments extracted from the input passage. The model is trained on source languages and is then directly applied to target languages for event argument extraction. Experiments demonstrate that the proposed model outperforms the current state-of-the-art models on zero-shot cross-lingual EAE. Comprehensive studies and error analyses are presented to better understand the advantages and the current limitations of using generative language models for zero-shot cross-lingual transfer EAE.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.317.pdf",
        "keywords": [
            "event argument extraction",
            "generative language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Comprehensive studies and error analyses are presented to better understand the advantages and the current limitations of using generative language models for zero-shot cross-lingual transfer EAE.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Comprehensive studies and error analyses are presented to better understand the advantages and the current limitations of using generative language models for zero-shot cross-lingual transfer EAE.\""
    },
    {
        "title": "Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System",
        "authors": [
            "Yixuan Su",
            "Lei Shu",
            "Elman Mansimov",
            "Arshit Gupta",
            "Deng Cai",
            "Yi-An Lai",
            "Yi Zhang"
        ],
        "published": "2022",
        "summary": "Pre-trained language models have been recently shown to benefit task-oriented dialogue (TOD) systems. Despite their success, existing methods often formulate this task as a cascaded generation problem which can lead to error accumulation across different sub-tasks and greater data annotation overhead. In this study, we present PPTOD, a unified plug-and-play model for task-oriented dialogue. In addition, we introduce a new dialogue multi-task pre-training strategy that allows the model to learn the primary TOD task completion skills from heterogeneous dialog corpora. We extensively test our model on three benchmark TOD tasks, including end-to-end dialogue modelling, dialogue state tracking, and intent classification. Experimental results show that PPTOD achieves new state of the art on all evaluated tasks in both high-resource and low-resource scenarios. Furthermore, comparisons against previous SOTA methods show that the responses generated by PPTOD are more factually correct and semantically coherent as judged by human annotators.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.319.pdf",
        "keywords": [
            "plug and play",
            "dialogue",
            "dialogue system",
            "task",
            "task oriented dialogue",
            "intent classification"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"existing methods often formulate this task as a cascaded generation problem which can lead to error accumulation across different sub-tasks\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"existing methods often formulate this task as a cascaded generation problem which can lead to error accumulation across different sub-tasks\""
    },
    {
        "title": "Cross-Lingual Ability of Multilingual Masked Language Models: A Study of Language Structure",
        "authors": [
            "Yuan Chai",
            "Yaobo Liang",
            "Nan Duan"
        ],
        "published": "2022",
        "summary": "Multilingual pre-trained language models, such as mBERT and XLM-R, have shown impressive cross-lingual ability. Surprisingly, both of them use multilingual masked language model (MLM) without any cross-lingual supervision or aligned data. Despite the encouraging results, we still lack a clear understanding of why cross-lingual ability could emerge from multilingual MLM. In our work, we argue that cross-language ability comes from the commonality between languages. Specifically, we study three language properties: constituent order, composition and word co-occurrence. First, we create an artificial language by modifying property in source language. Then we study the contribution of modified property through the change of cross-language transfer results on target language. We conduct experiments on six languages and two cross-lingual NLP tasks (textual entailment, sentence retrieval). Our main conclusion is that the contribution of constituent order and word co-occurrence is limited, while the composition is more crucial to the success of cross-linguistic transfer.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.322.pdf",
        "keywords": [
            "cross lingual ability",
            "cross language ability",
            "masked language models",
            "language models",
            "co occurrence"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the encouraging results, we still lack a clear understanding of why cross-lingual ability could emerge from multilingual MLM.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Despite the encouraging results, we still lack a clear understanding of why cross-lingual ability could emerge from multilingual MLM.\""
    },
    {
        "title": "Rare and Zero-shot Word Sense Disambiguation using Z-Reweighting",
        "authors": [
            "Ying Su",
            "Hongming Zhang",
            "Yangqiu Song",
            "Tong Zhang"
        ],
        "published": "2022",
        "summary": "Word sense disambiguation (WSD) is a crucial problem in the natural language processing (NLP) community. Current methods achieve decent performance by utilizing supervised learning and large pre-trained language models. However, the imbalanced training dataset leads to poor performance on rare senses and zero-shot senses. There are more training instances and senses for words with top frequency ranks than those with low frequency ranks in the training dataset. We investigate the statistical relation between word frequency rank and word sense number distribution. Based on the relation, we propose a Z-reweighting method on the word level to adjust the training on the imbalanced dataset. The experiments show that the Z-reweighting strategy achieves performance gain on the standard English all words WSD benchmark. Moreover, the strategy can help models generalize better on rare and zero-shot senses.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.323.pdf",
        "keywords": [
            "z reweighting",
            "word sense disambiguation",
            "imbalanced dataset",
            "natural language processing"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the imbalanced training dataset leads to poor performance on rare senses and zero-shot senses.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the imbalanced training dataset leads to poor performance on rare senses and zero-shot senses.\""
    },
    {
        "title": "Nibbling at the Hard Core of Word Sense Disambiguation",
        "authors": [
            "Marco Maru",
            "Simone Conia",
            "Michele Bevilacqua",
            "Roberto Navigli"
        ],
        "published": "2022",
        "summary": "With state-of-the-art systems having finally attained estimated human performance, Word Sense Disambiguation (WSD) has now joined the array of Natural Language Processing tasks that have seemingly been solved, thanks to the vast amounts of knowledge encoded into Transformer-based pre-trained language models. And yet, if we look below the surface of raw figures, it is easy to realize that current approaches still make trivial mistakes that a human would never make. In this work, we provide evidence showing why the F1 score metric should not simply be taken at face value and present an exhaustive analysis of the errors that seven of the most representative state-of-the-art systems for English all-words WSD make on traditional evaluation benchmarks. In addition, we produce and release a collection of test sets featuring (a) an amended version of the standard evaluation benchmark that fixes its lexical and semantic inaccuracies, (b) 42D, a challenge set devised to assess the resilience of systems with respect to least frequent word senses and senses not seen at training time, and (c) hardEN, a challenge set made up solely of instances which none of the investigated state-of-the-art systems can solve. We make all of the test sets and model predictions available to the research community at https://github.com/SapienzaNLP/wsd-hard-benchmark.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.324.pdf",
        "keywords": [
            "word sense disambiguation",
            "nibbling"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"if we look below the surface of raw figures, it is easy to realize that current approaches still make trivial mistakes that a human would never make.\"; \"we provide evidence showing why the F1 score metric should not simply be taken at face value and present an exhaustive analysis of the errors that seven of the most representative state-of-the-art systems for English all-words WSD make on",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"if we look below the surface of raw figures, it is easy to realize that current approaches still make trivial mistakes that a human would never make.\"; \"we provide evidence showing why the F1 score metric should not simply be taken at face value and present an exhaustive analysis of the errors that seven of the most representative state-of-the-art systems for English all-words WSD make on"
    },
    {
        "title": "Large Scale Substitution-based Word Sense Induction",
        "authors": [
            "Matan Eyal",
            "Shoval Sadde",
            "Hillel Taub-Tabib",
            "Yoav Goldberg"
        ],
        "published": "2022",
        "summary": "We present a word-sense induction method based on pre-trained masked language models (MLMs), which can cheaply scale to large vocabularies and large corpora. The result is a corpus which is sense-tagged according to a corpus-derived sense inventory and where each sense is associated with indicative words. Evaluation on English Wikipedia that was sense-tagged using our method shows that both the induced senses, and the per-instance sense assignment, are of high quality even compared to WSD methods, such as Babelfy. Furthermore, by training a static word embeddings algorithm on the sense-tagged corpus, we obtain high-quality static senseful embeddings. These outperform existing senseful embeddings methods on the WiC dataset and on a new outlier detection dataset we developed. The data driven nature of the algorithm allows to induce corpora-specific senses, which may not appear in standard sense inventories, as we demonstrate using a case study on the scientific domain.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.325.pdf",
        "keywords": [
            "large scale substitution",
            "word sense induction",
            "outlier detection"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Rewire-then-Probe: A Contrastive Recipe for Probing Biomedical Knowledge of Pre-trained Language Models",
        "authors": [
            "Zaiqiao Meng",
            "Fangyu Liu",
            "Ehsan Shareghi",
            "Yixuan Su",
            "Charlotte Collins",
            "Nigel Collier"
        ],
        "published": "2022",
        "summary": "Knowledge probing is crucial for understanding the knowledge transfer mechanism behind the pre-trained language models (PLMs). Despite the growing progress of probing knowledge for PLMs in the general domain, specialised areas such as the biomedical domain are vastly under-explored. To facilitate this, we release a well-curated biomedical knowledge probing benchmark, MedLAMA, constructed based on the Unified Medical Language System (UMLS) Metathesaurus. We test a wide spectrum of state-of-the-art PLMs and probing approaches on our benchmark, reaching at most 3% of acc@10. While highlighting various sources of domain-specific challenges that amount to this underwhelming performance, we illustrate that the underlying PLMs have a higher potential for probing tasks. To achieve this, we propose Contrastive-Probe, a novel self-supervised contrastive probing approach, that adjusts the underlying PLMs without using any probing data. While Contrastive-Probe pushes the acc@10 to 28%, the performance gap still remains notable. Our human expert evaluation suggests that the probing performance of our Contrastive-Probe is still under-estimated as UMLS still does not include the full spectrum of factual knowledge. We hope MedLAMA and Contrastive-Probe facilitate further developments of more suited probing techniques for this domain. Our code and dataset are publicly available at https://github.com/cambridgeltl/medlama.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.329.pdf",
        "keywords": [
            "trained language models",
            "knowledge",
            "knowledge probing",
            "probe",
            "knowledge transfer",
            "contrastive probe"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While highlighting various sources of domain-specific challenges that amount to this underwhelming performance, we illustrate that the underlying PLMs have a higher potential for probing tasks... the performance gap still remains notable.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"While highlighting various sources of domain-specific challenges that amount to this underwhelming performance, we illustrate that the underlying PLMs have a higher potential for probing tasks... the performance gap still remains notable.\""
    },
    {
        "title": "Compression of Generative Pre-trained Language Models via Quantization",
        "authors": [
            "Chaofan Tao",
            "Lu Hou",
            "Wei Zhang",
            "Lifeng Shang",
            "Xin Jiang",
            "Qun Liu",
            "Ping Luo",
            "Ngai Wong"
        ],
        "published": "2022",
        "summary": "The increasing size of generative Pre-trained Language Models (PLMs) have greatly increased the demand for model compression. Despite various methods to compress BERT or its variants, there are few attempts to compress generative PLMs, and the underlying difficulty remains unclear. In this paper, we compress generative PLMs by quantization. We find that previous quantization methods fail on generative tasks due to the homogeneous word embeddings caused by reduced capacity and the varied distribution of weights. Correspondingly, we propose a token-level contrastive distillation to learn distinguishable word embeddings, and a module-wise dynamic scaling to make quantizers adaptive to different modules. Empirical results on various tasks show that our proposed method outperforms the state-of-the-art compression methods on generative PLMs by a clear margin. With comparable performance with the full-precision models, we achieve 14.4x and 13.4x compression rate on GPT-2 and BART, respectively.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.331.pdf",
        "keywords": [
            "quantizers",
            "trained language models",
            "generative"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite various methods to compress BERT or its variants, there are few attempts to compress generative PLMs, and the underlying difficulty remains unclear.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite various methods to compress BERT or its variants, there are few attempts to compress generative PLMs, and the underlying difficulty remains unclear.\""
    },
    {
        "title": "DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation",
        "authors": [
            "Wei Chen",
            "Yeyun Gong",
            "Song Wang",
            "Bolun Yao",
            "Weizhen Qi",
            "Zhongyu Wei",
            "Xiaowu Hu",
            "Bartuer Zhou",
            "Yi Mao",
            "Weizhu Chen",
            "Biao Cheng",
            "Nan Duan"
        ],
        "published": "2022",
        "summary": "Dialog response generation in open domain is an important research topic where the main challenge is to generate relevant and diverse responses. In this paper, we propose a new dialog pre-training framework called DialogVED, which introduces continuous latent variables into the enhanced encoder-decoder pre-training framework to increase the relevance and diversity of responses. With the help of a large dialog corpus (Reddit), we pre-train the model using the following 4 tasks, used in training language models (LMs) and Variational Autoencoders (VAEs) literature: 1) masked language model; 2) response generation; 3) bag-of-words prediction; and 4) KL divergence reduction. We also add additional parameters to model the turn structure in dialogs to improve the performance of the pre-trained model. We conduct experiments on PersonaChat, DailyDialog, and DSTC7-AVSD benchmarks for response generation. Experimental results show that our model achieves the new state-of-the-art results on all these datasets.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.333.pdf",
        "keywords": [
            "dialog corpus",
            "dialog",
            "dialog response generation",
            "response generation",
            "latent variable encoder decoder",
            "continuous latent variables",
            "training"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"used in training language models (LMs)\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"used in training language models (LMs)\""
    },
    {
        "title": "A Contrastive Framework for Learning Sentence Representations from Pairwise and Triple-wise Perspective in Angular Space",
        "authors": [
            "Yuhao Zhang",
            "Hongji Zhu",
            "Yongliang Wang",
            "Nan Xu",
            "Xiaobo Li",
            "Binqiang Zhao"
        ],
        "published": "2022",
        "summary": "Learning high-quality sentence representations is a fundamental problem of natural language processing which could benefit a wide range of downstream tasks. Though the BERT-like pre-trained language models have achieved great success, using their sentence representations directly often results in poor performance on the semantic textual similarity task. Recently, several contrastive learning methods have been proposed for learning sentence representations and have shown promising results. However, most of them focus on the constitution of positive and negative representation pairs and pay little attention to the training objective like NT-Xent, which is not sufficient enough to acquire the discriminating power and is unable to model the partial order of semantics between sentences. So in this paper, we propose a new method ArcCSE, with training objectives designed to enhance the pairwise discriminative power and model the entailment relation of triplet sentences. We conduct extensive experiments which demonstrate that our approach outperforms the previous state-of-the-art on diverse sentence related tasks, including STS and SentEval.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.336.pdf",
        "keywords": [],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"using their sentence representations directly often results in poor performance on the semantic textual similarity task.\"\n\nThis paper mentions a limitation of pre-trained language models (poor performance on semantic textual similarity tasks) but does not elaborate on it and focuses on proposing a new method to address this issue.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"using their sentence representations directly often results in poor performance on the semantic textual similarity task.\"\n\nThis paper mentions a limitation of pre-trained language models (poor performance on semantic textual similarity tasks) but does not elaborate on it and focuses on proposing a new method to address this issue."
    },
    {
        "title": "Which side are you on? Insider-Outsider classification in conspiracy-theoretic social media",
        "authors": [
            "Pavan Holur",
            "Tianyi Wang",
            "Shadi Shahsavari",
            "Timothy Tangherlini",
            "Vwani Roychowdhury"
        ],
        "published": "2022",
        "summary": "Social media is a breeding ground for threat narratives and related conspiracy theories. In these, an outside group threatens the integrity of an inside group, leading to the emergence of sharply defined group identities: Insiders – agents with whom the authors identify and Outsiders – agents who threaten the insiders. Inferring the members of these groups constitutes a challenging new NLP task: (i) Information is distributed over many poorly-constructed posts; (ii) Threats and threat agents are highly contextual, with the same post potentially having multiple agents assigned to membership in either group; (iii) An agent’s identity is often implicit and transitive; and (iv) Phrases used to imply Outsider status often do not follow common negative sentiment patterns. To address these challenges, we define a novel Insider-Outsider classification task. Because we are not aware of any appropriate existing datasets or attendant models, we introduce a labeled dataset (CT5K) and design a model (NP2IO) to address this task. NP2IO leverages pretrained language modeling to classify Insiders and Outsiders. NP2IO is shown to be robust, generalizing to noun phrases not seen during training, and exceeding the performance of non-trivial baseline models by 20%.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.341.pdf",
        "keywords": [
            "social media",
            "insider outsider classification"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"NP2IO leverages pretrained language modeling to classify Insiders and Outsiders.\"\n\nThis paper mentions LLMs, but does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"NP2IO leverages pretrained language modeling to classify Insiders and Outsiders.\"\n\nThis paper mentions LLMs, but does not discuss any limitations of LLMs."
    },
    {
        "title": "SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer",
        "authors": [
            "Tu Vu",
            "Brian Lester",
            "Noah Constant",
            "Rami Al-Rfou’",
            "Daniel Cer"
        ],
        "published": "2022",
        "summary": "There has been growing interest in parameter-efficient methods to apply pre-trained language models to downstream tasks. Building on the Prompt Tuning approach of Lester et al. (2021), which learns task-specific soft prompts to condition a frozen pre-trained model to perform different tasks, we propose a novel prompt-based transfer learning approach called SPoT: Soft Prompt Transfer. SPoT first learns a prompt on one or more source tasks and then uses it to initialize the prompt for a target task. We show that SPoT significantly boosts the performance of Prompt Tuning across many tasks. More remarkably, across all model sizes, SPoT matches or outperforms standard Model Tuning (which fine-tunes all model parameters) on the SuperGLUE benchmark, while using up to 27,000× fewer task-specific parameters. To understand where SPoT is most effective, we conduct a large-scale study on task transferability with 26 NLP tasks in 160 combinations, and demonstrate that many tasks can benefit each other via prompt transfer. Finally, we propose an efficient retrieval approach that interprets task prompts as task embeddings to identify similar tasks and predict the most transferable source tasks for a novel target task.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.346.pdf",
        "keywords": [
            "tuning",
            "task transferability",
            "soft prompt transfer",
            "model tuning",
            "soft",
            "prompt based transfer",
            "spot"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions \"parameter-efficient methods\" which could be related to limitations of LLMs, but it is not explicitly stated.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions \"parameter-efficient methods\" which could be related to limitations of LLMs, but it is not explicitly stated."
    },
    {
        "title": "Pass off Fish Eyes for Pearls: Attacking Model Selection of Pre-trained Models",
        "authors": [
            "Biru Zhu",
            "Yujia Qin",
            "Fanchao Qi",
            "Yangdong Deng",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Ming Gu"
        ],
        "published": "2022",
        "summary": "Selecting an appropriate pre-trained model (PTM) for a specific downstream task typically requires significant efforts of fine-tuning. To accelerate this process, researchers propose feature-based model selection (FMS) methods, which assess PTMs’ transferability to a specific task in a fast way without fine-tuning. In this work, we argue that current FMS methods are vulnerable, as the assessment mainly relies on the static features extracted from PTMs. However, such features are derived without training PTMs on downstream tasks, and are not necessarily reliable indicators for the PTM’s transferability. To validate our viewpoints, we design two methods to evaluate the robustness of FMS: (1) model disguise attack, which post-trains an inferior PTM with a contrastive objective, and (2) evaluation data selection, which selects a subset of the data points for FMS evaluation based on K-means clustering. Experimental results prove that both methods can successfully make FMS mistakenly judge the transferability of PTMs. Moreover, we find that these two methods can further be combined with the backdoor attack to misguide the FMS to select poisoned models. To the best of our knowledge, this is the first work to demonstrate the defects of current FMS algorithms and evaluate their potential security risks. By identifying previously unseen risks of FMS, our study indicates new directions for improving the robustness of FMS.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.347.pdf",
        "keywords": [
            "pre trained models",
            "model selection",
            "pass off fish eyes",
            "feature based model selection",
            "s transferability"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "HeterMPC: A Heterogeneous Graph Neural Network for Response Generation in Multi-Party Conversations",
        "authors": [
            "Jia-Chen Gu",
            "Chao-Hong Tan",
            "Chongyang Tao",
            "Zhen-Hua Ling",
            "Huang Hu",
            "Xiubo Geng",
            "Daxin Jiang"
        ],
        "published": "2022",
        "summary": "Recently, various response generation models for two-party conversations have achieved impressive improvements, but less effort has been paid to multi-party conversations (MPCs) which are more practical and complicated. Compared with a two-party conversation where a dialogue context is a sequence of utterances, building a response generation model for MPCs is more challenging, since there exist complicated context structures and the generated responses heavily rely on both interlocutors (i.e., speaker and addressee) and history utterances. To address these challenges, we present HeterMPC, a heterogeneous graph-based neural network for response generation in MPCs which models the semantics of utterances and interlocutors simultaneously with two types of nodes in a graph. Besides, we also design six types of meta relations with node-edge-type-dependent parameters to characterize the heterogeneous interactions within the graph. Through multi-hop updating, HeterMPC can adequately utilize the structural knowledge of conversations for response generation. Experimental results on the Ubuntu Internet Relay Chat (IRC) channel benchmark show that HeterMPC outperforms various baseline models for response generation in MPCs.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.349.pdf",
        "keywords": [
            "response generation",
            "multi party conversations",
            "two party conversations",
            "heterogeneous graph",
            "heterogeneous interactions",
            "heterogeneous graph neural network",
            "neural network"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "A Multi-Document Coverage Reward for RELAXed Multi-Document Summarization",
        "authors": [
            "Jacob Parnell",
            "Inigo Jauregi Unanue",
            "Massimo Piccardi"
        ],
        "published": "2022",
        "summary": "Multi-document summarization (MDS) has made significant progress in recent years, in part facilitated by the availability of new, dedicated datasets and capacious language models. However, a standing limitation of these models is that they are trained against limited references and with plain maximum-likelihood objectives. As for many other generative tasks, reinforcement learning (RL) offers the potential to improve the training of MDS models; yet, it requires a carefully-designed reward that can ensure appropriate leverage of both the reference summaries and the input documents. For this reason, in this paper we propose fine-tuning an MDS baseline with a reward that balances a reference-based metric such as ROUGE with coverage of the input documents. To implement the approach, we utilize RELAX (Grathwohl et al., 2018), a contemporary gradient estimator which is both low-variance and unbiased, and we fine-tune the baseline in a few-shot style for both stability and computational efficiency. Experimental results over the Multi-News and WCEP MDS datasets show significant improvements of up to +0.95 pp average ROUGE score and +3.17 pp METEOR score over the baseline, and competitive results with the literature. In addition, they show that the coverage of the input documents is increased, and evenly across all documents.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.351.pdf",
        "keywords": [
            "multi document summarization",
            "reinforcement learning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, a standing limitation of these models is that they are trained against limited references and with plain maximum-likelihood objectives.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, a standing limitation of these models is that they are trained against limited references and with plain maximum-likelihood objectives.\""
    },
    {
        "title": "Beyond Goldfish Memory: Long-Term Open-Domain Conversation",
        "authors": [
            "Jing Xu",
            "Arthur Szlam",
            "Jason Weston"
        ],
        "published": "2022",
        "summary": "Despite recent improvements in open-domain dialogue models, state of the art models are trained and evaluated on short conversations with little context. In contrast, the long-term conversation setting has hardly been studied. In this work we collect and release a human-human dataset consisting of multiple chat sessions whereby the speaking partners learn about each other’s interests and discuss the things they have learnt from past sessions. We show how existing models trained on existing datasets perform poorly in this long-term conversation setting in both automatic and human evaluations, and we study long-context models that can perform much better. In particular, we find retrieval-augmented methods and methods with an ability to summarize and recall previous conversations outperform the standard encoder-decoder architectures currently considered state of the art.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.356.pdf",
        "keywords": [
            "conversation",
            "dialogue models",
            "chat"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We show how existing models trained on existing datasets perform poorly in this long-term conversation setting in both automatic and human evaluations\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We show how existing models trained on existing datasets perform poorly in this long-term conversation setting in both automatic and human evaluations\""
    },
    {
        "title": "E-LANG: Energy-Based Joint Inferencing of Super and Swift Language Models",
        "authors": [
            "Mohammad Akbari",
            "Amin Banitalebi-Dehkordi",
            "Yong Zhang"
        ],
        "published": "2022",
        "summary": "Building huge and highly capable language models has been a trend in the past years. Despite their great performance, they incur high computational cost. A common solution is to apply model compression or choose light-weight architectures, which often need a separate fixed-size model for each desirable computational budget, and may lose performance in case of heavy compression. This paper proposes an effective dynamic inference approach, called E-LANG, which distributes the inference between large accurate Super-models and light-weight Swift models. To this end, a decision making module routes the inputs to Super or Swift models based on the energy characteristics of the representations in the latent space. This method is easily adoptable and architecture agnostic. As such, it can be applied to black-box pre-trained models without a need for architectural manipulations, reassembling of modules, or re-training. Unlike existing methods that are only applicable to encoder-only backbones and classification tasks, our method also works for encoder-decoder structures and sequence-to-sequence tasks such as translation. The E-LANG performance is verified through a set of experiments with T5 and BERT backbones on GLUE, SuperGLUE, and WMT. In particular, we outperform T5-11B with an average computations speed-up of 3.3X on GLUE and 2.9X on SuperGLUE. We also achieve BERT-based SOTA on GLUE with 3.2X less computations. Code and demo are available in supplementary materials.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.359.pdf",
        "keywords": [
            "swift models",
            "swift language models",
            "light weight swift models",
            "language models",
            "super models",
            "decision making"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite their great performance, they incur high computational cost.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite their great performance, they incur high computational cost.\""
    },
    {
        "title": "Measuring the Impact of (Psycho-)Linguistic and Readability Features and Their Spill Over Effects on the Prediction of Eye Movement Patterns",
        "authors": [
            "Daniel Wiechmann",
            "Yu Qiao",
            "Elma Kerz",
            "Justus Mattern"
        ],
        "published": "2022",
        "summary": "There is a growing interest in the combined use of NLP and machine learning methods to predict gaze patterns during naturalistic reading. While promising results have been obtained through the use of transformer-based language models, little work has been undertaken to relate the performance of such models to general text characteristics. In this paper we report on experiments with two eye-tracking corpora of naturalistic reading and two language models (BERT and GPT-2). In all experiments, we test effects of a broad spectrum of features for predicting human reading behavior that fall into five categories (syntactic complexity, lexical richness, register-based multiword combinations, readability and psycholinguistic word properties). Our experiments show that both the features included and the architecture of the transformer-based language models play a role in predicting multiple eye-tracking measures during naturalistic reading. We also report the results of experiments aimed at determining the relative importance of features from different groups using SP-LIME.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.362.pdf",
        "keywords": [
            "readability",
            "naturalistic reading",
            "language",
            "predicting"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While promising results have been obtained through the use of transformer-based language models, little work has been undertaken to relate the performance of such models to general text characteristics.\"\n\nNote that the paper mentions a limitation of transformer-based language models in that their performance has not been related to general text characteristics, but this limitation is not the main focus of the paper and is only briefly mentioned.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While promising results have been obtained through the use of transformer-based language models, little work has been undertaken to relate the performance of such models to general text characteristics.\"\n\nNote that the paper mentions a limitation of transformer-based language models in that their performance has not been related to general text characteristics, but this limitation is not the main focus of the paper and is only briefly mentioned."
    },
    {
        "title": "Phone-ing it in: Towards Flexible Multi-Modal Language Model Training by Phonetic Representations of Data",
        "authors": [
            "Colin Leong",
            "Daniel Whitenack"
        ],
        "published": "2022",
        "summary": "Multi-modal techniques offer significant untapped potential to unlock improved NLP technology for local languages. However, many advances in language model pre-training are focused on text, a fact that only increases systematic inequalities in the performance of NLP tasks across the world’s languages. In this work, we propose a multi-modal approach to train language models using whatever text and/or audio data might be available in a language. Initial experiments using Swahili and Kinyarwanda data suggest the viability of the approach for downstream Named Entity Recognition (NER) tasks, with models pre-trained on phone data showing an improvement of up to 6% F1-score above models that are trained from scratch. Preprocessing and training code will be uploaded to https://github.com/sil-ai/phone-it-in.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.364.pdf",
        "keywords": [
            "local languages",
            "whatever",
            "language model",
            "named entity recognition",
            "swahili",
            "phonetic representations",
            "phone",
            "modal language"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"many advances in language model pre-training are focused on text, a fact that only increases systematic inequalities in the performance of NLP tasks across the world’s languages.\"\n\nThis paper mentions a limitation of LLMs in passing, specifically that they are often pre-trained on text data, which can increase performance inequalities across languages. However, the primary focus of the paper is on proposing a multi",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"many advances in language model pre-training are focused on text, a fact that only increases systematic inequalities in the performance of NLP tasks across the world’s languages.\"\n\nThis paper mentions a limitation of LLMs in passing, specifically that they are often pre-trained on text data, which can increase performance inequalities across languages. However, the primary focus of the paper is on proposing a multi"
    },
    {
        "title": "Noisy Channel Language Model Prompting for Few-Shot Text Classification",
        "authors": [
            "Sewon Min",
            "Mike Lewis",
            "Hannaneh Hajishirzi",
            "Luke Zettlemoyer"
        ],
        "published": "2022",
        "summary": "We introduce a noisy channel approach for language model prompting in few-shot text classification. Instead of computing the likelihood of the label given the input (referred as direct models), channel models compute the conditional probability of the input given the label, and are thereby required to explain every word in the input. We use channel models for recently proposed few-shot learning methods with no or very limited updates to the language model parameters, via either in-context demonstration or prompt tuning. Our experiments show that, for both methods, channel models significantly outperform their direct counterparts, which we attribute to their stability, i.e., lower variance and higher worst-case accuracy. We also present extensive ablations that provide recommendations for when to use channel prompt tuning instead of other competitive models (e.g., direct head tuning): channel prompt tuning is preferred when the number of training examples is small, labels in the training data are imbalanced, or generalization to unseen labels is required.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.365.pdf",
        "keywords": [
            "shot text classification",
            "few shot text classification",
            "language model prompting",
            "noisy channel language model",
            "learning"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions \"no or very limited updates to the language model parameters\", which implies a limitation of LLMs, however it is not explicitly stated as a limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions \"no or very limited updates to the language model parameters\", which implies a limitation of LLMs, however it is not explicitly stated as a limitation."
    },
    {
        "title": "Multilingual unsupervised sequence segmentation transfers to extremely low-resource languages",
        "authors": [
            "C. Downey",
            "Shannon Drizin",
            "Levon Haroutunian",
            "Shivin Thukral"
        ],
        "published": "2022",
        "summary": "We show that unsupervised sequence-segmentation performance can be transferred to extremely low-resource languages by pre-training a Masked Segmental Language Model (Downey et al., 2021) multilingually. Further, we show that this transfer can be achieved by training over a collection of low-resource languages that are typologically similar (but phylogenetically unrelated) to the target language. In our experiments, we transfer from a collection of 10 Indigenous American languages (AmericasNLP, Mager et al., 2021) to K’iche’, a Mayan language. We compare our multilingual model to a monolingual (from-scratch) baseline, as well as a model pre-trained on Quechua only. We show that the multilingual pre-trained approach yields consistent segmentation quality across target dataset sizes, exceeding the monolingual baseline in 6/10 experimental settings. Our model yields especially strong results at small target sizes, including a zero-shot performance of 20.6 F1. These results have promising implications for low-resource NLP pipelines involving human-like linguistic units, such as the sparse transcription framework proposed by Bird (2020).",
        "pdf_link": "https://aclanthology.org/2022.acl-long.366.pdf",
        "keywords": [
            "sequence segmentation",
            "segmentation",
            "unsupervised sequence segmentation"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "KinyaBERT: a Morphology-aware Kinyarwanda Language Model",
        "authors": [
            "Antoine Nzeyimana",
            "Andre Niyongabo Rubungo"
        ],
        "published": "2022",
        "summary": "Pre-trained language models such as BERT have been successful at tackling many natural language processing tasks. However, the unsupervised sub-word tokenization methods commonly used in these models (e.g., byte-pair encoding - BPE) are sub-optimal at handling morphologically rich languages. Even given a morphological analyzer, naive sequencing of morphemes into a standard BERT architecture is inefficient at capturing morphological compositionality and expressing word-relative syntactic regularities. We address these challenges by proposing a simple yet effective two-tier BERT architecture that leverages a morphological analyzer and explicitly represents morphological compositionality.Despite the success of BERT, most of its evaluations have been conducted on high-resource languages, obscuring its applicability on low-resource languages. We evaluate our proposed method on the low-resource morphologically rich Kinyarwanda language, naming the proposed model architecture KinyaBERT. A robust set of experimental results reveal that KinyaBERT outperforms solid baselines by 2% in F1 score on a named entity recognition task and by 4.3% in average score of a machine-translated GLUE benchmark. KinyaBERT fine-tuning has better convergence and achieves more robust results on multiple tasks even in the presence of translation noise.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.367.pdf",
        "keywords": [
            "kinyabert",
            "pre trained language models",
            "named entity recognition",
            "resource languages",
            "morphology"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the unsupervised sub-word tokenization methods commonly used in these models (e.g., byte-pair encoding - BPE) are sub-optimal at handling morphologically rich languages. Even given a morphological analyzer, naive sequencing of morphemes into a standard BERT architecture is inefficient at capturing morphological compositionality and expressing word-relative syntactic regularities.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the unsupervised sub-word tokenization methods commonly used in these models (e.g., byte-pair encoding - BPE) are sub-optimal at handling morphologically rich languages. Even given a morphological analyzer, naive sequencing of morphemes into a standard BERT architecture is inefficient at capturing morphological compositionality and expressing word-relative syntactic regularities.\""
    },
    {
        "title": "On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency",
        "authors": [
            "Seo Yeon Park",
            "Cornelia Caragea"
        ],
        "published": "2022",
        "summary": "A well-calibrated neural model produces confidence (probability outputs) closely approximated by the expected accuracy. While prior studies have shown that mixup training as a data augmentation technique can improve model calibration on image classification tasks, little is known about using mixup for model calibration on natural language understanding (NLU) tasks. In this paper, we explore mixup for model calibration on several NLU tasks and propose a novel mixup strategy for pre-trained language models that improves model calibration further. Our proposed mixup is guided by both the Area Under the Margin (AUM) statistic (Pleiss et al., 2020) and the saliency map of each sample (Simonyan et al., 2013). Moreover, we combine our mixup strategy with model miscalibration correction techniques (i.e., label smoothing and temperature scaling) and provide detailed analyses of their impact on our proposed mixup. We focus on systematically designing experiments on three NLU tasks: natural language inference, paraphrase detection, and commonsense reasoning. Our method achieves the lowest expected calibration error compared to strong baselines on both in-domain and out-of-domain test samples while maintaining competitive accuracy.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.368.pdf",
        "keywords": [
            "model miscalibration",
            "language",
            "label smoothing",
            "mixup",
            "pre trained language models",
            "area under the margin",
            "model calibration",
            "inference"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While prior studies have shown that mixup training as a data augmentation technique can improve model calibration on image classification tasks, little is known about using mixup for model calibration on natural language understanding (NLU) tasks.\"\n\nThis rating is given because the paper mentions a limitation of pre-trained language models (poor calibration on NLU tasks) but does not explore it in depth, instead",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While prior studies have shown that mixup training as a data augmentation technique can improve model calibration on image classification tasks, little is known about using mixup for model calibration on natural language understanding (NLU) tasks.\"\n\nThis rating is given because the paper mentions a limitation of pre-trained language models (poor calibration on NLU tasks) but does not explore it in depth, instead"
    },
    {
        "title": "Prix-LM: Pretraining for Multilingual Knowledge Base Construction",
        "authors": [
            "Wenxuan Zhou",
            "Fangyu Liu",
            "Ivan Vulić",
            "Nigel Collier",
            "Muhao Chen"
        ],
        "published": "2022",
        "summary": "Knowledge bases (KBs) contain plenty of structured world and commonsense knowledge. As such, they often complement distributional text-based information and facilitate various downstream tasks. Since their manual construction is resource- and time-intensive, recent efforts have tried leveraging large pretrained language models (PLMs) to generate additional monolingual knowledge facts for KBs. However, such methods have not been attempted for building and enriching multilingual KBs. Besides wider application, such multilingual KBs can provide richer combined knowledge than monolingual (e.g., English) KBs. Knowledge expressed in different languages may be complementary and unequally distributed: this implies that the knowledge available in high-resource languages can be transferred to low-resource ones. To achieve this, it is crucial to represent multilingual knowledge in a shared/unified space. To this end, we propose a unified representation model, Prix-LM, for multilingual KB construction and completion. We leverage two types of knowledge, monolingual triples and cross-lingual links, extracted from existing multilingual KBs, and tune a multilingual language encoder XLM-R via a causal language modeling objective. Prix-LM integrates useful multilingual and KB-based factual knowledge into a single model. Experiments on standard entity-related tasks, such as link prediction in multiple languages, cross-lingual entity linking and bilingual lexicon induction, demonstrate its effectiveness, with gains reported over strong task-specialised baselines.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.371.pdf",
        "keywords": [
            "knowledge bases",
            "knowledge base construction",
            "pretraining",
            "multilingual kb construction",
            "language models",
            "multilingual",
            "link prediction"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, such methods have not been attempted for building and enriching multilingual KBs.\"\n\nNote: The abstract mentions a limitation of existing methods (not being able to build and enrich multilingual KBs) but it's not a limitation of LLMs itself, however, it's related to the application of LLMs in this context.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, such methods have not been attempted for building and enriching multilingual KBs.\"\n\nNote: The abstract mentions a limitation of existing methods (not being able to build and enrich multilingual KBs) but it's not a limitation of LLMs itself, however, it's related to the application of LLMs in this context."
    },
    {
        "title": "HOLM: Hallucinating Objects with Language Models for Referring Expression Recognition in Partially-Observed Scenes",
        "authors": [
            "Volkan Cirik",
            "Louis-Philippe Morency",
            "Taylor Berg-Kirkpatrick"
        ],
        "published": "2022",
        "summary": "AI systems embodied in the physical world face a fundamental challenge of partial observability; operating with only a limited view and knowledge of the environment. This creates challenges when AI systems try to reason about language and its relationship with the environment: objects referred to through language (e.g. giving many instructions) are not immediately visible. Actions by the AI system may be required to bring these objects in view. A good benchmark to study this challenge is Dynamic Referring Expression Recognition (dRER) task, where the goal is to find a target location by dynamically adjusting the field of view (FoV) in a partially observed 360 scenes. In this paper, we introduce HOLM, Hallucinating Objects with Language Models, to address the challenge of partial observability. HOLM uses large pre-trained language models (LMs) to infer object hallucinations for the unobserved part of the environment. Our core intuition is that if a pair of objects co-appear in an environment frequently, our usage of language should reflect this fact about the world. Based on this intuition, we prompt language models to extract knowledge about object affinities which gives us a proxy for spatial relationships of objects. Our experiments show that HOLM performs better than the state-of-the-art approaches on two datasets for dRER; allowing to study generalization for both indoor and outdoor settings.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.373.pdf",
        "keywords": [
            "hallucinating objects",
            "partial observability",
            "language models",
            "trained language models",
            "holm"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations, but the paper's goal is to address a challenge related to LLMs, implying a limitation in handling partial observability.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations, but the paper's goal is to address a challenge related to LLMs, implying a limitation in handling partial observability."
    },
    {
        "title": "Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models",
        "authors": [
            "Kabir Ahuja",
            "Shanu Kumar",
            "Sandipan Dandapat",
            "Monojit Choudhury"
        ],
        "published": "2022",
        "summary": "Massively Multilingual Transformer based Language Models have been observed to be surprisingly effective on zero-shot transfer across languages, though the performance varies from language to language depending on the pivot language(s) used for fine-tuning. In this work, we build upon some of the existing techniques for predicting the zero-shot performance on a task, by modeling it as a multi-task learning problem. We jointly train predictive models for different tasks which helps us build more accurate predictors for tasks where we have test data in very few languages to measure the actual performance of the model. Our approach also lends us the ability to perform a much more robust feature selection, and identify a common set of features that influence zero-shot performance across a variety of tasks.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.374.pdf",
        "keywords": [
            "multi task learning",
            "performance prediction",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"though the performance varies from language to language depending on the pivot language(s) used for fine-tuning.\"\n\nThis evidence suggests that the paper mentions a limitation of multilingual models, specifically that their zero-shot performance varies across languages depending on the pivot language used for fine-tuning. However, this limitation is not explored in depth and is primarily used to motivate the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"though the performance varies from language to language depending on the pivot language(s) used for fine-tuning.\"\n\nThis evidence suggests that the paper mentions a limitation of multilingual models, specifically that their zero-shot performance varies across languages depending on the pivot language used for fine-tuning. However, this limitation is not explored in depth and is primarily used to motivate the proposed solution."
    },
    {
        "title": "Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View",
        "authors": [
            "Boxi Cao",
            "Hongyu Lin",
            "Xianpei Han",
            "Fangchao Liu",
            "Le Sun"
        ],
        "published": "2022",
        "summary": "Prompt-based probing has been widely used in evaluating the abilities of pretrained language models (PLMs). Unfortunately, recent studies have discovered such an evaluation may be inaccurate, inconsistent and unreliable. Furthermore, the lack of understanding its inner workings, combined with its wide applicability, has the potential to lead to unforeseen risks for evaluating and applying PLMs in real-world applications. To discover, understand and quantify the risks, this paper investigates the prompt-based probing from a causal view, highlights three critical biases which could induce biased results and conclusions, and proposes to conduct debiasing via causal intervention. This paper provides valuable insights for the design of unbiased datasets, better probing frameworks and more reliable evaluations of pretrained language models. Furthermore, our conclusions also echo that we need to rethink the criteria for identifying better pretrained language models.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.398.pdf",
        "keywords": [
            "probing frameworks",
            "language models",
            "language",
            "prompt based probing",
            "risks"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Unfortunately, recent studies have discovered such an evaluation may be inaccurate, inconsistent and unreliable.\"; \"the lack of understanding its inner workings, combined with its wide applicability, has the potential to lead to unforeseen risks for evaluating and applying PLMs in real-world applications.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Unfortunately, recent studies have discovered such an evaluation may be inaccurate, inconsistent and unreliable.\"; \"the lack of understanding its inner workings, combined with its wide applicability, has the potential to lead to unforeseen risks for evaluating and applying PLMs in real-world applications.\""
    },
    {
        "title": "Probing Simile Knowledge from Pre-trained Language Models",
        "authors": [
            "Weijie Chen",
            "Yongzhu Chang",
            "Rongsheng Zhang",
            "Jiashu Pu",
            "Guandan Chen",
            "Le Zhang",
            "Yadong Xi",
            "Yijiang Chen",
            "Chang Su"
        ],
        "published": "2022",
        "summary": "Simile interpretation (SI) and simile generation (SG) are challenging tasks for NLP because models require adequate world knowledge to produce predictions. Previous works have employed many hand-crafted resources to bring knowledge-related into models, which is time-consuming and labor-intensive. In recent years, pre-trained language models (PLMs) based approaches have become the de-facto standard in NLP since they learn generic knowledge from a large corpus. The knowledge embedded in PLMs may be useful for SI and SG tasks. Nevertheless, there are few works to explore it. In this paper, we probe simile knowledge from PLMs to solve the SI and SG tasks in the unified framework of simile triple completion for the first time. The backbone of our framework is to construct masked sentences with manual patterns and then predict the candidate words in the masked position. In this framework, we adopt a secondary training process (Adjective-Noun mask Training) with the masked language model (MLM) loss to enhance the prediction diversity of candidate words in the masked position. Moreover, pattern ensemble (PE) and pattern search (PS) are applied to improve the quality of predicted words. Finally, automatic and human evaluations demonstrate the effectiveness of our framework in both SI and SG tasks.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.404.pdf",
        "keywords": [
            "masked language model",
            "predicted",
            "pattern search",
            "trained language models",
            "pattern ensemble"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Nevertheless, there are few works to explore it.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Nevertheless, there are few works to explore it.\""
    },
    {
        "title": "Continual Pre-training of Language Models for Math Problem Understanding with Syntax-Aware Memory Network",
        "authors": [
            "Zheng Gong",
            "Kun Zhou",
            "Xin Zhao",
            "Jing Sha",
            "Shijin Wang",
            "Ji-Rong Wen"
        ],
        "published": "2022",
        "summary": "In this paper, we study how to continually pre-train language models for improving the understanding of math problems. Specifically, we focus on solving a fundamental challenge in modeling math problems, how to fuse the semantics of textual description and formulas, which are highly different in essence. To address this issue, we propose a new approach called COMUS to continually pre-train language models for math problem understanding with syntax-aware memory network. In this approach, we first construct the math syntax graph to model the structural semantic information, by combining the parsing trees of the text and formulas, and then design the syntax-aware memory networks to deeply fuse the features from the graph and text. With the help of syntax relations, we can model the interaction between the token from the text and its semantic-related nodes within the formulas, which is helpful to capture fine-grained semantic correlations between texts and formulas. Besides, we devise three continual pre-training tasks to further align and fuse the representations of the text and math syntax graph. Experimental results on four tasks in the math domain demonstrate the effectiveness of our approach. Our code and data are publicly available at the link: bluehttps://github.com/RUCAIBox/COMUS.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.408.pdf",
        "keywords": [
            "syntax",
            "math problems",
            "math syntax",
            "math problem understanding",
            "syntax aware memory network",
            "memory network",
            "continual pre training",
            "language models",
            "math syntax graph"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"how to fuse the semantics of textual description and formulas, which are highly different in essence.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"how to fuse the semantics of textual description and formulas, which are highly different in essence.\""
    },
    {
        "title": "Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction",
        "authors": [
            "Zhanming Jie",
            "Jierui Li",
            "Wei Lu"
        ],
        "published": "2022",
        "summary": "Solving math word problems requires deductive reasoning over the quantities in the text. Various recent research efforts mostly relied on sequence-to-sequence or sequence-to-tree models to generate mathematical expressions without explicitly performing relational reasoning between quantities in the given context. While empirically effective, such approaches typically do not provide explanations for the generated expressions. In this work, we view the task as a complex relation extraction problem, proposing a novel approach that presents explainable deductive reasoning steps to iteratively construct target expressions, where each step involves a primitive operation over two quantities defining their relation. Through extensive experiments on four benchmark datasets, we show that the proposed model significantly outperforms existing strong baselines. We further demonstrate that the deductive procedure not only presents more explainable steps but also enables us to make more accurate predictions on questions that require more complex reasoning.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.410.pdf",
        "keywords": [
            "complex relation extraction"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Toward Interpretable Semantic Textual Similarity via Optimal Transport-based Contrastive Sentence Learning",
        "authors": [
            "Seonghyeon Lee",
            "Dongha Lee",
            "Seongbo Jang",
            "Hwanjo Yu"
        ],
        "published": "2022",
        "summary": "Recently, finetuning a pretrained language model to capture the similarity between sentence embeddings has shown the state-of-the-art performance on the semantic textual similarity (STS) task. However, the absence of an interpretation method for the sentence similarity makes it difficult to explain the model output. In this work, we explicitly describe the sentence distance as the weighted sum of contextualized token distances on the basis of a transportation problem, and then present the optimal transport-based distance measure, named RCMD; it identifies and leverages semantically-aligned token pairs. In the end, we propose CLRCMD, a contrastive learning framework that optimizes RCMD of sentence pairs, which enhances the quality of sentence similarity and their interpretation. Extensive experiments demonstrate that our learning framework outperforms other baselines on both STS and interpretable-STS benchmarks, indicating that it computes effective sentence similarity and also provides interpretation consistent with human judgement.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.412.pdf",
        "keywords": [
            "similarity",
            "sentence similarity",
            "sentence learning",
            "finetuning",
            "semantic textual similarity"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the absence of an interpretation method for the sentence similarity makes it difficult to explain the model output.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the absence of an interpretation method for the sentence similarity makes it difficult to explain the model output.\""
    },
    {
        "title": "Pre-training and Fine-tuning Neural Topic Model: A Simple yet Effective Approach to Incorporating External Knowledge",
        "authors": [
            "Linhai Zhang",
            "Xuemeng Hu",
            "Boyu Wang",
            "Deyu Zhou",
            "Qian-Wen Zhang",
            "Yunbo Cao"
        ],
        "published": "2022",
        "summary": "Recent years have witnessed growing interests in incorporating external knowledge such as pre-trained word embeddings (PWEs) or pre-trained language models (PLMs) into neural topic modeling. However, we found that employing PWEs and PLMs for topic modeling only achieved limited performance improvements but with huge computational overhead. In this paper, we propose a novel strategy to incorporate external knowledge into neural topic modeling where the neural topic model is pre-trained on a large corpus and then fine-tuned on the target dataset. Experiments have been conducted on three datasets and results show that the proposed approach significantly outperforms both current state-of-the-art neural topic models and some topic modeling approaches enhanced with PWEs or PLMs. Moreover, further study shows that the proposed approach greatly reduces the need for the huge size of training data.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.413.pdf",
        "keywords": [
            "topic modeling",
            "neural topic modeling",
            "tuning neural topic model",
            "fine tuned",
            "pre training",
            "language"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, we found that employing PWEs and PLMs for topic modeling only achieved limited performance improvements but with huge computational overhead.\"\n\nThis paper mentions a limitation of using pre-trained language models (PLMs), which is a type of LLM, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, we found that employing PWEs and PLMs for topic modeling only achieved limited performance improvements but with huge computational overhead.\"\n\nThis paper mentions a limitation of using pre-trained language models (PLMs), which is a type of LLM, but it is not the primary focus of the paper."
    },
    {
        "title": "Graph Pre-training for AMR Parsing and Generation",
        "authors": [
            "Xuefeng Bai",
            "Yulong Chen",
            "Yue Zhang"
        ],
        "published": "2022",
        "summary": "Abstract meaning representation (AMR) highlights the core semantic information of text in a graph structure. Recently, pre-trained language models (PLMs) have advanced tasks of AMR parsing and AMR-to-text generation, respectively. However, PLMs are typically pre-trained on textual data, thus are sub-optimal for modeling structural knowledge. To this end, we investigate graph self-supervised training to improve the structure awareness of PLMs over AMR graphs. In particular, we introduce two graph auto-encoding strategies for graph-to-graph pre-training and four tasks to integrate text and graph information during pre-training. We further design a unified framework to bridge the gap between pre-training and fine-tuning tasks. Experiments on both AMR parsing and AMR-to-text generation show the superiority of our model. To our knowledge, we are the first to consider pre-training on semantic graphs.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.415.pdf",
        "keywords": [
            "training",
            "pre training",
            "text generation",
            "graph pre training",
            "parsing",
            "auto encoding",
            "amr parsing",
            "graph",
            "semantic graphs",
            "trained language models",
            "graph self supervised training",
            "abstract meaning representation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"PLMs are typically pre-trained on textual data, thus are sub-optimal for modeling structural knowledge.\"\n\nThis evidence suggests that the paper mentions a limitation of pre-trained language models (PLMs), which can be classified as a type of LLM, in modeling structural knowledge due to their pre-training on textual data.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"PLMs are typically pre-trained on textual data, thus are sub-optimal for modeling structural knowledge.\"\n\nThis evidence suggests that the paper mentions a limitation of pre-trained language models (PLMs), which can be classified as a type of LLM, in modeling structural knowledge due to their pre-training on textual data."
    },
    {
        "title": "Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills",
        "authors": [
            "Ori Yoran",
            "Alon Talmor",
            "Jonathan Berant"
        ],
        "published": "2022",
        "summary": "Models pre-trained with a language modeling objective possess ample world knowledge and language skills, but are known to struggle in tasks that require reasoning. In this work, we propose to leverage semi-structured tables, and automatically generate at scale question-paragraph pairs, where answering the question requires reasoning over multiple facts in the paragraph. We add a pre-training step over this synthetic data, which includes examples that require 16 different reasoning skills such as number comparison, conjunction, and fact composition. To improve data efficiency, we sample examples from reasoning skills where the model currently errs. We evaluate our approach on three reasoning-focused reading comprehension datasets, and show that our model, PReasM, substantially outperforms T5, a popular pre-trained encoder-decoder model. Moreover, sampling examples based on model errors leads to faster training and higher performance.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.416.pdf",
        "keywords": [
            "reasoning skills",
            "turning tables",
            "reasoning focused",
            "language"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Models pre-trained with a language modeling objective possess ample world knowledge and language skills, but are known to struggle in tasks that require reasoning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Models pre-trained with a language modeling objective possess ample world knowledge and language skills, but are known to struggle in tasks that require reasoning.\""
    },
    {
        "title": "Debiased Contrastive Learning of Unsupervised Sentence Representations",
        "authors": [
            "Kun Zhou",
            "Beichen Zhang",
            "Xin Zhao",
            "Ji-Rong Wen"
        ],
        "published": "2022",
        "summary": "Recently, contrastive learning has been shown to be effective in improving pre-trained language models (PLM) to derive high-quality sentence representations. It aims to pull close positive examples to enhance the alignment while push apart irrelevant negatives for the uniformity of the whole representation space. However, previous works mostly adopt in-batch negatives or sample from training data at random. Such a way may cause the sampling bias that improper negatives (false negatives and anisotropy representations) are used to learn sentence representations, which will hurt the uniformity of the representation space. To address it, we present a new framework DCLR (Debiased Contrastive Learning of unsupervised sentence Representations) to alleviate the influence of these improper negatives.In DCLR, we design an instance weighting method to punish false negatives and generate noise-based negatives to guarantee the uniformity of the representation space.Experiments on seven semantic textual similarity tasks show that our approach is more effective than competitive baselines. Our code and data are publicly available at the link: bluehttps://github.com/RUCAIBox/DCLR.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.423.pdf",
        "keywords": [
            "contrastive learning",
            "sentence representations",
            "negatives",
            "unsupervised sentence representations",
            "unsupervised sentence",
            "pre trained language models",
            "noise"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Such a way may cause the sampling bias that improper negatives (false negatives and anisotropy representations) are used to learn sentence representations, which will hurt the uniformity of the representation space.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Such a way may cause the sampling bias that improper negatives (false negatives and anisotropy representations) are used to learn sentence representations, which will hurt the uniformity of the representation space.\""
    },
    {
        "title": "MSP: Multi-Stage Prompting for Making Pre-trained Language Models Better Translators",
        "authors": [
            "Zhixing Tan",
            "Xiangwen Zhang",
            "Shuo Wang",
            "Yang Liu"
        ],
        "published": "2022",
        "summary": "Prompting has recently been shown as a promising approach for applying pre-trained language models to perform downstream tasks. We present Multi-Stage Prompting, a simple and automatic approach for leveraging pre-trained language models to translation tasks. To better mitigate the discrepancy between pre-training and translation, MSP divides the translation process via pre-trained language models into three separate stages: the encoding stage, the re-encoding stage, and the decoding stage. During each stage, we independently apply different continuous prompts for allowing pre-trained language models better shift to translation tasks. We conduct extensive experiments on three translation tasks. Experiments show that our method can significantly improve the translation performance of pre-trained language models.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.424.pdf",
        "keywords": [
            "language models",
            "prompting",
            "translators",
            "msp",
            "trained language models",
            "pre trained language models",
            "multi stage prompting"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"To better mitigate the discrepancy between pre-training and translation\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"To better mitigate the discrepancy between pre-training and translation\""
    },
    {
        "title": "SalesBot: Transitioning from Chit-Chat to Task-Oriented Dialogues",
        "authors": [
            "Ssu Chiu",
            "Maolin Li",
            "Yen-Ting Lin",
            "Yun-Nung Chen"
        ],
        "published": "2022",
        "summary": "Dialogue systems are usually categorized into two types, open-domain and task-oriented. The first one focuses on chatting with users and making them engage in the conversations, where selecting a proper topic to fit the dialogue context is essential for a successful dialogue. The other one focuses on a specific task instead of casual talks, e.g., finding a movie on Friday night, playing a song. These two directions have been studied separately due to their different purposes. However, how to smoothly transition from social chatting to task-oriented dialogues is important for triggering the business opportunities, and there is no any public data focusing on such scenarios. Hence, this paper focuses on investigating the conversations starting from open-domain social chatting and then gradually transitioning to task-oriented purposes, and releases a large-scale dataset with detailed annotations for encouraging this research direction. To achieve this goal, this paper proposes a framework to automatically generate many dialogues without human involvement, in which any powerful open-domain dialogue generation model can be easily leveraged. The human evaluation shows that our generated dialogue data has a natural flow at a reasonable quality, showing that our released data has a great potential of guiding future research directions and commercial activities. Furthermore, the released models allow researchers to automatically generate unlimited dialogues in the target scenarios, which can greatly benefit semi-supervised and unsupervised approaches.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.425.pdf",
        "keywords": [
            "dialogues",
            "conversations",
            "dialogue generation",
            "dialogue systems"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "XLM-E: Cross-lingual Language Model Pre-training via ELECTRA",
        "authors": [
            "Zewen Chi",
            "Shaohan Huang",
            "Li Dong",
            "Shuming Ma",
            "Bo Zheng",
            "Saksham Singhal",
            "Payal Bajaj",
            "Xia Song",
            "Xian-Ling Mao",
            "Heyan Huang",
            "Furu Wei"
        ],
        "published": "2022",
        "summary": "In this paper, we introduce ELECTRA-style tasks to cross-lingual language model pre-training. Specifically, we present two pre-training tasks, namely multilingual replaced token detection, and translation replaced token detection. Besides, we pretrain the model, named as XLM-E, on both multilingual and parallel corpora. Our model outperforms the baseline models on various cross-lingual understanding tasks with much less computation cost. Moreover, analysis shows that XLM-E tends to obtain better cross-lingual transferability.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.427.pdf",
        "keywords": [
            "cross lingual language model",
            "parallel corpora",
            "electra"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations of LLMs are mentioned in the abstract, but the introduction of a new pre-training method implies that existing methods may have limitations, such as high computation cost.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations of LLMs are mentioned in the abstract, but the introduction of a new pre-training method implies that existing methods may have limitations, such as high computation cost."
    },
    {
        "title": "ReACC: A Retrieval-Augmented Code Completion Framework",
        "authors": [
            "Shuai Lu",
            "Nan Duan",
            "Hojae Han",
            "Daya Guo",
            "Seung-won Hwang",
            "Alexey Svyatkovskiy"
        ],
        "published": "2022",
        "summary": "Code completion, which aims to predict the following code token(s) according to the code context, can improve the productivity of software development. Recent work has proved that statistical language modeling with transformers can greatly improve the performance in the code completion task via learning from large-scale source code datasets. However, current approaches focus only on code context within the file or project, i.e. internal context. Our distinction is utilizing ”external” context, inspired by human behaviors of copying from the related code snippets when writing code. Specifically, we propose a retrieval-augmented code completion framework, leveraging both lexical copying and referring to code with similar semantics by retrieval. We adopt a stage-wise training approach that combines a source code retriever and an auto-regressive language model for programming language. We evaluate our approach in the code completion task in Python and Java programming languages, achieving a state-of-the-art performance on CodeXGLUE benchmark.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.431.pdf",
        "keywords": [
            "code completion",
            "code completion framework",
            "java",
            "programming language"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, current approaches focus only on code context within the file or project, i.e. internal context.\"\n\nThis evidence indicates a limitation of current language modeling approaches for code completion, but it is not a major focus of the paper and is used to justify the proposed method.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, current approaches focus only on code context within the file or project, i.e. internal context.\"\n\nThis evidence indicates a limitation of current language modeling approaches for code completion, but it is not a major focus of the paper and is used to justify the proposed method."
    },
    {
        "title": "UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning",
        "authors": [
            "Yuning Mao",
            "Lambert Mathias",
            "Rui Hou",
            "Amjad Almahairi",
            "Hao Ma",
            "Jiawei Han",
            "Scott Yih",
            "Madian Khabsa"
        ],
        "published": "2022",
        "summary": "Recent parameter-efficient language model tuning (PELT) methods manage to match the performance of fine-tuning with much fewer trainable parameters and perform especially well when training data is limited. However, different PELT methods may perform rather differently on the same task, making it nontrivial to select the most appropriate method for a specific task, especially considering the fast-growing number of new PELT methods and tasks. In light of model diversity and the difficulty of model selection, we propose a unified framework, UniPELT, which incorporates different PELT methods as submodules and learns to activate the ones that best suit the current data or task setup via gating mechanism. On the GLUE benchmark, UniPELT consistently achieves 1 4% gains compared to the best individual PELT method that it incorporates and even outperforms fine-tuning under different setups. Moreover, UniPELT generally surpasses the upper bound that takes the best performance of all its submodules used individually on each task, indicating that a mixture of multiple PELT methods may be inherently more effective than single methods.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.433.pdf",
        "keywords": [
            "parameter efficient"
        ],
        "Rate_Llama-3.1-70b": "not extracted",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs:  \nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Universal Conditional Masked Language Pre-training for Neural Machine Translation",
        "authors": [
            "Pengfei Li",
            "Liangyou Li",
            "Meng Zhang",
            "Minghao Wu",
            "Qun Liu"
        ],
        "published": "2022",
        "summary": "Pre-trained sequence-to-sequence models have significantly improved Neural Machine Translation (NMT). Different from prior works where pre-trained models usually adopt an unidirectional decoder, this paper demonstrates that pre-training a sequence-to-sequence model but with a bidirectional decoder can produce notable performance gains for both Autoregressive and Non-autoregressive NMT. Specifically, we propose CeMAT, a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. We also introduce two simple but effective methods to enhance the CeMAT, aligned code-switching & masking and dynamic dual-masking. We conduct extensive experiments and show that our CeMAT can achieve significant performance improvement for all scenarios from low- to extremely high-resource languages, i.e., up to +14.4 BLEU on low resource and +7.9 BLEU improvements on average for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it can also produce consistent performance gains, i.e., up to +5.3 BLEU. To the best of our knowledge, this is the first work to pre-train a unified model for fine-tuning on both NMT tasks. Code, data, and pre-trained models are available at https://github.com/huawei-noah/Pretrained-Language-Model/CeMAT",
        "pdf_link": "https://aclanthology.org/2022.acl-long.442.pdf",
        "keywords": [
            "neural machine translation"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicitly mentioned, but the paper proposes a new pre-training method to improve performance, implying that existing methods may have limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None explicitly mentioned, but the paper proposes a new pre-training method to improve performance, implying that existing methods may have limitations."
    },
    {
        "title": "Transformers in the loop: Polarity in neural models of language",
        "authors": [
            "Lisa Bylinina",
            "Alexey Tikhonov"
        ],
        "published": "2022",
        "summary": "Representation of linguistic phenomena in computational language models is typically assessed against the predictions of existing linguistic theories of these phenomena. Using the notion of polarity as a case study, we show that this is not always the most adequate set-up. We probe polarity via so-called ‘negative polarity items’ (in particular, English ‘any’) in two pre-trained Transformer-based models (BERT and GPT-2). We show that – at least for polarity – metrics derived from language models are more consistent with data from psycholinguistic experiments than linguistic theory predictions. Establishing this allows us to more adequately evaluate the performance of language models and also to use language models to discover new insights into natural language grammar beyond existing linguistic theories. This work contributes to establishing closer ties between psycholinguistic experiments and experiments with language models.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.455.pdf",
        "keywords": [
            "polarity",
            "language",
            "language models",
            "negative polarity",
            "transformers",
            "computational language models",
            "linguistic"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper discusses the performance of pre-trained Transformer-based models (BERT and GPT-2) in relation to linguistic theories, which implies a limitation in the context of evaluating their performance.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper discusses the performance of pre-trained Transformer-based models (BERT and GPT-2) in relation to linguistic theories, which implies a limitation in the context of evaluating their performance."
    },
    {
        "title": "Bridging the Data Gap between Training and Inference for Unsupervised Neural Machine Translation",
        "authors": [
            "Zhiwei He",
            "Xing Wang",
            "Rui Wang",
            "Shuming Shi",
            "Zhaopeng Tu"
        ],
        "published": "2022",
        "summary": "Back-translation is a critical component of Unsupervised Neural Machine Translation (UNMT), which generates pseudo parallel data from target monolingual data. A UNMT model is trained on the pseudo parallel data with translated source, and translates natural source sentences in inference. The source discrepancy between training and inference hinders the translation performance of UNMT models. By carefully designing experiments, we identify two representative characteristics of the data gap in source: (1) style gap (i.e., translated vs. natural text style) that leads to poor generalization capability; (2) content gap that induces the model to produce hallucination content biased towards the target language. To narrow the data gap, we propose an online self-training approach, which simultaneously uses the pseudo parallel data {natural source, translated target} to mimic the inference scenario. Experimental results on several widely-used language pairs show that our approach outperforms two strong baselines (XLM and MASS) by remedying the style and content gaps.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.456.pdf",
        "keywords": [
            "translated",
            "back translation",
            "unsupervised neural machine translation",
            "inference",
            "hallucination",
            "training"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Reinforcement Guided Multi-Task Learning Framework for Low-Resource Stereotype Detection",
        "authors": [
            "Rajkumar Pujari",
            "Erik Oveson",
            "Priyanka Kulkarni",
            "Elnaz Nouri"
        ],
        "published": "2022",
        "summary": "As large Pre-trained Language Models (PLMs) trained on large amounts of data in an unsupervised manner become more ubiquitous, identifying various types of bias in the text has come into sharp focus. Existing ‘Stereotype Detection’ datasets mainly adopt a diagnostic approach toward large PLMs. Blodgett et. al. (2021) show that there are significant reliability issues with the existing benchmark datasets. Annotating a reliable dataset requires a precise understanding of the subtle nuances of how stereotypes manifest in text. In this paper, we annotate a focused evaluation set for ‘Stereotype Detection’ that addresses those pitfalls by de-constructing various ways in which stereotypes manifest in text. Further, we present a multi-task model that leverages the abundance of data-rich neighboring tasks such as hate speech detection, offensive language detection, misogyny detection, etc., to improve the empirical performance on ‘Stereotype Detection’. We then propose a reinforcement-learning agent that guides the multi-task learning model by learning to identify the training examples from the neighboring tasks that help the target task the most. We show that the proposed models achieve significant empirical gains over existing baselines on all the tasks.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.462.pdf",
        "keywords": [
            "‘ stereotype detection",
            "multi task",
            "multi task learning",
            "multi task learning framework",
            "reinforcement learning agent",
            "low resource stereotype detection",
            "reinforcement"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Blodgett et. al. (2021) show that there are significant reliability issues with the existing benchmark datasets.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Blodgett et. al. (2021) show that there are significant reliability issues with the existing benchmark datasets.\""
    },
    {
        "title": "A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation",
        "authors": [
            "Tianyu Liu",
            "Yizhe Zhang",
            "Chris Brockett",
            "Yi Mao",
            "Zhifang Sui",
            "Weizhu Chen",
            "Bill Dolan"
        ],
        "published": "2022",
        "summary": "Large pretrained generative models like GPT-3 often suffer from hallucinating non-existent or incorrect content, which undermines their potential merits in real applications. Existing work usually attempts to detect these hallucinations based on a corresponding oracle reference at a sentence or document level. However ground-truth references may not be readily available for many free-form text generation applications, and sentence- or document-level detection may fail to provide the fine-grained signals that would prevent fallacious content in real time. As a first step to addressing these issues, we propose a novel token-level, reference-free hallucination detection task and an associated annotated dataset named HaDeS (HAllucination DEtection dataSet). To create this dataset, we first perturb a large number of text segments extracted from English language Wikipedia, and then verify these with crowd-sourced annotations. To mitigate label imbalance during annotation, we utilize an iterative model-in-loop strategy. We conduct comprehensive data analyses and create multiple baseline models.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.464.pdf",
        "keywords": [
            "hallucination",
            "hallucination detection",
            "truth"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large pretrained generative models like GPT-3 often suffer from hallucinating non-existent or incorrect content, which undermines their potential merits in real applications.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Large pretrained generative models like GPT-3 often suffer from hallucinating non-existent or incorrect content, which undermines their potential merits in real applications.\""
    },
    {
        "title": "Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in Practice",
        "authors": [
            "Andreas Grivas",
            "Nikolay Bogoychev",
            "Adam Lopez"
        ],
        "published": "2022",
        "summary": "Classifiers in natural language processing (NLP) often have a large number of output classes. For example, neural language models (LMs) and machine translation (MT) models both predict tokens from a vocabulary of thousands. The Softmax output layer of these models typically receives as input a dense feature representation, which has much lower dimensionality than the output. In theory, the result is some words may be impossible to be predicted via argmax, irrespective of input features, and empirically, there is evidence this happens in small language models (Demeter et al., 2020). In this paper we ask whether it can happen in practical large language models and translation models. To do so, we develop algorithms to detect such unargmaxable tokens in public models. We find that 13 out of 150 models do indeed have such tokens; however, they are very infrequent and unlikely to impact model quality. We release our algorithms and code to the public.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.465.pdf",
        "keywords": [
            "algorithms",
            "translation",
            "machine translation",
            "language models",
            "natural language processing"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"In theory, the result is some words may be impossible to be predicted via argmax, irrespective of input features, and empirically, there is evidence this happens in small language models (Demeter et al., 2020).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"In theory, the result is some words may be impossible to be predicted via argmax, irrespective of input features, and empirically, there is evidence this happens in small language models (Demeter et al., 2020).\""
    },
    {
        "title": "Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction",
        "authors": [
            "Yubo Ma",
            "Zehao Wang",
            "Yixin Cao",
            "Mukai Li",
            "Meiqi Chen",
            "Kun Wang",
            "Jing Shao"
        ],
        "published": "2022",
        "summary": "In this paper, we propose an effective yet efficient model PAIE for both sentence-level and document-level Event Argument Extraction (EAE), which also generalizes well when there is a lack of training data. On the one hand, PAIE utilizes prompt tuning for extractive objectives to take the best advantages of Pre-trained Language Models (PLMs). It introduces two span selectors based on the prompt to select start/end tokens among input texts for each role. On the other hand, it captures argument interactions via multi-role prompts and conducts joint optimization with optimal span assignments via a bipartite matching loss. Also, with a flexible prompt design, PAIE can extract multiple arguments with the same role instead of conventional heuristic threshold tuning. We have conducted extensive experiments on three benchmarks, including both sentence- and document-level EAE. The results present promising improvements from PAIE (3.5% and 2.3% F1 gains in average on three benchmarks, for PAIE-base and PAIE-large respectively). Further analysis demonstrates the efficiency, generalization to few-shot settings, and effectiveness of different extractive prompt tuning strategies. Our code is available at https://github.com/mayubo2333/PAIE.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.466.pdf",
        "keywords": [
            "extractive",
            "event argument extraction",
            "argument interactions",
            "prompting argument interaction"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but mentions \"Pre-trained Language Models (PLMs)\" which implies Large Language Models.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but mentions \"Pre-trained Language Models (PLMs)\" which implies Large Language Models."
    },
    {
        "title": "Fine-Grained Controllable Text Generation Using Non-Residual Prompting",
        "authors": [
            "Fredrik Carlsson",
            "Joey Öhman",
            "Fangyu Liu",
            "Severine Verlinden",
            "Joakim Nivre",
            "Magnus Sahlgren"
        ],
        "published": "2022",
        "summary": "The introduction of immensely large Causal Language Models (CLMs) has rejuvenated the interest in open-ended text generation. However, controlling the generative process for these Transformer-based models is at large an unsolved problem. Earlier work has explored either plug-and-play decoding strategies, or more powerful but blunt approaches such as prompting. There hence currently exists a trade-off between fine-grained control, and the capability for more expressive high-level instructions. To alleviate this trade-off, we propose an encoder-decoder architecture that enables intermediate text prompts at arbitrary time steps. We propose a resource-efficient method for converting a pre-trained CLM into this architecture, and demonstrate its potential on various experiments, including the novel task of contextualized word inclusion. Our method provides strong results on multiple experimental settings, proving itself to be both expressive and versatile.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.471.pdf",
        "keywords": [
            "prompting",
            "text generation",
            "controllable text generation",
            "non residual prompting",
            "fine grained control",
            "language models",
            "expressive",
            "encoder"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, controlling the generative process for these Transformer-based models is at large an unsolved problem.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, controlling the generative process for these Transformer-based models is at large an unsolved problem.\""
    },
    {
        "title": "TwittIrish: A Universal Dependencies Treebank of Tweets in Modern Irish",
        "authors": [
            "Lauren Cassidy",
            "Teresa Lynn",
            "James Barry",
            "Jennifer Foster"
        ],
        "published": "2022",
        "summary": "Modern Irish is a minority language lacking sufficient computational resources for the task of accurate automatic syntactic parsing of user-generated content such as tweets. Although language technology for the Irish language has been developing in recent years, these tools tend to perform poorly on user-generated content. As with other languages, the linguistic style observed in Irish tweets differs, in terms of orthography, lexicon, and syntax, from that of standard texts more commonly used for the development of language models and parsers. We release the first Universal Dependencies treebank of Irish tweets, facilitating natural language processing of user-generated content in Irish. In this paper, we explore the differences between Irish tweets and standard Irish text, and the challenges associated with dependency parsing of Irish tweets. We describe our bootstrapping method of treebank development and report on preliminary parsing experiments.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.473.pdf",
        "keywords": [
            "treebank",
            "irish",
            "irish tweets",
            "dependency parsing",
            "language"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs or their limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs or their limitations."
    },
    {
        "title": "Word Order Does Matter and Shuffled Language Models Know It",
        "authors": [
            "Mostafa Abdou",
            "Vinit Ravishankar",
            "Artur Kulmizev",
            "Anders Søgaard"
        ],
        "published": "2022",
        "summary": "Recent studies have shown that language models pretrained and/or fine-tuned on randomly permuted sentences exhibit competitive performance on GLUE, putting into question the importance of word order information. Somewhat counter-intuitively, some of these studies also report that position embeddings appear to be crucial for models’ good performance with shuffled text. We probe these language models for word order information and investigate what position embeddings learned from shuffled text encode, showing that these models retain a notion of word order information. We show this is in part due to a subtlety in how shuffling is implemented in previous work – before rather than after subword segmentation. Surprisingly, we find even Language models trained on text shuffled after subword segmentation retain some semblance of information about word order because of the statistical dependencies between sentence length and unigram probabilities. Finally, we show that beyond GLUE, a variety of language understanding tasks do require word order information, often to an extent that cannot be learned through fine-tuning.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.476.pdf",
        "keywords": [
            "word order",
            "shuffled",
            "word order information",
            "shuffled language",
            "language"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Finally, we show that beyond GLUE, a variety of language understanding tasks do require word order information, often to an extent that cannot be learned through fine-tuning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Finally, we show that beyond GLUE, a variety of language understanding tasks do require word order information, often to an extent that cannot be learned through fine-tuning.\""
    },
    {
        "title": "Prototypical Verbalizer for Prompt-based Few-shot Tuning",
        "authors": [
            "Ganqu Cui",
            "Shengding Hu",
            "Ning Ding",
            "Longtao Huang",
            "Zhiyuan Liu"
        ],
        "published": "2022",
        "summary": "Prompt-based tuning for pre-trained language models (PLMs) has shown its effectiveness in few-shot learning. Typically, prompt-based tuning wraps the input text into a cloze question. To make predictions, the model maps the output words to labels via a verbalizer, which is either manually designed or automatically built. However, manual verbalizers heavily depend on domain-specific prior knowledge and human efforts, while finding appropriate label words automatically still remains challenging. In this work, we propose the prototypical verbalizer (ProtoVerb) which is built directly from training data. Specifically, ProtoVerb learns prototype vectors as verbalizers by contrastive learning. In this way, the prototypes summarize training instances and are able to enclose rich class-level semantics. We conduct experiments on both topic classification and entity typing tasks, and the results demonstrate that ProtoVerb significantly outperforms current automatic verbalizers, especially when training data is extremely scarce. More surprisingly, ProtoVerb consistently boosts prompt-based tuning even on untuned PLMs, indicating an elegant non-tuning way to utilize PLMs. Our codes are avaliable at https://github.com/thunlp/OpenPrompt.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.483.pdf",
        "keywords": [
            "verbalizer",
            "entity typing",
            "tuning",
            "protoverb",
            "prompt based tuning",
            "non tuning",
            "topic classification",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, manual verbalizers heavily depend on domain-specific prior knowledge and human efforts, while finding appropriate label words automatically still remains challenging.\"\n\nThis paper mentions a limitation of LLMs in passing, specifically the challenge of finding appropriate label words automatically and the dependence on domain-specific prior knowledge when using manual verbalizers.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, manual verbalizers heavily depend on domain-specific prior knowledge and human efforts, while finding appropriate label words automatically still remains challenging.\"\n\nThis paper mentions a limitation of LLMs in passing, specifically the challenge of finding appropriate label words automatically and the dependence on domain-specific prior knowledge when using manual verbalizers."
    },
    {
        "title": "Signal in Noise: Exploring Meaning Encoded in Random Character Sequences with Character-Aware Language Models",
        "authors": [
            "Mark Chu",
            "Bhargav Srinivasa Desikan",
            "Ethan Nadler",
            "Donald Ruggiero Lo Sardo",
            "Elise Darragh-Ford",
            "Douglas Guilbeault"
        ],
        "published": "2022",
        "summary": "Natural language processing models learn word representations based on the distributional hypothesis, which asserts that word context (e.g., co-occurrence) correlates with meaning. We propose that n-grams composed of random character sequences, or garble, provide a novel context for studying word meaning both within and beyond extant language. In particular, randomly generated character n-grams lack meaning but contain primitive information based on the distribution of characters they contain. By studying the embeddings of a large corpus of garble, extant language, and pseudowords using CharacterBERT, we identify an axis in the model’s high-dimensional embedding space that separates these classes of n-grams. Furthermore, we show that this axis relates to structure within extant language, including word part-of-speech, morphology, and concept concreteness. Thus, in contrast to studies that are mainly limited to extant language, our work reveals that meaning and primitive information are intrinsically linked.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.492.pdf",
        "keywords": [
            "random character sequences",
            "language models",
            "word meaning",
            "meaning encoded",
            "natural language processing models",
            "word context"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "CAMERO: Consistency Regularized Ensemble of Perturbed Language Models with Weight Sharing",
        "authors": [
            "Chen Liang",
            "Pengcheng He",
            "Yelong Shen",
            "Weizhu Chen",
            "Tuo Zhao"
        ],
        "published": "2022",
        "summary": "Model ensemble is a popular approach to produce a low-variance and well-generalized model. However, it induces large memory and inference costs, which is often not affordable for real-world deployment. Existing work has resorted to sharing weights among models. However, when increasing the proportion of the shared weights, the resulting models tend to be similar, and the benefits of using model ensemble diminish. To retain ensemble benefits while maintaining a low memory cost, we propose a consistency-regularized ensemble learning approach based on perturbed models, named CAMERO. Specifically, we share the weights of bottom layers across all models and apply different perturbations to the hidden representations for different models, which can effectively promote the model diversity. Meanwhile, we apply a prediction consistency regularizer across the perturbed models to control the variance due to the model diversity. Our experiments using large language models demonstrate that CAMERO significantly improves the generalization performance of the ensemble model. Specifically, CAMERO outperforms the standard ensemble of 8 BERT-base models on the GLUE benchmark by 0.7 with a significantly smaller model size (114.2M vs. 880.6M).",
        "pdf_link": "https://aclanthology.org/2022.acl-long.495.pdf",
        "keywords": [
            "consistency",
            "consistency regularized ensemble",
            "ensemble",
            "language models",
            "model ensemble",
            "perturbed language models",
            "weight sharing",
            "consistency regularized ensemble learning",
            "bert base models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, when increasing the proportion of the shared weights, the resulting models tend to be similar, and the benefits of using model ensemble diminish.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, when increasing the proportion of the shared weights, the resulting models tend to be similar, and the benefits of using model ensemble diminish.\""
    },
    {
        "title": "UniXcoder: Unified Cross-Modal Pre-training for Code Representation",
        "authors": [
            "Daya Guo",
            "Shuai Lu",
            "Nan Duan",
            "Yanlin Wang",
            "Ming Zhou",
            "Jian Yin"
        ],
        "published": "2022",
        "summary": "Pre-trained models for programming languages have recently demonstrated great success on code intelligence. To support both code-related understanding and generation tasks, recent works attempt to pre-train unified encoder-decoder models. However, such encoder-decoder framework is sub-optimal for auto-regressive tasks, especially code completion that requires a decoder-only manner for efficient inference. In this paper, we present UniXcoder, a unified cross-modal pre-trained model for programming language. The model utilizes mask attention matrices with prefix adapters to control the behavior of the model and leverages cross-modal contents like AST and code comment to enhance code representation. To encode AST that is represented as a tree in parallel, we propose a one-to-one mapping method to transform AST in a sequence structure that retains all structural information from the tree. Furthermore, we propose to utilize multi-modal contents to learn representation of code fragment with contrastive learning, and then align representations among programming languages using a cross-modal generation task. We evaluate UniXcoder on five code-related tasks over nine datasets. To further evaluate the performance of code fragment representation, we also construct a dataset for a new task, called zero-shot code-to-code search. Results show that our model achieves state-of-the-art performance on most tasks and analysis reveals that comment and AST can both enhance UniXcoder.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.499.pdf",
        "keywords": [
            "fragment",
            "code fragment representation",
            "code representation",
            "code intelligence",
            "programming languages",
            "cross modal generation",
            "transform ast"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, such encoder-decoder framework is sub-optimal for auto-regressive tasks, especially code completion that requires a decoder-only manner for efficient inference.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, such encoder-decoder framework is sub-optimal for auto-regressive tasks, especially code completion that requires a decoder-only manner for efficient inference.\""
    },
    {
        "title": "Is GPT-3 Text Indistinguishable from Human Text? Scarecrow: A Framework for Scrutinizing Machine Text",
        "authors": [
            "Yao Dou",
            "Maxwell Forbes",
            "Rik Koncel-Kedziorski",
            "Noah A. Smith",
            "Yejin Choi"
        ],
        "published": "2022",
        "summary": "Modern neural language models can produce remarkably fluent and grammatical text. So much, in fact, that recent work by Clark et al. (2021) has reported that conventional crowdsourcing can no longer reliably distinguish between machine-authored (GPT-3) and human-authored writing. As errors in machine generations become ever subtler and harder to spot, it poses a new challenge to the research community for robust machine text evaluation. We propose a new framework called Scarecrow for scrutinizing machine text via crowd annotation. To support the broad range of real machine errors that can be identified by laypeople, the ten error categories of Scarecrow—such as redundancy, commonsense errors, and incoherence—are identified through several rounds of crowd annotation experiments without a predefined ontology. We then use Scarecrow to collect over 41k error spans in human-written and machine-generated paragraphs of English language news text. We isolate factors for detailed analysis, including parameter count, training data, and various decoding-time configurations. Our approach successfully quantifies measurable gaps between human authored text and generations from models of several sizes, including fourteen configurations of GPT-3. In addition, our analysis unveils new insights, with detailed rationales provided by laypeople, e.g., that the commonsense capabilities have been improving with larger models while math capabilities have not, and that the choices of simple decoding hyperparameters can make remarkable differences on the perceived quality of machine text. We release our training material, annotation toolkit and dataset at https://yao-dou.github.io/scarecrow/.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.501.pdf",
        "keywords": [
            "crowd",
            "neural language models",
            "machine text",
            "grammatical text",
            "commonsense",
            "crowdsourcing",
            "ontology"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"As errors in machine generations become ever subtler and harder to spot, it poses a new challenge to the research community for robust machine text evaluation.\"; \"our analysis unveils new insights, with detailed rationales provided by laypeople, e.g., that the commonsense capabilities have been improving with larger models while math capabilities have not.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"As errors in machine generations become ever subtler and harder to spot, it poses a new challenge to the research community for robust machine text evaluation.\"; \"our analysis unveils new insights, with detailed rationales provided by laypeople, e.g., that the commonsense capabilities have been improving with larger models while math capabilities have not.\""
    },
    {
        "title": "Pretraining with Artificial Language: Studying Transferable Knowledge in Language Models",
        "authors": [
            "Ryokan Ri",
            "Yoshimasa Tsuruoka"
        ],
        "published": "2022",
        "summary": "We investigate what kind of structural knowledge learned in neural network encoders is transferable to processing natural language. We design artificial languages with structural properties that mimic natural language, pretrain encoders on the data, and see how much performance the encoder exhibits on downstream tasks in natural language.Our experimental results show that pretraining with an artificial language with a nesting dependency structure provides some knowledge transferable to natural language.A follow-up probing analysis indicates that its success in the transfer is related to the amount of encoded contextual information and what is transferred is the knowledge of position-aware context dependence of language.Our results provide insights into how neural network encoders process human languages and the source of cross-lingual transferability of recent multilingual language models.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.504.pdf",
        "keywords": [
            "pretraining",
            "artificial language",
            "transferable",
            "transferable knowledge",
            "encoders",
            "language models",
            "knowledge",
            "multilingual language models",
            "neural network encoders",
            "human languages"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicitly mentioned, but the focus on understanding the source of cross-lingual transferability of recent multilingual language models implies potential limitations in their ability to generalize across languages.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None explicitly mentioned, but the focus on understanding the source of cross-lingual transferability of recent multilingual language models implies potential limitations in their ability to generalize across languages."
    },
    {
        "title": "mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models",
        "authors": [
            "Ryokan Ri",
            "Ikuya Yamada",
            "Yoshimasa Tsuruoka"
        ],
        "published": "2022",
        "summary": "Recent studies have shown that multilingual pretrained language models can be effectively improved with cross-lingual alignment information from Wikipedia entities. However, existing methods only exploit entity information in pretraining and do not explicitly use entities in downstream tasks. In this study, we explore the effectiveness of leveraging entity representations for downstream cross-lingual tasks. We train a multilingual language model with 24 languages with entity representations and showthe model consistently outperforms word-based pretrained models in various cross-lingual transfer tasks. We also analyze the model and the key insight is that incorporating entity representations into the input allows us to extract more language-agnostic features. We also evaluate the model with a multilingual cloze prompt task with the mLAMA dataset. We show that entity-based prompt elicits correct factual knowledge more likely than using only word representations.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.505.pdf",
        "keywords": [
            "entity representations",
            "language models",
            "multilingual pretrained language models",
            "multilingual language model"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "Sharpness-Aware Minimization Improves Language Model Generalization",
        "authors": [
            "Dara Bahri",
            "Hossein Mobahi",
            "Yi Tay"
        ],
        "published": "2022",
        "summary": "The allure of superhuman-level capabilities has led to considerable interest in language models like GPT-3 and T5, wherein the research has, by and large, revolved around new model architectures, training tasks, and loss objectives, along with substantial engineering efforts to scale up model capacity and dataset size. Comparatively little work has been done to improve the generalization of these models through better optimization. In this work, we show that Sharpness-Aware Minimization (SAM), a recently proposed optimization procedure that encourages convergence to flatter minima, can substantially improve the generalization of language models without much computational overhead. We show that SAM is able to boost performance on SuperGLUE, GLUE, Web Questions, Natural Questions, Trivia QA, and TyDiQA, with particularly large gains when training data for these tasks is limited.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.508.pdf",
        "keywords": [
            "sharpness",
            "sharpness aware minimization"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Comparatively little work has been done to improve the generalization of these models through better optimization.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (poor generalization) in passing, but does not elaborate on it and focuses on the proposed solution (Sharpness-Aware Minimization) to address this limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Comparatively little work has been done to improve the generalization of these models through better optimization.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (poor generalization) in passing, but does not elaborate on it and focuses on the proposed solution (Sharpness-Aware Minimization) to address this limitation."
    },
    {
        "title": "Neural reality of argument structure constructions",
        "authors": [
            "Bai Li",
            "Zining Zhu",
            "Guillaume Thomas",
            "Frank Rudzicz",
            "Yang Xu"
        ],
        "published": "2022",
        "summary": "In lexicalist linguistic theories, argument structure is assumed to be predictable from the meaning of verbs. As a result, the verb is the primary determinant of the meaning of a clause. In contrast, construction grammarians propose that argument structure is encoded in constructions (or form-meaning pairs) that are distinct from verbs. Two decades of psycholinguistic research have produced substantial empirical evidence in favor of the construction view. Here we adapt several psycholinguistic studies to probe for the existence of argument structure constructions (ASCs) in Transformer-based language models (LMs). First, using a sentence sorting experiment, we find that sentences sharing the same construction are closer in embedding space than sentences sharing the same verb. Furthermore, LMs increasingly prefer grouping by construction with more input data, mirroring the behavior of non-native language learners. Second, in a “Jabberwocky” priming-based experiment, we find that LMs associate ASCs with meaning, even in semantically nonsensical sentences. Our work offers the first evidence for ASCs in LMs and highlights the potential to devise novel probing methods grounded in psycholinguistic research.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.512.pdf",
        "keywords": [
            "argument structure",
            "verbs",
            "argument structure constructions",
            "constructions",
            "sentence sorting experiment",
            "meaning pairs"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs, but mentions LMs in the context of psycholinguistic research and probing methods.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No evidence of discussion of limitations of LLMs, but mentions LMs in the context of psycholinguistic research and probing methods."
    },
    {
        "title": "ABC: Attention with Bounded-memory Control",
        "authors": [
            "Hao Peng",
            "Jungo Kasai",
            "Nikolaos Pappas",
            "Dani Yogatama",
            "Zhaofeng Wu",
            "Lingpeng Kong",
            "Roy Schwartz",
            "Noah A. Smith"
        ],
        "published": "2022",
        "summary": "Transformer architectures have achieved state- of-the-art results on a variety of natural language processing (NLP) tasks. However, their attention mechanism comes with a quadratic complexity in sequence lengths, making the computational overhead prohibitive, especially for long sequences. Attention context can be seen as a random-access memory with each token taking a slot. Under this perspective, the memory size grows linearly with the sequence length, and so does the overhead of reading from it. One way to improve the efficiency is to bound the memory size. We show that disparate approaches can be subsumed into one abstraction, attention with bounded-memory control (ABC), and they vary in their organization of the memory. ABC reveals new, unexplored possibilities. First, it connects several efficient attention variants that would otherwise seem apart. Second, this abstraction gives new insights—an established approach (Wang et al., 2020b) previously thought to not be applicable in causal attention, actually is. Last, we present a new instance of ABC, which draws inspiration from existing ABC approaches, but replaces their heuristic memory-organizing functions with a learned, contextualized one. Our experiments on language modeling, machine translation, and masked language model finetuning show that our approach outperforms previous efficient attention models; compared to the strong transformer baselines, it significantly improves the inference time and space efficiency with no or negligible accuracy loss.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.515.pdf",
        "keywords": [
            "bounded memory control",
            "natural language processing",
            "attention",
            "language modeling",
            "attention models",
            "inference time",
            "attention context",
            "random access"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, their attention mechanism comes with a quadratic complexity in sequence lengths, making the computational overhead prohibitive, especially for long sequences.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, their attention mechanism comes with a quadratic complexity in sequence lengths, making the computational overhead prohibitive, especially for long sequences.\""
    },
    {
        "title": "Analyzing Generalization of Vision and Language Navigation to Unseen Outdoor Areas",
        "authors": [
            "Raphael Schumann",
            "Stefan Riezler"
        ],
        "published": "2022",
        "summary": "Vision and language navigation (VLN) is a challenging visually-grounded language understanding task. Given a natural language navigation instruction, a visual agent interacts with a graph-based environment equipped with panorama images and tries to follow the described route. Most prior work has been conducted in indoor scenarios where best results were obtained for navigation on routes that are similar to the training routes, with sharp drops in performance when testing on unseen environments. We focus on VLN in outdoor scenarios and find that in contrast to indoor VLN, most of the gain in outdoor VLN on unseen data is due to features like junction type embedding or heading delta that are specific to the respective environment graph, while image information plays a very minor role in generalizing VLN to unseen outdoor areas. These findings show a bias to specifics of graph representations of urban environments, demanding that VLN tasks grow in scale and diversity of geographical environments.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.518.pdf",
        "keywords": [
            "navigation",
            "language navigation",
            "agent"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Memorisation versus Generalisation in Pre-trained Language Models",
        "authors": [
            "Michael Tänzer",
            "Sebastian Ruder",
            "Marek Rei"
        ],
        "published": "2022",
        "summary": "State-of-the-art pre-trained language models have been shown to memorise facts and perform well with limited amounts of training data. To gain a better understanding of how these models learn, we study their generalisation and memorisation capabilities in noisy and low-resource scenarios. We find that the training of these models is almost unaffected by label noise and that it is possible to reach near-optimal results even on extremely noisy datasets. However, our experiments also show that they mainly learn from high-frequency patterns and largely fail when tested on low-resource tasks such as few-shot learning and rare entity recognition. To mitigate such limitations, we propose an extension based on prototypical networks that improves performance in low-resource named entity recognition tasks.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.521.pdf",
        "keywords": [
            "generalisation",
            "language models",
            "memorisation",
            "entity recognition"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, our experiments also show that they mainly learn from high-frequency patterns and largely fail when tested on low-resource tasks such as few-shot learning and rare entity recognition.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, our experiments also show that they mainly learn from high-frequency patterns and largely fail when tested on low-resource tasks such as few-shot learning and rare entity recognition.\""
    },
    {
        "title": "ChatMatch: Evaluating Chatbots by Autonomous Chat Tournaments",
        "authors": [
            "Ruolan Yang",
            "Zitong Li",
            "Haifeng Tang",
            "Kenny Zhu"
        ],
        "published": "2022",
        "summary": "Existing automatic evaluation systems of chatbots mostly rely on static chat scripts as ground truth, which is hard to obtain, and requires access to the models of the bots as a form of “white-box testing”. Interactive evaluation mitigates this problem but requires human involvement. In our work, we propose an interactive chatbot evaluation framework in which chatbots compete with each other like in a sports tournament, using flexible scoring metrics. This framework can efficiently rank chatbots independently from their model architectures and the domains for which they are trained.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.522.pdf",
        "keywords": [
            "chatbots",
            "interactive chatbot",
            "chat tournaments"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Dependency-based Mixture Language Models",
        "authors": [
            "Zhixian Yang",
            "Xiaojun Wan"
        ],
        "published": "2022",
        "summary": "Various models have been proposed to incorporate knowledge of syntactic structures into neural language models. However, previous works have relied heavily on elaborate components for a specific language model, usually recurrent neural network (RNN), which makes themselves unwieldy in practice to fit into other neural language models, such as Transformer and GPT-2. In this paper, we introduce the Dependency-based Mixture Language Models. In detail, we first train neural language models with a novel dependency modeling objective to learn the probability distribution of future dependent tokens given context. We then formulate the next-token probability by mixing the previous dependency modeling probability distributions with self-attention. Extensive experiments and human evaluations show that our method can be easily and effectively applied to different neural language models while improving neural text generation on various tasks.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.535.pdf",
        "keywords": [
            "mixture language",
            "mixture language models",
            "neural language",
            "neural language models",
            "dependency modeling",
            "neural network",
            "token probability"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, previous works have relied heavily on elaborate components for a specific language model, usually recurrent neural network (RNN), which makes themselves unwieldy in practice to fit into other neural language models, such as Transformer and GPT-2.\"\n\nThis rating is given because the paper mentions a limitation of previous works in incorporating knowledge of syntactic structures into neural language models, but does",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, previous works have relied heavily on elaborate components for a specific language model, usually recurrent neural network (RNN), which makes themselves unwieldy in practice to fit into other neural language models, such as Transformer and GPT-2.\"\n\nThis rating is given because the paper mentions a limitation of previous works in incorporating knowledge of syntactic structures into neural language models, but does"
    },
    {
        "title": "Can Unsupervised Knowledge Transfer from Social Discussions Help Argument Mining?",
        "authors": [
            "Subhabrata Dutta",
            "Jeevesh Juneja",
            "Dipankar Das",
            "Tanmoy Chakraborty"
        ],
        "published": "2022",
        "summary": "Identifying argument components from unstructured texts and predicting the relationships expressed among them are two primary steps of argument mining. The intrinsic complexity of these tasks demands powerful learning models. While pretrained Transformer-based Language Models (LM) have been shown to provide state-of-the-art results over different NLP tasks, the scarcity of manually annotated data and the highly domain-dependent nature of argumentation restrict the capabilities of such models. In this work, we propose a novel transfer learning strategy to overcome these challenges. We utilize argumentation-rich social discussions from the ChangeMyView subreddit as a source of unsupervised, argumentative discourse-aware knowledge by finetuning pretrained LMs on a selectively masked language modeling task. Furthermore, we introduce a novel prompt-based strategy for inter-component relation prediction that compliments our proposed finetuning method while leveraging on the discourse context. Exhaustive experiments show the generalization capability of our method on these two tasks over within-domain as well as out-of-domain datasets, outperforming several existing and employed strong baselines.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.536.pdf",
        "keywords": [
            "transfer learning",
            "argument mining",
            "finetuning",
            "unsupervised knowledge transfer",
            "inter component relation prediction"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the scarcity of manually annotated data and the highly domain-dependent nature of argumentation restrict the capabilities of such models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the scarcity of manually annotated data and the highly domain-dependent nature of argumentation restrict the capabilities of such models.\""
    },
    {
        "title": "Entity-based Neural Local Coherence Modeling",
        "authors": [
            "Sungho Jeon",
            "Michael Strube"
        ],
        "published": "2022",
        "summary": "In this paper, we propose an entity-based neural local coherence model which is linguistically more sound than previously proposed neural coherence models. Recent neural coherence models encode the input document using large-scale pretrained language models. Hence their basis for computing local coherence are words and even sub-words. The analysis of their output shows that these models frequently compute coherence on the basis of connections between (sub-)words which, from a linguistic perspective, should not play a role. Still, these models achieve state-of-the-art performance in several end applications. In contrast to these models, we compute coherence on the basis of entities by constraining the input to noun phrases and proper names. This provides us with an explicit representation of the most important items in sentences leading to the notion of focus. This brings our model linguistically in line with pre-neural models of computing coherence. It also gives us better insight into the behaviour of the model thus leading to better explainability. Our approach is also in accord with a recent study (O’Connor and Andreas, 2021), which shows that most usable information is captured by nouns and verbs in transformer-based language models. We evaluate our model on three downstream tasks showing that it is not only linguistically more sound than previous models but also that it outperforms them in end applications.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.537.pdf",
        "keywords": [
            "linguistically",
            "verbs",
            "coherence",
            "local coherence",
            "language",
            "neural local coherence modeling",
            "neural coherence models",
            "compute coherence"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent neural coherence models encode the input document using large-scale pretrained language models... The analysis of their output shows that these models frequently compute coherence on the basis of connections between (sub-)words which, from a linguistic perspective, should not play a role.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Recent neural coherence models encode the input document using large-scale pretrained language models... The analysis of their output shows that these models frequently compute coherence on the basis of connections between (sub-)words which, from a linguistic perspective, should not play a role.\""
    },
    {
        "title": "Fair and Argumentative Language Modeling for Computational Argumentation",
        "authors": [
            "Carolin Holtermann",
            "Anne Lauscher",
            "Simone Ponzetto"
        ],
        "published": "2022",
        "summary": "Although much work in NLP has focused on measuring and mitigating stereotypical bias in semantic spaces, research addressing bias in computational argumentation is still in its infancy. In this paper, we address this research gap and conduct a thorough investigation of bias in argumentative language models. To this end, we introduce ABBA, a novel resource for bias measurement specifically tailored to argumentation. We employ our resource to assess the effect of argumentative fine-tuning and debiasing on the intrinsic bias found in transformer-based language models using a lightweight adapter-based approach that is more sustainable and parameter-efficient than full fine-tuning. Finally, we analyze the potential impact of language model debiasing on the performance in argument quality prediction, a downstream task of computational argumentation. Our results show that we are able to successfully and sustainably remove bias in general and argumentative language models while preserving (and sometimes improving) model performance in downstream tasks. We make all experimental code and data available at https://github.com/umanlp/FairArgumentativeLM.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.541.pdf",
        "keywords": [
            "argumentative language",
            "language models",
            "argumentative language modeling",
            "argumentation",
            "bias",
            "argument quality prediction",
            "computational argumentation",
            "intrinsic bias",
            "language model debiasing",
            "bias measurement"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although much work in NLP has focused on measuring and mitigating stereotypical bias in semantic spaces, research addressing bias in computational argumentation is still in its infancy.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although much work in NLP has focused on measuring and mitigating stereotypical bias in semantic spaces, research addressing bias in computational argumentation is still in its infancy.\""
    },
    {
        "title": "Can Pre-trained Language Models Interpret Similes as Smart as Human?",
        "authors": [
            "Qianyu He",
            "Sijie Cheng",
            "Zhixu Li",
            "Rui Xie",
            "Yanghua Xiao"
        ],
        "published": "2022",
        "summary": "Simile interpretation is a crucial task in natural language processing. Nowadays, pre-trained language models (PLMs) have achieved state-of-the-art performance on many tasks. However, it remains under-explored whether PLMs can interpret similes or not. In this paper, we investigate the ability of PLMs in simile interpretation by designing a novel task named Simile Property Probing, i.e., to let the PLMs infer the shared properties of similes. We construct our simile property probing datasets from both general textual corpora and human-designed questions, containing 1,633 examples covering seven main categories. Our empirical study based on the constructed datasets shows that PLMs can infer similes’ shared properties while still underperforming humans. To bridge the gap with human performance, we additionally design a knowledge-enhanced training objective by incorporating the simile knowledge into PLMs via knowledge embedding methods. Our method results in a gain of 8.58% in the probing task and 1.37% in the downstream task of sentiment classification. The datasets and code are publicly available at https://github.com/Abbey4799/PLMs-Interpret-Simile.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.543.pdf",
        "keywords": [
            "knowledge embedding",
            "sentiment classification",
            "simile property probing",
            "simile interpretation",
            "property probing",
            "natural language processing",
            "interpretation",
            "pre trained language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our empirical study based on the constructed datasets shows that PLMs can infer similes’ shared properties while still underperforming humans.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Our empirical study based on the constructed datasets shows that PLMs can infer similes’ shared properties while still underperforming humans.\""
    },
    {
        "title": "LinkBERT: Pretraining Language Models with Document Links",
        "authors": [
            "Michihiro Yasunaga",
            "Jure Leskovec",
            "Percy Liang"
        ],
        "published": "2022",
        "summary": "Language model (LM) pretraining captures various knowledge from text corpora, helping downstream tasks. However, existing methods such as BERT model a single document, and do not capture dependencies or knowledge that span across documents. In this work, we propose LinkBERT, an LM pretraining method that leverages links between documents, e.g., hyperlinks. Given a text corpus, we view it as a graph of documents and create LM inputs by placing linked documents in the same context. We then pretrain the LM with two joint self-supervised objectives: masked language modeling and our new proposal, document relation prediction. We show that LinkBERT outperforms BERT on various downstream tasks across two domains: the general domain (pretrained on Wikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with citation links). LinkBERT is especially effective for multi-hop reasoning and few-shot QA (+5% absolute improvement on HotpotQA and TriviaQA), and our biomedical LinkBERT sets new states of the art on various BioNLP tasks (+7% on BioASQ and USMLE). We release our pretrained models, LinkBERT and BioLinkBERT, as well as code and data.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.551.pdf",
        "keywords": [
            "document relation prediction",
            "wikipedia",
            "pretraining language models",
            "pretraining method",
            "biolinkbert"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing methods such as BERT model a single document, and do not capture dependencies or knowledge that span across documents.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, existing methods such as BERT model a single document, and do not capture dependencies or knowledge that span across documents.\""
    },
    {
        "title": "Softmax Bottleneck Makes Language Models Unable to Represent Multi-mode Word Distributions",
        "authors": [
            "Haw-Shiuan Chang",
            "Andrew McCallum"
        ],
        "published": "2022",
        "summary": "Neural language models (LMs) such as GPT-2 estimate the probability distribution over the next word by a softmax over the vocabulary. The softmax layer produces the distribution based on the dot products of a single hidden state and the embeddings of words in the vocabulary. However, we discover that this single hidden state cannot produce all probability distributions regardless of the LM size or training data size because the single hidden state embedding cannot be close to the embeddings of all the possible next words simultaneously when there are other interfering word embeddings between them. In this work, we demonstrate the importance of this limitation both theoretically and practically. Our work not only deepens our understanding of softmax bottleneck and mixture of softmax (MoS) but also inspires us to propose multi-facet softmax (MFS) to address the limitations of MoS. Extensive empirical analyses confirm our findings and show that against MoS, the proposed MFS achieves two-fold improvements in the perplexity of GPT-2 and BERT.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.554.pdf",
        "keywords": [
            "dot products",
            "softmax bottleneck",
            "neural language models",
            "softmax layer",
            "multi facet softmax"
        ],
        "Rate_Llama-3.1-70b": "5",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, we discover that this single hidden state cannot produce all probability distributions regardless of the LM size or training data size because the single hidden state embedding cannot be close to the embeddings of all the possible next words simultaneously when there are other interfering word embeddings between them.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 5\nEvidence: \"However, we discover that this single hidden state cannot produce all probability distributions regardless of the LM size or training data size because the single hidden state embedding cannot be close to the embeddings of all the possible next words simultaneously when there are other interfering word embeddings between them.\""
    },
    {
        "title": "Ditch the Gold Standard: Re-evaluating Conversational Question Answering",
        "authors": [
            "Huihan Li",
            "Tianyu Gao",
            "Manan Goenka",
            "Danqi Chen"
        ],
        "published": "2022",
        "summary": "Conversational question answering aims to provide natural-language answers to users in information-seeking conversations. Existing conversational QA benchmarks compare models with pre-collected human-human conversations, using ground-truth answers provided in conversational history. It remains unclear whether we can rely on this static evaluation for model development and whether current systems can well generalize to real-world human-machine conversations. In this work, we conduct the first large-scale human evaluation of state-of-the-art conversational QA systems, where human evaluators converse with models and judge the correctness of their answers. We find that the distribution of human machine conversations differs drastically from that of human-human conversations, and there is a disagreement between human and gold-history evaluation in terms of model ranking. We further investigate how to improve automatic evaluations, and propose a question rewriting mechanism based on predicted history, which better correlates with human judgments. Finally, we analyze the impact of various modeling strategies and discuss future directions towards building better conversational question answering systems.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.555.pdf",
        "keywords": [
            "conversational question answering",
            "conversational",
            "ditch the gold"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"It remains unclear whether we can rely on this static evaluation for model development and whether current systems can well generalize to real-world human-machine conversations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"It remains unclear whether we can rely on this static evaluation for model development and whether current systems can well generalize to real-world human-machine conversations.\""
    },
    {
        "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
        "authors": [
            "Yao Lu",
            "Max Bartolo",
            "Alastair Moore",
            "Sebastian Riedel",
            "Pontus Stenetorp"
        ],
        "published": "2022",
        "summary": "When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are “fantastic” and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13% relative improvement for GPT-family models across eleven different established text classification tasks.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.556.pdf",
        "keywords": [
            "prompts",
            "language models",
            "shot prompt order sensitivity",
            "artificial development",
            "entropy"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data.\"\n\nThis rating is based on the fact that the paper mentions a limitation of LLMs in the context of few-shot learning, specifically the need for additional annotated data to determine good permutations, but this limitation is not the primary",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data.\"\n\nThis rating is based on the fact that the paper mentions a limitation of LLMs in the context of few-shot learning, specifically the need for additional annotated data to determine good permutations, but this limitation is not the primary"
    },
    {
        "title": "Situated Dialogue Learning through Procedural Environment Generation",
        "authors": [
            "Prithviraj Ammanabrolu",
            "Renee Jia",
            "Mark Riedl"
        ],
        "published": "2022",
        "summary": "We teach goal-driven agents to interactively act and speak in situated environments by training on generated curriculums. Our agents operate in LIGHT (Urbanek et al. 2019)—a large-scale crowd-sourced fantasy text adventure game wherein an agent perceives and interacts with the world through textual natural language. Goals in this environment take the form of character-based quests, consisting of personas and motivations. We augment LIGHT by learning to procedurally generate additional novel textual worlds and quests to create a curriculum of steadily increasing difficulty for training agents to achieve such goals. In particular, we measure curriculum difficulty in terms of the rarity of the quest in the original training distribution—an easier environment is one that is more likely to have been found in the unaugmented dataset. An ablation study shows that this method of learning from the tail of a distribution results in significantly higher generalization abilities as measured by zero-shot performance on never-before-seen quests.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.557.pdf",
        "keywords": [
            "situated dialogue learning",
            "curriculum"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs or their limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs or their limitations."
    },
    {
        "title": "Coherence boosting: When your pretrained language model is not paying enough attention",
        "authors": [
            "Nikolay Malkin",
            "Zhen Wang",
            "Nebojsa Jojic"
        ],
        "published": "2022",
        "summary": "Long-range semantic coherence remains a challenge in automatic language generation and understanding. We demonstrate that large language models have insufficiently learned the effect of distant words on next-token prediction. We present coherence boosting, an inference procedure that increases a LM’s focus on a long context. We show the benefits of coherence boosting with pretrained models by distributional analyses of generated ordinary text and dialog responses. It is also found that coherence boosting with state-of-the-art models for various zero-shot NLP tasks yields performance gains with no additional training.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.565.pdf",
        "keywords": [
            "coherence boosting",
            "language model",
            "semantic coherence",
            "token prediction",
            "attention"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Long-range semantic coherence remains a challenge in automatic language generation and understanding. We demonstrate that large language models have insufficiently learned the effect of distant words on next-token prediction.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Long-range semantic coherence remains a challenge in automatic language generation and understanding. We demonstrate that large language models have insufficiently learned the effect of distant words on next-token prediction.\""
    },
    {
        "title": "VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena",
        "authors": [
            "Letitia Parcalabescu",
            "Michele Cafagna",
            "Lilitta Muradjan",
            "Anette Frank",
            "Iacer Calixto",
            "Albert Gatt"
        ],
        "published": "2022",
        "summary": "We propose VALSE (Vision And Language Structured Evaluation), a novel benchmark designed for testing general-purpose pretrained vision and language (V&L) models for their visio-linguistic grounding capabilities on specific linguistic phenomena. VALSE offers a suite of six tests covering various linguistic constructs. Solving these requires models to ground linguistic phenomena in the visual modality, allowing more fine-grained evaluations than hitherto possible. We build VALSE using methods that support the construction of valid foils, and report results from evaluating five widely-used V&L models. Our experiments suggest that current models have considerable difficulty addressing most phenomena. Hence, we expect VALSE to serve as an important benchmark to measure future progress of pretrained V&L models from a linguistic perspective, complementing the canonical task-centred V&L evaluations.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.567.pdf",
        "keywords": [
            "language models",
            "benchmark"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our experiments suggest that current models have considerable difficulty addressing most phenomena.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Our experiments suggest that current models have considerable difficulty addressing most phenomena.\""
    },
    {
        "title": "The Grammar-Learning Trajectories of Neural Language Models",
        "authors": [
            "Leshem Choshen",
            "Guy Hacohen",
            "Daphna Weinshall",
            "Omri Abend"
        ],
        "published": "2022",
        "summary": "The learning trajectories of linguistic phenomena in humans provide insight into linguistic representation, beyond what can be gleaned from inspecting the behavior of an adult speaker. To apply a similar approach to analyze neural language models (NLM), it is first necessary to establish that different models are similar enough in the generalizations they make. In this paper, we show that NLMs with different initialization, architecture, and training data acquire linguistic phenomena in a similar order, despite their different end performance. These findings suggest that there is some mutual inductive bias that underlies these models’ learning of linguistic phenomena. Taking inspiration from psycholinguistics, we argue that studying this inductive bias is an opportunity to study the linguistic representation implicit in NLMs.Leveraging these findings, we compare the relative performance on different phenomena at varying learning stages with simpler reference models. Results suggest that NLMs exhibit consistent “developmental” stages. Moreover, we find the learning trajectory to be approximately one-dimensional: given an NLM with a certain overall performance, it is possible to predict what linguistic generalizations it has already acquired. Initial analysis of these stages presents phenomena clusters (notably morphological ones), whose performance progresses in unison, suggesting a potential link between the generalizations behind them.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.568.pdf",
        "keywords": [
            "neural language models",
            "linguistic generalizations",
            "data acquire",
            "inductive bias",
            "psycholinguistics",
            "grammar learning"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicitly mentioned, but the paper implies potential limitations in the learning process of NLMs by stating that the models \"exhibit consistent “developmental” stages\" and that their learning trajectory is \"approximately one-dimensional\".",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None explicitly mentioned, but the paper implies potential limitations in the learning process of NLMs by stating that the models \"exhibit consistent “developmental” stages\" and that their learning trajectory is \"approximately one-dimensional\"."
    },
    {
        "title": "PPT: Pre-trained Prompt Tuning for Few-shot Learning",
        "authors": [
            "Yuxian Gu",
            "Xu Han",
            "Zhiyuan Liu",
            "Minlie Huang"
        ],
        "published": "2022",
        "summary": "Prompts for pre-trained language models (PLMs) have shown remarkable performance by bridging the gap between pre-training tasks and various downstream tasks. Among these methods, prompt tuning, which freezes PLMs and only tunes soft prompts, provides an efficient and effective solution for adapting large-scale PLMs to downstream tasks. However, prompt tuning is yet to be fully explored. In our pilot experiments, we find that prompt tuning performs comparably with conventional full-model tuning when downstream data are sufficient, whereas it is much worse under few-shot learning settings, which may hinder the application of prompt tuning. We attribute this low performance to the manner of initializing soft prompts. Therefore, in this work, we propose to pre-train prompts by adding soft prompts into the pre-training stage to obtain a better initialization. We name this Pre-trained Prompt Tuning framework “PPT”. To ensure the generalization of PPT, we formulate similar classification tasks into a unified task form and pre-train soft prompts for this unified task. Extensive experiments show that tuning pre-trained prompts for downstream tasks can reach or even outperform full-model fine-tuning under both full-data and few-shot settings. Our approach is effective and efficient for using large-scale PLMs in practice.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.576.pdf",
        "keywords": [
            "tuning",
            "pre trained language models",
            "soft prompts",
            "fine tuning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, prompt tuning is yet to be fully explored. In our pilot experiments, we find that prompt tuning performs comparably with conventional full-model tuning when downstream data are sufficient, whereas it is much worse under few-shot learning settings, which may hinder the application of prompt tuning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, prompt tuning is yet to be fully explored. In our pilot experiments, we find that prompt tuning performs comparably with conventional full-model tuning when downstream data are sufficient, whereas it is much worse under few-shot learning settings, which may hinder the application of prompt tuning.\""
    },
    {
        "title": "Deduplicating Training Data Makes Language Models Better",
        "authors": [
            "Katherine Lee",
            "Daphne Ippolito",
            "Andrew Nystrom",
            "Chiyuan Zhang",
            "Douglas Eck",
            "Chris Callison-Burch",
            "Nicholas Carlini"
        ],
        "published": "2022",
        "summary": "We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings. As a result, over 1% of the unprompted output of language models trained on these datasets is copied verbatim from the training data. We develop two tools that allow us to deduplicate training datasets—for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times. Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer training steps to achieve the same or better accuracy. We can also reduce train-test overlap, which affects over 4% of the validation set of standard datasets, thus allowing for more accurate evaluation. Code for deduplication is released at https://github.com/google-research/deduplicate-text-datasets.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.577.pdf",
        "keywords": [
            "language models",
            "deduplication",
            "text",
            "validation",
            "code",
            "test"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"As a result, over 1% of the unprompted output of language models trained on these datasets is copied verbatim from the training data.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"As a result, over 1% of the unprompted output of language models trained on these datasets is copied verbatim from the training data.\""
    },
    {
        "title": "Internet-Augmented Dialogue Generation",
        "authors": [
            "Mojtaba Komeili",
            "Kurt Shuster",
            "Jason Weston"
        ],
        "published": "2022",
        "summary": "The largest store of continually updating knowledge on our planet can be accessed via internet search. In this work we study giving access to this information to conversational agents. Large language models, even though they store an impressive amount of knowledge within their weights, are known to hallucinate facts when generating dialogue (Shuster et al., 2021); moreover, those facts are frozen in time at the point of model training. In contrast, we propose an approach that learns to generate an internet search query based on the context, and then conditions on the search results to finally generate a response, a method that can employ up-to-the-minute relevant information. We train and evaluate such models on a newly collected dataset of human-human conversations whereby one of the speakers is given access to internet search during knowledgedriven discussions in order to ground their responses. We find that search-query based access of the internet in conversation provides superior performance compared to existing approaches that either use no augmentation or FAISS-based retrieval (Lewis et al., 2020b).",
        "pdf_link": "https://aclanthology.org/2022.acl-long.579.pdf",
        "keywords": [
            "internet augmented dialogue generation",
            "language models",
            "internet",
            "knowledge",
            "conversational agents",
            "internet search"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models... are known to hallucinate facts when generating dialogue...; moreover, those facts are frozen in time at the point of model training.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Large language models... are known to hallucinate facts when generating dialogue...; moreover, those facts are frozen in time at the point of model training.\""
    },
    {
        "title": "Knowledge Neurons in Pretrained Transformers",
        "authors": [
            "Damai Dai",
            "Li Dong",
            "Yaru Hao",
            "Zhifang Sui",
            "Baobao Chang",
            "Furu Wei"
        ],
        "published": "2022",
        "summary": "Large-scale pretrained language models are surprisingly good at recalling factual knowledge presented in the training corpus. In this paper, we present preliminary studies on how factual knowledge is stored in pretrained Transformers by introducing the concept of knowledge neurons. Specifically, we examine the fill-in-the-blank cloze task for BERT. Given a relational fact, we propose a knowledge attribution method to identify the neurons that express the fact. We find that the activation of such knowledge neurons is positively correlated to the expression of their corresponding facts. In our case studies, we attempt to leverage knowledge neurons to edit (such as update, and erase) specific factual knowledge without fine-tuning. Our results shed light on understanding the storage of knowledge within pretrained Transformers.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.581.pdf",
        "keywords": [
            "transformers",
            "neurons",
            "knowledge neurons",
            "factual knowledge"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper focuses on understanding how knowledge is stored in LLMs, which might be related to limitations in knowledge representation or updating.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper focuses on understanding how knowledge is stored in LLMs, which might be related to limitations in knowledge representation or updating."
    },
    {
        "title": "French CrowS-Pairs: Extending a challenge dataset for measuring social bias in masked language models to a language other than English",
        "authors": [
            "Aurélie Névéol",
            "Yoann Dupont",
            "Julien Bezançon",
            "Karën Fort"
        ],
        "published": "2022",
        "summary": "Warning: This paper contains explicit statements of offensive stereotypes which may be upsetting. Much work on biases in natural language processing has addressed biases linked to the social and cultural experience of English speaking individuals in the United States. We seek to widen the scope of bias studies by creating material to measure social bias in language models (LMs) against specific demographic groups in France. We build on the US-centered CrowS-pairs dataset to create a multilingual stereotypes dataset that allows for comparability across languages while also characterizing biases that are specific to each country and language. We introduce 1,679 sentence pairs in French that cover stereotypes in ten types of bias like gender and age. 1,467 sentence pairs are translated from CrowS-pairs and 212 are newly crowdsourced. The sentence pairs contrast stereotypes concerning underadvantaged groups with the same sentence concerning advantaged groups. We find that four widely used language models (three French, one multilingual) favor sentences that express stereotypes in most bias categories. We report on the translation process from English into French, which led to a characterization of stereotypes in CrowS-pairs including the identification of US-centric cultural traits. We offer guidelines to further extend the dataset to other languages and cultural environments.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.583.pdf",
        "keywords": [
            "social bias",
            "bias",
            "language",
            "language models",
            "masked language models"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that four widely used language models (three French, one multilingual) favor sentences that express stereotypes in most bias categories.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We find that four widely used language models (three French, one multilingual) favor sentences that express stereotypes in most bias categories.\""
    },
    {
        "title": "Text-Free Prosody-Aware Generative Spoken Language Modeling",
        "authors": [
            "Eugene Kharitonov",
            "Ann Lee",
            "Adam Polyak",
            "Yossi Adi",
            "Jade Copet",
            "Kushal Lakhotia",
            "Tu Anh Nguyen",
            "Morgane Riviere",
            "Abdelrahman Mohamed",
            "Emmanuel Dupoux",
            "Wei-Ning Hsu"
        ],
        "published": "2022",
        "summary": "Speech pre-training has primarily demonstrated efficacy on classification tasks, while its capability of generating novel speech, similar to how GPT-2 can generate coherent paragraphs, has barely been explored. Generative Spoken Language Modeling (GSLM) (CITATION) is the only prior work addressing the generative aspect of speech pre-training, which builds a text-free language model using discovered units. Unfortunately, because the units used in GSLM discard most prosodic information, GSLM fails to leverage prosody for better comprehension and does not generate expressive speech. In this work, we present a prosody-aware generative spoken language model (pGSLM). It is composed of a multi-stream transformer language model (MS-TLM) of speech, represented as discovered unit and prosodic feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to waveforms. Experimental results show that the pGSLM can utilize prosody to improve both prosody and content modeling, and also generate natural, meaningful, and coherent speech given a spoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm. Codes and models are available at https://github.com/pytorch/fairseq/tree/main/examples/textless_nlp/pgslm.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.593.pdf",
        "keywords": [
            "spoken language modeling",
            "prosody",
            "text free prosody",
            "generative spoken language model"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Unfortunately, because the units used in GSLM discard most prosodic information, GSLM fails to leverage prosody for better comprehension and does not generate expressive speech.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Unfortunately, because the units used in GSLM discard most prosodic information, GSLM fails to leverage prosody for better comprehension and does not generate expressive speech.\""
    },
    {
        "title": "Bilingual alignment transfers to multilingual alignment for unsupervised parallel text mining",
        "authors": [
            "Chih-chan Tien",
            "Shane Steinert-Threlkeld"
        ],
        "published": "2022",
        "summary": "This work presents methods for learning cross-lingual sentence representations using paired or unpaired bilingual texts. We hypothesize that the cross-lingual alignment strategy is transferable, and therefore a model trained to align only two languages can encode multilingually more aligned representations. We thus introduce dual-pivot transfer: training on one language pair and evaluating on other pairs. To study this theory, we design unsupervised models trained on unpaired sentences and single-pair supervised models trained on bitexts, both based on the unsupervised language model XLM-R with its parameters frozen. The experiments evaluate the models as universal sentence encoders on the task of unsupervised bitext mining on two datasets, where the unsupervised model reaches the state of the art of unsupervised retrieval, and the alternative single-pair supervised model approaches the performance of multilingually supervised models. The results suggest that bilingual training techniques as proposed can be applied to get sentence representations with multilingual alignment.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.595.pdf",
        "keywords": [
            "bilingual training",
            "bilingual texts"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.\n\nNote: Although the paper mentions \"unsupervised language model XLM-R\", it does not discuss any limitations of Large Language Models.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs.\n\nNote: Although the paper mentions \"unsupervised language model XLM-R\", it does not discuss any limitations of Large Language Models."
    },
    {
        "title": "Pyramid-BERT: Reducing Complexity via Successive Core-set based Token Selection",
        "authors": [
            "Xin Huang",
            "Ashish Khetan",
            "Rene Bidart",
            "Zohar Karnin"
        ],
        "published": "2022",
        "summary": "Transformer-based language models such as BERT (CITATION) have achieved the state-of-the-art performance on various NLP tasks, but are computationally prohibitive. A recent line of works use various heuristics to successively shorten sequence length while transforming tokens through encoders, in tasks such as classification and ranking that require a single token embedding for prediction. We present a novel solution to this problem, called Pyramid-BERT where we replace previously used heuristics with a core-set based token selection method justified by theoretical results. The core-set based token selection technique allows us to avoid expensive pre-training, gives a space-efficient fine tuning, and thus makes it suitable to handle longer sequence lengths. We provide extensive experiments establishing advantages of pyramid BERT over several baselines and existing works on the GLUE benchmarks and Long Range Arena (CITATION) datasets.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.602.pdf",
        "keywords": [
            "token selection",
            "complexity",
            "pyramid",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Transformer-based language models... are computationally prohibitive.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Transformer-based language models... are computationally prohibitive.\""
    },
    {
        "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
        "authors": [
            "Elad Ben Zaken",
            "Yoav Goldberg",
            "Shauli Ravfogel"
        ],
        "published": "2022",
        "summary": "We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods. Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.",
        "pdf_link": "https://aclanthology.org/2022.acl-short.1.pdf",
        "keywords": [
            "fine tuning",
            "finetuning",
            "sparse fine tuning",
            "sparse finetuning",
            "language modeling",
            "parameter"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.\"\n\nThis abstract mentions a limitation of LLMs in passing, implying that fine-tuning may not be learning new task-specific linguistic knowledge, but rather exposing existing knowledge. However, this limitation is not the primary focus of the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.\"\n\nThis abstract mentions a limitation of LLMs in passing, implying that fine-tuning may not be learning new task-specific linguistic knowledge, but rather exposing existing knowledge. However, this limitation is not the primary focus of the abstract."
    },
    {
        "title": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks",
        "authors": [
            "Xiao Liu",
            "Kaixuan Ji",
            "Yicheng Fu",
            "Weng Tam",
            "Zhengxiao Du",
            "Zhilin Yang",
            "Jie Tang"
        ],
        "published": "2022",
        "summary": "Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (CITATION) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.",
        "pdf_link": "https://aclanthology.org/2022.acl-short.8.pdf",
        "keywords": [
            "tuning",
            "universality",
            "finetuning",
            "p tuning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality.\""
    },
    {
        "title": "Automatic Detection of Entity-Manipulated Text using Factual Knowledge",
        "authors": [
            "Ganesh Jawahar",
            "Muhammad Abdul-Mageed",
            "Laks Lakshmanan"
        ],
        "published": "2022",
        "summary": "In this work, we focus on the problem of distinguishing a human written news article from a news article that is created by manipulating entities in a human written news article (e.g., replacing entities with factually incorrect entities). Such manipulated articles can mislead the reader by posing as a human written news article. We propose a neural network based detector that detects manipulated news articles by reasoning about the facts mentioned in the article. Our proposed detector exploits factual knowledge via graph convolutional neural network along with the textual information in the news article. We also create challenging datasets for this task by considering various strategies to generate the new replacement entity (e.g., entity generation from GPT-2). In all the settings, our proposed model either matches or outperforms the state-of-the-art detector in terms of accuracy. Our code and data are available at https://github.com/UBC-NLP/manipulated_entity_detection.",
        "pdf_link": "https://aclanthology.org/2022.acl-short.10.pdf",
        "keywords": [
            "factual knowledge",
            "entity manipulated text",
            "neural network",
            "manipulated news articles"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"entity generation from GPT-2\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"entity generation from GPT-2\""
    },
    {
        "title": "Buy Tesla, Sell Ford: Assessing Implicit Stock Market Preference in Pre-trained Language Models",
        "authors": [
            "Chengyu Chuang",
            "Yi Yang"
        ],
        "published": "2022",
        "summary": "Pretrained language models such as BERT have achieved remarkable success in several NLP tasks. With the wide adoption of BERT in real-world applications, researchers begin to investigate the implicit biases encoded in the BERT. In this paper, we assess the implicit stock market preferences in BERT and its finance domain-specific model FinBERT. We find some interesting patterns. For example, the language models are overall more positive towards the stock market, but there are significant differences in preferences between a pair of industry sectors, or even within a sector. Given the prevalence of NLP models in financial decision making systems, this work raises the awareness of their potential implicit preferences in the stock markets. Awareness of such problems can help practitioners improve robustness and accountability of their financial NLP pipelines .",
        "pdf_link": "https://aclanthology.org/2022.acl-short.12.pdf",
        "keywords": [
            "stock market",
            "language models",
            "implicit stock market",
            "buy tesla",
            "trained language"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Given the prevalence of NLP models in financial decision making systems, this work raises the awareness of their potential implicit preferences in the stock markets.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Given the prevalence of NLP models in financial decision making systems, this work raises the awareness of their potential implicit preferences in the stock markets.\""
    },
    {
        "title": "Predicting Difficulty and Discrimination of Natural Language Questions",
        "authors": [
            "Matthew Byrd",
            "Shashank Srivastava"
        ],
        "published": "2022",
        "summary": "Item Response Theory (IRT) has been extensively used to numerically characterize question difficulty and discrimination for human subjects in domains including cognitive psychology and education (Primi et al., 2014; Downing, 2003). More recently, IRT has been used to similarly characterize item difficulty and discrimination for natural language models across various datasets (Lalor et al., 2019; Vania et al., 2021; Rodriguez et al., 2021). In this work, we explore predictive models for directly estimating and explaining these traits for natural language questions in a question-answering context. We use HotpotQA for illustration. Our experiments show that it is possible to predict both difficulty and discrimination parameters for new questions, and these traits are correlated with features of questions, answers, and associated contexts. Our findings can have significant implications for the creation of new datasets and tests on the one hand and strategies such as active learning and curriculum learning on the other.",
        "pdf_link": "https://aclanthology.org/2022.acl-short.15.pdf",
        "keywords": [
            "natural language",
            "item response theory",
            "natural language models",
            "item difficulty",
            "predicting difficulty",
            "discrimination",
            "predictive",
            "natural language questions"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicitly mentioned, but the paper discusses the difficulty and discrimination of natural language questions which could be related to limitations of LLMs in understanding and answering questions, however, this is not clearly stated in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None explicitly mentioned, but the paper discusses the difficulty and discrimination of natural language questions which could be related to limitations of LLMs in understanding and answering questions, however, this is not clearly stated in the abstract."
    },
    {
        "title": "How does the pre-training objective affect what large language models learn about linguistic properties?",
        "authors": [
            "Ahmed Alajrami",
            "Nikolaos Aletras"
        ],
        "published": "2022",
        "summary": "Several pre-training objectives, such as masked language modeling (MLM), have been proposed to pre-train language models (e.g. BERT) with the aim of learning better language representations. However, to the best of our knowledge, no previous work so far has investigated how different pre-training objectives affect what BERT learns about linguistics properties. We hypothesize that linguistically motivated objectives such as MLM should help BERT to acquire better linguistic knowledge compared to other non-linguistically motivated objectives that are not intuitive or hard for humans to guess the association between the input and the label to be predicted. To this end, we pre-train BERT with two linguistically motivated objectives and three non-linguistically motivated ones. We then probe for linguistic characteristics encoded in the representation of the resulting models. We find strong evidence that there are only small differences in probing performance between the representations learned by the two different types of objectives. These surprising results question the dominant narrative of linguistically informed pre-training.",
        "pdf_link": "https://aclanthology.org/2022.acl-short.16.pdf",
        "keywords": [
            "linguistically",
            "linguistically motivated objectives",
            "training"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find strong evidence that there are only small differences in probing performance between the representations learned by the two different types of objectives. These surprising results question the dominant narrative of linguistically informed pre-training.\"\n\nThis rating is based on the fact that the paper discusses a limitation of LLMs, specifically that linguistically motivated pre-training objectives may not lead to better linguistic knowledge acquisition, which challenges",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We find strong evidence that there are only small differences in probing performance between the representations learned by the two different types of objectives. These surprising results question the dominant narrative of linguistically informed pre-training.\"\n\nThis rating is based on the fact that the paper discusses a limitation of LLMs, specifically that linguistically motivated pre-training objectives may not lead to better linguistic knowledge acquisition, which challenges"
    },
    {
        "title": "The Power of Prompt Tuning for Low-Resource Semantic Parsing",
        "authors": [
            "Nathan Schucher",
            "Siva Reddy",
            "Harm de Vries"
        ],
        "published": "2022",
        "summary": "Prompt tuning has recently emerged as an effective method for adapting pre-trained language models to a number of language understanding and generation tasks. In this paper, we investigate prompt tuning for semantic parsing—the task of mapping natural language utterances onto formal meaning representations. On the low-resource splits of Overnight and TOPv2, we find that a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart, as well as strong GPT-3 and BART baselines. We also conduct ablation studies across different model scales and target representations, finding that, with increasing model scale, prompt tuned T5 models improve at generating target representations that are far from the pre-training distribution.",
        "pdf_link": "https://aclanthology.org/2022.acl-short.17.pdf",
        "keywords": [
            "tuning",
            "fine tuned",
            "pre training",
            "adapting",
            "formal meaning representations",
            "mapping natural language",
            "semantic parsing"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations are mentioned, but the paper implies that there are limitations in fine-tuning by stating that prompt tuning outperforms fine-tuning.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations are mentioned, but the paper implies that there are limitations in fine-tuning by stating that prompt tuning outperforms fine-tuning."
    },
    {
        "title": "Data Contamination: From Memorization to Exploitation",
        "authors": [
            "Inbal Magar",
            "Roy Schwartz"
        ],
        "published": "2022",
        "summary": "Pretrained language models are typically trained on massive web-based datasets, which are often “contaminated” with downstream test sets. It is not clear to what extent models exploit the contaminated data for downstream tasks. We present a principled method to study this question. We pretrain BERT models on joint corpora of Wikipedia and labeled downstream datasets, and fine-tune them on the relevant task. Comparing performance between samples seen and unseen during pretraining enables us to define and quantify levels of memorization and exploitation. Experiments with two models and three downstream tasks show that exploitation exists in some cases, but in others the models memorize the contaminated data, but do not exploit it. We show that these two measures are affected by different factors such as the number of duplications of the contaminated data and the model size. Our results highlight the importance of analyzing massive web-scale datasets to verify that progress in NLP is obtained by better language understanding and not better data exploitation.",
        "pdf_link": "https://aclanthology.org/2022.acl-short.18.pdf",
        "keywords": [
            "memorization",
            "data contamination",
            "exploitation",
            "wikipedia"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We show that these two measures are affected by different factors such as the number of duplications of the contaminated data and the model size. Our results highlight the importance of analyzing massive web-scale datasets to verify that progress in NLP is obtained by better language understanding and not better data exploitation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We show that these two measures are affected by different factors such as the number of duplications of the contaminated data and the model size. Our results highlight the importance of analyzing massive web-scale datasets to verify that progress in NLP is obtained by better language understanding and not better data exploitation.\""
    },
    {
        "title": "Learning-by-Narrating: Narrative Pre-Training for Zero-Shot Dialogue Comprehension",
        "authors": [
            "Chao Zhao",
            "Wenlin Yao",
            "Dian Yu",
            "Kaiqiang Song",
            "Dong Yu",
            "Jianshu Chen"
        ],
        "published": "2022",
        "summary": "Comprehending a dialogue requires a model to capture diverse kinds of key information in the utterances, which are either scattered around or implicitly implied in different turns of conversations. Therefore, dialogue comprehension requires diverse capabilities such as paraphrasing, summarizing, and commonsense reasoning. Towards the objective of pre-training a zero-shot dialogue comprehension model, we develop a novel narrative-guided pre-training strategy that learns by narrating the key information from a dialogue input. However, the dialogue-narrative parallel corpus for such a pre-training strategy is currently unavailable. For this reason, we first construct a dialogue-narrative parallel corpus by automatically aligning movie subtitles and their synopses. We then pre-train a BART model on the data and evaluate its performance on four dialogue-based tasks that require comprehension. Experimental results show that our model not only achieves superior zero-shot performance but also exhibits stronger fine-grained dialogue comprehension capabilities. The data and code are available at https://github.com/zhaochaocs/Diana.",
        "pdf_link": "https://aclanthology.org/2022.acl-short.23.pdf",
        "keywords": [
            "narrative",
            "dialogue narrative",
            "dialogue",
            "dialogue comprehension",
            "zero shot dialogue",
            "dialogue narrative parallel",
            "narrative pre"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations of LLMs are mentioned in the abstract, but the title mentions \"Zero-Shot Dialogue Comprehension\" which implies that current models may struggle with this task.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations of LLMs are mentioned in the abstract, but the title mentions \"Zero-Shot Dialogue Comprehension\" which implies that current models may struggle with this task."
    },
    {
        "title": "Kronecker Decomposition for GPT Compression",
        "authors": [
            "Ali Edalati",
            "Marzieh Tahaei",
            "Ahmad Rashid",
            "Vahid Nia",
            "James Clark",
            "Mehdi Rezagholizadeh"
        ],
        "published": "2022",
        "summary": "GPT is an auto-regressive Transformer-based pre-trained language model which has attracted a lot of attention in the natural language processing (NLP) domain. The success of GPT is mostly attributed to its pre-training on huge amount of data and its large number of parameters. Despite the superior performance of GPT, this overparameterized nature of GPT can be very prohibitive for deploying this model on devices with limited computational power or memory. This problem can be mitigated using model compression techniques; however, compressing GPT models has not been investigated much in the literature. In this work, we use Kronecker decomposition to compress the linear mappings of the GPT-2 model. Our Kronecker GPT-2 model (KnGPT2) is initialized based on the Kronecker decomposed version of the GPT-2 model and then is undergone a very light pre- training on only a small portion of the training data with intermediate layer knowledge distillation (ILKD). Finally, our KnGPT2 is fine-tuned on downstream tasks using ILKD as well. We evaluate our model on both language modeling and General Language Understanding Evaluation benchmark tasks and show that with more efficient pre-training and similar number of parameters, our KnGPT2 outperforms the existing DistilGPT2 model significantly.",
        "pdf_link": "https://aclanthology.org/2022.acl-short.24.pdf",
        "keywords": [
            "model compression",
            "kronecker",
            "gpt compression",
            "layer knowledge distillation",
            "kronecker decomposition",
            "natural language processing"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the superior performance of GPT, this overparameterized nature of GPT can be very prohibitive for deploying this model on devices with limited computational power or memory.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite the superior performance of GPT, this overparameterized nature of GPT can be very prohibitive for deploying this model on devices with limited computational power or memory.\""
    },
    {
        "title": "LM-BFF-MS: Improving Few-Shot Fine-tuning of Language Models based on Multiple Soft Demonstration Memory",
        "authors": [
            "Eunhwan Park",
            "Donghyeon Jeon",
            "Seonhoon Kim",
            "Inho Kang",
            "Seung-Hoon Na"
        ],
        "published": "2022",
        "summary": "LM-BFF (CITATION) achieves significant few-shot performance by using auto-generated prompts and adding demonstrations similar to an input example. To improve the approach of LM-BFF, this paper proposes LM-BFF-MS—better few-shot fine-tuning of language models with multiple soft demonstrations by making its further extensions, which include 1) prompts with multiple demonstrations based on automatic generation of multiple label words; and 2) soft demonstration memory which consists of multiple sequences of globally shared word embeddings for a similar context. Experiments conducted on eight NLP tasks show that LM-BFF-MS leads to improvements over LM-BFF on five tasks, particularly achieving 94.0 and 90.4 on SST-2 and MRPC, respectively.",
        "pdf_link": "https://aclanthology.org/2022.acl-short.34.pdf",
        "keywords": [
            "soft demonstration",
            "lm bff",
            "lm bff ms",
            "multiple soft demonstration memory",
            "multiple soft demonstrations",
            "language models",
            "lm",
            "sst",
            "fine tuning"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the paper discusses improving few-shot fine-tuning of language models but does not mention any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the paper discusses improving few-shot fine-tuning of language models but does not mention any limitations of LLMs."
    },
    {
        "title": "Exploiting Language Model Prompts Using Similarity Measures: A Case Study on the Word-in-Context Task",
        "authors": [
            "Mohsen Tabasi",
            "Kiamehr Rezaee",
            "Mohammad Taher Pilehvar"
        ],
        "published": "2022",
        "summary": "As a recent development in few-shot learning, prompt-based techniques have demonstrated promising potential in a variety of natural language processing tasks. However, despite proving competitive on most tasks in the GLUE and SuperGLUE benchmarks, existing prompt-based techniques fail on the semantic distinction task of the Word-in-Context (WiC) dataset. Specifically, none of the existing few-shot approaches (including the in-context learning of GPT-3) can attain a performance that is meaningfully different from the random baseline. Trying to fill this gap, we propose a new prompting technique, based on similarity metrics, which boosts few-shot performance to the level of fully supervised methods. Our simple adaptation shows that the failure of existing prompt-based techniques in semantic distinction is due to their improper configuration, rather than lack of relevant knowledge in the representations. We also show that this approach can be effectively extended to other downstream tasks for which a single prompt is sufficient.",
        "pdf_link": "https://aclanthology.org/2022.acl-short.36.pdf",
        "keywords": [
            "similarity metrics",
            "similarity measures",
            "semantic distinction",
            "context task",
            "language model"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, despite proving competitive on most tasks in the GLUE and SuperGLUE benchmarks, existing prompt-based techniques fail on the semantic distinction task of the Word-in-Context (WiC) dataset. Specifically, none of the existing few-shot approaches (including the in-context learning of GPT-3) can attain a performance that is meaningfully different from the random baseline.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, despite proving competitive on most tasks in the GLUE and SuperGLUE benchmarks, existing prompt-based techniques fail on the semantic distinction task of the Word-in-Context (WiC) dataset. Specifically, none of the existing few-shot approaches (including the in-context learning of GPT-3) can attain a performance that is meaningfully different from the random baseline.\""
    },
    {
        "title": "An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers",
        "authors": [
            "Valentin Hofmann",
            "Hinrich Schuetze",
            "Janet Pierrehumbert"
        ],
        "published": "2022",
        "summary": "We introduce FLOTA (Few Longest Token Approximation), a simple yet effective method to improve the tokenization of pretrained language models (PLMs). FLOTA uses the vocabulary of a standard tokenizer but tries to preserve the morphological structure of words during tokenization. We evaluate FLOTA on morphological gold segmentations as well as a text classification task, using BERT, GPT-2, and XLNet as example PLMs. FLOTA leads to performance gains, makes inference more efficient, and enhances the robustness of PLMs with respect to whitespace noise.",
        "pdf_link": "https://aclanthology.org/2022.acl-short.43.pdf",
        "keywords": [
            "tokenization",
            "language model tokenizers",
            "text classification",
            "xlnet"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"undesirable Properties of Pretrained Language Model Tokenizers\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"undesirable Properties of Pretrained Language Model Tokenizers\""
    },
    {
        "title": "Investigating person-specific errors in chat-oriented dialogue systems",
        "authors": [
            "Koh Mitsuda",
            "Ryuichiro Higashinaka",
            "Tingxuan Li",
            "Sen Yoshida"
        ],
        "published": "2022",
        "summary": "Creating chatbots to behave like real people is important in terms of believability. Errors in general chatbots and chatbots that follow a rough persona have been studied, but those in chatbots that behave like real people have not been thoroughly investigated. We collected a large amount of user interactions of a generation-based chatbot trained from large-scale dialogue data of a specific character, i.e., target person, and analyzed errors related to that person. We found that person-specific errors can be divided into two types: errors in attributes and those in relations, each of which can be divided into two levels: self and other. The correspondence with an existing taxonomy of errors was also investigated, and person-specific errors that should be addressed in the future were clarified.",
        "pdf_link": "https://aclanthology.org/2022.acl-short.50.pdf",
        "keywords": [
            "chatbots",
            "chat",
            "errors",
            "dialogue systems",
            "specific errors"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Fire Burns, Sword Cuts: Commonsense Inductive Bias for Exploration in Text-based Games",
        "authors": [
            "Dongwon Ryu",
            "Ehsan Shareghi",
            "Meng Fang",
            "Yunqiu Xu",
            "Shirui Pan",
            "Reza Haf"
        ],
        "published": "2022",
        "summary": "Text-based games (TGs) are exciting testbeds for developing deep reinforcement learning techniques due to their partially observed environments and large action spaces. In these games, the agent learns to explore the environment via natural language interactions with the game simulator. A fundamental challenge in TGs is the efficient exploration of the large action space when the agent has not yet acquired enough knowledge about the environment. We propose CommExpl, an exploration technique that injects external commonsense knowledge, via a pretrained language model (LM), into the agent during training when the agent is the most uncertain about its next action. Our method exhibits improvement on the collected game scores during the training in four out of nine games from Jericho. Additionally, the produced trajectory of actions exhibit lower perplexity, when tested with a pretrained LM, indicating better closeness to human language.",
        "pdf_link": "https://aclanthology.org/2022.acl-short.56.pdf",
        "keywords": [
            "sword cuts",
            "commonsense",
            "games",
            "text based games",
            "language model",
            "fire burns",
            "commonsense inductive bias"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions a \"pretrained language model (LM)\" which is related to LLMs, but it does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions a \"pretrained language model (LM)\" which is related to LLMs, but it does not discuss any limitations of LLMs."
    },
    {
        "title": "A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models",
        "authors": [
            "Deming Ye",
            "Yankai Lin",
            "Peng Li",
            "Maosong Sun",
            "Zhiyuan Liu"
        ],
        "published": "2022",
        "summary": "Pre-trained language models (PLMs) cannot well recall rich factual knowledge of entities exhibited in large-scale corpora, especially those rare entities. In this paper, we propose to build a simple but effective Pluggable Entity Lookup Table (PELT) on demand by aggregating the entity’s output representations of multiple occurrences in the corpora. PELT can be compatibly plugged as inputs to infuse supplemental entity knowledge into PLMs. Compared to previous knowledge-enhanced PLMs, PELT only requires 0.2%-5% pre-computation with capability of acquiring knowledge from out-of-domain corpora for domain adaptation scenario. The experiments on knowledge-related tasks demonstrate that our method, PELT, can flexibly and effectively transfer entity knowledge from related corpora into PLMs with different architectures. Our code and models are publicly available at https://github.com/thunlp/PELT",
        "pdf_link": "https://aclanthology.org/2022.acl-short.57.pdf",
        "keywords": [
            "plugged",
            "pluggable entity lookup table",
            "pre trained language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Pre-trained language models (PLMs) cannot well recall rich factual knowledge of entities exhibited in large-scale corpora, especially those rare entities.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Pre-trained language models (PLMs) cannot well recall rich factual knowledge of entities exhibited in large-scale corpora, especially those rare entities.\""
    },
    {
        "title": "S4-Tuning: A Simple Cross-lingual Sub-network Tuning Method",
        "authors": [
            "Runxin Xu",
            "Fuli Luo",
            "Baobao Chang",
            "Songfang Huang",
            "Fei Huang"
        ],
        "published": "2022",
        "summary": "The emergence of multilingual pre-trained language models makes it possible to adapt to target languages with only few labeled examples. However, vanilla fine-tuning tends to achieve degenerated and unstable results, owing to the Language Interference among different languages, and Parameter Overload under the few-sample transfer learning scenarios. To address two problems elegantly, we propose S4-Tuning, a Simple Cross-lingual Sub-network Tuning method. S4-Tuning first detects the most essential sub-network for each target language, and only updates it during fine-tuning.In this way, the language sub-networks lower the scale of trainable parameters, and hence better suit the low-resource scenarios.Meanwhile, the commonality and characteristics across languages are modeled by the overlapping and non-overlapping parts to ease the interference among languages.Simple but effective, S4-Tuning gains consistent improvements over vanilla fine-tuning on three multi-lingual tasks involving 37 different languages in total (XNLI, PAWS-X, and Tatoeba).",
        "pdf_link": "https://aclanthology.org/2022.acl-short.58.pdf",
        "keywords": [
            "sub network tuning",
            "s4 tuning",
            "fine tuning",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, vanilla fine-tuning tends to achieve degenerated and unstable results, owing to the Language Interference among different languages, and Parameter Overload under the few-sample transfer learning scenarios.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, vanilla fine-tuning tends to achieve degenerated and unstable results, owing to the Language Interference among different languages, and Parameter Overload under the few-sample transfer learning scenarios.\""
    },
    {
        "title": "Developmental Negation Processing in Transformer Language Models",
        "authors": [
            "Antonio Laverghetta Jr.",
            "John Licato"
        ],
        "published": "2022",
        "summary": "Reasoning using negation is known to be difficult for transformer-based language models. While previous studies have used the tools of psycholinguistics to probe a transformer’s ability to reason over negation, none have focused on the types of negation studied in developmental psychology. We explore how well transformers can process such categories of negation, by framing the problem as a natural language inference (NLI) task. We curate a set of diagnostic questions for our target categories from popular NLI datasets and evaluate how well a suite of models reason over them. We find that models perform consistently better only on certain categories, suggesting clear distinctions in how they are processed.",
        "pdf_link": "https://aclanthology.org/2022.acl-short.60.pdf",
        "keywords": [
            "transformer",
            "transformer language models",
            "transformer based language models",
            "natural language inference",
            "developmental negation processing"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Reasoning using negation is known to be difficult for transformer-based language models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Reasoning using negation is known to be difficult for transformer-based language models.\""
    },
    {
        "title": "On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations",
        "authors": [
            "Yang Trista Cao",
            "Yada Pruksachatkun",
            "Kai-Wei Chang",
            "Rahul Gupta",
            "Varun Kumar",
            "Jwala Dhamala",
            "Aram Galstyan"
        ],
        "published": "2022",
        "summary": "Multiple metrics have been introduced to measure fairness in various natural language processing tasks. These metrics can be roughly categorized into two categories: 1) extrinsic metrics for evaluating fairness in downstream applications and 2) intrinsic metrics for estimating fairness in upstream contextualized language representation models. In this paper, we conduct an extensive correlation study between intrinsic and extrinsic metrics across bias notions using 19 contextualized language models. We find that intrinsic and extrinsic metrics do not necessarily correlate in their original setting, even when correcting for metric misalignments, noise in evaluation datasets, and confounding factors such as experiment configuration for extrinsic metrics.",
        "pdf_link": "https://aclanthology.org/2022.acl-short.62.pdf",
        "keywords": [
            "fairness",
            "metrics",
            "extrinsic metrics",
            "contextualized language representations",
            "contextualized language models",
            "intrinsic metrics"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that intrinsic and extrinsic metrics do not necessarily correlate in their original setting, even when correcting for metric misalignments, noise in evaluation datasets, and confounding factors such as experiment configuration for extrinsic metrics.\"\n\nThis abstract mentions a limitation of LLMs in passing, specifically that intrinsic and extrinsic metrics do not necessarily correlate, but does not elaborate on this limitation further",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We find that intrinsic and extrinsic metrics do not necessarily correlate in their original setting, even when correcting for metric misalignments, noise in evaluation datasets, and confounding factors such as experiment configuration for extrinsic metrics.\"\n\nThis abstract mentions a limitation of LLMs in passing, specifically that intrinsic and extrinsic metrics do not necessarily correlate, but does not elaborate on this limitation further"
    },
    {
        "title": "Zero-Shot Dependency Parsing with Worst-Case Aware Automated Curriculum Learning",
        "authors": [
            "Miryam de Lhoneux",
            "Sheng Zhang",
            "Anders Søgaard"
        ],
        "published": "2022",
        "summary": "Large multilingual pretrained language models such as mBERT and XLM-RoBERTa have been found to be surprisingly effective for cross-lingual transfer of syntactic parsing models Wu and Dredze (2019), but only between related languages. However, source and training languages are rarely related, when parsing truly low-resource languages. To close this gap, we adopt a method from multi-task learning, which relies on automated curriculum learning, to dynamically optimize for parsing performance on outlier languages. We show that this approach is significantly better than uniform and size-proportional sampling in the zero-shot setting.",
        "pdf_link": "https://aclanthology.org/2022.acl-short.64.pdf",
        "keywords": [
            "automated curriculum learning",
            "zero shot dependency parsing"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large multilingual pretrained language models such as mBERT and XLM-RoBERTa have been found to be surprisingly effective for cross-lingual transfer of syntactic parsing models Wu and Dredze (2019), but only between related languages.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large multilingual pretrained language models such as mBERT and XLM-RoBERTa have been found to be surprisingly effective for cross-lingual transfer of syntactic parsing models Wu and Dredze (2019), but only between related languages.\""
    },
    {
        "title": "When classifying grammatical role, BERT doesn’t care about word order... except when it matters",
        "authors": [
            "Isabel Papadimitriou",
            "Richard Futrell",
            "Kyle Mahowald"
        ],
        "published": "2022",
        "summary": "Because meaning can often be inferred from lexical semantics alone, word order is often a redundant cue in natural language. For example, the words chopped, chef, and onion are more likely used to convey “The chef chopped the onion,” not “The onion chopped the chef.” Recent work has shown large language models to be surprisingly word order invariant, but crucially has largely considered natural prototypical inputs, where compositional meaning mostly matches lexical expectations. To overcome this confound, we probe grammatical role representation in English BERT and GPT-2, on instances where lexical expectations are not sufficient, and word order knowledge is necessary for correct classification. Such non-prototypical instances are naturally occurring English sentences with inanimate subjects or animate objects, or sentences where we systematically swap the arguments to make sentences like “The onion chopped the chef”. We find that, while early layer embeddings are largely lexical, word order is in fact crucial in defining the later-layer representations of words in semantically non-prototypical positions. Our experiments isolate the effect of word order on the contextualization process, and highlight how models use context in the uncommon, but critical, instances where it matters.",
        "pdf_link": "https://aclanthology.org/2022.acl-short.71.pdf",
        "keywords": [
            "grammatical",
            "language models",
            "word order",
            "word order knowledge",
            "word order invariant",
            "grammatical role",
            "lexical expectations",
            "context",
            "contextualization"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent work has shown large language models to be surprisingly word order invariant, but crucially has largely considered natural prototypical inputs, where compositional meaning mostly matches lexical expectations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Recent work has shown large language models to be surprisingly word order invariant, but crucially has largely considered natural prototypical inputs, where compositional meaning mostly matches lexical expectations.\""
    },
    {
        "title": "Triangular Transfer: Freezing the Pivot for Triangular Machine Translation",
        "authors": [
            "Meng Zhang",
            "Liangyou Li",
            "Qun Liu"
        ],
        "published": "2022",
        "summary": "Triangular machine translation is a special case of low-resource machine translation where the language pair of interest has limited parallel data, but both languages have abundant parallel data with a pivot language. Naturally, the key to triangular machine translation is the successful exploitation of such auxiliary data. In this work, we propose a transfer-learning-based approach that utilizes all types of auxiliary data. As we train auxiliary source-pivot and pivot-target translation models, we initialize some parameters of the pivot side with a pre-trained language model and freeze them to encourage both translation models to work in the same pivot language space, so that they can be smoothly transferred to the source-target translation model. Experiments show that our approach can outperform previous ones.",
        "pdf_link": "https://aclanthology.org/2022.acl-short.72.pdf",
        "keywords": [
            "pivot",
            "triangular machine translation",
            "transfer learning"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better",
        "authors": [
            "Chuhan Wu",
            "Fangzhao Wu",
            "Tao Qi",
            "Yongfeng Huang"
        ],
        "published": "2022",
        "summary": "Effectively finetuning pretrained language models (PLMs) is critical for their success in downstream tasks. However, PLMs may have risks in overfitting the pretraining tasks and data, which usually have gap with the target downstream tasks. Such gap may be difficult for existing PLM finetuning methods to overcome and lead to suboptimal performance. In this paper, we propose a very simple yet effective method named NoisyTune to help better finetune PLMs on downstream tasks by adding some noise to the parameters of PLMs before fine-tuning. More specifically, we propose a matrix-wise perturbing method which adds different uniform noises to different parameter matrices based on their standard deviations. In this way, the varied characteristics of different types of parameters in PLMs can be considered. Extensive experiments on both GLUE English benchmark and XTREME multilingual benchmark show NoisyTune can consistently empower the finetuning of different PLMs on different downstream tasks.",
        "pdf_link": "https://aclanthology.org/2022.acl-short.76.pdf",
        "keywords": [
            "finetuning",
            "language models",
            "noisytune",
            "noise",
            "little noise"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, PLMs may have risks in overfitting the pretraining tasks and data, which usually have gap with the target downstream tasks. Such gap may be difficult for existing PLM finetuning methods to overcome and lead to suboptimal performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, PLMs may have risks in overfitting the pretraining tasks and data, which usually have gap with the target downstream tasks. Such gap may be difficult for existing PLM finetuning methods to overcome and lead to suboptimal performance.\""
    },
    {
        "title": "Can a Transformer Pass the Wug Test? Tuning Copying Bias in Neural Morphological Inflection Models",
        "authors": [
            "Ling Liu",
            "Mans Hulden"
        ],
        "published": "2022",
        "summary": "Deep learning sequence models have been successful with morphological inflection generation. The SIGMORPHON shared task results in the past several years indicate that such models can perform well, but only if the training data covers a good amount of different lemmata, or if the lemmata to be inflected at test time have also been seen in training, as has indeed been largely the case in these tasks. Surprisingly, we find that standard models such as the Transformer almost completely fail at generalizing inflection patterns when trained on a limited number of lemmata and asked to inflect previously unseen lemmata—i.e. under “wug test”-like circumstances. This is true even though the actual number of training examples is very large. While established data augmentation techniques can be employed to alleviate this shortcoming by introducing a copying bias through hallucinating synthetic new word forms using the alphabet in the language at hand, our experiment results show that, to be more effective, the hallucination process needs to pay attention to substrings of syllable-like length rather than individual characters.",
        "pdf_link": "https://aclanthology.org/2022.acl-short.84.pdf",
        "keywords": [
            "copying bias",
            "deep learning sequence"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Surprisingly, we find that standard models such as the Transformer almost completely fail at generalizing inflection patterns when trained on a limited number of lemmata and asked to inflect previously unseen lemmata—i.e. under “wug test”-like circumstances.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Surprisingly, we find that standard models such as the Transformer almost completely fail at generalizing inflection patterns when trained on a limited number of lemmata and asked to inflect previously unseen lemmata—i.e. under “wug test”-like circumstances.\""
    },
    {
        "title": "CoDA21: Evaluating Language Understanding Capabilities of NLP Models With Context-Definition Alignment",
        "authors": [
            "Lütfi Kerem Senel",
            "Timo Schick",
            "Hinrich Schuetze"
        ],
        "published": "2022",
        "summary": "Pretrained language models (PLMs) have achieved superhuman performance on many benchmarks, creating a need for harder tasks. We introduce CoDA21 (Context Definition Alignment), a challenging benchmark that measures natural language understanding (NLU) capabilities of PLMs: Given a definition and a context each for k words, but not the words themselves, the task is to align the k definitions with the k contexts. CoDA21 requires a deep understanding of contexts and definitions, including complex inference and world knowledge. We find that there is a large gap between human and PLM performance, suggesting that CoDA21 measures an aspect of NLU that is not sufficiently covered in existing benchmarks.",
        "pdf_link": "https://aclanthology.org/2022.acl-short.92.pdf",
        "keywords": [
            "context definition alignment",
            "language models",
            "coda21",
            "world knowledge",
            "performance"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that there is a large gap between human and PLM performance, suggesting that CoDA21 measures an aspect of NLU that is not sufficiently covered in existing benchmarks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We find that there is a large gap between human and PLM performance, suggesting that CoDA21 measures an aspect of NLU that is not sufficiently covered in existing benchmarks.\""
    },
    {
        "title": "On the Importance of Effectively Adapting Pretrained Language Models for Active Learning",
        "authors": [
            "Katerina Margatina",
            "Loic Barrault",
            "Nikolaos Aletras"
        ],
        "published": "2022",
        "summary": "Recent active learning (AL) approaches in Natural Language Processing (NLP) proposed using off-the-shelf pretrained language models (LMs). In this paper, we argue that these LMs are not adapted effectively to the downstream task during AL and we explore ways to address this issue. We suggest to first adapt the pretrained LM to the target task by continuing training with all the available unlabeled data and then use it for AL. We also propose a simple yet effective fine-tuning method to ensure that the adapted LM is properly trained in both low and high resource scenarios during AL. Our experiments demonstrate that our approach provides substantial data efficiency improvements compared to the standard fine-tuning approach, suggesting that a poor training strategy can be catastrophic for AL.",
        "pdf_link": "https://aclanthology.org/2022.acl-short.93.pdf",
        "keywords": [
            "active learning",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"we argue that these LMs are not adapted effectively to the downstream task during AL\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"we argue that these LMs are not adapted effectively to the downstream task during AL\""
    },
    {
        "title": "A Recipe for Arbitrary Text Style Transfer with Large Language Models",
        "authors": [
            "Emily Reif",
            "Daphne Ippolito",
            "Ann Yuan",
            "Andy Coenen",
            "Chris Callison-Burch",
            "Jason Wei"
        ],
        "published": "2022",
        "summary": "In this paper, we leverage large language models (LLMs) to perform zero-shot text style transfer. We present a prompting method that we call augmented zero-shot learning, which frames style transfer as a sentence rewriting task and requires only a natural language instruction, without model fine-tuning or exemplars in the target style. Augmented zero-shot learning is simple and demonstrates promising results not just on standard style transfer tasks such as sentiment, but also on arbitrary transformations such as ‘make this melodramatic’ or ‘insert a metaphor.’",
        "pdf_link": "https://aclanthology.org/2022.acl-short.94.pdf",
        "keywords": [
            "shot learning",
            "language models",
            "zero shot text",
            "text style transfer"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the paper only discusses leveraging LLMs for zero-shot text style transfer without mentioning any limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the paper only discusses leveraging LLMs for zero-shot text style transfer without mentioning any limitations."
    },
    {
        "title": "Text Smoothing: Enhance Various Data Augmentation Methods on Text Classification Tasks",
        "authors": [
            "Xing Wu",
            "Chaochen Gao",
            "Meng Lin",
            "Liangjun Zang",
            "Songlin Hu"
        ],
        "published": "2022",
        "summary": "Before entering the neural network, a token needs to be converted to its one-hot representation, which is a discrete distribution of the vocabulary. Smoothed representation is the probability of candidate tokens obtained from the pre-trained masked language model, which can be seen as a more informative augmented substitution to the one-hot representation. We propose an efficient data augmentation method, dub as text smoothing, by converting a sentence from its one-hot representation to controllable smoothed representation. We evaluate text smoothing on different datasets in a low-resource regime. Experimental results show that text smoothing outperforms various mainstream data augmentation methods by a substantial margin. Moreover, text smoothing can be combined with these data augmentation methods to achieve better performance.",
        "pdf_link": "https://aclanthology.org/2022.acl-short.97.pdf",
        "keywords": [
            "smoothed",
            "text smoothing",
            "data augmentation",
            "text classification",
            "smoothed representation",
            "mainstream data augmentation"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"a pre-trained masked language model\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"a pre-trained masked language model\""
    },
    {
        "title": "Compilable Neural Code Generation with Compiler Feedback",
        "authors": [
            "Xin Wang",
            "Yasheng Wang",
            "Yao Wan",
            "Fei Mi",
            "Yitong Li",
            "Pingyi Zhou",
            "Jin Liu",
            "Hao Wu",
            "Xin Jiang",
            "Qun Liu"
        ],
        "published": "2022",
        "summary": "Automatically generating compilable programs with (or without) natural language descriptions has always been a touchstone problem for computational linguistics and automated software engineering. Existing deep-learning approaches model code generation as text generation, either constrained by grammar structures in decoder, or driven by pre-trained language models on large-scale code corpus (e.g., CodeGPT, PLBART, and CodeT5). However, few of them account for compilability of the generated programs. To improve compilability of the generated programs, this paper proposes COMPCODER, a three-stage pipeline utilizing compiler feedback for compilable code generation, including language model fine-tuning, compilability reinforcement, and compilability discrimination. Comprehensive experiments on two code generation tasks demonstrate the effectiveness of our proposed approach, improving the success rate of compilation from 44.18 to 89.18 in code completion on average and from 70.3 to 96.2 in text-to-code generation, respectively, when comparing with the state-of-the-art CodeGPT.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.2.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing deep-learning approaches... driven by pre-trained language models on large-scale code corpus (e.g., CodeGPT, PLBART, and CodeT5). However, few of them account for compilability of the generated programs.\"\n\nThis paper mentions a limitation of LLMs in passing, specifically that they often do not account for compilability of the generated programs, but does",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing deep-learning approaches... driven by pre-trained language models on large-scale code corpus (e.g., CodeGPT, PLBART, and CodeT5). However, few of them account for compilability of the generated programs.\"\n\nThis paper mentions a limitation of LLMs in passing, specifically that they often do not account for compilability of the generated programs, but does"
    },
    {
        "title": "RelationPrompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction",
        "authors": [
            "Yew Ken Chia",
            "Lidong Bing",
            "Soujanya Poria",
            "Luo Si"
        ],
        "published": "2022",
        "summary": "Despite the importance of relation extraction in building and representing knowledge, less research is focused on generalizing to unseen relations types. We introduce the task setting of Zero-Shot Relation Triplet Extraction (ZeroRTE) to encourage further research in low-resource relation extraction methods. Given an input sentence, each extracted triplet consists of the head entity, relation label, and tail entity where the relation label is not seen at the training stage. To solve ZeroRTE, we propose to synthesize relation examples by prompting language models to generate structured texts. Concretely, we unify language model prompts and structured text approaches to design a structured prompt template for generating synthetic relation samples when conditioning on relation label prompts (RelationPrompt). To overcome the limitation for extracting multiple relation triplets in a sentence, we design a novel Triplet Search Decoding method. Experiments on FewRel and Wiki-ZSL datasets show the efficacy of RelationPrompt for the ZeroRTE task and zero-shot relation classification. Our code and data are available at github.com/declare-lab/RelationPrompt.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.5.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"To overcome the limitation for extracting multiple relation triplets in a sentence, we design a novel Triplet Search Decoding method.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs in extracting multiple relation triplets in a sentence, but it is a minor detail and not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"To overcome the limitation for extracting multiple relation triplets in a sentence, we design a novel Triplet Search Decoding method.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs in extracting multiple relation triplets in a sentence, but it is a minor detail and not the primary focus of the paper."
    },
    {
        "title": "MR-P: A Parallel Decoding Algorithm for Iterative Refinement Non-Autoregressive Translation",
        "authors": [
            "Hao Cheng",
            "Zhihua Zhang"
        ],
        "published": "2022",
        "summary": "Non-autoregressive translation (NAT) predicts all the target tokens in parallel and significantly speeds up the inference process. The Conditional Masked Language Model (CMLM) is a strong baseline of NAT. It decodes with the Mask-Predict algorithm which iteratively refines the output. Most works about CMLM focus on the model structure and the training objective. However, the decoding algorithm is equally important. We propose a simple, effective, and easy-to-implement decoding algorithm that we call MaskRepeat-Predict (MR-P). The MR-P algorithm gives higher priority to consecutive repeated tokens when selecting tokens to mask for the next iteration and stops the iteration after target tokens converge. We conduct extensive experiments on six translation directions with varying data sizes. The results show that MR-P significantly improves the performance with the same model parameters. Specifically, we achieve a BLEU increase of 1.39 points in the WMT’14 En-De translation task.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.25.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Open Relation Modeling: Learning to Define Relations between Entities",
        "authors": [
            "Jie Huang",
            "Kevin Chang",
            "Jinjun Xiong",
            "Wen-mei Hwu"
        ],
        "published": "2022",
        "summary": "Relations between entities can be represented by different instances, e.g., a sentence containing both entities or a fact in a Knowledge Graph (KG). However, these instances may not well capture the general relations between entities, may be difficult to understand by humans, even may not be found due to the incompleteness of the knowledge source. In this paper, we introduce the Open Relation Modeling problem - given two entities, generate a coherent sentence describing the relation between them. To solve this problem, we propose to teach machines to generate definition-like relation descriptions by letting them learn from defining entities. Specifically, we fine-tune Pre-trained Language Models (PLMs) to produce definitions conditioned on extracted entity pairs. To help PLMs reason between entities and provide additional relational knowledge to PLMs for open relation modeling, we incorporate reasoning paths in KGs and include a reasoning path selection mechanism. Experimental results show that our model can generate concise but informative relation descriptions that capture the representative characteristics of entities.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.26.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these instances may not well capture the general relations between entities, may be difficult to understand by humans, even may not be found due to the incompleteness of the knowledge source.\"\n\nThis paper should be rated with 1 since even though it talks about LLMs (specifically Pre-trained Language Models), it does not mention any explicit limitation of the models in the abstract",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, these instances may not well capture the general relations between entities, may be difficult to understand by humans, even may not be found due to the incompleteness of the knowledge source.\"\n\nThis paper should be rated with 1 since even though it talks about LLMs (specifically Pre-trained Language Models), it does not mention any explicit limitation of the models in the abstract"
    },
    {
        "title": "Towards Large-Scale Interpretable Knowledge Graph Reasoning for Dialogue Systems",
        "authors": [
            "Yi-Lin Tuan",
            "Sajjad Beygi",
            "Maryam Fazel-Zarandi",
            "Qiaozi Gao",
            "Alessandra Cervone",
            "William Yang Wang"
        ],
        "published": "2022",
        "summary": "Users interacting with voice assistants today need to phrase their requests in a very specific manner to elicit an appropriate response. This limits the user experience, and is partly due to the lack of reasoning capabilities of dialogue platforms and the hand-crafted rules that require extensive labor. One possible solution to improve user experience and relieve the manual efforts of designers is to build an end-to-end dialogue system that can do reasoning itself while perceiving user’s utterances. In this work, we propose a novel method to incorporate the knowledge reasoning capability into dialog systems in a more scalable and generalizable manner. Our proposed method allows a single transformer model to directly walk on a large-scale knowledge graph to generate responses. To the best of our knowledge, this is the first work to have transformer models generate responses by reasoning over differentiable knowledge graphs. We investigate the reasoning abilities of the proposed method on both task-oriented and domain-specific chit-chat dialogues. Empirical results show that this method can effectively and efficiently incorporate a knowledge graph into a dialogue system with fully-interpretable reasoning paths.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.33.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Users interacting with voice assistants today need to phrase their requests in a very specific manner to elicit an appropriate response. This limits the user experience, and is partly due to the lack of reasoning capabilities of dialogue platforms...\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Users interacting with voice assistants today need to phrase their requests in a very specific manner to elicit an appropriate response. This limits the user experience, and is partly due to the lack of reasoning capabilities of dialogue platforms...\""
    },
    {
        "title": "Plug-and-Play Adaptation for Continuously-updated QA",
        "authors": [
            "Kyungjae Lee",
            "Wookje Han",
            "Seung-won Hwang",
            "Hwaran Lee",
            "Joonsuk Park",
            "Sang-Woo Lee"
        ],
        "published": "2022",
        "summary": "Language models (LMs) have shown great potential as implicit knowledge bases (KBs). And for their practical use, knowledge in LMs need to be updated periodically. However, existing tasks to assess LMs’ efficacy as KBs do not adequately consider multiple large-scale updates. To this end, we first propose a novel task—Continuously-updated QA (CuQA)—in which multiple large-scale updates are made to LMs, and the performance is measured with respect to the success in adding and updating knowledge while retaining existing knowledge. We then present LMs with plug-in modules that effectively handle the updates. Experiments conducted on zsRE QA and NQ datasets show that our method outperforms existing approaches. We find that our method is 4x more effective in terms of updates/forgets ratio, compared to a fine-tuning baseline.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.37.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing tasks to assess LMs’ efficacy as KBs do not adequately consider multiple large-scale updates.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, existing tasks to assess LMs’ efficacy as KBs do not adequately consider multiple large-scale updates.\""
    },
    {
        "title": "What Works and Doesn’t Work, A Deep Decoder for Neural Machine Translation",
        "authors": [
            "Zuchao Li",
            "Yiran Wang",
            "Masao Utiyama",
            "Eiichiro Sumita",
            "Hai Zhao",
            "Taro Watanabe"
        ],
        "published": "2022",
        "summary": "Deep learning has demonstrated performance advantages in a wide range of natural language processing tasks, including neural machine translation (NMT). Transformer NMT models are typically strengthened by deeper encoder layers, but deepening their decoder layers usually results in failure. In this paper, we first identify the cause of the failure of the deep decoder in the Transformer model. Inspired by this discovery, we then propose approaches to improving it, with respect to model structure and model training, to make the deep decoder practical in NMT. Specifically, with respect to model structure, we propose a cross-attention drop mechanism to allow the decoder layers to perform their own different roles, to reduce the difficulty of deep-decoder learning. For model training, we propose a collapse reducing training approach to improve the stability and effectiveness of deep-decoder training. We experimentally evaluated our proposed Transformer NMT model structure modification and novel training methods on several popular machine translation benchmarks. The results showed that deepening the NMT model by increasing the number of decoder layers successfully prevented the deepened decoder from degrading to an unconditional language model. In contrast to prior work on deepening an NMT model on the encoder, our method can deepen the model on both the encoder and decoder at the same time, resulting in a deeper model and improved performance.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.39.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "NEWTS: A Corpus for News Topic-Focused Summarization",
        "authors": [
            "Seyed Ali Bahrainian",
            "Sheridan Feucht",
            "Carsten Eickhoff"
        ],
        "published": "2022",
        "summary": "Text summarization models are approaching human levels of fidelity. Existing benchmarking corpora provide concordant pairs of full and abridged versions of Web, news or professional content. To date, all summarization datasets operate under a one-size-fits-all paradigm that may not reflect the full range of organic summarization needs. Several recently proposed models (e.g., plug and play language models) have the capacity to condition the generated summaries on a desired range of themes. These capacities remain largely unused and unevaluated as there is no dedicated dataset that would support the task of topic-focused summarization. This paper introduces the first topical summarization corpus NEWTS, based on the well-known CNN/Dailymail dataset, and annotated via online crowd-sourcing. Each source article is paired with two reference summaries, each focusing on a different theme of the source document. We evaluate a representative range of existing techniques and analyze the effectiveness of different prompting methods.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.42.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"These capacities remain largely unused and unevaluated as there is no dedicated dataset that would support the task of topic-focused summarization.\"\n\nThis rating is chosen because the abstract mentions a limitation of LLMs (plug and play language models) in that their capacity for topic-focused summarization remains unused and unevaluated due to the lack of a dedicated dataset. However, this limitation is",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"These capacities remain largely unused and unevaluated as there is no dedicated dataset that would support the task of topic-focused summarization.\"\n\nThis rating is chosen because the abstract mentions a limitation of LLMs (plug and play language models) in that their capacity for topic-focused summarization remains unused and unevaluated due to the lack of a dedicated dataset. However, this limitation is"
    },
    {
        "title": "End-to-End Segmentation-based News Summarization",
        "authors": [
            "Yang Liu",
            "Chenguang Zhu",
            "Michael Zeng"
        ],
        "published": "2022",
        "summary": "In this paper, we bring a new way of digesting news content by introducing the task of segmenting a news article into multiple sections and generating the corresponding summary to each section. We make two contributions towards this new task. First, we create and make available a dataset, SegNews, consisting of 27k news articles with sections and aligned heading-style section summaries. Second, we propose a novel segmentation-based language generation model adapted from pre-trained language models that can jointly segment a document and produce the summary for each section. Experimental results on SegNews demonstrate that our model can outperform several state-of-the-art sequence-to-sequence generation models for this new task.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.46.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"we propose a novel segmentation-based language generation model adapted from pre-trained language models\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"we propose a novel segmentation-based language generation model adapted from pre-trained language models\""
    },
    {
        "title": "Extracting Latent Steering Vectors from Pretrained Language Models",
        "authors": [
            "Nishant Subramani",
            "Nivedita Suresh",
            "Matthew Peters"
        ],
        "published": "2022",
        "summary": "Prior work on controllable text generation has focused on learning how to control language models through trainable decoding, smart-prompt design, or fine-tuning based on a desired objective. We hypothesize that the information needed to steer the model to generate a target sentence is already encoded within the model. Accordingly, we explore a different approach altogether: extracting latent vectors directly from pretrained language model decoders without fine-tuning. Experiments show that there exist steering vectors, which, when added to the hidden states of the language model, generate a target sentence nearly perfectly (> 99 BLEU) for English sentences from a variety of domains. We show that vector arithmetic can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark, with performance comparable to models tailored to this task. We find that distances between steering vectors reflect sentence similarity when evaluated on a textual similarity benchmark (STS-B), outperforming pooled hidden states of models. Finally, we present an analysis of the intrinsic properties of the steering vectors. Taken together, our results suggest that frozen LMs can be effectively controlled through their latent steering space.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.48.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "Reframing Instructional Prompts to GPTk’s Language",
        "authors": [
            "Swaroop Mishra",
            "Daniel Khashabi",
            "Chitta Baral",
            "Yejin Choi",
            "Hannaneh Hajishirzi"
        ],
        "published": "2022",
        "summary": "What kinds of instructional prompts are easier to follow for Language Models (LMs)? We study this question by conducting extensive empirical analysis that shed light on important features of successful instructional prompts. Specifically, we study several classes of reframing techniques for manual reformulation of prompts into more effective ones. Some examples include decomposing a complex task instruction into multiple simpler tasks or itemizing instructions into sequential steps. Our experiments compare the zero-shot and few-shot performance of LMs prompted with reframed instructions on 12 NLP tasks across 6 categories. Compared with original instructions, our reframed instructions lead to significant improvements across LMs with different sizes. For example, the same reframed prompts boost few-shot performance of GPT3-series and GPT2-series by 12.5% and 6.7% respectively averaged over all tasks. Furthermore, reframed instructions reduce the number of examples required to prompt LMs in the few-shot setting. We hope these empirically-driven techniques will pave the way towards more effective future prompting algorithms.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.50.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations, but implies that LMs may struggle with complex tasks or instructions, as reframing techniques are proposed to improve their performance.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations, but implies that LMs may struggle with complex tasks or instructions, as reframing techniques are proposed to improve their performance."
    },
    {
        "title": "Inverse is Better! Fast and Accurate Prompt for Few-shot Slot Tagging",
        "authors": [
            "Yutai Hou",
            "Cheng Chen",
            "Xianzhen Luo",
            "Bohan Li",
            "Wanxiang Che"
        ],
        "published": "2022",
        "summary": "Prompting methods recently achieve impressive success in few-shot learning. These methods modify input samples with prompt sentence pieces, and decode label tokens to map samples to corresponding labels. However, such a paradigm is very inefficient for the task of slot tagging. Since slot tagging samples are multiple consecutive words in a sentence, the prompting methods have to enumerate all n-grams token spans to find all the possible slots, which greatly slows down the prediction. To tackle this, we introduce an inverse paradigm for prompting. Different from the classic prompts mapping tokens to labels, we reversely predict slot values given slot types. Such inverse prompting only requires a one-turn prediction for each slot type and greatly speeds up the prediction. Besides, we propose a novel Iterative Prediction Strategy, from which the model learns to refine predictions by considering the relations between different slot types. We find, somewhat surprisingly, the proposed method not only predicts faster but also significantly improves the effect (improve over 6.1 F1-scores on 10-shot setting) and achieves new state-of-the-art performance.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.53.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Cross-Modal Cloze Task: A New Task to Brain-to-Word Decoding",
        "authors": [
            "Shuxian Zou",
            "Shaonan Wang",
            "Jiajun Zhang",
            "Chengqing Zong"
        ],
        "published": "2022",
        "summary": "Decoding language from non-invasive brain activity has attracted increasing attention from both researchers in neuroscience and natural language processing. Due to the noisy nature of brain recordings, existing work has simplified brain-to-word decoding as a binary classification task which is to discriminate a brain signal between its corresponding word and a wrong one. This pairwise classification task, however, cannot promote the development of practical neural decoders for two reasons. First, it has to enumerate all pairwise combinations in the test set, so it is inefficient to predict a word in a large vocabulary. Second, a perfect pairwise decoder cannot guarantee the performance on direct classification. To overcome these and go a step further to a realistic neural decoder, we propose a novel Cross-Modal Cloze (CMC) task which is to predict the target word encoded in the neural image with a context as prompt. Furthermore, to address this task, we propose a general approach that leverages the pre-trained language model to predict the target word. To validate our method, we perform experiments on more than 20 participants from two brain imaging datasets. Our method achieves 28.91% top-1 accuracy and 54.19% top-5 accuracy on average across all participants, significantly outperforming several baselines. This result indicates that our model can serve as a state-of-the-art baseline for the CMC task. More importantly, it demonstrates that it is feasible to decode a certain word within a large vocabulary from its neural brain activity.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.54.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"we propose a general approach that leverages the pre-trained language model to predict the target word.\"\n\nThis paper discusses LLMs but does not mention any explicit limitation of the models in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"we propose a general approach that leverages the pre-trained language model to predict the target word.\"\n\nThis paper discusses LLMs but does not mention any explicit limitation of the models in the abstract."
    },
    {
        "title": "Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal",
        "authors": [
            "Umang Gupta",
            "Jwala Dhamala",
            "Varun Kumar",
            "Apurv Verma",
            "Yada Pruksachatkun",
            "Satyapriya Krishna",
            "Rahul Gupta",
            "Kai-Wei Chang",
            "Greg Ver Steeg",
            "Aram Galstyan"
        ],
        "published": "2022",
        "summary": "Language models excel at generating coherent text, and model compression techniques such as knowledge distillation have enabled their use in resource-constrained settings. However, these models can be biased in multiple ways, including the unfounded association of male and female genders with gender-neutral professions. Therefore, knowledge distillation without any fairness constraints may preserve or exaggerate the teacher model’s biases onto the distilled model. To this end, we present a novel approach to mitigate gender disparity in text generation by learning a fair model during knowledge distillation. We propose two modifications to the base knowledge distillation based on counterfactual role reversal—modifying teacher probabilities and augmenting the training set. We evaluate gender polarity across professions in open-ended text generated from the resulting distilled and finetuned GPT–2 models and demonstrate a substantial reduction in gender disparity with only a minor compromise in utility. Finally, we observe that language models that reduce gender polarity in language generation do not improve embedding fairness or downstream classification fairness.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.55.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these models can be biased in multiple ways, including the unfounded association of male and female genders with gender-neutral professions. Therefore, knowledge distillation without any fairness constraints may preserve or exaggerate the teacher model’s biases onto the distilled model.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these models can be biased in multiple ways, including the unfounded association of male and female genders with gender-neutral professions. Therefore, knowledge distillation without any fairness constraints may preserve or exaggerate the teacher model’s biases onto the distilled model.\""
    },
    {
        "title": "Hierarchical Inductive Transfer for Continual Dialogue Learning",
        "authors": [
            "Shaoxiong Feng",
            "Xuancheng Ren",
            "Kan Li",
            "Xu Sun"
        ],
        "published": "2022",
        "summary": "Pre-trained models have achieved excellent performance on the dialogue task. However, for the continual increase of online chit-chat scenarios, directly fine-tuning these models for each of the new tasks not only explodes the capacity of the dialogue system on the embedded devices but also causes knowledge forgetting on pre-trained models and knowledge interference among diverse dialogue tasks. In this work, we propose a hierarchical inductive transfer framework to learn and deploy the dialogue skills continually and efficiently. First, we introduce the adapter module into pre-trained models for learning new dialogue tasks. As the only trainable module, it is beneficial for the dialogue system on the embedded devices to acquire new dialogue skills with negligible additional parameters. Then, for alleviating knowledge interference between tasks yet benefiting the regularization between them, we further design hierarchical inductive transfer that enables new tasks to use general knowledge in the base adapter without being misled by diverse knowledge in task-specific adapters. Empirical evaluation and analysis indicate that our framework obtains comparable performance under deployment-friendly model capacity.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.57.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"directly fine-tuning these models for each of the new tasks not only explodes the capacity of the dialogue system on the embedded devices but also causes knowledge forgetting on pre-trained models and knowledge interference among diverse dialogue tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"directly fine-tuning these models for each of the new tasks not only explodes the capacity of the dialogue system on the embedded devices but also causes knowledge forgetting on pre-trained models and knowledge interference among diverse dialogue tasks.\""
    },
    {
        "title": "Why Exposure Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation",
        "authors": [
            "Kushal Arora",
            "Layla El Asri",
            "Hareesh Bahuleyan",
            "Jackie Cheung"
        ],
        "published": "2022",
        "summary": "Current language generation models suffer from issues such as repetition, incoherence, and hallucinations. An often-repeated hypothesis for this brittleness of generation models is that it is caused by the training and the generation procedure mismatch, also referred to as exposure bias. In this paper, we verify this hypothesis by analyzing exposure bias from an imitation learning perspective. We show that exposure bias leads to an accumulation of errors during generation, analyze why perplexity fails to capture this accumulation of errors, and empirically show that this accumulation results in poor generation quality.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.58.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Current language generation models suffer from issues such as repetition, incoherence, and hallucinations... exposure bias leads to an accumulation of errors during generation, analyze why perplexity fails to capture this accumulation of errors, and empirically show that this accumulation results in poor generation quality.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Current language generation models suffer from issues such as repetition, incoherence, and hallucinations... exposure bias leads to an accumulation of errors during generation, analyze why perplexity fails to capture this accumulation of errors, and empirically show that this accumulation results in poor generation quality.\""
    },
    {
        "title": "MIMICause: Representation and automatic extraction of causal relation types from clinical notes",
        "authors": [
            "Vivek Khetan",
            "Md Imbesat Rizvi",
            "Jessica Huber",
            "Paige Bartusiak",
            "Bogdan Sacaleanu",
            "Andrew Fano"
        ],
        "published": "2022",
        "summary": "Understanding causal narratives communicated in clinical notes can help make strides towards personalized healthcare. Extracted causal information from clinical notes can be combined with structured EHR data such as patients’ demographics, diagnoses, and medications. This will enhance healthcare providers’ ability to identify aspects of a patient’s story communicated in the clinical notes and help make more informed decisions. In this work, we propose annotation guidelines, develop an annotated corpus and provide baseline scores to identify types and direction of causal relations between a pair of biomedical concepts in clinical notes; communicated implicitly or explicitly, identified either in a single sentence or across multiple sentences. We annotate a total of 2714 de-identified examples sampled from the 2018 n2c2 shared task dataset and train four different language model based architectures. Annotation based on our guidelines achieved a high inter-annotator agreement i.e. Fleiss’ kappa (𝜅) score of 0.72, and our model for identification of causal relations achieved a macro F1 score of 0.56 on the test data. The high inter-annotator agreement for clinical text shows the quality of our annotation guidelines while the provided baseline F1 score sets the direction for future research towards understanding narratives in clinical texts.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.63.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Compressing Sentence Representation for Semantic Retrieval via Homomorphic Projective Distillation",
        "authors": [
            "Xuandong Zhao",
            "Zhiguo Yu",
            "Ming Wu",
            "Lei Li"
        ],
        "published": "2022",
        "summary": "How to learn highly compact yet effective sentence representation? Pre-trained language models have been effective in many NLP tasks. However, these models are often huge and produce large sentence embeddings. Moreover, there is a big performance gap between large and small models. In this paper, we propose Homomorphic Projective Distillation (HPD) to learn compressed sentence embeddings. Our method augments a small Transformer encoder model with learnable projection layers to produce compact representations while mimicking a large pre-trained language model to retain the sentence representation quality. We evaluate our method with different model sizes on both semantic textual similarity (STS) and semantic retrieval (SR) tasks. Experiments show that our method achieves 2.7-4.5 points performance gain on STS tasks compared with previous best representations of the same size. In SR tasks, our method improves retrieval speed (8.2×) and memory usage (8.0×) compared with state-of-the-art large models. Our implementation is available at https://github.com/XuandongZhao/HPD.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.64.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these models are often huge and produce large sentence embeddings. Moreover, there is a big performance gap between large and small models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these models are often huge and produce large sentence embeddings. Moreover, there is a big performance gap between large and small models.\""
    },
    {
        "title": "DeepStruct: Pretraining of Language Models for Structure Prediction",
        "authors": [
            "Chenguang Wang",
            "Xiao Liu",
            "Zui Chen",
            "Haoyun Hong",
            "Jie Tang",
            "Dawn Song"
        ],
        "published": "2022",
        "summary": "We introduce a method for improving the structural understanding abilities of language models. Unlike previous approaches that finetune the models with task-specific augmentation, we pretrain language models to generate structures from the text on a collection of task-agnostic corpora. Our structure pretraining enables zero-shot transfer of the learned knowledge that models have about the structure tasks. We study the performance of this approach on 28 datasets, spanning 10 structure prediction tasks including open information extraction, joint entity and relation extraction, named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, factual probe, intent detection, and dialogue state tracking. We further enhance the pretraining with the task-specific training sets. We show that a 10B parameter language model transfers non-trivially to most tasks and obtains state-of-the-art performance on 21 of 28 datasets that we evaluate. Our code and datasets will be made publicly available.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.67.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations of LLMs are mentioned in the abstract, but the focus on improving structural understanding abilities implies that current LLMs may have limitations in this area.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations of LLMs are mentioned in the abstract, but the focus on improving structural understanding abilities implies that current LLMs may have limitations in this area."
    },
    {
        "title": "DS-TOD: Efficient Domain Specialization for Task-Oriented Dialog",
        "authors": [
            "Chia-Chien Hung",
            "Anne Lauscher",
            "Simone Ponzetto",
            "Goran Glavaš"
        ],
        "published": "2022",
        "summary": "Recent work has shown that self-supervised dialog-specific pretraining on large conversational datasets yields substantial gains over traditional language modeling (LM) pretraining in downstream task-oriented dialog (TOD). These approaches, however, exploit general dialogic corpora (e.g., Reddit) and thus presumably fail to reliably embed domain-specific knowledge useful for concrete downstream TOD domains. In this work, we investigate the effects of domain specialization of pretrained language models (PLMs) for TOD. Within our DS-TOD framework, we first automatically extract salient domain-specific terms, and then use them to construct DomainCC and DomainReddit – resources that we leverage for domain-specific pretraining, based on (i) masked language modeling (MLM) and (ii) response selection (RS) objectives, respectively. We further propose a resource-efficient and modular domain specialization by means of domain adapters – additional parameter-light layers in which we encode the domain knowledge. Our experiments with prominent TOD tasks – dialog state tracking (DST) and response retrieval (RR) – encompassing five domains from the MultiWOZ benchmark demonstrate the effectiveness of DS-TOD. Moreover, we show that the light-weight adapter-based specialization (1) performs comparably to full fine-tuning in single domain setups and (2) is particularly suitable for multi-domain specialization, where besides advantageous computational footprint, it can offer better TOD performance.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.72.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"These approaches, however, exploit general dialogic corpora (e.g., Reddit) and thus presumably fail to reliably embed domain-specific knowledge useful for concrete downstream TOD domains.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"These approaches, however, exploit general dialogic corpora (e.g., Reddit) and thus presumably fail to reliably embed domain-specific knowledge useful for concrete downstream TOD domains.\""
    },
    {
        "title": "Distinguishing Non-natural from Natural Adversarial Samples for More Robust Pre-trained Language Model",
        "authors": [
            "Jiayi Wang",
            "Rongzhou Bao",
            "Zhuosheng Zhang",
            "Hai Zhao"
        ],
        "published": "2022",
        "summary": "Recently, the problem of robustness of pre-trained language models (PrLMs) has received increasing research interest. Latest studies on adversarial attacks achieve high attack success rates against PrLMs, claiming that PrLMs are not robust. However, we find that the adversarial samples that PrLMs fail are mostly non-natural and do not appear in reality. We question the validity of the current evaluation of robustness of PrLMs based on these non-natural adversarial samples and propose an anomaly detector to evaluate the robustness of PrLMs with more natural adversarial samples. We also investigate two applications of the anomaly detector: (1) In data augmentation, we employ the anomaly detector to force generating augmented data that are distinguished as non-natural, which brings larger gains to the accuracy of PrLMs. (2) We apply the anomaly detector to a defense framework to enhance the robustness of PrLMs. It can be used to defend all types of attacks and achieves higher accuracy on both adversarial samples and compliant samples than other defense frameworks.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.73.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "not extracted",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, we find that the adversarial samples that PrLMs fail are mostly non-natural and do not appear in reality.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs:  \nEvidence: \"However, we find that the adversarial samples that PrLMs fail are mostly non-natural and do not appear in reality.\""
    },
    {
        "title": "From BERT‘s Point of View: Revealing the Prevailing Contextual Differences",
        "authors": [
            "Carolin M. Schuster",
            "Simon Hegelich"
        ],
        "published": "2022",
        "summary": "Though successfully applied in research and industry large pretrained language models of the BERT family are not yet fully understood. While much research in the field of BERTology has tested whether specific knowledge can be extracted from layer activations, we invert the popular probing design to analyze the prevailing differences and clusters in BERT’s high dimensional space. By extracting coarse features from masked token representations and predicting them by probing models with access to only partial information we can apprehend the variation from ‘BERT’s point of view’. By applying our new methodology to different datasets we show how much the differences can be described by syntax but further how they are to a great extent shaped by the most simple positional information.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.89.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Though successfully applied in research and industry large pretrained language models of the BERT family are not yet fully understood.\"\n\nThis abstract briefly mentions a limitation of understanding large pretrained language models, but does not explore it in depth.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Though successfully applied in research and industry large pretrained language models of the BERT family are not yet fully understood.\"\n\nThis abstract briefly mentions a limitation of understanding large pretrained language models, but does not explore it in depth."
    },
    {
        "title": "CoCoLM: Complex Commonsense Enhanced Language Model with Discourse Relations",
        "authors": [
            "Changlong Yu",
            "Hongming Zhang",
            "Yangqiu Song",
            "Wilfred Ng"
        ],
        "published": "2022",
        "summary": "Large-scale pre-trained language models have demonstrated strong knowledge representation ability. However, recent studies suggest that even though these giant models contain rich simple commonsense knowledge (e.g., bird can fly and fish can swim.), they often struggle with complex commonsense knowledge that involves multiple eventualities (verb-centric phrases, e.g., identifying the relationship between “Jim yells at Bob” and “Bob is upset”). To address this issue, in this paper, we propose to help pre-trained language models better incorporate complex commonsense knowledge. Unlike direct fine-tuning approaches, we do not focus on a specific task and instead propose a general language model named CoCoLM. Through the careful training over a large-scale eventuality knowledge graph ASER, we successfully teach pre-trained language models (i.e., BERT and RoBERTa) rich multi-hop commonsense knowledge among eventualities. Experiments on multiple commonsense tasks that require the correct understanding of eventualities demonstrate the effectiveness of CoCoLM.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.93.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, recent studies suggest that even though these giant models contain rich simple commonsense knowledge... they often struggle with complex commonsense knowledge that involves multiple eventualities\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, recent studies suggest that even though these giant models contain rich simple commonsense knowledge... they often struggle with complex commonsense knowledge that involves multiple eventualities\""
    },
    {
        "title": "MDCSpell: A Multi-task Detector-Corrector Framework for Chinese Spelling Correction",
        "authors": [
            "Chenxi Zhu",
            "Ziqiang Ying",
            "Boyu Zhang",
            "Feng Mao"
        ],
        "published": "2022",
        "summary": "Chinese Spelling Correction (CSC) is a task to detect and correct misspelled characters in Chinese texts. CSC is challenging since many Chinese characters are visually or phonologically similar but with quite different semantic meanings. Many recent works use BERT-based language models to directly correct each character of the input sentence. However, these methods can be sub-optimal since they correct every character of the sentence only by the context which is easily negatively affected by the misspelled characters. Some other works propose to use an error detector to guide the correction by masking the detected errors. Nevertheless, these methods dampen the visual or phonological features from the misspelled characters which could be critical for correction. In this work, we propose a novel general detector-corrector multi-task framework where the corrector uses BERT to capture the visual and phonological features from each character in the raw sentence and uses a late fusion strategy to fuse the hidden states of the corrector with that of the detector to minimize the negative impact from the misspelled characters. Comprehensive experiments on benchmarks demonstrate that our proposed method can significantly outperform the state-of-the-art methods in the CSC task.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.98.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these methods can be sub-optimal since they correct every character of the sentence only by the context which is easily negatively affected by the misspelled characters.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these methods can be sub-optimal since they correct every character of the sentence only by the context which is easily negatively affected by the misspelled characters.\""
    },
    {
        "title": "HiStruct+: Improving Extractive Text Summarization with Hierarchical Structure Information",
        "authors": [
            "Qian Ruan",
            "Malte Ostendorff",
            "Georg Rehm"
        ],
        "published": "2022",
        "summary": "Transformer-based language models usually treat texts as linear sequences. However, most texts also have an inherent hierarchical structure, i.e., parts of a text can be identified using their position in this hierarchy. In addition, section titles usually indicate the common topic of their respective sentences. We propose a novel approach to formulate, extract, encode and inject hierarchical structure information explicitly into an extractive summarization model based on a pre-trained, encoder-only Transformer language model (HiStruct+ model), which improves SOTA ROUGEs for extractive summarization on PubMed and arXiv substantially. Using various experimental settings on three datasets (i.e., CNN/DailyMail, PubMed and arXiv), our HiStruct+ model outperforms a strong baseline collectively, which differs from our model only in that the hierarchical structure information is not injected. It is also observed that the more conspicuous hierarchical structure the dataset has, the larger improvements our method gains. The ablation study demonstrates that the hierarchical position information is the main contributor to our model’s SOTA performance.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.102.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Transformer-based language models usually treat texts as linear sequences. However, most texts also have an inherent hierarchical structure, i.e., parts of a text can be identified using their position in this hierarchy.\"\n\nThis rating is given because the abstract mentions a limitation of transformer-based language models (treating texts as linear sequences) but does not elaborate on it in detail, instead focusing on the proposed",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Transformer-based language models usually treat texts as linear sequences. However, most texts also have an inherent hierarchical structure, i.e., parts of a text can be identified using their position in this hierarchy.\"\n\nThis rating is given because the abstract mentions a limitation of transformer-based language models (treating texts as linear sequences) but does not elaborate on it in detail, instead focusing on the proposed"
    },
    {
        "title": "Multi-Stage Prompting for Knowledgeable Dialogue Generation",
        "authors": [
            "Zihan Liu",
            "Mostofa Patwary",
            "Ryan Prenger",
            "Shrimai Prabhumoye",
            "Wei Ping",
            "Mohammad Shoeybi",
            "Bryan Catanzaro"
        ],
        "published": "2022",
        "summary": "Existing knowledge-grounded dialogue systems typically use finetuned versions of a pretrained language model (LM) and large-scale knowledge bases. These models typically fail to generalize on topics outside of the knowledge base, and require maintaining separate potentially large checkpoints each time finetuning is needed. In this paper, we aim to address these limitations by leveraging the inherent knowledge stored in the pretrained LM as well as its powerful generation ability. We propose a multi-stage prompting approach to generate knowledgeable responses from a single pretrained LM. We first prompt the LM to generate knowledge based on the dialogue context. Then, we further prompt it to generate responses based on the dialogue context and the previously generated knowledge. Results show that our knowledge generator outperforms the state-of-the-art retrieval-based model by 5.8% when combining knowledge relevance and correctness. In addition, our multi-stage prompting outperforms the finetuning-based dialogue model in terms of response knowledgeability and engagement by up to 10% and 5%, respectively. Furthermore, we scale our model up to 530 billion parameters and demonstrate that larger LMs improve the generation correctness score by up to 10%, and response relevance, knowledgeability and engagement by up to 10%. Our code is available at: https://github.com/NVIDIA/Megatron-LM.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.104.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing knowledge-grounded dialogue systems typically use finetuned versions of a pretrained language model (LM) and large-scale knowledge bases. These models typically fail to generalize on topics outside of the knowledge base, and require maintaining separate potentially large checkpoints each time finetuning is needed.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing knowledge-grounded dialogue systems typically use finetuned versions of a pretrained language model (LM) and large-scale knowledge bases. These models typically fail to generalize on topics outside of the knowledge base, and require maintaining separate potentially large checkpoints each time finetuning is needed.\""
    },
    {
        "title": "Coloring the Blank Slate: Pre-training Imparts a Hierarchical Inductive Bias to Sequence-to-sequence Models",
        "authors": [
            "Aaron Mueller",
            "Robert Frank",
            "Tal Linzen",
            "Luheng Wang",
            "Sebastian Schuster"
        ],
        "published": "2022",
        "summary": "Relations between words are governed by hierarchical structure rather than linear ordering. Sequence-to-sequence (seq2seq) models, despite their success in downstream NLP applications, often fail to generalize in a hierarchy-sensitive manner when performing syntactic transformations—for example, transforming declarative sentences into questions. However, syntactic evaluations of seq2seq models have only observed models that were not pre-trained on natural language data before being trained to perform syntactic transformations, in spite of the fact that pre-training has been found to induce hierarchical linguistic generalizations in language models; in other words, the syntactic capabilities of seq2seq models may have been greatly understated. We address this gap using the pre-trained seq2seq models T5 and BART, as well as their multilingual variants mT5 and mBART. We evaluate whether they generalize hierarchically on two transformations in two languages: question formation and passivization in English and German. We find that pre-trained seq2seq models generalize hierarchically when performing syntactic transformations, whereas models trained from scratch on syntactic transformations do not. This result presents evidence for the learnability of hierarchical syntactic information from non-annotated natural language text while also demonstrating that seq2seq models are capable of syntactic generalization, though only after exposure to much more language data than human learners receive.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.106.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Sequence-to-sequence (seq2seq) models... often fail to generalize in a hierarchy-sensitive manner when performing syntactic transformations—for example, transforming declarative sentences into questions.\"\n\nThis abstract mentions a limitation of sequence-to-sequence models, but it is not the primary focus of the paper. The paper aims to address this limitation by evaluating pre-trained seq2seq models, but the",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Sequence-to-sequence (seq2seq) models... often fail to generalize in a hierarchy-sensitive manner when performing syntactic transformations—for example, transforming declarative sentences into questions.\"\n\nThis abstract mentions a limitation of sequence-to-sequence models, but it is not the primary focus of the paper. The paper aims to address this limitation by evaluating pre-trained seq2seq models, but the"
    },
    {
        "title": "Finding the Dominant Winning Ticket in Pre-Trained Language Models",
        "authors": [
            "Zhuocheng Gong",
            "Di He",
            "Yelong Shen",
            "Tie-Yan Liu",
            "Weizhu Chen",
            "Dongyan Zhao",
            "Ji-Rong Wen",
            "Rui Yan"
        ],
        "published": "2022",
        "summary": "The Lottery Ticket Hypothesis suggests that for any over-parameterized model, a small subnetwork exists to achieve competitive performance compared to the backbone architecture. In this paper, we study whether there is a winning lottery ticket for pre-trained language models, which allow the practitioners to fine-tune the parameters in the ticket but achieve good downstream performance. To achieve this, we regularize the fine-tuning process with L1 distance and explore the subnetwork structure (what we refer to as the “dominant winning ticket”). Empirically, we show that (a) the dominant winning ticket can achieve performance that is comparable with that of the full-parameter model, (b) the dominant winning ticket is transferable across different tasks, (c) and the dominant winning ticket has a natural structure within each parameter matrix. Strikingly, we find that a dominant winning ticket that takes up 0.05% of the parameters can already achieve satisfactory performance, indicating that the PLM is significantly reducible during fine-tuning.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.115.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"a dominant winning ticket that takes up 0.05% of the parameters can already achieve satisfactory performance, indicating that the PLM is significantly reducible during fine-tuning.\"\n\nThis rating is based on the fact that the paper mentions a limitation of pre-trained language models (being over-parameterized) and discusses the potential of finding a smaller subnetwork (the dominant winning ticket)",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"a dominant winning ticket that takes up 0.05% of the parameters can already achieve satisfactory performance, indicating that the PLM is significantly reducible during fine-tuning.\"\n\nThis rating is based on the fact that the paper mentions a limitation of pre-trained language models (being over-parameterized) and discusses the potential of finding a smaller subnetwork (the dominant winning ticket)"
    },
    {
        "title": "Thai Nested Named Entity Recognition Corpus",
        "authors": [
            "Weerayut Buaphet",
            "Can Udomcharoenchaikit",
            "Peerat Limkonchotiwat",
            "Attapol Rutherford",
            "Sarana Nutanong"
        ],
        "published": "2022",
        "summary": "This paper presents the first Thai Nested Named Entity Recognition (N-NER) dataset. Thai N-NER consists of 264,798 mentions, 104 classes, and a maximum depth of 8 layers obtained from 4,894 documents in the domains of news articles and restaurant reviews. Our work, to the best of our knowledge, presents the largest non-English N-NER dataset and the first non-English one with fine-grained classes. To understand the new challenges our proposed dataset brings to the field, we conduct an experimental study on (i) cutting edge N-NER models with the state-of-the-art accuracy in English and (ii) baseline methods based on well-known language model architectures. From the experimental results, we obtained two key findings. First, all models produced poor F1 scores in the tail region of the class distribution. There is little or no performance improvement provided by these models with respect to the baseline methods with our Thai dataset. These findings suggest that further investigation is required to make a multilingual N-NER solution that works well across different languages.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.116.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"There is little or no performance improvement provided by these models with respect to the baseline methods with our Thai dataset.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"There is little or no performance improvement provided by these models with respect to the baseline methods with our Thai dataset.\""
    },
    {
        "title": "Semantically Distributed Robust Optimization for Vision-and-Language Inference",
        "authors": [
            "Tejas Gokhale",
            "Abhishek Chaudhary",
            "Pratyay Banerjee",
            "Chitta Baral",
            "Yezhou Yang"
        ],
        "published": "2022",
        "summary": "Analysis of vision-and-language models has revealed their brittleness under linguistic phenomena such as paraphrasing, negation, textual entailment, and word substitutions with synonyms or antonyms. While data augmentation techniques have been designed to mitigate against these failure modes, methods that can integrate this knowledge into the training pipeline remain under-explored. In this paper, we present SDRO, a model-agnostic method that utilizes a set linguistic transformations in a distributed robust optimization setting, along with an ensembling technique to leverage these transformations during inference.Experiments on benchmark datasets with images (NLVR2) and video (VIOLIN) demonstrate performance improvements as well as robustness to adversarial attacks.Experiments on binary VQA explore the generalizability of this method to other V&L tasks.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.118.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Analysis of vision-and-language models has revealed their brittleness under linguistic phenomena such as paraphrasing, negation, textual entailment, and word substitutions with synonyms or antonyms.\"\n\n(Note: Although the paper discusses limitations of vision-and-language models, which include LLMs, it does not focus extensively on these limitations, but rather uses them to motivate the proposed method.)",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Analysis of vision-and-language models has revealed their brittleness under linguistic phenomena such as paraphrasing, negation, textual entailment, and word substitutions with synonyms or antonyms.\"\n\n(Note: Although the paper discusses limitations of vision-and-language models, which include LLMs, it does not focus extensively on these limitations, but rather uses them to motivate the proposed method.)"
    },
    {
        "title": "Learning from Missing Relations: Contrastive Learning with Commonsense Knowledge Graphs for Commonsense Inference",
        "authors": [
            "Yong-Ho Jung",
            "Jun-Hyung Park",
            "Joon-Young Choi",
            "Mingyu Lee",
            "Junho Kim",
            "Kang-Min Kim",
            "SangKeun Lee"
        ],
        "published": "2022",
        "summary": "Commonsense inference poses a unique challenge to reason and generate the physical, social, and causal conditions of a given event. Existing approaches to commonsense inference utilize commonsense transformers, which are large-scale language models that learn commonsense knowledge graphs. However, they suffer from a lack of coverage and expressive diversity of the graphs, resulting in a degradation of the representation quality. In this paper, we focus on addressing missing relations in commonsense knowledge graphs, and propose a novel contrastive learning framework called SOLAR. Our framework contrasts sets of semantically similar and dissimilar events, learning richer inferential knowledge compared to existing approaches. Empirical results demonstrate the efficacy of SOLAR in commonsense inference of diverse commonsense knowledge graphs. Specifically, SOLAR outperforms the state-of-the-art commonsense transformer on commonsense inference with ConceptNet by 1.84% on average among 8 automatic evaluation metrics. In-depth analysis of SOLAR sheds light on the effects of the missing relations utilized in learning commonsense knowledge graphs.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.119.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing approaches to commonsense inference utilize commonsense transformers, which are large-scale language models that learn commonsense knowledge graphs. However, they suffer from a lack of coverage and expressive diversity of the graphs, resulting in a degradation of the representation quality.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing approaches to commonsense inference utilize commonsense transformers, which are large-scale language models that learn commonsense knowledge graphs. However, they suffer from a lack of coverage and expressive diversity of the graphs, resulting in a degradation of the representation quality.\""
    },
    {
        "title": "TegTok: Augmenting Text Generation via Task-specific and Open-world Knowledge",
        "authors": [
            "Chao-Hong Tan",
            "Jia-Chen Gu",
            "Chongyang Tao",
            "Zhen-Hua Ling",
            "Can Xu",
            "Huang Hu",
            "Xiubo Geng",
            "Daxin Jiang"
        ],
        "published": "2022",
        "summary": "Generating natural and informative texts has been a long-standing problem in NLP. Much effort has been dedicated into incorporating pre-trained language models (PLMs) with various open-world knowledge, such as knowledge graphs or wiki pages. However, their ability to access and manipulate the task-specific knowledge is still limited on downstream tasks, as this type of knowledge is usually not well covered in PLMs and is hard to acquire. To address the problem, we propose augmenting TExt Generation via Task-specific and Open-world Knowledge (TegTok) in a unified framework. Our model selects knowledge entries from two types of knowledge sources through dense retrieval and then injects them into the input encoding and output decoding stages respectively on the basis of PLMs. With the help of these two types of knowledge, our model can learn what and how to generate. Experiments on two text generation tasks of dialogue generation and question generation, and on two datasets show that our method achieves better performance than various baseline models.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.125.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, their ability to access and manipulate the task-specific knowledge is still limited on downstream tasks, as this type of knowledge is usually not well covered in PLMs and is hard to acquire.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, their ability to access and manipulate the task-specific knowledge is still limited on downstream tasks, as this type of knowledge is usually not well covered in PLMs and is hard to acquire.\""
    },
    {
        "title": "N-Shot Learning for Augmenting Task-Oriented Dialogue State Tracking",
        "authors": [
            "Taha Aksu",
            "Zhengyuan Liu",
            "Min-Yen Kan",
            "Nancy Chen"
        ],
        "published": "2022",
        "summary": "Augmentation of task-oriented dialogues has followed standard methods used for plain-text such as back-translation, word-level manipulation, and paraphrasing despite its richly annotated structure. In this work, we introduce an augmentation framework that utilizes belief state annotations to match turns from various dialogues and form new synthetic dialogues in a bottom-up manner. Unlike other augmentation strategies, it operates with as few as five examples. Our augmentation strategy yields significant improvements when both adapting a DST model to a new domain, and when adapting a language model to the DST task, on evaluations with TRADE and TOD-BERT models. Further analysis shows that our model performs better on seen values during training, and it is also more robust to unseen values. We conclude that exploiting belief state annotations enhances dialogue augmentation and results in improved models in n-shot training scenarios.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.131.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"when adapting a language model to the DST task\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"when adapting a language model to the DST task\""
    },
    {
        "title": "Morphosyntactic Tagging with Pre-trained Language Models for Arabic and its Dialects",
        "authors": [
            "Go Inoue",
            "Salam Khalifa",
            "Nizar Habash"
        ],
        "published": "2022",
        "summary": "We present state-of-the-art results on morphosyntactic tagging across different varieties of Arabic using fine-tuned pre-trained transformer language models. Our models consistently outperform existing systems in Modern Standard Arabic and all the Arabic dialects we study, achieving 2.6% absolute improvement over the previous state-of-the-art in Modern Standard Arabic, 2.8% in Gulf, 1.6% in Egyptian, and 8.3% in Levantine. We explore different training setups for fine-tuning pre-trained transformer language models, including training data size, the use of external linguistic resources, and the use of annotated data from other dialects in a low-resource scenario. Our results show that strategic fine-tuning using datasets from other high-resource dialects is beneficial for a low-resource dialect. Additionally, we show that high-quality morphological analyzers as external linguistic resources are beneficial especially in low-resource settings.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.135.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis",
        "authors": [
            "Shaobo Li",
            "Xiaoguang Li",
            "Lifeng Shang",
            "Zhenhua Dong",
            "Chengjie Sun",
            "Bingquan Liu",
            "Zhenzhou Ji",
            "Xin Jiang",
            "Qun Liu"
        ],
        "published": "2022",
        "summary": "Recently, there has been a trend to investigate the factual knowledge captured by Pre-trained Language Models (PLMs). Many works show the PLMs’ ability to fill in the missing factual words in cloze-style prompts such as ”Dante was born in [MASK].” However, it is still a mystery how PLMs generate the results correctly: relying on effective clues or shortcut patterns? We try to answer this question by a causal-inspired analysis that quantitatively measures and evaluates the word-level patterns that PLMs depend on to generate the missing words. We check the words that have three typical associations with the missing words: knowledge-dependent, positionally close, and highly co-occurred. Our analysis shows: (1) PLMs generate the missing factual words more by the positionally close and highly co-occurred words than the knowledge-dependent words; (2) the dependence on the knowledge-dependent words is more effective than the positionally close and highly co-occurred words. Accordingly, we conclude that the PLMs capture the factual knowledge ineffectively because of depending on the inadequate associations.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.136.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"we conclude that the PLMs capture the factual knowledge ineffectively because of depending on the inadequate associations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"we conclude that the PLMs capture the factual knowledge ineffectively because of depending on the inadequate associations.\""
    },
    {
        "title": "Metadata Shaping: A Simple Approach for Knowledge-Enhanced Language Models",
        "authors": [
            "Simran Arora",
            "Sen Wu",
            "Enci Liu",
            "Christopher Re"
        ],
        "published": "2022",
        "summary": "Popular language models (LMs) struggle to capture knowledge about rare tail facts and entities. Since widely used systems such as search and personal-assistants must support the long tail of entities that users ask about, there has been significant effort towards enhancing these base LMs with factual knowledge. We observe proposed methods typically start with a base LM and data that has been annotated with entity metadata, then change the model, by modifying the architecture or introducing auxiliary loss terms to better capture entity knowledge. In this work, we question this typical process and ask to what extent can we match the quality of model modifications, with a simple alternative: using a base LM and only changing the data. We propose metadata shaping, a method which inserts substrings corresponding to the readily available entity metadata, e.g. types and descriptions, into examples at train and inference time based on mutual information. Despite its simplicity, metadata shaping is quite effective. On standard evaluation benchmarks for knowledge-enhanced LMs, the method exceeds the base-LM baseline by an average of 4.3 F1 points and achieves state-of-the-art results. We further show the gains are on average 4.4x larger for the slice of examples containing tail vs. popular entities.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.137.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Popular language models (LMs) struggle to capture knowledge about rare tail facts and entities.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Popular language models (LMs) struggle to capture knowledge about rare tail facts and entities.\""
    },
    {
        "title": "Enhancing Natural Language Representation with Large-Scale Out-of-Domain Commonsense",
        "authors": [
            "Wanyun Cui",
            "Xingran Chen"
        ],
        "published": "2022",
        "summary": "We study how to enhance text representation via textual commonsense. We point out that commonsense has the nature of domain discrepancy. Namely, commonsense has different data formats and is domain-independent from the downstream task. This nature brings challenges to introducing commonsense in general text understanding tasks. A typical method of introducing textual knowledge is continuing pre-training over the commonsense corpus. However, it will cause catastrophic forgetting to the downstream task due to the domain discrepancy. In addition, previous methods of directly using textual descriptions as extra input information cannot apply to large-scale commonsense. In this paper, we propose to use large-scale out-of-domain commonsense to enhance text representation. In order to effectively incorporate the commonsense, we proposed OK-Transformer (Out-of-domain Knowledge enhanced Transformer). OK-Transformer effectively integrates commonsense descriptions and enhances them to the target text representation. In addition, OK-Transformer can adapt to the Transformer-based language models (e.g. BERT, RoBERTa) for free, without pre-training on large-scale unsupervised corpora. We have verified the effectiveness of OK-Transformer in multiple applications such as commonsense reasoning, general text classification, and low-resource commonsense settings.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.138.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it will cause catastrophic forgetting to the downstream task due to the domain discrepancy.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, it will cause catastrophic forgetting to the downstream task due to the domain discrepancy.\""
    },
    {
        "title": "Hey AI, Can You Solve Complex Tasks by Talking to Agents?",
        "authors": [
            "Tushar Khot",
            "Kyle Richardson",
            "Daniel Khashabi",
            "Ashish Sabharwal"
        ],
        "published": "2022",
        "summary": "Training giant models from scratch for each complex task is resource- and data-inefficient. To help develop models that can leverage existing systems, we propose a new challenge: Learning to solve complex tasks by communicating with existing agents (or models) in natural language. We design a synthetic benchmark, CommaQA, with three complex reasoning tasks (explicit, implicit, numeric) designed to be solved by communicating with existing QA agents. For instance, using text and table QA agents to answer questions such as “Who had the longest javelin throw from USA?”. We show that black-box models struggle to learn this task from scratch (accuracy under 50%) even with access to each agent’s knowledge and gold facts supervision. In contrast, models that learn to communicate with agents outperform black-box models, reaching scores of 100% when given gold decomposition supervision. However, we show that the challenge of learning to solve complex tasks by communicating with existing agents without relying on any auxiliary supervision or data still remains highly elusive. We will release CommaQA, along with a compositional generalization test split, to advance research in this direction.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.142.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, we show that the challenge of learning to solve complex tasks by communicating with existing agents without relying on any auxiliary supervision or data still remains highly elusive.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, we show that the challenge of learning to solve complex tasks by communicating with existing agents without relying on any auxiliary supervision or data still remains highly elusive.\""
    },
    {
        "title": "Improving Relation Extraction through Syntax-induced Pre-training with Dependency Masking",
        "authors": [
            "Yuanhe Tian",
            "Yan Song",
            "Fei Xia"
        ],
        "published": "2022",
        "summary": "Relation extraction (RE) is an important natural language processing task that predicts the relation between two given entities, where a good understanding of the contextual information is essential to achieve an outstanding model performance. Among different types of contextual information, the auto-generated syntactic information (namely, word dependencies) has shown its effectiveness for the task. However, most existing studies require modifications to the existing baseline architectures (e.g., adding new components, such as GCN, on the top of an encoder) to leverage the syntactic information. To offer an alternative solution, we propose to leverage syntactic information to improve RE by training a syntax-induced encoder on auto-parsed data through dependency masking. Specifically, the syntax-induced encoder is trained by recovering the masked dependency connections and types in first, second, and third orders, which significantly differs from existing studies that train language models or word embeddings by predicting the context words along the dependency paths. Experimental results on two English benchmark datasets, namely, ACE2005EN and SemEval 2010 Task 8 datasets, demonstrate the effectiveness of our approach for RE, where our approach outperforms strong baselines and achieve state-of-the-art results on both datasets.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.147.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Diversifying Content Generation for Commonsense Reasoning with Mixture of Knowledge Graph Experts",
        "authors": [
            "Wenhao Yu",
            "Chenguang Zhu",
            "Lianhui Qin",
            "Zhihan Zhang",
            "Tong Zhao",
            "Meng Jiang"
        ],
        "published": "2022",
        "summary": "Generative commonsense reasoning (GCR) in natural language is to reason about the commonsense while generating coherent text. Recent years have seen a surge of interest in improving the generation quality of commonsense reasoning tasks. Nevertheless, these approaches have seldom investigated diversity in the GCR tasks, which aims to generate alternative explanations for a real-world situation or predict all possible outcomes. Diversifying GCR is challenging as it expects to generate multiple outputs that are not only semantically different but also grounded in commonsense knowledge. In this paper, we propose MoKGE, a novel method that diversifies the generative reasoning by a mixture of expert (MoE) strategy on commonsense knowledge graphs (KG). A set of knowledge experts seek diverse reasoning on KG to encourage various generation outputs. Empirical experiments demonstrated that MoKGE can significantly improve the diversity while achieving on par performance on accuracy on two GCR benchmarks, based on both automatic and human evaluations.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.149.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Dict-BERT: Enhancing Language Model Pre-training with Dictionary",
        "authors": [
            "Wenhao Yu",
            "Chenguang Zhu",
            "Yuwei Fang",
            "Donghan Yu",
            "Shuohang Wang",
            "Yichong Xu",
            "Michael Zeng",
            "Meng Jiang"
        ],
        "published": "2022",
        "summary": "Pre-trained language models (PLMs) aim to learn universal language representations by conducting self-supervised training tasks on large-scale corpora. Since PLMs capture word semantics in different contexts, the quality of word representations highly depends on word frequency, which usually follows a heavy-tailed distributions in the pre-training corpus. Therefore, the embeddings of rare words on the tail are usually poorly optimized. In this work, we focus on enhancing language model pre-training by leveraging definitions of the rare words in dictionaries (e.g., Wiktionary). To incorporate a rare word definition as a part of input, we fetch its definition from the dictionary and append it to the end of the input text sequence. In addition to training with the masked language modeling objective, we propose two novel self-supervised pre-training tasks on word and sentence-level alignment between input text sequence and rare word definitions to enhance language modeling representation with dictionary. We evaluate the proposed Dict-BERT model on the language understanding benchmark GLUE and eight specialized domain benchmark datasets. Extensive experiments demonstrate that Dict-BERT can significantly improve the understanding of rare words and boost model performance on various NLP downstream tasks.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.150.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the quality of word representations highly depends on word frequency, which usually follows a heavy-tailed distributions in the pre-training corpus. Therefore, the embeddings of rare words on the tail are usually poorly optimized.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the quality of word representations highly depends on word frequency, which usually follows a heavy-tailed distributions in the pre-training corpus. Therefore, the embeddings of rare words on the tail are usually poorly optimized.\""
    },
    {
        "title": "CaM-Gen: Causally Aware Metric-Guided Text Generation",
        "authors": [
            "Navita Goyal",
            "Roodram Paneri",
            "Ayush Agarwal",
            "Udit Kalani",
            "Abhilasha Sancheti",
            "Niyati Chhaya"
        ],
        "published": "2022",
        "summary": "Content is created for a well-defined purpose, often described by a metric or signal represented in the form of structured information. The relationship between the goal (metrics) of target content and the content itself is non-trivial. While large-scale language models show promising text generation capabilities, guiding the generated text with external metrics is challenging. These metrics and content tend to have inherent relationships and not all of them may be of consequence. We introduce CaM-Gen: Causally aware Generative Networks guided by user-defined target metrics incorporating the causal relationships between the metric and content features. We leverage causal inference techniques to identify causally significant aspects of a text that lead to the target metric and then explicitly guide generative models towards these by a feedback mechanism. We propose this mechanism for variational autoencoder and Transformer-based generative models. The proposed models beat baselines in terms of the target metric control while maintaining fluency and language quality of the generated text. To the best of our knowledge, this is one of the early attempts at controlled generation incorporating a metric guide using causal inference.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.162.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"guiding the generated text with external metrics is challenging.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"guiding the generated text with external metrics is challenging.\""
    },
    {
        "title": "Training Dynamics for Text Summarization Models",
        "authors": [
            "Tanya Goyal",
            "Jiacheng Xu",
            "Junyi Jessy Li",
            "Greg Durrett"
        ],
        "published": "2022",
        "summary": "Pre-trained language models (e.g. BART) have shown impressive results when fine-tuned on large summarization datasets. However, little is understood about this fine-tuning process, including what knowledge is retained from pre-training time or how content selection and generation strategies are learnt across iterations. In this work, we analyze the training dynamics for generation models, focusing on summarization. Across different datasets (CNN/DM, XSum, MediaSum) and summary properties, such as abstractiveness and hallucination, we study what the model learns at different stages of its fine-tuning process. We find that a propensity to copy the input is learned early in the training process consistently across all datasets studied. On the other hand, factual errors, such as hallucination of unsupported facts, are learnt in the later stages, though this behavior is more varied across domains. Based on these observations, we explore complementary approaches for modifying training: first, disregarding high-loss tokens that are challenging to learn and second, disregarding low-loss tokens that are learnt very quickly in the latter stages of the training process. We show that these simple training modifications allow us to configure our model to achieve different goals, such as improving factuality or improving abstractiveness.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.163.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"factual errors, such as hallucination of unsupported facts, are learnt in the later stages, though this behavior is more varied across domains.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"factual errors, such as hallucination of unsupported facts, are learnt in the later stages, though this behavior is more varied across domains.\""
    },
    {
        "title": "Question Generation for Reading Comprehension Assessment by Modeling How and What to Ask",
        "authors": [
            "Bilal Ghanem",
            "Lauren Lutz Coleman",
            "Julia Rivard Dexter",
            "Spencer von der Ohe",
            "Alona Fyshe"
        ],
        "published": "2022",
        "summary": "Reading is integral to everyday life, and yet learning to read is a struggle for many young learners. During lessons, teachers can use comprehension questions to increase engagement, test reading skills, and improve retention. Historically such questions were written by skilled teachers, but recently language models have been used to generate comprehension questions. However, many existing Question Generation (QG) systems focus on generating extractive questions from the text, and have no way to control the type of the generated question. In this paper, we study QG for reading comprehension where inferential questions are critical and extractive techniques cannot be used. We propose a two-step model (HTA-WTA) that takes advantage of previous datasets, and can generate questions for a specific targeted comprehension skill. We propose a new reading comprehension dataset that contains questions annotated with story-based reading comprehension skills (SBRCS), allowing for a more complete reader assessment. Across several experiments, our results show that HTA-WTA outperforms multiple strong baselines on this new dataset. We show that the HTA-WTA model tests for strong SCRS by asking deep inferential questions.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.168.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, many existing Question Generation (QG) systems focus on generating extractive questions from the text, and have no way to control the type of the generated question.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, many existing Question Generation (QG) systems focus on generating extractive questions from the text, and have no way to control the type of the generated question.\""
    },
    {
        "title": "Revisiting Uncertainty-based Query Strategies for Active Learning with Transformers",
        "authors": [
            "Christopher Schröder",
            "Andreas Niekler",
            "Martin Potthast"
        ],
        "published": "2022",
        "summary": "Active learning is the iterative construction of a classification model through targeted labeling, enabling significant labeling cost savings. As most research on active learning has been carried out before transformer-based language models (“transformers”) became popular, despite its practical importance, comparably few papers have investigated how transformers can be combined with active learning to date. This can be attributed to the fact that using state-of-the-art query strategies for transformers induces a prohibitive runtime overhead, which effectively nullifies, or even outweighs the desired cost savings. For this reason, we revisit uncertainty-based query strategies, which had been largely outperformed before, but are particularly suited in the context of fine-tuning transformers. In an extensive evaluation, we connect transformers to experiments from previous research, assessing their performance on five widely used text classification benchmarks. For active learning with transformers, several other uncertainty-based approaches outperform the well-known prediction entropy query strategy, thereby challenging its status as most popular uncertainty baseline in active learning for text classification.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.172.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"using state-of-the-art query strategies for transformers induces a prohibitive runtime overhead, which effectively nullifies, or even outweighs the desired cost savings.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"using state-of-the-art query strategies for transformers induces a prohibitive runtime overhead, which effectively nullifies, or even outweighs the desired cost savings.\""
    },
    {
        "title": "Your fairness may vary: Pretrained language model fairness in toxic text classification",
        "authors": [
            "Ioana Baldini",
            "Dennis Wei",
            "Karthikeyan Natesan Ramamurthy",
            "Moninder Singh",
            "Mikhail Yurochkin"
        ],
        "published": "2022",
        "summary": "The popularity of pretrained language models in natural language processing systems calls for a careful evaluation of such models in down-stream tasks, which have a higher potential for societal impact. The evaluation of such systems usually focuses on accuracy measures. Our findings in this paper call for attention to be paid to fairness measures as well. Through the analysis of more than a dozen pretrained language models of varying sizes on two toxic text classification tasks (English), we demonstrate that focusing on accuracy measures alone can lead to models with wide variation in fairness characteristics. Specifically, we observe that fairness can vary even more than accuracy with increasing training data size and different random initializations. At the same time, we find that little of the fairness variation is explained by model size, despite claims in the literature. To improve model fairness without retraining, we show that two post-processing methods developed for structured, tabular data can be successfully applied to a range of pretrained language models. Warning: This paper contains samples of offensive text.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.176.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Through the analysis of more than a dozen pretrained language models of varying sizes on two toxic text classification tasks (English), we demonstrate that focusing on accuracy measures alone can lead to models with wide variation in fairness characteristics.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Through the analysis of more than a dozen pretrained language models of varying sizes on two toxic text classification tasks (English), we demonstrate that focusing on accuracy measures alone can lead to models with wide variation in fairness characteristics.\""
    },
    {
        "title": "ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning",
        "authors": [
            "Ahmed Masry",
            "Xuan Long Do",
            "Jia Qing Tan",
            "Shafiq Joty",
            "Enamul Hoque"
        ],
        "published": "2022",
        "summary": "Charts are very popular for analyzing data. When exploring charts, people often ask a variety of complex reasoning questions that involve several logical and arithmetic operations. They also commonly refer to visual features of a chart in their questions. However, most existing datasets do not focus on such complex reasoning questions as their questions are template-based and answers come from a fixed-vocabulary. In this work, we present a large-scale benchmark covering 9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries. To address the unique challenges in our benchmark involving visual and logical reasoning over charts, we present two transformer-based models that combine visual features and the data table of the chart in a unified way to answer questions. While our models achieve the state-of-the-art results on the previous datasets as well as on our benchmark, the evaluation also reveals several challenges in answering complex reasoning questions.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.177.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the evaluation also reveals several challenges in answering complex reasoning questions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the evaluation also reveals several challenges in answering complex reasoning questions.\""
    },
    {
        "title": "Does BERT really agree ? Fine-grained Analysis of Lexical Dependence on a Syntactic Task",
        "authors": [
            "Karim Lasri",
            "Alessandro Lenci",
            "Thierry Poibeau"
        ],
        "published": "2022",
        "summary": "Although transformer-based Neural Language Models demonstrate impressive performance on a variety of tasks, their generalization abilities are not well understood. They have been shown to perform strongly on subject-verb number agreement in a wide array of settings, suggesting that they learned to track syntactic dependencies during their training even without explicit supervision. In this paper, we examine the extent to which BERT is able to perform lexically-independent subject-verb number agreement (NA) on targeted syntactic templates. To do so, we disrupt the lexical patterns found in naturally occurring stimuli for each targeted structure in a novel fine-grained analysis of BERT’s behavior. Our results on nonce sentences suggest that the model generalizes well for simple templates, but fails to perform lexically-independent syntactic generalization when as little as one attractor is present.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.181.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"They have been shown to perform strongly on subject-verb number agreement in a wide array of settings... Our results on nonce sentences suggest that the model generalizes well for simple templates, but fails to perform lexically-independent syntactic generalization when as little as one attractor is present.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"They have been shown to perform strongly on subject-verb number agreement in a wide array of settings... Our results on nonce sentences suggest that the model generalizes well for simple templates, but fails to perform lexically-independent syntactic generalization when as little as one attractor is present.\""
    },
    {
        "title": "Combining Static and Contextualised Multilingual Embeddings",
        "authors": [
            "Katharina Hämmerl",
            "Jindřich Libovický",
            "Alexander Fraser"
        ],
        "published": "2022",
        "summary": "Static and contextual multilingual embeddings have complementary strengths. Static embeddings, while less expressive than contextual language models, can be more straightforwardly aligned across multiple languages. We combine the strengths of static and contextual models to improve multilingual representations. We extract static embeddings for 40 languages from XLM-R, validate those embeddings with cross-lingual word retrieval, and then align them using VecMap. This results in high-quality, highly multilingual static embeddings. Then we apply a novel continued pre-training approach to XLM-R, leveraging the high quality alignment of our static embeddings to better align the representation space of XLM-R. We show positive results for multiple complex semantic tasks. We release the static embeddings and the continued pre-training code. Unlike most previous work, our continued pre-training approach does not require parallel text.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.182.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Static embeddings, while less expressive than contextual language models...\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Static embeddings, while less expressive than contextual language models...\""
    },
    {
        "title": "Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation",
        "authors": [
            "Wenliang Dai",
            "Lu Hou",
            "Lifeng Shang",
            "Xin Jiang",
            "Qun Liu",
            "Pascale Fung"
        ],
        "published": "2022",
        "summary": "The recent large-scale vision-language pre-training (VLP) of dual-stream architectures (e.g., CLIP) with a tremendous amount of image-text pair data, has shown its superiority on various multimodal alignment tasks. Despite its success, the resulting models are not capable of multimodal generative tasks due to the weak text encoder. To tackle this problem, we propose to augment the dual-stream VLP model with a textual pre-trained language model (PLM) via vision-language knowledge distillation (VLKD), enabling the capability for multimodal generation. VLKD is pretty data- and computation-efficient compared to the pre-training from scratch. Experimental results show that the resulting model has strong zero-shot performance on multimodal generation tasks, such as open-ended visual question answering and image captioning. For example, it achieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous state-of-the-art zero-shot model with 7× fewer parameters. Furthermore, the original textual language understanding and generation ability of the PLM is maintained after VLKD, which makes our model versatile for both multimodal and unimodal tasks.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.187.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite its success, the resulting models are not capable of multimodal generative tasks due to the weak text encoder.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite its success, the resulting models are not capable of multimodal generative tasks due to the weak text encoder.\""
    },
    {
        "title": "Co-VQA : Answering by Interactive Sub Question Sequence",
        "authors": [
            "Ruonan Wang",
            "Yuxi Qian",
            "Fangxiang Feng",
            "Xiaojie Wang",
            "Huixing Jiang"
        ],
        "published": "2022",
        "summary": "Most existing approaches to Visual Question Answering (VQA) answer questions directly, however, people usually decompose a complex question into a sequence of simple sub questions and finally obtain the answer to the original question after answering the sub question sequence(SQS). By simulating the process, this paper proposes a conversation-based VQA (Co-VQA) framework, which consists of three components: Questioner, Oracle, and Answerer. Questioner raises the sub questions using an extending HRED model, and Oracle answers them one-by-one. An Adaptive Chain Visual Reasoning Model (ACVRM) for Answerer is also proposed, where the question-answer pair is used to update the visual representation sequentially. To perform supervised learning for each model, we introduce a well-designed method to build a SQS for each question on VQA 2.0 and VQA-CP v2 datasets. Experimental results show that our method achieves state-of-the-art on VQA-CP v2. Further analyses show that SQSs help build direct semantic connections between questions and images, provide question-adaptive variable-length reasoning chains, and with explicit interpretability as well as error traceability.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.188.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Syntax-guided Contrastive Learning for Pre-trained Language Model",
        "authors": [
            "Shuai Zhang",
            "Wang Lijie",
            "Xinyan Xiao",
            "Hua Wu"
        ],
        "published": "2022",
        "summary": "Syntactic information has been proved to be useful for transformer-based pre-trained language models. Previous studies often rely on additional syntax-guided attention components to enhance the transformer, which require more parameters and additional syntactic parsing in downstream tasks. This increase in complexity severely limits the application of syntax-enhanced language model in a wide range of scenarios. In order to inject syntactic knowledge effectively and efficiently into pre-trained language models, we propose a novel syntax-guided contrastive learning method which does not change the transformer architecture. Based on constituency and dependency structures of syntax trees, we design phrase-guided and tree-guided contrastive objectives, and optimize them in the pre-training stage, so as to help the pre-trained language model to capture rich syntactic knowledge in its representations. Experimental results show that our contrastive method achieves consistent improvements in a variety of tasks, including grammatical error detection, entity tasks, structural probing and GLUE. Detailed analysis further verifies that the improvements come from the utilization of syntactic information, and the learned attention weights are more explainable in terms of linguistics.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.191.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This increase in complexity severely limits the application of syntax-enhanced language model in a wide range of scenarios.\"\n\nThe paper mentions one limitation of LLMs, specifically the complexity of incorporating syntactic information, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"This increase in complexity severely limits the application of syntax-enhanced language model in a wide range of scenarios.\"\n\nThe paper mentions one limitation of LLMs, specifically the complexity of incorporating syntactic information, but it is not the primary focus of the paper."
    },
    {
        "title": "ASCM: An Answer Space Clustered Prompting Method without Answer Engineering",
        "authors": [
            "Zhen Wang",
            "Yating Yang",
            "Zhou Xi",
            "Bo Ma",
            "Lei Wang",
            "Rui Dong",
            "Azmat Anwar"
        ],
        "published": "2022",
        "summary": "Prompt-based learning, which exploits knowledge from pre-trained language models by providing textual prompts and designing appropriate answer-category mapping methods, has achieved impressive successes on few-shot text classification and natural language inference (NLI). Because of the diverse linguistic expression, there exist many answer tokens for the same category. However, both manual answer design and automatic answer search constrain answer space and therefore hardly achieve ideal performance. To address this issue, we propose an answer space clustered prompting model (ASCM) together with a synonym initialization method (SI) which automatically categorizes all answer tokens in a semantic-clustered embedding space. We also propose a stable semi-supervised method named stair learning (SL) that orderly distills knowledge from better models to weaker models. Extensive experiments demonstrate that our ASCM+SL significantly outperforms existing state-of-the-art techniques in few-shot settings.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.193.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, both manual answer design and automatic answer search constrain answer space and therefore hardly achieve ideal performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, both manual answer design and automatic answer search constrain answer space and therefore hardly achieve ideal performance.\""
    },
    {
        "title": "Deep Reinforcement Learning for Entity Alignment",
        "authors": [
            "Lingbing Guo",
            "Yuqiang Han",
            "Qiang Zhang",
            "Huajun Chen"
        ],
        "published": "2022",
        "summary": "Embedding-based methods have attracted increasing attention in recent entity alignment (EA) studies. Although great promise they can offer, there are still several limitations. The most notable is that they identify the aligned entities based on cosine similarity, ignoring the semantics underlying the embeddings themselves. Furthermore, these methods are shortsighted, heuristically selecting the closest entity as the target and allowing multiple entities to match the same candidate. To address these limitations, we model entity alignment as a sequential decision-making task, in which an agent sequentially decides whether two entities are matched or mismatched based on their representation vectors. The proposed reinforcement learning (RL)-based entity alignment framework can be flexibly adapted to most embedding-based EA methods. The experimental results demonstrate that it consistently advances the performance of several state-of-the-art methods, with a maximum improvement of 31.1% on Hits@1.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.217.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "ELLE: Efficient Lifelong Pre-training for Emerging Data",
        "authors": [
            "Yujia Qin",
            "Jiajie Zhang",
            "Yankai Lin",
            "Zhiyuan Liu",
            "Peng Li",
            "Maosong Sun",
            "Jie Zhou"
        ],
        "published": "2022",
        "summary": "Current pre-trained language models (PLM) are typically trained with static data, ignoring that in real-world scenarios, streaming data of various sources may continuously grow. This requires PLMs to integrate the information from all the sources in a lifelong manner. Although this goal could be achieved by exhaustive pre-training on all the existing data, such a process is known to be computationally expensive. To this end, we propose ELLE, aiming at efficient lifelong pre-training for emerging data. Specifically, ELLE consists of (1) function preserved model expansion, which flexibly expands an existing PLM’s width and depth to improve the efficiency of knowledge acquisition; and (2) pre-trained domain prompts, which disentangle the versatile knowledge learned during pre-training and stimulate the proper knowledge for downstream tasks. We experiment ELLE with streaming data from 5 domains on BERT and GPT. The results show the superiority of ELLE over various lifelong learning baselines in both pre-training efficiency and downstream performances. The codes are publicly available at https://github.com/thunlp/ELLE.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.220.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although this goal could be achieved by exhaustive pre-training on all the existing data, such a process is known to be computationally expensive.\"\n\nThis rating is given because the abstract mentions a limitation of pre-trained language models (computational expense of exhaustive pre-training on all existing data), but does not elaborate on it and focuses on the proposed solution (ELLE) to address this limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although this goal could be achieved by exhaustive pre-training on all the existing data, such a process is known to be computationally expensive.\"\n\nThis rating is given because the abstract mentions a limitation of pre-trained language models (computational expense of exhaustive pre-training on all existing data), but does not elaborate on it and focuses on the proposed solution (ELLE) to address this limitation."
    },
    {
        "title": "Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models",
        "authors": [
            "Robert Logan IV",
            "Ivana Balazevic",
            "Eric Wallace",
            "Fabio Petroni",
            "Sameer Singh",
            "Sebastian Riedel"
        ],
        "published": "2022",
        "summary": "Prompting language models (LMs) with training examples and task descriptions has been seen as critical to recent successes in few-shot learning. In this work, we show that finetuning LMs in the few-shot setting can considerably reduce the need for prompt engineering. In fact, one can use null prompts, prompts that contain neither task-specific templates nor training examples, and achieve competitive accuracy to manually-tuned prompts across a wide range of tasks. While finetuning LMs does introduce new parameters for each downstream task, we show that this memory overhead can be substantially reduced: finetuning only the bias terms can achieve comparable or better accuracy than standard finetuning while only updating 0.1% of the parameters. All in all, we recommend finetuning LMs for few-shot learning as it is more accurate, robust to different prompts, and can be made nearly as efficient as using frozen LMs.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.222.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While finetuning LMs does introduce new parameters for each downstream task, we show that this memory overhead can be substantially reduced:\"\n\nThis evidence is brief and doesn't go into detail about the limitations of LLMs, it only mentions one limitation (memory overhead) in passing.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"While finetuning LMs does introduce new parameters for each downstream task, we show that this memory overhead can be substantially reduced:\"\n\nThis evidence is brief and doesn't go into detail about the limitations of LLMs, it only mentions one limitation (memory overhead) in passing."
    },
    {
        "title": "Extracting Person Names from User Generated Text: Named-Entity Recognition for Combating Human Trafficking",
        "authors": [
            "Yifei Li",
            "Pratheeksha Nair",
            "Kellin Pelrine",
            "Reihaneh Rabbany"
        ],
        "published": "2022",
        "summary": "Online escort advertisement websites are widely used for advertising victims of human trafficking. Domain experts agree that advertising multiple people in the same ad is a strong indicator of trafficking. Thus, extracting person names from the text of these ads can provide valuable clues for further analysis. However, Named-Entity Recognition (NER) on escort ads is challenging because the text can be noisy, colloquial and often lacking proper grammar and punctuation. Most existing state-of-the-art NER models fail to demonstrate satisfactory performance in this task. In this paper, we propose NEAT (Name Extraction Against Trafficking) for extracting person names. It effectively combines classic rule-based and dictionary extractors with a contextualized language model to capture ambiguous names (e.g penny, hazel) and adapts to adversarial changes in the text by expanding its dictionary. NEAT shows 19% improvement on average in the F1 classification score for name extraction compared to previous state-of-the-art in two domain-specific datasets.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.225.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Most existing state-of-the-art NER models fail to demonstrate satisfactory performance in this task.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Most existing state-of-the-art NER models fail to demonstrate satisfactory performance in this task.\""
    },
    {
        "title": "Controllable Natural Language Generation with Contrastive Prefixes",
        "authors": [
            "Jing Qian",
            "Li Dong",
            "Yelong Shen",
            "Furu Wei",
            "Weizhu Chen"
        ],
        "published": "2022",
        "summary": "To guide the generation of large pretrained language models (LM), previous work has focused on directly fine-tuning the language model or utilizing an attribute discriminator. In this work, we propose a novel lightweight framework for controllable GPT2 generation, which utilizes a set of small attribute-specific vectors, called prefixes (Li and Liang, 2021), to steer natural language generation. Different from Li and Liang (2021), where each prefix is trained independently, we take the relationship among prefixes into consideration and train multiple prefixes simultaneously. We propose a novel supervised method and also an unsupervised method to train the prefixes for single-aspect control while the combination of these two methods can achieve multi-aspect control. Experimental results on both single-aspect and multi-aspect control show that our methods can guide generation towards the desired attributes while keeping high linguistic quality.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.229.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "Learning to Describe Solutions for Bug Reports Based on Developer Discussions",
        "authors": [
            "Sheena Panthaplackel",
            "Junyi Jessy Li",
            "Milos Gligoric",
            "Ray Mooney"
        ],
        "published": "2022",
        "summary": "When a software bug is reported, developers engage in a discussion to collaboratively resolve it. While the solution is likely formulated within the discussion, it is often buried in a large amount of text, making it difficult to comprehend and delaying its implementation. To expedite bug resolution, we propose generating a concise natural language description of the solution by synthesizing relevant content within the discussion, which encompasses both natural language and source code. We build a corpus for this task using a novel technique for obtaining noisy supervision from repository changes linked to bug reports, with which we establish benchmarks. We also design two systems for generating a description during an ongoing discussion by classifying when sufficient context for performing the task emerges in real-time. With automated and human evaluation, we find this task to form an ideal testbed for complex reasoning in long, bimodal dialogue context.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.231.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Perturbations in the Wild: Leveraging Human-Written Text Perturbations for Realistic Adversarial Attack and Defense",
        "authors": [
            "Thai Le",
            "Jooyoung Lee",
            "Kevin Yen",
            "Yifan Hu",
            "Dongwon Lee"
        ],
        "published": "2022",
        "summary": "We proposes a novel algorithm, ANTHRO, that inductively extracts over 600K human-written text perturbations in the wild and leverages them for realistic adversarial attack. Unlike existing character-based attacks which often deductively hypothesize a set of manipulation strategies, our work is grounded on actual observations from real-world texts. We find that adversarial texts generated by ANTHRO achieve the best trade-off between (1) attack success rate, (2) semantic preservation of the original text, and (3) stealthiness–i.e. indistinguishable from human writings hence harder to be flagged as suspicious. Specifically, our attacks accomplished around 83% and 91% attack success rates on BERT and RoBERTa, respectively. Moreover, it outperformed the TextBugger baseline with an increase of 50% and 40% in terms of semantic preservation and stealthiness when evaluated by both layperson and professional human workers. ANTHRO can further enhance a BERT classifier’s performance in understanding different variations of human-written toxic texts via adversarial training when compared to the Perspective API.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.232.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Unlike existing character-based attacks which often deductively hypothesize a set of manipulation strategies, our work is grounded on actual observations from real-world texts.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Unlike existing character-based attacks which often deductively hypothesize a set of manipulation strategies, our work is grounded on actual observations from real-world texts.\""
    },
    {
        "title": "Modular and Parameter-Efficient Multimodal Fusion with Prompting",
        "authors": [
            "Sheng Liang",
            "Mengjie Zhao",
            "Hinrich Schuetze"
        ],
        "published": "2022",
        "summary": "Recent research has made impressive progress in large-scale multimodal pre-training. In the context of the rapid growth of model size, it is necessary to seek efficient and flexible methods other than finetuning. In this paper, we propose to use prompt vectors to align the modalities. Our method achieves comparable performance to several other multimodal fusion methods in low-resource settings. We further show that our method is modular and parameter-efficient for processing tasks involving two or more data modalities.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.234.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"In the context of the rapid growth of model size, it is necessary to seek efficient and flexible methods other than finetuning.\"\n\n(Note: Although the abstract does not explicitly mention LLMs, it mentions \"large-scale multimodal pre-training\" which is related to LLMs. The evidence suggests a limitation of large-scale models, but it is not the primary focus of the",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"In the context of the rapid growth of model size, it is necessary to seek efficient and flexible methods other than finetuning.\"\n\n(Note: Although the abstract does not explicitly mention LLMs, it mentions \"large-scale multimodal pre-training\" which is related to LLMs. The evidence suggests a limitation of large-scale models, but it is not the primary focus of the"
    },
    {
        "title": "Composing Structure-Aware Batches for Pairwise Sentence Classification",
        "authors": [
            "Andreas Waldis",
            "Tilman Beck",
            "Iryna Gurevych"
        ],
        "published": "2022",
        "summary": "Identifying the relation between two sentences requires datasets with pairwise annotations. In many cases, these datasets contain instances that are annotated multiple times as part of different pairs. They constitute a structure that contains additional helpful information about the inter-relatedness of the text instances based on the annotations. This paper investigates how this kind of structural dataset information can be exploited during training. We propose three batch composition strategies to incorporate such information and measure their performance over 14 heterogeneous pairwise sentence classification tasks. Our results show statistically significant improvements (up to 3.9%) - independent of the pre-trained language model - for most tasks compared to baselines that follow a standard training procedure. Further, we see that even this baseline procedure can profit from having such structural information in a low-resource setting.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.239.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"independent of the pre-trained language model\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"independent of the pre-trained language model\""
    },
    {
        "title": "Factual Consistency of Multilingual Pretrained Language Models",
        "authors": [
            "Constanza Fierro",
            "Anders Søgaard"
        ],
        "published": "2022",
        "summary": "Pretrained language models can be queried for factual knowledge, with potential applications in knowledge base acquisition and tasks that require inference. However, for that, we need to know how reliable this knowledge is, and recent work has shown that monolingual English language models lack consistency when predicting factual knowledge, that is, they fill-in-the-blank differently for paraphrases describing the same fact. In this paper, we extend the analysis of consistency to a multilingual setting. We introduce a resource, mParaRel, and investigate (i) whether multilingual language models such as mBERT and XLM-R are more consistent than their monolingual counterparts;and (ii) if such models are equally consistent across languages. We find that mBERT is as inconsistent as English BERT in English paraphrases, but that both mBERT and XLM-R exhibit a high degree of inconsistency in English and even more so for all the other 45 languages.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.240.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"recent work has shown that monolingual English language models lack consistency when predicting factual knowledge, that is, they fill-in-the-blank differently for paraphrases describing the same fact.\"; \"We find that mBERT is as inconsistent as English BERT in English paraphrases, but that both mBERT and XLM-R exhibit a high degree of inconsistency in English and even more so for",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"recent work has shown that monolingual English language models lack consistency when predicting factual knowledge, that is, they fill-in-the-blank differently for paraphrases describing the same fact.\"; \"We find that mBERT is as inconsistent as English BERT in English paraphrases, but that both mBERT and XLM-R exhibit a high degree of inconsistency in English and even more so for"
    },
    {
        "title": "Using Pre-Trained Language Models for Producing Counter Narratives Against Hate Speech: a Comparative Study",
        "authors": [
            "Serra Sinem Tekiroğlu",
            "Helena Bonaldi",
            "Margherita Fanton",
            "Marco Guerini"
        ],
        "published": "2022",
        "summary": "In this work, we present an extensive study on the use of pre-trained language models for the task of automatic Counter Narrative (CN) generation to fight online hate speech in English. We first present a comparative study to determine whether there is a particular Language Model (or class of LMs) and a particular decoding mechanism that are the most appropriate to generate CNs. Findings show that autoregressive models combined with stochastic decodings are the most promising. We then investigate how an LM performs in generating a CN with regard to an unseen target of hate. We find out that a key element for successful ‘out of target’ experiments is not an overall similarity with the training data but the presence of a specific subset of training data, i. e. a target that shares some commonalities with the test target that can be defined a-priori. We finally introduce the idea of a pipeline based on the addition of an automatic post-editing step to refine generated CNs.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.245.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find out that a key element for successful ‘out of target’ experiments is not an overall similarity with the training data but the presence of a specific subset of training data, i. e. a target that shares some commonalities with the test target that can be defined a-priori.\"\n\nThis paper mentions a limitation of LLMs in generating counter narratives for unseen targets",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We find out that a key element for successful ‘out of target’ experiments is not an overall similarity with the training data but the presence of a specific subset of training data, i. e. a target that shares some commonalities with the test target that can be defined a-priori.\"\n\nThis paper mentions a limitation of LLMs in generating counter narratives for unseen targets"
    },
    {
        "title": "Improving Robustness of Language Models from a Geometry-aware Perspective",
        "authors": [
            "Bin Zhu",
            "Zhaoquan Gu",
            "Le Wang",
            "Jinyin Chen",
            "Qi Xuan"
        ],
        "published": "2022",
        "summary": "Recent studies have found that removing the norm-bounded projection and increasing search steps in adversarial training can significantly improve robustness. However, we observe that a too large number of search steps can hurt accuracy. We aim to obtain strong robustness efficiently using fewer steps. Through a toy experiment, we find that perturbing the clean data to the decision boundary but not crossing it does not degrade the test accuracy. Inspired by this, we propose friendly adversarial data augmentation (FADA) to generate friendly adversarial data. On top of FADA, we propose geometry-aware adversarial training (GAT) to perform adversarial training on friendly adversarial data so that we can save a large number of search steps. Comprehensive experiments across two widely used datasets and three pre-trained language models demonstrate that GAT can obtain stronger robustness via fewer steps. In addition, we provide extensive empirical results and in-depth analyses on robustness to facilitate future studies.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.246.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, we observe that a too large number of search steps can hurt accuracy.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, we observe that a too large number of search steps can hurt accuracy.\""
    },
    {
        "title": "Task-guided Disentangled Tuning for Pretrained Language Models",
        "authors": [
            "Jiali Zeng",
            "Yufan Jiang",
            "Shuangzhi Wu",
            "Yongjing Yin",
            "Mu Li"
        ],
        "published": "2022",
        "summary": "Pretrained language models (PLMs) trained on large-scale unlabeled corpus are typically fine-tuned on task-specific downstream datasets, which have produced state-of-the-art results on various NLP tasks. However, the data discrepancy issue in domain and scale makes fine-tuning fail to efficiently capture task-specific patterns, especially in low data regime. To address this issue, we propose Task-guided Disentangled Tuning (TDT) for PLMs, which enhances the generalization of representations by disentangling task-relevant signals from the entangled representations. For a given task, we introduce a learnable confidence model to detect indicative guidance from context, and further propose a disentangled regularization to mitigate the over-reliance problem. Experimental results on GLUE and CLUE benchmarks show that TDT gives consistently better results than fine-tuning with different PLMs, and extensive analysis demonstrates the effectiveness and robustness of our method. Code is available at https://github.com/lemon0830/TDT.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.247.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the data discrepancy issue in domain and scale makes fine-tuning fail to efficiently capture task-specific patterns, especially in low data regime.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the data discrepancy issue in domain and scale makes fine-tuning fail to efficiently capture task-specific patterns, especially in low data regime.\""
    },
    {
        "title": "The Inefficiency of Language Models in Scholarly Retrieval: An Experimental Walk-through",
        "authors": [
            "Shruti Singh",
            "Mayank Singh"
        ],
        "published": "2022",
        "summary": "Language models are increasingly becoming popular in AI-powered scientific IR systems. This paper evaluates popular scientific language models in handling (i) short-query texts and (ii) textual neighbors. Our experiments showcase the inability to retrieve relevant documents for a short-query text even under the most relaxed conditions. Additionally, we leverage textual neighbors, generated by small perturbations to the original text, to demonstrate that not all perturbations lead to close neighbors in the embedding space. Further, an exhaustive categorization yields several classes of orthographically and semantically related, partially related and completely unrelated neighbors. Retrieval performance turns out to be more influenced by the surface form rather than the semantics of the text.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.249.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our experiments showcase the inability to retrieve relevant documents for a short-query text even under the most relaxed conditions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Our experiments showcase the inability to retrieve relevant documents for a short-query text even under the most relaxed conditions.\""
    },
    {
        "title": "The Past Mistake is the Future Wisdom: Error-driven Contrastive Probability Optimization for Chinese Spell Checking",
        "authors": [
            "Yinghui Li",
            "Qingyu Zhou",
            "Yangning Li",
            "Zhongli Li",
            "Ruiyang Liu",
            "Rongyi Sun",
            "Zizhen Wang",
            "Chao Li",
            "Yunbo Cao",
            "Hai-Tao Zheng"
        ],
        "published": "2022",
        "summary": "Chinese Spell Checking (CSC) aims to detect and correct Chinese spelling errors, which are mainly caused by the phonological or visual similarity. Recently, pre-trained language models (PLMs) promote the progress of CSC task. However, there exists a gap between the learned knowledge of PLMs and the goal of CSC task. PLMs focus on the semantics in text and tend to correct the erroneous characters to semantically proper or commonly used ones, but these aren’t the ground-truth corrections. To address this issue, we propose an Error-driven COntrastive Probability Optimization (ECOPO) framework for CSC task. ECOPO refines the knowledge representations of PLMs, and guides the model to avoid predicting these common characters through an error-driven way. Particularly, ECOPO is model-agnostic and it can be combined with existing CSC methods to achieve better performance. Extensive experiments and detailed analyses on SIGHAN datasets demonstrate that ECOPO is simple yet effective.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.252.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, there exists a gap between the learned knowledge of PLMs and the goal of CSC task. PLMs focus on the semantics in text and tend to correct the erroneous characters to semantically proper or commonly used ones, but these aren’t the ground-truth corrections.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, there exists a gap between the learned knowledge of PLMs and the goal of CSC task. PLMs focus on the semantics in text and tend to correct the erroneous characters to semantically proper or commonly used ones, but these aren’t the ground-truth corrections.\""
    },
    {
        "title": "Leveraging Knowledge in Multilingual Commonsense Reasoning",
        "authors": [
            "Yuwei Fang",
            "Shuohang Wang",
            "Yichong Xu",
            "Ruochen Xu",
            "Siqi Sun",
            "Chenguang Zhu",
            "Michael Zeng"
        ],
        "published": "2022",
        "summary": "Commonsense reasoning (CSR) requires models to be equipped with general world knowledge. While CSR is a language-agnostic process, most comprehensive knowledge sources are restricted to a small number of languages, especially English. Thus, it remains unclear how to effectively conduct multilingual commonsense reasoning (XCSR) for various languages. In this work, we propose to use English as a pivot language, utilizing English knowledge sources for our our commonsense reasoning framework via a translate-retrieve-translate (TRT) strategy. For multilingual commonsense questions and answer candidates, we collect related knowledge via translation and retrieval from the knowledge in the source language. The retrieved knowledge is then translated into the target language and integrated into a pre-trained multilingual language model via visible knowledge attention. Then we utilize a diverse of four English knowledge sources to provide more comprehensive coverage of knowledge in different formats. Extensive results on the XCSR benchmark demonstrate that TRT with external knowledge can significantly improve multilingual commonsense reasoning in both zero-shot and translate-train settings, consistently outperforming the state-of-the-art by more than 3% on the multilingual commonsense reasoning benchmark X-CSQA and X-CODAH.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.255.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "One Agent To Rule Them All: Towards Multi-agent Conversational AI",
        "authors": [
            "Christopher Clarke",
            "Joseph Peper",
            "Karthik Krishnamurthy",
            "Walter Talamonti",
            "Kevin Leach",
            "Walter Lasecki",
            "Yiping Kang",
            "Lingjia Tang",
            "Jason Mars"
        ],
        "published": "2022",
        "summary": "The increasing volume of commercially available conversational agents (CAs) on the market has resulted in users being burdened with learning and adopting multiple agents to accomplish their tasks. Though prior work has explored supporting a multitude of domains within the design of a single agent, the interaction experience suffers due to the large action space of desired capabilities. To address these problems, we introduce a new task BBAI: Black-Box Agent Integration, focusing on combining the capabilities of multiple black-box CAs at scale. We explore two techniques: question agent pairing and question response pairing aimed at resolving this task. Leveraging these techniques, we design One For All (OFA), a scalable system that provides a unified interface to interact with multiple CAs. Additionally, we introduce MARS: Multi-Agent Response Selection, a new encoder model for question response pairing that jointly encodes user question and agent response pairs. We demonstrate that OFA is able to automatically and accurately integrate an ensemble of commercially available CAs spanning disparate domains. Specifically, using the MARS encoder we achieve the highest accuracy on our BBAI task, outperforming strong baselines.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.257.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Bridging Pre-trained Language Models and Hand-crafted Features for Unsupervised POS Tagging",
        "authors": [
            "Houquan Zhou",
            "Yang Li",
            "Zhenghua Li",
            "Min Zhang"
        ],
        "published": "2022",
        "summary": "In recent years, large-scale pre-trained language models (PLMs) have made extraordinary progress in most NLP tasks. But, in the unsupervised POS tagging task, works utilizing PLMs are few and fail to achieve state-of-the-art (SOTA) performance. The recent SOTA performance is yielded by a Guassian HMM variant proposed by He et al. (2018). However, as a generative model, HMM makes very strong independence assumptions, making it very challenging to incorporate contexualized word representations from PLMs. In this work, we for the first time propose a neural conditional random field autoencoder (CRF-AE) model for unsupervised POS tagging. The discriminative encoder of CRF-AE can straightforwardly incorporate ELMo word representations. Moreover, inspired by feature-rich HMM, we reintroduce hand-crafted features into the decoder of CRF-AE. Finally, experiments clearly show that our model outperforms previous state-of-the-art models by a large margin on Penn Treebank and multilingual Universal Dependencies treebank v2.0.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.259.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"But, in the unsupervised POS tagging task, works utilizing PLMs are few and fail to achieve state-of-the-art (SOTA) performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"But, in the unsupervised POS tagging task, works utilizing PLMs are few and fail to achieve state-of-the-art (SOTA) performance.\""
    },
    {
        "title": "A Natural Diet: Towards Improving Naturalness of Machine Translation Output",
        "authors": [
            "Markus Freitag",
            "David Vilar",
            "David Grangier",
            "Colin Cherry",
            "George Foster"
        ],
        "published": "2022",
        "summary": "Machine translation (MT) evaluation often focuses on accuracy and fluency, without paying much attention to translation style. This means that, even when considered accurate and fluent, MT output can still sound less natural than high quality human translations or text originally written in the target language. Machine translation output notably exhibits lower lexical diversity, and employs constructs that mirror those in the source sentence. In this work we propose a method for training MT systems to achieve a more natural style, i.e. mirroring the style of text originally written in the target language. Our method tags parallel training data according to the naturalness of the target side by contrasting language models trained on natural and translated data. Tagging data allows us to put greater emphasis on target sentences originally written in the target language. Automatic metrics show that the resulting models achieve lexical richness on par with human translations, mimicking a style much closer to sentences originally written in the target language. Furthermore, we find that their output is preferred by human experts when compared to the baseline translations.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.263.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Machine translation output notably exhibits lower lexical diversity, and employs constructs that mirror those in the source sentence.\"\n\nThis abstract mentions a limitation of LLMs (lower lexical diversity and mirroring source sentence constructs) but does not explore it in depth, and focuses on the proposed solution to improve the naturalness of machine translation output.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Machine translation output notably exhibits lower lexical diversity, and employs constructs that mirror those in the source sentence.\"\n\nThis abstract mentions a limitation of LLMs (lower lexical diversity and mirroring source sentence constructs) but does not explore it in depth, and focuses on the proposed solution to improve the naturalness of machine translation output."
    },
    {
        "title": "CUE Vectors: Modular Training of Language Models Conditioned on Diverse Contextual Signals",
        "authors": [
            "Scott Novotney",
            "Sreeparna Mukherjee",
            "Zeeshan Ahmed",
            "Andreas Stolcke"
        ],
        "published": "2022",
        "summary": "We propose a framework to modularize the training of neural language models that use diverse forms of context by eliminating the need to jointly train context and within-sentence encoders. Our approach, contextual universal embeddings (CUE), trains LMs on one type of contextual data and adapts to novel context types. The model consists of a pretrained neural sentence LM, a BERT-based contextual encoder, and a masked transfomer decoder that estimates LM probabilities using sentence-internal and contextual evidence. When contextually annotated data is unavailable, our model learns to combine contextual and sentence-internal information using noisy oracle unigram embeddings as a proxy. Real context data can be introduced later and used to adapt a small number of parameters that map contextual data into the decoder’s embedding space. We validate the CUE framework on a NYTimes text corpus with multiple metadata types, for which the LM perplexity can be lowered from 36.6 to 27.4 by conditioning on context. Bootstrapping a contextual LM with only a subset of the metadata during training retains 85% of the achievable gain. Training the model initially with proxy context retains 67% of the perplexity gain after adapting to real context. Furthermore, we can swap one type of pretrained sentence LM for another without retraining the context encoders, by only adapting the decoder model. Overall, we obtain a modular framework that allows incremental, scalable training of context-enhanced LMs.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.265.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions \"when contextually annotated data is unavailable\", which could be considered a limitation of the training process rather than the LLM itself.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions \"when contextually annotated data is unavailable\", which could be considered a limitation of the training process rather than the LLM itself."
    },
    {
        "title": "Cross-Lingual UMLS Named Entity Linking using UMLS Dictionary Fine-Tuning",
        "authors": [
            "Rina Galperin",
            "Shachar Schnapp",
            "Michael Elhadad"
        ],
        "published": "2022",
        "summary": "We study cross-lingual UMLS named entity linking, where mentions in a given source language are mapped to UMLS concepts, most of which are labeled in English. Our cross-lingual framework includes an offline unsupervised construction of a translated UMLS dictionary and a per-document pipeline which identifies UMLS candidate mentions and uses a fine-tuned pretrained transformer language model to filter candidates according to context. Our method exploits a small dataset of manually annotated UMLS mentions in the source language and uses this supervised data in two ways: to extend the unsupervised UMLS dictionary and to fine-tune the contextual filtering of candidate mentions in full documents. We demonstrate results of our approach on both Hebrew and English. We achieve new state-of-the-art (SOTA) results on the Hebrew Camoni corpus, +8.9 F1 on average across three communities in the dataset. We also achieve new SOTA on the English dataset MedMentions with +7.3 F1.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.266.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions using a \"pretrained transformer language model\" which is a type of LLM, but does not discuss its limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions using a \"pretrained transformer language model\" which is a type of LLM, but does not discuss its limitations."
    },
    {
        "title": "Aligned Weight Regularizers for Pruning Pretrained Neural Networks",
        "authors": [
            "James O’ Neill",
            "Sourav Dutta",
            "Haytham Assem"
        ],
        "published": "2022",
        "summary": "Pruning aims to reduce the number of parameters while maintaining performance close to the original network. This work proposes a novel self-distillation based pruning strategy, whereby the representational similarity between the pruned and unpruned versions of the same network is maximized. Unlike previous approaches that treat distillation and pruning separately, we use distillation to inform the pruning criteria, without requiring a separate student network as in knowledge distillation. We show that the proposed cross-correlation objective for self-distilled pruning implicitly encourages sparse solutions, naturally complementing magnitude-based pruning criteria. Experiments on the GLUE and XGLUE benchmarks show that self-distilled pruning increases mono- and cross-lingual language model performance. Self-distilled pruned models also outperform smaller Transformers with an equal number of parameters and are competitive against (6 times) larger distilled networks. We also observe that self-distillation (1) maximizes class separability, (2) increases the signal-to-noise ratio, and (3) converges faster after pruning steps, providing further insights into why self-distilled pruning improves generalization.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.267.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Self-distilled pruned models also outperform smaller Transformers with an equal number of parameters and are competitive against (6 times) larger distilled networks.\"\n\n(Note: Although the paper discusses LLMs, it does not mention any explicit limitations of the models. The evidence provided is about the performance of self-distilled pruned models in comparison to other models, but it does not discuss any",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Self-distilled pruned models also outperform smaller Transformers with an equal number of parameters and are competitive against (6 times) larger distilled networks.\"\n\n(Note: Although the paper discusses LLMs, it does not mention any explicit limitations of the models. The evidence provided is about the performance of self-distilled pruned models in comparison to other models, but it does not discuss any"
    },
    {
        "title": "Improving Controllable Text Generation with Position-Aware Weighted Decoding",
        "authors": [
            "Yuxuan Gu",
            "Xiaocheng Feng",
            "Sicheng Ma",
            "Jiaming Wu",
            "Heng Gong",
            "Bing Qin"
        ],
        "published": "2022",
        "summary": "Weighted decoding methods composed of the pretrained language model (LM) and the controller have achieved promising results for controllable text generation. However, these models often suffer from a control strength/fluency trade-off problem as higher control strength is more likely to generate incoherent and repetitive text. In this paper, we illustrate this trade-off is arisen by the controller imposing the target attribute on the LM at improper positions. And we propose a novel framework based on existing weighted decoding methods called CAT-PAW, which introduces a lightweight regulator to adjust bias signals from the controller at different decoding positions. Experiments on positive sentiment control, topic control, and language detoxification show the effectiveness of our CAT-PAW upon 4 SOTA models.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.272.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these models often suffer from a control strength/fluency trade-off problem as higher control strength is more likely to generate incoherent and repetitive text.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these models often suffer from a control strength/fluency trade-off problem as higher control strength is more likely to generate incoherent and repetitive text.\""
    },
    {
        "title": "Prompt Tuning for Discriminative Pre-trained Language Models",
        "authors": [
            "Yuan Yao",
            "Bowen Dong",
            "Ao Zhang",
            "Zhengyan Zhang",
            "Ruobing Xie",
            "Zhiyuan Liu",
            "Leyu Lin",
            "Maosong Sun",
            "Jianyong Wang"
        ],
        "published": "2022",
        "summary": "Recent works have shown promising results of prompt tuning in stimulating pre-trained language models (PLMs) for natural language processing (NLP) tasks. However, to the best of our knowledge, existing works focus on prompt-tuning generative PLMs that are pre-trained to generate target tokens, such as BERT. It is still unknown whether and how discriminative PLMs, e.g., ELECTRA, can be effectively prompt-tuned. In this work, we present DPT, the first prompt tuning framework for discriminative PLMs, which reformulates NLP tasks into a discriminative language modeling problem. Comprehensive experiments on text classification and question answering show that, compared with vanilla fine-tuning, DPT achieves significantly higher performance, and also prevents the unstable problem in tuning large PLMs in both full-set and low-resource settings.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.273.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"It is still unknown whether and how discriminative PLMs, e.g., ELECTRA, can be effectively prompt-tuned.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"It is still unknown whether and how discriminative PLMs, e.g., ELECTRA, can be effectively prompt-tuned.\""
    },
    {
        "title": "THE-X: Privacy-Preserving Transformer Inference with Homomorphic Encryption",
        "authors": [
            "Tianyu Chen",
            "Hangbo Bao",
            "Shaohan Huang",
            "Li Dong",
            "Binxing Jiao",
            "Daxin Jiang",
            "Haoyi Zhou",
            "Jianxin Li",
            "Furu Wei"
        ],
        "published": "2022",
        "summary": "As more and more pre-trained language models adopt on-cloud deployment, the privacy issues grow quickly, mainly for the exposure of plain-text user data (e.g., search history, medical record, bank account). Privacy-preserving inference of transformer models is on the demand of cloud service users. To protect privacy, it is an attractive choice to compute only with ciphertext in homomorphic encryption (HE). However, enabling pre-trained models inference on ciphertext data is difficult due to the complex computations in transformer blocks, which are not supported by current HE tools yet. In this work, we introduce THE-X, an approximation approach for transformers, which enables privacy-preserving inference of pre-trained models developed by popular frameworks. THE-X proposes a workflow to deal with complex computation in transformer networks, including all the non-polynomial functions like GELU, softmax, and LayerNorm. Experiments reveal our proposed THE-X can enable transformer inference on encrypted data for different downstream tasks, all with negligible performance drop but enjoying the theory-guaranteed privacy-preserving advantage.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.277.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"enabling pre-trained models inference on ciphertext data is difficult due to the complex computations in transformer blocks, which are not supported by current HE tools yet.\"\n\nThis rating is given because the abstract mentions a limitation of transformer models (difficulty in enabling inference on ciphertext data due to complex computations) but does not explore it in depth, and the primary focus of the paper is on the proposed solution",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"enabling pre-trained models inference on ciphertext data is difficult due to the complex computations in transformer blocks, which are not supported by current HE tools yet.\"\n\nThis rating is given because the abstract mentions a limitation of transformer models (difficulty in enabling inference on ciphertext data due to complex computations) but does not explore it in depth, and the primary focus of the paper is on the proposed solution"
    },
    {
        "title": "Do Pre-trained Models Benefit Knowledge Graph Completion? A Reliable Evaluation and a Reasonable Approach",
        "authors": [
            "Xin Lv",
            "Yankai Lin",
            "Yixin Cao",
            "Lei Hou",
            "Juanzi Li",
            "Zhiyuan Liu",
            "Peng Li",
            "Jie Zhou"
        ],
        "published": "2022",
        "summary": "In recent years, pre-trained language models (PLMs) have been shown to capture factual knowledge from massive texts, which encourages the proposal of PLM-based knowledge graph completion (KGC) models. However, these models are still quite behind the SOTA KGC models in terms of performance. In this work, we find two main reasons for the weak performance: (1) Inaccurate evaluation setting. The evaluation setting under the closed-world assumption (CWA) may underestimate the PLM-based KGC models since they introduce more external knowledge; (2) Inappropriate utilization of PLMs. Most PLM-based KGC models simply splice the labels of entities and relations as inputs, leading to incoherent sentences that do not take full advantage of the implicit knowledge in PLMs. To alleviate these problems, we highlight a more accurate evaluation setting under the open-world assumption (OWA), which manual checks the correctness of knowledge that is not in KGs. Moreover, motivated by prompt tuning, we propose a novel PLM-based KGC model named PKGC. The basic idea is to convert each triple and its support information into natural prompt sentences, which is further fed into PLMs for classification. Experiment results on two KGC datasets demonstrate OWA is more reliable for evaluating KGC, especially on the link prediction, and the effectiveness of our PKCG model on both CWA and OWA settings.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.282.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these models are still quite behind the SOTA KGC models in terms of performance. In this work, we find two main reasons for the weak performance: (1) Inaccurate evaluation setting. The evaluation setting under the closed-world assumption (CWA) may underestimate the PLM-based KGC models since they introduce more external knowledge; (2) Inappropriate utilization",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, these models are still quite behind the SOTA KGC models in terms of performance. In this work, we find two main reasons for the weak performance: (1) Inaccurate evaluation setting. The evaluation setting under the closed-world assumption (CWA) may underestimate the PLM-based KGC models since they introduce more external knowledge; (2) Inappropriate utilization"
    },
    {
        "title": "EICO: Improving Few-Shot Text Classification via Explicit and Implicit Consistency Regularization",
        "authors": [
            "Lei Zhao",
            "Cheng Yao"
        ],
        "published": "2022",
        "summary": "While the prompt-based fine-tuning methods had advanced few-shot natural language understanding tasks, self-training methods are also being explored. This work revisits the consistency regularization in self-training and presents explicit and implicit consistency regularization enhanced language model (EICO). By employing both explicit and implicit consistency regularization, EICO advances the performance of prompt-based few-shot text classification. For implicit consistency regularization, we generate pseudo-label from the weakly-augmented view and predict pseudo-label from the strongly-augmented view. For explicit consistency regularization, we minimize the difference between the prediction of the augmentation view and the prediction of the original view. We conducted extensive experiments on six text classification datasets and found that with sixteen labeled examples, EICO achieves competitive performance compared to existing self-training few-shot learning methods.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.283.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit evidence, but the paper is focused on improving few-shot text classification, which implies that existing LLMs may struggle with this task.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit evidence, but the paper is focused on improving few-shot text classification, which implies that existing LLMs may struggle with this task."
    },
    {
        "title": "Incorporating Dynamic Semantics into Pre-Trained Language Model for Aspect-based Sentiment Analysis",
        "authors": [
            "Kai Zhang",
            "Kun Zhang",
            "Mengdi Zhang",
            "Hongke Zhao",
            "Qi Liu",
            "Wei Wu",
            "Enhong Chen"
        ],
        "published": "2022",
        "summary": "Aspect-based sentiment analysis (ABSA) predicts sentiment polarity towards a specific aspect in the given sentence. While pre-trained language models such as BERT have achieved great success, incorporating dynamic semantic changes into ABSA remains challenging. To this end, in this paper, we propose to address this problem by Dynamic Re-weighting BERT (DR-BERT), a novel method designed to learn dynamic aspect-oriented semantics for ABSA. Specifically, we first take the Stack-BERT layers as a primary encoder to grasp the overall semantic of the sentence and then fine-tune it by incorporating a lightweight Dynamic Re-weighting Adapter (DRA). Note that the DRA can pay close attention to a small region of the sentences at each step and re-weigh the vitally important words for better aspect-aware sentiment understanding. Finally, experimental results on three benchmark datasets demonstrate the effectiveness and the rationality of our proposed model and provide good interpretable insights for future semantic modeling.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.285.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"incorporating dynamic semantic changes into ABSA remains challenging.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"incorporating dynamic semantic changes into ABSA remains challenging.\""
    },
    {
        "title": "Divide and Conquer: Text Semantic Matching with Disentangled Keywords and Intents",
        "authors": [
            "Yicheng Zou",
            "Hongwei Liu",
            "Tao Gui",
            "Junzhe Wang",
            "Qi Zhang",
            "Meng Tang",
            "Haixiang Li",
            "Daniell Wang"
        ],
        "published": "2022",
        "summary": "Text semantic matching is a fundamental task that has been widely used in various scenarios, such as community question answering, information retrieval, and recommendation. Most state-of-the-art matching models, e.g., BERT, directly perform text comparison by processing each word uniformly. However, a query sentence generally comprises content that calls for different levels of matching granularity. Specifically, keywords represent factual information such as action, entity, and event that should be strictly matched, while intents convey abstract concepts and ideas that can be paraphrased into various expressions. In this work, we propose a simple yet effective training strategy for text semantic matching in a divide-and-conquer manner by disentangling keywords from intents. Our approach can be easily combined with pre-trained language models (PLM) without influencing their inference efficiency, achieving stable performance improvements against a wide range of PLMs on three benchmarks.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.287.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Most state-of-the-art matching models, e.g., BERT, directly perform text comparison by processing each word uniformly.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Most state-of-the-art matching models, e.g., BERT, directly perform text comparison by processing each word uniformly.\""
    },
    {
        "title": "ED2LM: Encoder-Decoder to Language Model for Faster Document Re-ranking Inference",
        "authors": [
            "Kai Hui",
            "Honglei Zhuang",
            "Tao Chen",
            "Zhen Qin",
            "Jing Lu",
            "Dara Bahri",
            "Ji Ma",
            "Jai Gupta",
            "Cicero Nogueira dos Santos",
            "Yi Tay",
            "Donald Metzler"
        ],
        "published": "2022",
        "summary": "State-of-the-art neural models typically encode document-query pairs using cross-attention for re-ranking. To this end, models generally utilize an encoder-only (like BERT) paradigm or an encoder-decoder (like T5) approach. These paradigms, however, are not without flaws, i.e., running the model on all query-document pairs at inference-time incurs a significant computational cost. This paper proposes a new training and inference paradigm for re-ranking. We propose to finetune a pretrained encoder-decoder model using in the form of document to query generation. Subsequently, we show that this encoder-decoder architecture can be decomposed into a decoder-only language model during inference. This results in significant inference time speedups since the decoder-only architecture only needs to learn to interpret static encoder embeddings during inference. Our experiments show that this new paradigm achieves results that are comparable to the more expensive cross-attention ranking approaches while being up to 6.8X faster. We believe this work paves the way for more efficient neural rankers that leverage large pretrained models.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.295.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"These paradigms, however, are not without flaws, i.e., running the model on all query-document pairs at inference-time incurs a significant computational cost.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"These paradigms, however, are not without flaws, i.e., running the model on all query-document pairs at inference-time incurs a significant computational cost.\""
    },
    {
        "title": "Dialogue Summaries as Dialogue States (DS2), Template-Guided Summarization for Few-shot Dialogue State Tracking",
        "authors": [
            "Jamin Shin",
            "Hangyeol Yu",
            "Hyeongdon Moon",
            "Andrea Madotto",
            "Juneyoung Park"
        ],
        "published": "2022",
        "summary": "Annotating task-oriented dialogues is notorious for the expensive and difficult data collection process. Few-shot dialogue state tracking (DST) is a realistic solution to this problem. In this paper, we hypothesize that dialogue summaries are essentially unstructured dialogue states; hence, we propose to reformulate dialogue state tracking as a dialogue summarization problem. To elaborate, we train a text-to-text language model with synthetic template-based dialogue summaries, generated by a set of rules from the dialogue states. Then, the dialogue states can be recovered by inversely applying the summary generation rules. We empirically show that our method DS2 outperforms previous works on few-shot DST in MultiWoZ 2.0 and 2.1, in both cross-domain and multi-domain settings. Our method also exhibits vast speedup during both training and inference as it can generate all states at once. Finally, based on our analysis, we discover that the naturalness of the summary templates plays a key role for successful training.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.302.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper uses a \"text-to-text language model\" which is a type of LLM, however, it does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper uses a \"text-to-text language model\" which is a type of LLM, however, it does not discuss any limitations of LLMs."
    },
    {
        "title": "Unsupervised Chinese Word Segmentation with BERT Oriented Probing and Transformation",
        "authors": [
            "Wei Li",
            "Yuhan Song",
            "Qi Su",
            "Yanqiu Shao"
        ],
        "published": "2022",
        "summary": "Word Segmentation is a fundamental step for understanding Chinese language. Previous neural approaches for unsupervised Chinese Word Segmentation (CWS) only exploits shallow semantic information, which can miss important context. Large scale Pre-trained language models (PLM) have achieved great success in many areas because of its ability to capture the deep contextual semantic relation. In this paper, we propose to take advantage of the deep semantic information embedded in PLM (e.g., BERT) with a self-training manner, which iteratively probes and transforms the semantic information in PLM into explicit word segmentation ability. Extensive experiment results show that our proposed approach achieves state-of-the-art F1 score on two CWS benchmark datasets.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.310.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation mentioned, but the abstract implies a limitation of previous neural approaches for unsupervised Chinese Word Segmentation (CWS) and suggests that LLMs can be used to address this limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation mentioned, but the abstract implies a limitation of previous neural approaches for unsupervised Chinese Word Segmentation (CWS) and suggests that LLMs can be used to address this limitation."
    },
    {
        "title": "Probing BERT’s priors with serial reproduction chains",
        "authors": [
            "Takateru Yamakoshi",
            "Thomas Griffiths",
            "Robert Hawkins"
        ],
        "published": "2022",
        "summary": "Sampling is a promising bottom-up method for exposing what generative models have learned about language, but it remains unclear how to generate representative samples from popular masked language models (MLMs) like BERT. The MLM objective yields a dependency network with no guarantee of consistent conditional distributions, posing a problem for naive approaches. Drawing from theories of iterated learning in cognitive science, we explore the use of serial reproduction chains to sample from BERT’s priors. In particular, we observe that a unique and consistent estimator of the ground-truth joint distribution is given by a Generative Stochastic Network (GSN) sampler, which randomly selects which token to mask and reconstruct on each step. We show that the lexical and syntactic statistics of sentences from GSN chains closely match the ground-truth corpus distribution and perform better than other methods in a large corpus of naturalness judgments. Our findings establish a firmer theoretical foundation for bottom-up probing and highlight richer deviations from human priors.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.314.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The MLM objective yields a dependency network with no guarantee of consistent conditional distributions, posing a problem for naive approaches.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The MLM objective yields a dependency network with no guarantee of consistent conditional distributions, posing a problem for naive approaches.\""
    },
    {
        "title": "A Few-Shot Semantic Parser for Wizard-of-Oz Dialogues with the Precise ThingTalk Representation",
        "authors": [
            "Giovanni Campagna",
            "Sina Semnani",
            "Ryan Kearns",
            "Lucas Jun Koba Sato",
            "Silei Xu",
            "Monica Lam"
        ],
        "published": "2022",
        "summary": "Previous attempts to build effective semantic parsers for Wizard-of-Oz (WOZ) conversations suffer from the difficulty in acquiring a high-quality, manually annotated training set. Approaches based only on dialogue synthesis are insufficient, as dialogues generated from state-machine based models are poor approximations of real-life conversations. Furthermore, previously proposed dialogue state representations are ambiguous and lack the precision necessary for building an effective agent. This paper proposes a new dialogue representation and a sample-efficient methodology that can predict precise dialogue states in WOZ conversations. We extended the ThingTalk representation to capture all information an agent needs to respond properly. Our training strategy is sample-efficient: we combine (1) few-shot data sparsely sampling the full dialogue space and (2) synthesized data covering a subset space of dialogues generated by a succinct state-based dialogue model. The completeness of the extended ThingTalk language is demonstrated with a fully operational agent, which is also used in training data synthesis. We demonstrate the effectiveness of our methodology on MultiWOZ 3.0, a reannotation of the MultiWOZ 2.1 dataset in ThingTalk. ThingTalk can represent 98% of the test turns, while the simulator can emulate 85% of the validation set. We train a contextual semantic parser using our strategy, and obtain 79% turn-by-turn exact match accuracy on the reannotated test set.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.317.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "CrossAligner & Co: Zero-Shot Transfer Methods for Task-Oriented Cross-lingual Natural Language Understanding",
        "authors": [
            "Milan Gritta",
            "Ruoyu Hu",
            "Ignacio Iacobacci"
        ],
        "published": "2022",
        "summary": "Task-oriented personal assistants enable people to interact with a host of devices and services using natural language. One of the challenges of making neural dialogue systems available to more users is the lack of training data for all but a few languages. Zero-shot methods try to solve this issue by acquiring task knowledge in a high-resource language such as English with the aim of transferring it to the low-resource language(s). To this end, we introduce CrossAligner, the principal method of a variety of effective approaches for zero-shot cross-lingual transfer based on learning alignment from unlabelled parallel data. We present a quantitative analysis of individual methods as well as their weighted combinations, several of which exceed state-of-the-art (SOTA) scores as evaluated across nine languages, fifteen test sets and three benchmark multilingual datasets. A detailed qualitative error analysis of the best methods shows that our fine-tuned language models can zero-shot transfer the task knowledge better than anticipated.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.319.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"One of the challenges of making neural dialogue systems available to more users is the lack of training data for all but a few languages.\"\n\nThis rating is given because the paper mentions a limitation of LLMs, but it is not the primary focus of the paper and is only briefly mentioned to motivate the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"One of the challenges of making neural dialogue systems available to more users is the lack of training data for all but a few languages.\"\n\nThis rating is given because the paper mentions a limitation of LLMs, but it is not the primary focus of the paper and is only briefly mentioned to motivate the proposed solution."
    },
    {
        "title": "Attention as Grounding: Exploring Textual and Cross-Modal Attention on Entities and Relations in Language-and-Vision Transformer",
        "authors": [
            "Nikolai Ilinykh",
            "Simon Dobnik"
        ],
        "published": "2022",
        "summary": "We explore how a multi-modal transformer trained for generation of longer image descriptions learns syntactic and semantic representations about entities and relations grounded in objects at the level of masked self-attention (text generation) and cross-modal attention (information fusion). We observe that cross-attention learns the visual grounding of noun phrases into objects and high-level semantic information about spatial relations, while text-to-text attention captures low-level syntactic knowledge between words. This concludes that language models in a multi-modal task learn different semantic information about objects and relations cross-modally and uni-modally (text-only). Our code is available here: https://github.com/GU-CLASP/attention-as-grounding.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.320.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicitly mentioned, but \"This concludes that language models in a multi-modal task learn different semantic information about objects and relations cross-modally and uni-modally (text-only)\" implies limitations in how LLMs learn and represent information in different modalities.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None explicitly mentioned, but \"This concludes that language models in a multi-modal task learn different semantic information about objects and relations cross-modally and uni-modally (text-only)\" implies limitations in how LLMs learn and represent information in different modalities."
    },
    {
        "title": "Exploring the Capacity of a Large-scale Masked Language Model to Recognize Grammatical Errors",
        "authors": [
            "Ryo Nagata",
            "Manabu Kimura",
            "Kazuaki Hanawa"
        ],
        "published": "2022",
        "summary": "In this paper, we explore the capacity of a language model-based method for grammatical error detection in detail. We first show that 5 to 10% of training data are enough for a BERT-based error detection method to achieve performance equivalent to what a non-language model-based method can achieve with the full training data; recall improves much faster with respect to training data size in the BERT-based method than in the non-language model method. This suggests that (i) the BERT-based method should have a good knowledge of the grammar required to recognize certain types of error and that (ii) it can transform the knowledge into error detection rules by fine-tuning with few training samples, which explains its high generalization ability in grammatical error detection. We further show with pseudo error data that it actually exhibits such nice properties in learning rules for recognizing various types of error. Finally, based on these findings, we discuss a cost-effective method for detecting grammatical errors with feedback comments explaining relevant grammatical rules to learners.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.324.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None directly mentioned, but the discussion of the model's capacity and its generalization ability in grammatical error detection implies some limitations are being explored, albeit briefly.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None directly mentioned, but the discussion of the model's capacity and its generalization ability in grammatical error detection implies some limitations are being explored, albeit briefly."
    },
    {
        "title": "On the data requirements of probing",
        "authors": [
            "Zining Zhu",
            "Jixuan Wang",
            "Bai Li",
            "Frank Rudzicz"
        ],
        "published": "2022",
        "summary": "As large and powerful neural language models are developed, researchers have been increasingly interested in developing diagnostic tools to probe them. There are many papers with conclusions of the form “observation X is found in model Y”, using their own datasets with varying sizes. Larger probing datasets bring more reliability, but are also expensive to collect. There is yet to be a quantitative method for estimating reasonable probing dataset sizes. We tackle this omission in the context of comparing two probing configurations: after we have collected a small dataset from a pilot study, how many additional data samples are sufficient to distinguish two different configurations? We present a novel method to estimate the required number of data samples in such experiments and, across several case studies, we verify that our estimations have sufficient statistical power. Our framework helps to systematically construct probing datasets to diagnose neural NLP models.",
        "pdf_link": "https://aclanthology.org/2022.findings-acl.326.pdf",
        "source": "acl2022",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Larger probing datasets bring more reliability, but are also expensive to collect.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Larger probing datasets bring more reliability, but are also expensive to collect.\""
    }
]