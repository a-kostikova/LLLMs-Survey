[
    {
        "title": "Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision",
        "authors": "Haoning Wu;Zicheng Zhang;Erli Zhang;Chaofeng Chen;Liang Liao;Annan Wang;Chunyi Li;Wenxiu Sun;Qiong Yan;Guangtao Zhai;Weisi Lin",
        "site": "https://iclr.cc/virtual/2024/poster/19616",
        "summary": "The rapid evolution of Multi-modality Large Language Models (MLLMs) has catalyzed a shift in computer vision from specialized models to general-purpose foundation models. Nevertheless, there is still an inadequacy in assessing the abilities of MLLMs onlow-level visual perception and understanding. To address this gap, we presentQ-Bench, a holistic benchmark crafted to systematically evaluate potential abilities of MLLMs on three realms: low-level visual perception, low-level visual description, and overall visual quality assessment.a)To evaluate the low-levelperceptionability, we construct theLLVisionQAdataset, consisting of 2,990 diverse-sourced images, each equipped with a human-asked question focusing on its low-level attributes. We then measure the correctness of MLLMs on answering these questions.b)To examine thedescriptionability of MLLMs on low-level information, we propose theLLDescribedataset consisting of long expert-labelledgoldenlow-level text descriptions on 499 images, and a GPT-involved comparison pipeline between outputs of MLLMs and thegoldendescriptions.c)Besides these two tasks, we further measure their visual qualityassessmentability to align with human opinion scores. Specifically, we design a softmax-based strategy that enables MLLMs to predictquantifiablequality scores, and evaluate them on various existing image quality assessment (IQA) datasets. Our evaluation across the three abilities confirms that MLLMs possess preliminary low-level visual skills. However, these skills are still unstable and relatively imprecise, indicating the need for specific enhancements on MLLMs towards these abilities. We hope that our benchmark can encourage the research community to delve deeper to discover and enhance these untapped potentials of MLLMs.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19616",
        "keywords": [
            "low level vision",
            "low levelperceptionability",
            "low level information",
            "image quality assessment"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these skills are still unstable and relatively imprecise, indicating the need for specific enhancements on MLLMs towards these abilities.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, these skills are still unstable and relatively imprecise, indicating the need for specific enhancements on MLLMs towards these abilities.\""
    },
    {
        "title": "Self-Alignment with Instruction Backtranslation",
        "authors": "Xian Li;Ping Yu;Chunting Zhou;Timo Schick;Omer Levy;Luke Zettlemoyer;Jason E Weston;Mike Lewis",
        "site": "https://iclr.cc/virtual/2024/poster/19571",
        "summary": "We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then  selecting high quality examples from among these candidates (self-curation).  This data is then used to finetune a stronger model.  Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19571",
        "keywords": [
            "self curation",
            "self alignment",
            "instruction backtranslation",
            "instruction",
            "self augmentation",
            "language"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions \"finetuned on a small amount of seed data\", which implies that the model may have limitations in terms of data requirements, but it is not explicitly stated as a limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions \"finetuned on a small amount of seed data\", which implies that the model may have limitations in terms of data requirements, but it is not explicitly stated as a limitation."
    },
    {
        "title": "Unified Human-Scene Interaction via Prompted Chain-of-Contacts",
        "authors": "Zeqi Xiao;Tai Wang;Jingbo Wang;Jinkun Cao;Wenwei Zhang;Bo Dai;Dahua Lin;Jiangmiao Pang",
        "site": "https://iclr.cc/virtual/2024/poster/19566",
        "summary": "Human-Scene Interaction (HSI) is a vital component of fields like embodied AI and virtual reality. Despite advancements in motion quality and physical plausibility, two pivotal factors, versatile interaction control and the development of a user-friendly interface, require further exploration before the practical application of HSI. This paper presents a unified HSI framework, UniHSI, which supports unified control of diverse interactions through language commands. The framework defines interaction as ``Chain of Contacts (CoC)\", representing steps involving human joint-object part pairs. This concept is inspired by the strong correlation between interaction types and corresponding contact regions. Based on the definition, UniHSI constitutes a Large Language Model (LLM) Planner to translate language prompts into task plans in the form of CoC, and a Unified Controller that turns CoC into uniform task execution. To facilitate training and evaluation, we collect a new dataset named ScenePlan that encompasses thousands of task plans generated by LLMs based on diverse scenarios. Comprehensive experiments demonstrate the effectiveness of our framework in versatile task execution and generalizability to real scanned scenes.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19566",
        "keywords": [
            "human scene interaction",
            "chain of contacts"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"To facilitate training and evaluation, we collect a new dataset named ScenePlan that encompasses thousands of task plans generated by LLMs based on diverse scenarios.\"\n\nThis paper discusses LLMs but does not mention any limitations of the models in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"To facilitate training and evaluation, we collect a new dataset named ScenePlan that encompasses thousands of task plans generated by LLMs based on diverse scenarios.\"\n\nThis paper discusses LLMs but does not mention any limitations of the models in the abstract."
    },
    {
        "title": "What does the Knowledge Neuron Thesis Have to do with Knowledge?",
        "authors": "Jingcheng Niu;Andrew Liu;Zining Zhu;Gerald Penn",
        "site": "https://iclr.cc/virtual/2024/poster/19556",
        "summary": "We reassess the Knowledge Neuron (KN) Thesis: an interpretation of the mechanism underlying the ability of large language models to recall facts from a training corpus. This nascent thesis proposes that facts are recalled from the training corpus through the MLP weights in a manner resembling key-value memory, implying in effect that \"knowledge\" is stored in the network. Furthermore, by modifying the MLP modules, one can control the language model's generation of factual information. The plausibility of the KN thesis has been demonstrated by the success of KN-inspired model editing methods (Dai et al., 2022; Meng et al., 2022).We find that this thesis is, at best, an oversimplification. Not only have we found that we can edit the expression of certain linguistic phenomena using the same model editing methods but, through a more comprehensive evaluation, we have found that the KN thesis does not adequately explain the process of factual expression. While it is possible to argue that the MLP weights store complex patterns that are interpretable both syntactically and semantically, these patterns do not constitute \"knowledge.\" To gain a more comprehensive understanding of the knowledge representation process, we must look beyond the MLP weights and explore recent models' complex layer structures  and attention mechanisms.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19556",
        "keywords": [
            "knowledge",
            "knowledge representation",
            "knowledge neuron",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that this thesis is, at best, an oversimplification. Not only have we found that we can edit the expression of certain linguistic phenomena using the same model editing methods but, through a more comprehensive evaluation, we have found that the KN thesis does not adequately explain the process of factual expression.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We find that this thesis is, at best, an oversimplification. Not only have we found that we can edit the expression of certain linguistic phenomena using the same model editing methods but, through a more comprehensive evaluation, we have found that the KN thesis does not adequately explain the process of factual expression.\""
    },
    {
        "title": "Time Travel in LLMs: Tracing Data Contamination in Large Language Models",
        "authors": "Shahriar Golchin;Mihai Surdeanu",
        "site": "https://iclr.cc/virtual/2024/poster/19550",
        "summary": "Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in measuring LLMs' real effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination at the instance level; using this information, our approach then assesses wider contamination at the partition level. To estimate contamination of individual instances, we employ \"guided instruction:\" a prompt consisting of the dataset name, partition type, and the random-length initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or nearly matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset partition as contaminated if the average overlap score with the reference instances (as measured by ROUGE-L or BLEURT) is statistically significantly better with the completions from guided instruction compared to a \"general instruction\" that does not include the dataset and partition name. The second idea marks a dataset partition as contaminated if a classifier based on GPT-4 with few-shot in-context learning prompt marks multiple generated completions as exact/near-exact matches of the corresponding reference instances. Our best method achieves an accuracy between 92% and 100% in detecting if an LLM is contaminated with seven datasets, containing train and test/validation partitions, when contrasted with manual evaluation by human experts. Further, our findings indicate that GPT-4 is contaminated with AG News, WNLI, and XSum datasets.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19550",
        "keywords": [
            "language models",
            "contaminated",
            "data contamination"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in measuring LLMs' real effectiveness on other tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in measuring LLMs' real effectiveness on other tasks.\""
    },
    {
        "title": "Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints",
        "authors": "Chaoqi Wang;Yibo Jiang;Chenghao Yang;Han Liu;Yuxin Chen",
        "site": "https://iclr.cc/virtual/2024/poster/19543",
        "summary": "The increasing capabilities of large language models (LLMs) raise opportunities for artificial general intelligence but concurrently amplify safety concerns, such as potential misuse of AI systems, necessitating effective AI alignment. Reinforcement Learning from Human Feedback (RLHF) has emerged as a promising pathway towards AI alignment but brings forth challenges due to its complexity and dependence on a separate reward model. Direct Preference Optimization (DPO) has been proposed as an alternative; and it remains equivalent to RLHF under the reverse KL regularization constraint. This paper presents $f$-DPO, a generalized approach to DPO by incorporating diverse divergence constraints. We show that under certain $f$-divergences, including Jensen-Shannon divergence, forward KL divergences and $\\alpha$-divergences, the complex relationship between the reward and optimal policy can also be simplified by addressing the Karush–Kuhn–Tucker conditions. This eliminates the need for estimating the normalizing constant in the Bradley-Terry model and enables a tractable mapping between the reward function and the optimal policy. Our approach optimizes LLMs to align with human preferences in a more efficient and supervised manner under a broad set of divergence constraints. Empirically, adopting these divergences ensures a balance between alignment performance and generation diversity. Importantly, our $f$-DPO outperforms PPO-based methods in divergence efficiency, and divergence constraints directly influence expected calibration error (ECE).",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19543",
        "keywords": [
            "divergence constraints",
            "diverse divergence constraints",
            "direct preference optimization",
            "divergence efficiency",
            "regularization constraint",
            "human feedback",
            "reinforcement learning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The increasing capabilities of large language models (LLMs) raise opportunities for artificial general intelligence but concurrently amplify safety concerns, such as potential misuse of AI systems, necessitating effective AI alignment.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The increasing capabilities of large language models (LLMs) raise opportunities for artificial general intelligence but concurrently amplify safety concerns, such as potential misuse of AI systems, necessitating effective AI alignment.\""
    },
    {
        "title": "Ferret: Refer and Ground Anything Anywhere at Any Granularity",
        "authors": "Haoxuan You;Haotian Zhang;Zhe Gan;Xianzhi Du;Bowen Zhang;Zirui Wang;Liangliang Cao;Shih-Fu Chang;Yinfei Yang",
        "site": "https://iclr.cc/virtual/2024/poster/19537",
        "summary": "We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of understanding spatial referring of any shape or granularity within an image and accurately grounding open-vocabulary descriptions. To unify referring and grounding in the LLM paradigm, Ferret employs a novel and powerful hybrid region representation that integrates discrete coordinates and continuous features jointly to represent a region in the image. To extract the continuous features of versatile regions,  we propose a spatial-aware visual sampler, adept at handling varying sparsity across different shapes. Consequently, Ferret can accept diverse region inputs, such as points, bounding boxes, and free-form shapes. To bolster the desired capability of Ferret, we curate GRIT, a comprehensive refer-and-ground instruction tuning dataset including 1.1M samples that contain rich hierarchical spatial knowledge, with an additional 130K hard negative data to promote model robustness. The resulting model not only achieves superior performance in classical referring and grounding tasks, but also greatly outperforms existing MLLMs in region-based and localization-demanded multimodal chatting. Our evaluations also reveal a significantly improved capability of describing image details and a remarkable alleviation in object hallucination.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19537",
        "keywords": [
            "granularity"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our evaluations also reveal... a remarkable alleviation in object hallucination.\"\n\nThis rating is given because the paper mentions a limitation (object hallucination) but does not explore it in depth, and the primary focus of the paper is on the proposed model and its capabilities.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Our evaluations also reveal... a remarkable alleviation in object hallucination.\"\n\nThis rating is given because the paper mentions a limitation (object hallucination) but does not explore it in depth, and the primary focus of the paper is on the proposed model and its capabilities."
    },
    {
        "title": "CrIBo: Self-Supervised Learning via Cross-Image Object-Level Bootstrapping",
        "authors": "Tim Lebailly;Thomas Stegmüller;Behzad Bozorgtabar;Jean-Philippe Thiran;Tinne Tuytelaars",
        "site": "https://iclr.cc/virtual/2024/poster/19518",
        "summary": "Leveraging nearest neighbor retrieval for self-supervised representation learning has proven beneficial with object-centric images. However, this approach faces limitations when applied to scene-centric datasets, where multiple objects within an image are only implicitly captured in the global representation. Such global bootstrapping can lead to undesirable entanglement of object representations. Furthermore, even object-centric datasets stand to benefit from a finer-grained bootstrapping approach. In response to these challenges, we introduce a novel $\\textbf{Cr}$oss-$\\textbf{I}$mage Object-Level $\\textbf{Bo}$otstrapping method tailored to enhance dense visual representation learning. By employing object-level nearest neighbor bootstrapping throughout the training, CrIBo emerges as a notably strong and adequate candidate for in-context learning, leveraging nearest neighbor retrieval at test time. CrIBo shows state-of-the-art performance on the latter task while being highly competitive in more standard downstream segmentation tasks. Our code and pretrained models are publicly available at https://github.com/tileb1/CrIBo.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19518",
        "keywords": [
            "bootstrapping",
            "object level bootstrapping",
            "self supervised",
            "self supervised learning",
            "neighbor bootstrapping"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "BTR: Binary Token Representations for Efficient Retrieval Augmented Language Models",
        "authors": "Qingqing Cao;Sewon Min;Yizhong Wang;Hannaneh Hajishirzi",
        "site": "https://iclr.cc/virtual/2024/poster/19511",
        "summary": "Retrieval augmentation addresses many critical problems in large language models such as hallucination, staleness, and privacy leaks.However, running retrieval-augmented language models (LMs) is slow and difficult to scale due to processing large amounts of retrieved text. We introduce binary token representations (BTR), which use 1-bit vectors to precompute every token in passages, significantly reducing computation during inference. Despite the potential loss of accuracy, our new calibration techniques and training objectives restore performance. Combined with offline and runtime compression, this only requires 127GB of disk space for encoding 3 billion tokens in Wikipedia.Our experiments show that on five knowledge-intensive NLP tasks, BTR accelerates state-of-the-art inference by up to 4x and reduces storage by over 100x while maintaining over 95% task performance. Our code is publicly available at https://github.com/csarron/BTR.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19511",
        "keywords": [
            "binary token representations",
            "augmented language models",
            "retrieval augmentation",
            "hallucination"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"running retrieval-augmented language models (LMs) is slow and difficult to scale due to processing large amounts of retrieved text.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"running retrieval-augmented language models (LMs) is slow and difficult to scale due to processing large amounts of retrieved text.\""
    },
    {
        "title": "LEGO-Prover: Neural Theorem Proving with Growing Libraries",
        "authors": "Haiming Wang;Huajian Xin;Chuanyang Zheng;Zhengying Liu;Qingxing Cao;Yinya Huang;Jing Xiong;Han Shi;Enze Xie;Jian Yin;Zhenguo Li;Xiaodan Liang",
        "site": "https://iclr.cc/virtual/2024/poster/19499",
        "summary": "Despite the success of large language models (LLMs), the task of theorem proving still remains one of the hardest reasoning tasks that is far from being fully solved. Prior methods using language models have demonstrated promising results, but they still struggle to prove even middle school level theorems. One common limitation of these methods is that they assume a fixed theorem library during the whole theorem proving process. However, as we all know, creating new useful theorems or even new theories is not only helpful but crucial and necessary for advancing mathematics and proving harder and deeper results. In this work, we present LEGO-Prover, which employs a growing skill library containing verified lemmas as skills to augment the capability of LLMs used in theorem proving. By constructing the proof modularly, LEGO-Prover enables LLMs to utilize existing skills retrieved from the library and to create new skills during the proving process. These skills are further evolved (by prompting an LLM) to enrich the library on another scale. Modular and reusable skills are constantly added to the library to enable tackling increasingly intricate mathematical problems. Moreover, the learned library further bridges the gap between human proofs and formal proofs by making it easier to impute missing steps. LEGO-Prover advances the state-of-the-art pass rate on miniF2F-valid (48.0\\% to 57.0\\%) and miniF2F-test (45.5\\% to 50.0\\%). During the proving process, LEGO-Prover also generates over 20,000 skills (theorems/lemmas) and adds them to the growing library. Our ablation study indicates that these newly added skills are indeed helpful for proving theorems, resulting in a 4.9\\% improvement in success rate",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19499",
        "keywords": [
            "theorem proving",
            "lego prover",
            "growing libraries",
            "language models",
            "neural theorem proving"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"One common limitation of these methods is that they assume a fixed theorem library during the whole theorem proving process.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"One common limitation of these methods is that they assume a fixed theorem library during the whole theorem proving process.\""
    },
    {
        "title": "One For All: Towards Training One Graph Model For All Classification Tasks",
        "authors": "Hao Liu;Jiarui Feng;Lecheng Kong;Ningyue Liang;Dacheng Tao;Yixin Chen;Muhan Zhang",
        "site": "https://iclr.cc/virtual/2024/poster/19474",
        "summary": "Designing a single model to address multiple tasks has been a long-standing objective in artificial intelligence. Recently, large language models have demonstrated exceptional capability in solving different tasks within the language domain. However, a unified model for various graph tasks remains underexplored, primarily due to the challenges unique to the graph learning domain. First, graph data from different areas carry distinct attributes and follow different distributions. Such discrepancy makes it hard to represent graphs in a single representation space. Second, tasks on graphs diversify into node, link, and graph tasks, requiring distinct embedding strategies. Finally, an appropriate graph prompting paradigm for in-context learning is unclear. We proposeOne for All (OFA), the first general framework that can use a single graph model to address the above challenges. Specifically, OFA proposes text-attributed graphs to unify different graph data by describing nodes and edges with natural language and uses language models to encode the diverse and possibly cross-domain text attributes to feature vectors in the same embedding space. Furthermore, OFA introduces the concept of nodes-of-interest to standardize different tasks with a single task representation. For in-context learning on graphs, OFA introduces a novel graph prompting paradigm that appends prompting substructures to the input graph, which enables it to address varied tasks without fine-tuning. We train the OFA model using graph data from multiple domains (including citation networks, molecular graphs, knowledge graphs, etc.) simultaneously and evaluate its ability in supervised, few-shot, and zero-shot learning scenarios. OFA performs well across different tasks, making it the first general-purpose across-domains classification model on graphs.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19474",
        "keywords": [
            "classification",
            "attributed graphs",
            "graph",
            "graph learning",
            "graph tasks",
            "classification model",
            "graph prompting paradigm",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, a unified model for various graph tasks remains underexplored, primarily due to the challenges unique to the graph learning domain.\"\n\nThis abstract mentions a limitation of LLMs in the context of graph learning, but it is not the primary focus of the paper. The limitation is mentioned briefly to motivate the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, a unified model for various graph tasks remains underexplored, primarily due to the challenges unique to the graph learning domain.\"\n\nThis abstract mentions a limitation of LLMs in the context of graph learning, but it is not the primary focus of the paper. The limitation is mentioned briefly to motivate the proposed solution."
    },
    {
        "title": "Image2Sentence based Asymmetrical Zero-shot Composed Image Retrieval",
        "authors": "Yongchao Du;Min Wang;Wengang Zhou;Shuping Hui;Houqiang Li",
        "site": "https://iclr.cc/virtual/2024/poster/19437",
        "summary": "The task of composed image retrieval (CIR) aims to retrieve images based on the query image and the text describing the users' intent. Existing methods have made great progress with the advanced large vision-language (VL) model in CIR task, however, they generally suffer from two main issues: lack of labeled triplets for model training and difficulty of deployment on resource-restricted environments when deploying the large vision-language model. To tackle the above problems, we propose Image2Sentence based Asymmetric zero-shot composed image retrieval (ISA), which takes advantage of the VL model and only relies on unlabeled images for composition learning. In the framework, we propose a new adaptive token learner that maps an image to a sentence in the word embedding space of VL model.  The sentence adaptively captures discriminative visual information and is further integrated with the text modifier. An asymmetric structure is devised for flexible deployment, in which the lightweight model is adopted for the query side while the large VL model is deployed on the gallery side. The global contrastive distillation and the local alignment regularization are adopted for the alignment between the light model and the VL model for CIR task.  Our experiments demonstrate that the proposed ISA could better cope with the real retrieval scenarios and further improve retrieval accuracy and efficiency.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19437",
        "keywords": [
            "image retrieval",
            "composed image retrieval",
            "image2sentence"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing methods have made great progress with the advanced large vision-language (VL) model in CIR task, however, they generally suffer from two main issues: lack of labeled triplets for model training and difficulty of deployment on resource-restricted environments when deploying the large vision-language model.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing methods have made great progress with the advanced large vision-language (VL) model in CIR task, however, they generally suffer from two main issues: lack of labeled triplets for model training and difficulty of deployment on resource-restricted environments when deploying the large vision-language model.\""
    },
    {
        "title": "Multiscale Positive-Unlabeled Detection of AI-Generated Texts",
        "authors": "Yuchuan Tian;Hanting Chen;Xutao Wang;Zheyuan Bai;QINGHUA ZHANG;Ruifeng Li;Chao Xu;Yunhe Wang",
        "site": "https://iclr.cc/virtual/2024/poster/19428",
        "summary": "Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are astonishing at generating human-like texts, but they may impact the authenticity of texts. Previous works proposed methods to detect these AI-generated texts, including simple ML classifiers, pretrained-model-based zero-shot methods, and finetuned language classification models. However, mainstream detectors always fail on short texts, like SMSes, Tweets, and reviews. In this paper, a Multiscale Positive-Unlabeled (MPU) training framework is proposed to address the difficulty of short-text detection without sacrificing long-texts. Firstly, we acknowledge the human-resemblance property of short machine texts, and rephrase AI text detection as a partial Positive-Unlabeled (PU) problem by regarding these short machine texts as partially \"unlabeled\". Then in this PU context, we propose the length-sensitive Multiscale PU Loss, where a recurrent model in abstraction is used to estimate positive priors of scale-variant corpora. Additionally, we introduce a Text Multiscaling module to enrich training corpora. Experiments show that our MPU method augments detection performance on long AI-generated texts, and significantly improves short-text detection of language model detectors. Language Models trained with MPU could outcompete existing detectors on various short-text and long-text detection benchmarks. The codes are available at https://github.com/mindspore-lab/mindone/tree/master/examples/detectchatgpt and https://github.com/YuchuanTian/AIGCtext_detector.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19428",
        "keywords": [
            "unlabeled detection",
            "text detection",
            "positive unlabeled",
            "partial positive unlabeled"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, mainstream detectors always fail on short texts, like SMSes, Tweets, and reviews.\"\n\nThis rating is given because the abstract mentions a limitation of LLM detectors (failing on short texts) in passing, but does not elaborate on it and focuses on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, mainstream detectors always fail on short texts, like SMSes, Tweets, and reviews.\"\n\nThis rating is given because the abstract mentions a limitation of LLM detectors (failing on short texts) in passing, but does not elaborate on it and focuses on the proposed solution."
    },
    {
        "title": "SaProt: Protein Language Modeling with Structure-aware Vocabulary",
        "authors": "Jin Su;Chenchen Han;Yuyang Zhou;Junjie Shan;Xibin Zhou;Fajie Yuan",
        "site": "https://iclr.cc/virtual/2024/poster/19394",
        "summary": "Large-scale protein language models (PLMs), such as the ESM family, have achieved remarkable performance in various downstream tasks related to protein structure and function by undergoing unsupervised training on residue sequences. They have become essential tools for researchers and practitioners in biology.  However, a limitation of vanilla PLMs is their lack of explicit consideration for protein structure information, which suggests the potential for further improvement. Motivated by this, we introduce the concept of a ``structure-aware vocabulary\" that  integrates residue tokens with structure tokens.    The structure tokens are  derived  by encoding the 3D structure of proteins using Foldseek. We then propose SaProt, a large-scale general-purpose PLM trained on an extensive dataset comprising approximately 40 million protein sequences and structures. Through extensive evaluation, our SaProt model surpasses well-established and renowned baselines across 10 significant downstream tasks, demonstrating its exceptional capacity and broad applicability. We have made the code, pre-trained model, and all relevant materials available at https://github.com/westlake-repl/SaProt.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19394",
        "keywords": [
            "foldseek",
            "protein language modeling",
            "structure",
            "structure tokens",
            "residue tokens",
            "structure aware vocabulary",
            "vocabulary",
            "saprot model"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, a limitation of vanilla PLMs is their lack of explicit consideration for protein structure information, which suggests the potential for further improvement.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, a limitation of vanilla PLMs is their lack of explicit consideration for protein structure information, which suggests the potential for further improvement.\""
    },
    {
        "title": "LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models",
        "authors": "Yukang Chen;Shengju Qian;Haotian Tang;Xin Lai;Zhijian Liu;Song Han;Jiaya Jia",
        "site": "https://iclr.cc/virtual/2024/poster/19390",
        "summary": "We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost.Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shifted sparse attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-efficient fine-tuning regime for context expansion. Notably, we find that LoRA for context extension works well under the premise of trainable embedding and normalization. LongLoRA combines this improved LoRA with S^2-Attn. LongLoRA demonstrates strong empirical results on various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends Llama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8x A100 machine. LongLoRA extends models' context while retaining their original architectures, and is compatible with most existing techniques, like Flash-Attention2. In addition, we further conduct supervised fine-tuning with LongLoRA and our long instruction-following LongAlpaca dataset. All our code, models, dataset, and demo are available at https://github.com/dvlab-research/LongLoRA.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19390",
        "keywords": [
            "fine tuning",
            "longlora",
            "self attention",
            "shifted sparse attention",
            "language models",
            "large language models",
            "long context sizes",
            "global attention",
            "costs",
            "context extension"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources.\""
    },
    {
        "title": "BooookScore: A systematic exploration of book-length summarization in the era of LLMs",
        "authors": "Yapei Chang;Kyle Lo;Tanya Goyal;Mohit Iyyer",
        "site": "https://iclr.cc/virtual/2024/poster/19359",
        "summary": "Summarizing book-length documents ($>$100K tokens)  that exceed the context window size of large language models (LLMs) requires first breaking the input document into smaller chunks and then prompting an LLM to merge, update, and compress chunk-level summaries. Despite the complexity and importance of this task, it has yet to be meaningfully studied due to the challenges of evaluation: existing book-length summarization datasets (e.g., BookSum) are in the pretraining data of most public LLMs, and existing evaluation methods struggle to capture errors made by modern LLM summarizers. In this paper, we present the first study of the coherence of LLM-based book-length summarizers implemented via two prompting workflows: (1) hierarchically merging chunk-level summaries, and (2) incrementally updating a running summary. We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. Because human evaluation is expensive and time-consuming, we develop an automatic metric, BooookScore, that measures the proportion of sentences in a summary that do not contain any of the identified error types. BooookScore has high agreement with human annotations and allows us to systematically evaluate the impact of many other critical parameters (e.g., chunk size, base LLM) while saving \\$15K USD and 500 hours in human evaluation costs. We find that closed-source LLMs such as GPT-4 and Claude 2 produce summaries with higher BooookScore than those generated by open-source models. While LLaMA 2 falls behind other models, Mixtral achieves performance on par with GPT-3.5-Turbo. Incremental updating yields lower BooookScore but higher level of detail than hierarchical merging, a trade-off sometimes preferred by annotators. We release code and annotations to spur more principled research on book-length summarization.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19359",
        "keywords": [
            "summarizing",
            "book length summarization",
            "prompting workflows"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs.\""
    },
    {
        "title": "Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks",
        "authors": "Vaidehi Patil;Peter Hase;Mohit Bansal",
        "site": "https://iclr.cc/virtual/2024/poster/19353",
        "summary": "Pretrained language models sometimes possess knowledge that we do not wish them to, including memorized personal information and knowledge that could be used to harm people. They can also output toxic or harmful text. To mitigate these safety and informational issues, we propose an attack-and-defense framework for studying the task of deleting sensitive information directly from model weights. We study direct edits to model weights because (1) this approach should guarantee that particular deleted information is never extracted by future prompt attacks, and (2) it should protect against whitebox attacks, which is necessary for making claims about safety/privacy in a setting where publicly available model weights could be used to elicit sensitive information. Our threat model assumes that an attack succeeds if the answer to a sensitive question is located among a set of B generated candidates, based on scenarios where the information would be insecure if the answer is among B candidates. Experimentally, we show that even state-of-the-art model editing methods such as ROME struggle to truly delete factual information from models like GPT-J, as our whitebox and blackbox attacks can recover “deleted” information from an edited model 38% of the time. These attacks leverage two key observations: (1) that traces of deleted information can be found in intermediate model hidden states, and (2) that applying an editing method for one question may not delete information across rephrased versions of the question. Finally, we provide new defense methods that protect against some extraction attacks, but we do not find a single universally effective defense method. Our results suggest that truly deleting sensitive information is a tractable but difficult problem, since even relatively low attack success rates have potentially severe implications for the deployment of language models in a world where individuals enjoy ownership of their personal data, a right to privacy, and safety from harmful model outputs.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19353",
        "keywords": [
            "deleted information",
            "extraction attacks",
            "defense method"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Experimentally, we show that even state-of-the-art model editing methods such as ROME struggle to truly delete factual information from models like GPT-J, as our whitebox and blackbox attacks can recover “deleted” information from an edited model 38% of the time.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Experimentally, we show that even state-of-the-art model editing methods such as ROME struggle to truly delete factual information from models like GPT-J, as our whitebox and blackbox attacks can recover “deleted” information from an edited model 38% of the time.\""
    },
    {
        "title": "EQA-MX: Embodied Question Answering using Multimodal Expression",
        "authors": "Md Mofijul Islam;Alexi Gladstone;Riashat Islam;Tariq Iqbal",
        "site": "https://iclr.cc/virtual/2024/poster/19349",
        "summary": "Humans predominantly use verbal utterances and nonverbal gestures (e.g., eye gaze and pointing gestures) in their natural interactions. For instance, pointing gestures and verbal information is often required to comprehend questions such as \"what object is that?\" Thus, this question-answering (QA) task involves complex reasoning of multimodal expressions (verbal utterances and nonverbal gestures). However, prior works have explored QA tasks in non-embodied settings, where questions solely contain verbal utterances from a single verbal and visual perspective. In this paper, we have introduced 8 novel embodied question answering (EQA) tasks to develop learning models to comprehend embodied questions with multimodal expressions. We have developed a novel large-scale dataset, EQA-MX, with over 8 million diverse embodied QA data samples involving multimodal expressions from multiple visual and verbal perspectives. To learn salient multimodal representations from discrete verbal embeddings and continuous wrapping of multiview visual representations, we propose a vector-quantization (VQ) based multimodal representation learning model, VQ-Fusion, for the EQA tasks. Our extensive experimental results suggest that VQ-Fusion can improve the performance of existing state-of-the-art visual-language models up to 13% across EQA tasks.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19349",
        "keywords": [
            "question answering",
            "embodied question answering",
            "embodied",
            "multimodal",
            "multimodal expressions",
            "vector quantization",
            "eqa tasks",
            "nonverbal gestures"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models",
        "authors": "Wenqi Shao;Mengzhao Chen;Zhaoyang Zhang;Peng Xu;Lirui Zhao;Zhiqian Li;Kaipeng Zhang;Peng Gao;Yu Qiao;Ping Luo",
        "site": "https://iclr.cc/virtual/2024/poster/19323",
        "summary": "Large language models (LLMs) have revolutionized natural language processing tasks. However, their practical deployment is hindered by their immense memory and computation requirements. Although recent post-training quantization (PTQ) methods are effective in reducing memory footprint and improving the computational efficiency of LLM, they hand-craft quantization parameters, leading to low performance, especially in extremely low-bit quantization. To tackle this issue, we introduce an Omnidirectionally calibrated Quantization ($\\textbf{OmniQuant}$) technique for LLMs, which achieves good performance in diverse quantization settings while maintaining the computational efficiency of PTQ by efficiently optimizing various quantization parameters. OmniQuant comprises two innovative components including Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET). LWC modulates the extreme values of weights by optimizing the clipping threshold. Meanwhile, LET tackles activation outliers by shifting the challenge of quantization from activations to weights. Operating within a differentiable framework using block-wise error minimization, OmniQuant can optimize the quantization process efficiently for both weight-only and weight-activation quantization. For instance, the LLaMA-2 model family size 7-70B can be processed with OmniQuant on a single A100-40G GPU within 1-16 hours using 128 samples. Extensive experiments validate OmniQuant's superior performance across diverse quantization configurations such as W4A4 (4-bit weight, 4-bit activation), W6A6, W4A16, W3A16, and W2A16. Additionally, OmniQuant demonstrates effectiveness in instruction-tuned models and delivers notable improvements in inference speed and memory reduction on real devices. Codes are available at \\url{https://github.com/OpenGVLab/OmniQuant}.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19323",
        "keywords": [
            "language models",
            "quantization",
            "large language models",
            "calibrated quantization",
            "clipping threshold",
            "weight activation quantization",
            "learnable weight clipping",
            "outliers"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, their practical deployment is hindered by their immense memory and computation requirements.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, their practical deployment is hindered by their immense memory and computation requirements.\""
    },
    {
        "title": "MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data",
        "authors": "Yinya Huang;Xiaohan Lin;Zhengying Liu;Qingxing Cao;Huajian Xin;Haiming Wang;Zhenguo Li;Linqi Song;Xiaodan Liang",
        "site": "https://iclr.cc/virtual/2024/poster/19310",
        "summary": "Recent large language models (LLMs) have witnessed significant advancement in various tasks, including mathematical reasoning and theorem proving. As these two tasks require strict and formal multi-step inference, they are appealing domains for exploring the reasoning ability of LLMs but still face important challenges. Previous studies such as Chain-of-Thought (CoT) have revealed the effectiveness of intermediate steps guidance. However, such step-wise annotation requires heavy labor, leading to insufficient training steps for current benchmarks. To fill this gap, this work introduces MUSTARD, a data generation framework that masters uniform synthesis of theorem and proof data of high quality and diversity. MUSTARD synthesizes data in three stages: (1) It samples a few mathematical concept seeds as the problem category. (2) Then, it prompts a generative language model with the sampled concepts to obtain both the problems and their step-wise formal solutions. (3) Lastly, the framework utilizes a proof assistant (e.g., Lean Prover) to filter the valid proofs. With the proposed MUSTARD, we present a theorem-and-proof benchmark MUSTARDSAUCE with 5,866 valid data points. Each data point contains an informal statement, an informal proof, and a translated formal proof that passes the prover validation. We perform extensive analysis and demonstrate that MUSTARD generates validated high-quality step-by-step data. We further apply the MUSTARDSAUCE for fine-tuning smaller language models. The fine-tuned Llama 2-7B achieves a 15.41% average relative performance gain in automated theorem proving, and 8.18% in math word problems. Codes and data are available at https://github.com/Eleanor-H/MUSTARD.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19310",
        "keywords": [
            "theorem proving",
            "performance",
            "validated"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"As these two tasks require strict and formal multi-step inference, they are appealing domains for exploring the reasoning ability of LLMs but still face important challenges.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"As these two tasks require strict and formal multi-step inference, they are appealing domains for exploring the reasoning ability of LLMs but still face important challenges.\""
    },
    {
        "title": "A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis",
        "authors": "Izzeddin Gur;Hiroki Furuta;Austin V Huang;Mustafa Safdari;Yutaka Matsuo;Douglas Eck;Aleksandra Faust",
        "site": "https://iclr.cc/virtual/2024/poster/19300",
        "summary": "Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation.However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML.We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions.WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those.We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization.We empirically demonstrate that our modular recipe improves the success on real websites by over 50%, and that HTML-T5 is the best model to solve various HTML understanding tasks; achieving 18.7% higher success rate than the prior method on MiniWoB web automation benchmark, and SoTA performance on Mind2Web, an offline task planning evaluation.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19300",
        "keywords": [
            "webagent",
            "web automation",
            "html t5",
            "synthesis"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML.\"\n\nThis paper briefly mentions three limitations of LLMs (open domainness, limited context length, and lack of inductive bias on HTML) but does not explore them in depth.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML.\"\n\nThis paper briefly mentions three limitations of LLMs (open domainness, limited context length, and lack of inductive bias on HTML) but does not explore them in depth."
    },
    {
        "title": "Towards Understanding Factual Knowledge of Large Language Models",
        "authors": "Xuming Hu;Junzhe Chen;Xiaochuan Li;Yufei Guo;Lijie Wen;Philip S. Yu;Zhijiang Guo",
        "site": "https://iclr.cc/virtual/2024/poster/19298",
        "summary": "Large language models (LLMs) have recently driven striking performance improvements across a range of natural language processing tasks. The factual knowledge acquired during pretraining and instruction tuning can be useful in various downstream tasks, such as question answering, and language generation. Unlike conventional Knowledge Bases (KBs) that explicitly store factual knowledge, LLMs implicitly store facts in their parameters. Content generated by the LLMs can often exhibit inaccuracies or deviations from the truth, due to facts that can be incorrectly induced or become obsolete over time. To this end, we aim to explore the extent and scope of factual knowledge within LLMs by designing the benchmark Pinocchio. Pinocchio contains 20K diverse factual questions that span different sources, timelines, domains, regions, and languages. Furthermore, we investigate whether LLMs can compose multiple facts, update factual knowledge temporally, reason over multiple pieces of facts, identify subtle factual differences, and resist adversarial examples. Extensive experiments on different sizes and types of LLMs show that existing LLMs still lack factual knowledge and suffer from various spurious correlations. We believe this is a critical bottleneck for realizing trustworthy artificial intelligence. The dataset Pinocchio and our codes are publicly available at: https://github.com/THU-BPM/Pinocchio.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19298",
        "keywords": [
            "large language models",
            "factual knowledge",
            "knowledge",
            "trustworthy artificial intelligence",
            "factual knowledge acquired",
            "spurious correlations"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Content generated by the LLMs can often exhibit inaccuracies or deviations from the truth, due to facts that can be incorrectly induced or become obsolete over time.\"; \"Extensive experiments on different sizes and types of LLMs show that existing LLMs still lack factual knowledge and suffer from various spurious correlations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Content generated by the LLMs can often exhibit inaccuracies or deviations from the truth, due to facts that can be incorrectly induced or become obsolete over time.\"; \"Extensive experiments on different sizes and types of LLMs show that existing LLMs still lack factual knowledge and suffer from various spurious correlations.\""
    },
    {
        "title": "Coeditor: Leveraging Repo-level Diffs for Code Auto-editing",
        "authors": "Jiayi Wei;Greg Durrett;Isil Dillig",
        "site": "https://iclr.cc/virtual/2024/poster/19265",
        "summary": "Developers often dedicate significant time to maintaining and refactoring existing code. However, most prior work on generative models for code focuses solely on creating new code, overlooking the distinctive needs of editing existing code. In this work, we explore a multi-round code auto-editing setting, aiming to predict edits to a code region based on recent changes within the same codebase. Our model, Coeditor, is a fine-tuned language model specifically designed for code editing tasks. We represent code changes using a line diff format and employ static analysis to form large customized model contexts, ensuring the availability of appropriate information for prediction. We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation. In a simplified single-round, single-edit task, Coeditor significantly outperforms GPT-3.5 and SOTA open-source code completion models (bringing exact-match accuracy from 34.7 up to 60.4), demonstrating the benefits of incorporating editing history for code completion. In a multi-round, multi-edit setting, we observe substantial gains by iteratively conditioning on additional user edits. We have open-sourced our code, data, and model weights to encourage future research and have released a VSCode extension powered by our model for interactive IDE usage.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19265",
        "keywords": [
            "repo level diffs",
            "refactoring",
            "code auto editing",
            "line diff"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"In a simplified single-round, single-edit task, Coeditor significantly outperforms GPT-3.5 and SOTA open-source code completion models (bringing exact-match accuracy from 34.7 up to 60.4)\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"In a simplified single-round, single-edit task, Coeditor significantly outperforms GPT-3.5 and SOTA open-source code completion models (bringing exact-match accuracy from 34.7 up to 60.4)\""
    },
    {
        "title": "LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset",
        "authors": "Lianmin Zheng;Wei-Lin Chiang;Ying Sheng;Tianle Li;Siyuan Zhuang;Zhanghao Wu;Yonghao Zhuang;Zhuohan Li;Zi Lin;Eric Xing;Joseph E. Gonzalez;Ion Stoica;Hao Zhang",
        "site": "https://iclr.cc/virtual/2024/poster/19219",
        "summary": "Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is publicly available at https://huggingface.co/datasets/lmsys/lmsys-chat-1m.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19219",
        "keywords": [
            "lmsys chat",
            "lmsys chat 1m",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the paper does not mention any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the paper does not mention any limitations of LLMs."
    },
    {
        "title": "Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions",
        "authors": "Juncheng Li;Kaihang Pan;Zhiqi Ge;Minghe Gao;Wei Ji;Wenqiao Zhang;Tat-Seng Chua;Siliang Tang;Hanwang Zhang;Yueting Zhuang",
        "site": "https://iclr.cc/virtual/2024/poster/19212",
        "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have been utilizing Visual Prompt Generators (VPGs) to convert visual features into tokens that LLMs can recognize. This is achieved by training the VPGs on millions of image-caption pairs, where the VPG-generated tokens of images are fed into a frozen LLM to generate the corresponding captions. However, this image-captioning based training objective inherently biases the VPG to concentrate solely on the primary visual contents sufficient for caption generation, often neglecting other visual details. This shortcoming results in MLLMs’ underperformance in comprehending demonstrative instructions consisting of multiple, interleaved, and multimodal instructions that demonstrate the required context to complete a task. To address this issue, we introduce a generic and lightweight Visual Prompt Generator Complete module (VPG-C), which can infer and complete the missing details essential for comprehending demonstrative instructions. Further, we propose a synthetic discriminative training strategy to fine-tune VPG-C, eliminating the need for supervised demonstrative instructions. As for evaluation, we build DEMON, a comprehensive benchmark for demonstrative instruction understanding. Synthetically trained with the proposed strategy, VPG-C achieves significantly stronger zero-shot performance across all tasks of DEMON. Further evaluation on the MME and OwlEval benchmarks also demonstrate the superiority of VPG-C. The code and models are available at https://github.com/DCDmllm/Cheetah.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19212",
        "keywords": [],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, this image-captioning based training objective inherently biases the VPG to concentrate solely on the primary visual contents sufficient for caption generation, often neglecting other visual details. This shortcoming results in MLLMs’ underperformance in comprehending demonstrative instructions consisting of multiple, interleaved, and multimodal instructions that demonstrate the required context to complete a task.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, this image-captioning based training objective inherently biases the VPG to concentrate solely on the primary visual contents sufficient for caption generation, often neglecting other visual details. This shortcoming results in MLLMs’ underperformance in comprehending demonstrative instructions consisting of multiple, interleaved, and multimodal instructions that demonstrate the required context to complete a task.\""
    },
    {
        "title": "WildChat: 1M ChatGPT Interaction Logs in the Wild",
        "authors": "Wenting Zhao;Xiang Ren;Jack Hessel;Claire Cardie;Yejin Choi;Yuntian Deng",
        "site": "https://iclr.cc/virtual/2024/poster/19206",
        "summary": "Chatbots such as GPT-4 and ChatGPT are now serving millions of users. Despite their widespread use, there remains a lack of public datasets showcasing how these tools are used by a population of users in practice. To bridge this gap, we offered free access to ChatGPT for online users in exchange for their affirmative, consensual opt-in to anonymously collect their chat transcripts and request headers. From this, we compiled WildChat, a corpus of 1 million user-ChatGPT conversations, which consists of over 2.5 million interaction turns. We compare WildChat with other popular user-chatbot interaction datasets, and find that our dataset offers the most diverse user prompts, contains the largest number of languages, and presents the richest variety of potentially toxic use-cases for researchers to study. In addition to timestamped chat transcripts, we enrich the dataset with demographic data, including state, country, and hashed IP addresses, alongside request headers. This augmentation allows for more detailed analysis of user behaviors across different geographical regions and temporal dimensions. Finally, because it captures a broad range of use cases, we demonstrate the dataset’s potential utility in fine-tuning instruction-following models. WildChat is released at https://wildchat.allen.ai under AI2 ImpACT Licenses.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19206",
        "keywords": [
            "chatgpt",
            "chatgpt conversations",
            "user chatbot interaction"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions \"potentially toxic use-cases\" which could be related to a limitation of LLMs, however it is not explicitly stated as a limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions \"potentially toxic use-cases\" which could be related to a limitation of LLMs, however it is not explicitly stated as a limitation."
    },
    {
        "title": "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets",
        "authors": "Seonghyeon Ye;Doyoung Kim;Sungdong Kim;Hyeonbin Hwang;Seungone Kim;Yongrae Jo;James Thorne;Juho Kim;Minjoon Seo",
        "site": "https://iclr.cc/virtual/2024/poster/19170",
        "summary": "Evaluation of Large Language Models (LLMs) is challenging because instruction-following necessitates alignment with human values and the required set of skills varies depending on the instruction. However, previous studies have mainly focused on coarse-grained evaluation (i.e. overall preference-based evaluation), which limits interpretability since it does not consider the nature of user instructions that require instance-wise skill composition. In this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on Alignment Skill Sets), a fine-grained evaluation protocol for both human-based and model-based evaluation which decomposes coarse-level scoring to a skill set-level scoring for each instruction. We experimentally observe that the fine-graininess of evaluation is crucial for attaining a holistic view of model performance and increasing the reliability of the evaluation. Using FLASK, we compare multiple open-source and proprietary LLMs and observe a high correlation between model-based and human-based evaluations.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19170",
        "keywords": [
            "skill sets",
            "skill composition",
            "alignment skill sets",
            "skill set level scoring"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Evaluation of Large Language Models (LLMs) is challenging because instruction-following necessitates alignment with human values and the required set of skills varies depending on the instruction.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Evaluation of Large Language Models (LLMs) is challenging because instruction-following necessitates alignment with human values and the required set of skills varies depending on the instruction.\""
    },
    {
        "title": "Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy",
        "authors": "Simon Ging;Maria Alejandra Bravo;Thomas Brox",
        "site": "https://iclr.cc/virtual/2024/poster/19102",
        "summary": "The evaluation of text-generative vision-language models is a challenging yet crucial endeavor. By addressing the limitations of existing Visual Question Answering (VQA) benchmarks and proposing innovative evaluation methodologies, our research seeks to advance our understanding of these models’ capabilities. We propose a novel VQA benchmark based on well-known visual classification datasets which allows a granular evaluation of text-generative vision-language models and their comparison with discriminative vision-language models. To improve the assessment of coarse answers on fine-grained classification tasks, we suggest using the semantic hierarchy of the label space to ask automatically generated follow-up questions about the ground-truth category. Finally, we compare traditional NLP and LLM-based metrics for the problem of evaluating model predictions given ground-truth answers. We perform a human evaluation study upon which we base our decision on the final metric. We apply our benchmark to a suite of vision-language models and show a detailed comparison of their abilities on object, action, and attribute classification. Our contributions aim to lay the foundation for more precise and meaningful assessments, facilitating targeted progress in the exciting field of vision-language modeling.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19102",
        "keywords": [
            "benchmark",
            "semantic hierarchy",
            "vision language models",
            "generative vision language models",
            "discriminative vision language models",
            "perform",
            "granular evaluation",
            "evaluation",
            "classification"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"By addressing the limitations of existing Visual Question Answering (VQA) benchmarks\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"By addressing the limitations of existing Visual Question Answering (VQA) benchmarks\""
    },
    {
        "title": "Illusory Attacks: Information-theoretic detectability matters in adversarial attacks",
        "authors": "Tim Franzmeyer;Stephen Marcus McAleer;Joao F. Henriques;Jakob Nicolaus Foerster;Philip Torr;Adel Bibi;Christian Schroeder de Witt",
        "site": "https://iclr.cc/virtual/2024/poster/19082",
        "summary": "Autonomous agents deployed in the real world need to be robust against adversarial attacks on sensory inputs. Robustifying agent policies requires anticipating the strongest attacks possible.We demonstrate that existing observation-space attacks on reinforcement learning agents have a common weakness: while effective, their lack of information-theoretic detectability constraints makes them \\textit{detectable} using automated means or human inspection. Detectability is undesirable to adversaries as it may trigger security escalations.We introduce \\textit{\\eattacks{}}, a novel form of adversarial attack on sequential decision-makers that is both effective and of $\\epsilon-$bounded statistical detectability. We propose a novel dual ascent algorithm to learn such attacks end-to-end.Compared to existing attacks, we empirically find \\eattacks{} to be significantly harder to detect with automated methods, and a small study with human participants\\footnote{IRB approval under reference R84123/RE001} suggests they are similarly harder to detect for humans. Our findings suggest the need for better anomaly detectors, as well as effective hardware- and system-level defenses. The project website can be found at https://tinyurl.com/illusory-attacks.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19082",
        "keywords": [
            "illusory attacks",
            "reinforcement learning agents",
            "dual ascent",
            "defenses",
            "detectability",
            "adversarial attack"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning",
        "authors": "Yuwei Guo;Ceyuan Yang;Anyi Rao;Zhengyang Liang;Yaohui Wang;Yu Qiao;Maneesh Agrawala;Dahua Lin;Bo Dai",
        "site": "https://iclr.cc/virtual/2024/poster/19044",
        "summary": "With the advance of text-to-image (T2I) diffusion models (e.g., Stable Diffusion) and corresponding personalization techniques such as DreamBooth and LoRA, everyone can manifest their imagination into high-quality images at an affordable cost. However, adding motion dynamics to existing high-quality personalized T2Is and enabling them to generate animations remains an open challenge. In this paper, we present AnimateDiff, a practical framework for animating personalized T2I models without requiring model-specific tuning. At the core of our framework is a plug-and-play motion module that can be trained once and seamlessly integrated into any personalized T2Is originating from the same base T2I. Through our proposed training strategy, the motion module effectively learns transferable motion priors from real-world videos. Once trained, the motion module can be inserted into a personalized T2I model to form a personalized animation generator. We further propose MotionLoRA, a lightweight fine-tuning technique for AnimateDiff that enables a pre-trained motion module to adapt to new motion patterns, such as different shot types, at a low training and data collection cost. We evaluate AnimateDiff and MotionLoRA on several public representative personalized T2I models collected from the community. The results demonstrate that our approaches help these models generate temporally smooth animation clips while preserving the visual quality and motion diversity. Codes and pre-trained weights are available at https://github.com/guoyww/AnimateDiff.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19044",
        "keywords": [
            "animate",
            "personalization",
            "diffusion",
            "animatediff",
            "image diffusion",
            "personalized text"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Identifying the Risks of LM Agents with an LM-Emulated Sandbox",
        "authors": "Yangjun Ruan;Honghua Dong;Andrew Wang;Silviu Pitis;Yongchao Zhou;Jimmy Ba;Yann Dubois;Chris J. Maddison;Tatsunori Hashimoto",
        "site": "https://iclr.cc/virtual/2024/poster/19037",
        "summary": "Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks—such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating implementing the tools, setting up the environment for each test scenario manually, and finding risky cases. As tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tail risks. To address these challenges, we introduce ToolEmu: a framework that uses an LM to emulate tool execution and enables scalable testing of LM agents against a diverse range of tools and scenarios. Alongside the emulator, we develop an LM-based automatic safety evaluator that examines agent failures and quantifies associated risks. We test both the tool emulator and evaluator through human evaluation and find that 68.8% of failures identified with ToolEmu would be valid real-world agent failures. Using our curated initial benchmark consisting of 36 high-stakes toolkits and 144 test cases, we provide a quantitative risk analysis of current LM agents and identify numerous failures with potentially severe outcomes. Notably, even the safest LM agent exhibits such failures 23.9% of the time according to our evaluator, underscoring the need to develop safer LM agents for real-world deployment.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19037",
        "keywords": [
            "tool emulator",
            "lm",
            "risks",
            "model"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent advances in Language Model (LM) agents and tool use... amplify potential risks—such as leaking private data or causing financial losses... Notably, even the safest LM agent exhibits such failures 23.9% of the time according to our evaluator, underscoring the need to develop safer LM agents for real-world deployment.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Recent advances in Language Model (LM) agents and tool use... amplify potential risks—such as leaking private data or causing financial losses... Notably, even the safest LM agent exhibits such failures 23.9% of the time according to our evaluator, underscoring the need to develop safer LM agents for real-world deployment.\""
    },
    {
        "title": "On the Humanity of Conversational AI: Evaluating the Psychological Portrayal of LLMs",
        "authors": "Jen-tse Huang;Wenxuan Wang;Eric John Li;Man Ho LAM;Shujie Ren;Youliang Yuan;Wenxiang Jiao;Zhaopeng Tu;Michael Lyu",
        "site": "https://iclr.cc/virtual/2024/poster/19008",
        "summary": "Large Language Models (LLMs) have recently showcased their remarkable capacities, not only in natural language processing tasks but also across diverse domains such as clinical medicine, legal consultation, and education. LLMs become more than mere applications, evolving into assistants capable of addressing diverse user requests. This narrows the distinction between human beings and artificial intelligence agents, raising intriguing questions regarding the potential manifestation of personalities, temperaments, and emotions within LLMs. In this paper, we propose a framework, PsychoBench, for evaluating diverse psychological aspects of LLMs. Comprising thirteen scales commonly used in clinical psychology, PsychoBench further classifies these scales into four distinct categories: personality traits, interpersonal relationships, motivational tests, and emotional abilities. Our study examines five popular models, namely text-davinci-003, ChatGPT, GPT-4, LLaMA-2-7b, and LLaMA-2-13b. Additionally, we employ a jailbreak approach to bypass the safety alignment protocols and test the intrinsic natures of LLMs. We have made PsychoBench openly accessible via https://github.com/CUHK-ARISE/PsychoBench.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/19008",
        "keywords": [
            "humanity",
            "language models",
            "emotions",
            "psychobench",
            "personality"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the abstract does not mention any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the abstract does not mention any limitations of LLMs."
    },
    {
        "title": "DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer",
        "authors": "Junyuan Hong;Jiachen T. Wang;Chenhui Zhang;Zhangheng LI;Bo Li;Zhangyang Wang",
        "site": "https://iclr.cc/virtual/2024/poster/18958",
        "summary": "Large Language Models (LLMs) have emerged as dominant tools for various tasks, particularly when tailored for a specific target by prompt tuning. Nevertheless, concerns surrounding data privacy present obstacles due to the tuned prompts' dependency on sensitive private information. A practical solution is to host a local LLM and optimize a soft prompt privately using data. Yet, hosting a local model becomes problematic when model ownership is protected. Alternative methods, like sending data to the model's provider for training, intensify these privacy issues facing an untrusted provider. In this paper, we present a novel solution called Differentially-Private Offsite Prompt Tuning (DP-OPT) to address this challenge. Our approach involves tuning a discrete prompt on the client side and then applying it to the desired cloud models. We demonstrate that prompts suggested by LLMs themselves can be transferred without compromising performance significantly. To ensure that the prompts do not leak private information, we introduce the first private prompt generation mechanism, by a differentially-private (DP) ensemble of in-context learning with private demonstrations.  With DP-OPT, generating privacy-preserving prompts by Vicuna-7b can yield competitive performance compared to non-private in-context learning on GPT3.5 or local private prompt tuning.Codes are available at https://github.com/VITA-Group/DP-OPT.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18958",
        "keywords": [
            "sensitive",
            "privacy"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"concerns surrounding data privacy present obstacles due to the tuned prompts' dependency on sensitive private information.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs (privacy concerns due to dependency on sensitive private information), but it is not the primary focus of the paper. The paper aims to propose a solution (DP-OPT) to address this challenge,",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"concerns surrounding data privacy present obstacles due to the tuned prompts' dependency on sensitive private information.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs (privacy concerns due to dependency on sensitive private information), but it is not the primary focus of the paper. The paper aims to propose a solution (DP-OPT) to address this challenge,"
    },
    {
        "title": "At Which Training Stage Does Code Data Help LLMs Reasoning?",
        "authors": "YINGWEI MA;Yue Liu;Yue Yu;Yuanliang Zhang;Yu Jiang;Changjian Wang;Shanshan Li",
        "site": "https://iclr.cc/virtual/2024/poster/18910",
        "summary": "Large Language models (LLMs) have exhibited remarkable reasoning capabilities and become the foundation of language technologies. Inspired by the great success of code data in training LLMs, we naturally wonder at which training stage introducing code data can really help LLMs reasoning. To this end, this paper systematically explores the impact of code data on LLMs at different stages. Concretely, we introduce the code data at the pre-training stage, instruction-tuning stage, and both of them, respectively. Then, the reasoning capability of LLMs is comprehensively and fairly evaluated via six reasoning tasks. We critically analyze the experimental results and provide conclusions with insights. First, pre-training LLMs with the mixture of code and text can significantly enhance LLMs' general reasoning capability almost without negative transfer on other tasks. Besides, at the instruction-tuning stage, code data endows LLMs the task-specific reasoning capability. Moreover, the dynamic mixing strategy of code and text data assists LLMs to learn reasoning capability step-by-step during training. These insights deepen the understanding of LLMs regarding reasoning ability for their application, such as scientific question answering, legal support, etc.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18910",
        "keywords": [
            "training",
            "training stage"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicit, but \"critically analyze the experimental results\" implies that the paper may discuss some limitations or challenges of LLMs in the context of their reasoning capability, although it is not the primary focus of the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None explicit, but \"critically analyze the experimental results\" implies that the paper may discuss some limitations or challenges of LLMs in the context of their reasoning capability, although it is not the primary focus of the abstract."
    },
    {
        "title": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization",
        "authors": "Weiran Yao;Shelby Heinecke;Juan Carlos Niebles;Zhiwei Liu;Yihao Feng;Le Xue;Rithesh R N;Zeyuan Chen;Jianguo Zhang;Devansh Arpit;Ran Xu;Phil L Mui;Huan Wang;Caiming Xiong;Silvio Savarese",
        "site": "https://iclr.cc/virtual/2024/poster/18908",
        "summary": "Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans. Experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18908",
        "keywords": [
            "policy gradient",
            "language agent",
            "retroformer",
            "agents",
            "policy gradient optimization",
            "large language agents",
            "retrospective"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards.\""
    },
    {
        "title": "Proving Test Set Contamination in Black-Box Language Models",
        "authors": "Yonatan Oren;Nicole Meister;Niladri S. Chatterji;Faisal Ladhak;Tatsunori Hashimoto",
        "site": "https://iclr.cc/virtual/2024/poster/18904",
        "summary": "Large language models are trained on vast amounts of internet data, prompting concerns that they have memorized public benchmarks. Detecting this type of contamination is challenging because the pretraining data used by proprietary models are often not publicly accessible.We propose a procedure for detecting test set contamination of language models with exact false positive guarantees and without access to pretraining data or model weights. Our approach leverages the fact that when there is no data contamination, all orderings of an exchangeable benchmark should be equally likely. In contrast, the tendency for language models to memorize example order means that a contaminated language model will find certain canonical orderings to be much more likely than others. Our test flags potential contamination whenever the likelihood of a canonically ordered benchmark dataset is significantly higher than the likelihood after shuffling the examples.We demonstrate that our procedure is sensitive enough to reliably detect contamination in challenging situations, including models as small as 1.4 billion parameters, on small test sets only 1000 examples, and datasets that appear only a few times in the pretraining corpus. Finally, we evaluate LLaMA-2 to apply our test in a realistic setting and find our results to be consistent with existing contamination evaluations.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18904",
        "keywords": [
            "test set contamination",
            "proving test set contamination",
            "language models",
            "data contamination",
            "test flags potential contamination"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"prompting concerns that they have memorized public benchmarks\"; \"the tendency for language models to memorize example order means that a contaminated language model will find certain canonical orderings to be much more likely than others.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"prompting concerns that they have memorized public benchmarks\"; \"the tendency for language models to memorize example order means that a contaminated language model will find certain canonical orderings to be much more likely than others.\""
    },
    {
        "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts",
        "authors": "Pan Lu;Hritik Bansal;Tony Xia;Jiacheng Liu;Chunyuan Li;Hannaneh Hajishirzi;Hao Cheng;Kai-Wei Chang;Michel Galley;Jianfeng Gao",
        "site": "https://iclr.cc/virtual/2024/poster/18900",
        "summary": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of GPT-4V is mainly attributed to its enhanced visual perception and mathematical reasoning. However, GPT-4V still falls short of human performance by 10.4%, as it often struggles to understand complex figures and perform rigorous reasoning. This significant gap underscores the critical role that MathVista will play in the development of general-purpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks. We further explore the new ability of self-verification, the application of self-consistency, and the interactive chatbot capabilities of GPT-4V, highlighting its promising potential for future research. The project is available at https://mathvista.github.io/.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18900",
        "keywords": [
            "foundation models",
            "mathematical reasoning",
            "large multimodal models",
            "large language models",
            "visual perception"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, GPT-4V still falls short of human performance by 10.4%, as it often struggles to understand complex figures and perform rigorous reasoning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, GPT-4V still falls short of human performance by 10.4%, as it often struggles to understand complex figures and perform rigorous reasoning.\""
    },
    {
        "title": "Cascading Reinforcement Learning",
        "authors": "Yihan Du;R. Srikant;Wei Chen",
        "site": "https://iclr.cc/virtual/2024/poster/18891",
        "summary": "Cascading bandits have gained popularity in recent years due to their applicability to recommendation systems and online advertising. In the cascading bandit model, at each timestep, an agent recommends an ordered subset of items (called an item list) from a pool of items, each associated with an unknown attraction probability. Then, the user examines the list, and clicks the first attractive item (if any), and after that, the agent receives a reward. The goal of the agent is to maximize the expected cumulative reward. However, the prior literature on cascading bandits ignores the influences of user states (e.g., historical behaviors) on recommendations and the change of states as the session proceeds. Motivated by this fact, we propose a generalized cascading RL framework, which considers the impact of user states and state transition into decisions. In cascading RL, we need to select items not only with  large attraction probabilities but also leading to good successor states. This imposes a huge computational challenge due to the combinatorial action space. To tackle this challenge, we delve into the properties of value functions, and design an oracle BestPerm to efficiently find the optimal item list. Equipped with BestPerm, we develop two algorithms CascadingVI and CascadingBPI, which are both computationally-efficient and sample-efficient, and provide near-optimal regret and sample complexity guarantees. Furthermore, we present experiments to show the improved computational and sample efficiencies of our algorithms compared to straightforward adaptations of existing RL algorithms in practice.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18891",
        "keywords": [
            "reinforcement learning",
            "efficiencies",
            "online advertising"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages",
        "authors": "Jinyi Hu;Yuan Yao;Chongyi Wang;SHAN WANG;Yinxu Pan;Qianyu Chen;Tianyu Yu;Hanghao Wu;Yue Zhao;Haoye Zhang;Xu Han;Yankai Lin;Jiao Xue;dahai li;Zhiyuan Liu;Maosong Sun",
        "site": "https://iclr.cc/virtual/2024/poster/18879",
        "summary": "Recently there has been a significant surge in multimodal learning in terms of both image-to-text and text-to-image generation. However, the success is typically limited to English, leaving other languages largely behind. Building a competitive counterpart in other languages is highly challenging due to the low-resource nature of non-English multimodal data (i.e., lack of large-scale, high-quality image-text data). In this work, we propose MPM, an effective training paradigm for training large multimodal models in low-resource languages. MPM demonstrates that Multilingual language models can Pivot zero-shot Multimodal learning across languages. Specifically, based on a strong multilingual large language model, multimodal models pretrained on English-only image-text data can well generalize to other languages in a (quasi)-zero-shot manner, even surpassing models trained on image-text data in native languages. Taking Chinese as a practice of MPM, we build large multimodal models VisCPM in image-to-text and text-to-image generation, which achieve state-of-the-art (open-source) performance in Chinese. To facilitate future research, we open-source codes and model weights at https://github.com/OpenBMB/VisCPM.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18879",
        "keywords": [
            "multimodal learning",
            "pivot",
            "multimodal",
            "multimodal models",
            "multilingual large language model",
            "multilingual language"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the success is typically limited to English, leaving other languages largely behind. Building a competitive counterpart in other languages is highly challenging due to the low-resource nature of non-English multimodal data (i.e., lack of large-scale, high-quality image-text data).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the success is typically limited to English, leaving other languages largely behind. Building a competitive counterpart in other languages is highly challenging due to the low-resource nature of non-English multimodal data (i.e., lack of large-scale, high-quality image-text data).\""
    },
    {
        "title": "The False Promise of Imitating Proprietary Language Models",
        "authors": "Arnav Gudibande;Eric Wallace;Charlie Victor Snell;Xinyang Geng;Hao Liu;Pieter Abbeel;Sergey Levine;Dawn Song",
        "site": "https://iclr.cc/virtual/2024/poster/18877",
        "summary": "An emerging method to cheaply improve a weaker language model is to finetune it on outputs from a stronger model, such as a proprietary system like ChatGPT (e.g., Alpaca, Self-Instruct, and others). In this work, we critically analyze this approach of imitating language models. We first finetune a series of LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data sources, and imitation data amounts (0.3M--150M tokens). We then evaluate the models using crowd raters and canonical NLP benchmarks. Initially, we were surprised by the output quality of our imitation models---they appear far better at following instructions, and crowd workers rate their outputs as competitive with ChatGPT. However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data. We show that these performance discrepancies may slip past human raters because imitation models are adept at mimicking ChatGPT’s style but not its factuality. Overall, we conclude that while model imitation can be useful for training models to follow instructions and avoid toxic outputs, it falls short its full promise in many ways. In particular, there exists a substantial capabilities gap between open and closed LMs that we find cannot be bridged merely by adding more imitation data. Instead, we find that fine-tuning more capable base LMs has a significantly more substantial effect on closing this gap. In turn, we argue that the higher leverage action for improving open-source models is to tackle the difficult challenge of developing better base LMs, rather than taking the shortcut of imitating proprietary systems.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18877",
        "keywords": [
            "false promise",
            "imitation",
            "model imitation",
            "imitating proprietary language"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data. We show that these performance discrepancies may slip past human raters because imitation models are adept at mimicking ChatGPT’s style but not its factuality.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data. We show that these performance discrepancies may slip past human raters because imitation models are adept at mimicking ChatGPT’s style but not its factuality.\""
    },
    {
        "title": "In-Context Pretraining: Language Modeling Beyond Document Boundaries",
        "authors": "Weijia Shi;Sewon Min;Maria Lomeli;Chunting Zhou;Margaret Li;Xi Victoria Lin;Noah A. Smith;Luke Zettlemoyer;Wen-tau Yih;Mike Lewis",
        "site": "https://iclr.cc/virtual/2024/poster/18861",
        "summary": "Language models are currently trained to predict tokens given document prefixes, enabling them to zero shot long form generation and prompting-style tasks which can be reduced to document completion. We instead present IN-CONTEXT PRETRAINING, a new approach where language models are trained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. Our approach builds on the fact that current pipelines train by concatenating random sets of shorter documents to create longer context windows; this improves efficiency even though the prior documents provide no signal for predicting the next document. Given this fact, we can do IN-CONTEXT PRETRAINING by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. However, this document sorting problem is challenging. There are billions of documents and we would like the sort to maximize contextual similarity for every document without repeating any data. To do this, we introduce approximate algorithms for finding related documents with efficient nearest neighbor search and constructing coherent batches with a graph cover algorithm. Our experiments show IN-CONTEXT PRETRAINING offers a scalable and simple approach to significantly enhance LM performance: we see notable improvements in tasks that require more complex contextual reasoning, including in-context learning (+8%), reading comprehension (+15%), faithfulness to previous contexts (+16%), long-context reasoning (+5%), and retrieval augmentation (+9%).",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18861",
        "keywords": [
            "language modeling",
            "pretraining",
            "context pretraining",
            "in context pretraining",
            "document boundaries"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Language models are currently trained to predict tokens given document prefixes, enabling them to zero shot long form generation and prompting-style tasks which can be reduced to document completion.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Language models are currently trained to predict tokens given document prefixes, enabling them to zero shot long form generation and prompting-style tasks which can be reduced to document completion.\""
    },
    {
        "title": "Instructive Decoding: Instruction-Tuned Large Language Models are Self-Refiner from Noisy Instructions",
        "authors": "Taehyeon Kim;Joonkee Kim;Gihun Lee;Se-Young Yun",
        "site": "https://iclr.cc/virtual/2024/poster/18856",
        "summary": "While instruction-tuned language models have demonstrated impressive zero-shot generalization, these models often struggle to generate accurate responses when faced with instructions that fall outside their training set. This paper presents Instructive Decoding (ID), a simple yet effective approach that augments the efficacy of instruction-tuned models. Specifically, ID adjusts the logits for next-token prediction in a contrastive manner, utilizing predictions generated from a manipulated version of the original instruction, referred to as a noisy instruction. This noisy instruction aims to elicit responses that could diverge from the intended instruction yet remain plausible. We conduct experiments across a spectrum of such noisy instructions, ranging from those that insert semantic noise via random words to others like 'opposite' that elicit the deviated responses. Our approach achieves considerable performance gains across various instruction-tuned models and tasks without necessitating any additional parameter updates. Notably, utilizing 'opposite' as the noisy instruction in ID, which shows the maximum divergence from the original instruction, consistently produces the most significant performance gains across multiple models and tasks.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18856",
        "keywords": [
            "instruction tuned",
            "instructive decoding",
            "instruction tuned language models",
            "self refiner"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While instruction-tuned language models have demonstrated impressive zero-shot generalization, these models often struggle to generate accurate responses when faced with instructions that fall outside their training set.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While instruction-tuned language models have demonstrated impressive zero-shot generalization, these models often struggle to generate accurate responses when faced with instructions that fall outside their training set.\""
    },
    {
        "title": "Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning",
        "authors": "Hyungho Na;Yunkyeong Seo;Il-chul Moon",
        "site": "https://iclr.cc/virtual/2024/poster/18851",
        "summary": "In cooperative multi-agent reinforcement learning (MARL), agents aim to achieve a common goal, such as defeating enemies or scoring a goal. Existing MARL algorithms are effective but still require significant learning time and often get trapped in local optima by complex tasks, subsequently failing to discover a goal-reaching policy. To address this, we introduce Efficient episodic Memory Utilization (EMU) for MARL, with two primary objectives: (a) accelerating reinforcement learning by leveraging semantically coherent memory from an episodic buffer and (b) selectively promoting desirable transitions to prevent local convergence. To achieve (a), EMU incorporates a trainable encoder/decoder structure alongside MARL, creating coherent memory embeddings that facilitate exploratory memory recall. To achieve (b), EMU introduces a novel reward structure called episodic incentive based on the desirability of states. This reward improves the TD target in Q-learning and acts as an additional incentive for desirable transitions. We provide theoretical support for the proposed incentive and demonstrate the effectiveness of EMU compared to conventional episodic control. The proposed method is evaluated in StarCraft II and Google Research Football, and empirical results indicate further performance improvement over state-of-the-art methods.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18851",
        "keywords": [
            "episodic memory utilization",
            "reinforcement learning",
            "multi agent reinforcement learning"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "InstructScene: Instruction-Driven 3D Indoor Scene Synthesis with Semantic Graph Prior",
        "authors": "Chenguo Lin;Yadong MU",
        "site": "https://iclr.cc/virtual/2024/poster/18845",
        "summary": "Comprehending natural language instructions is a charming property for 3D indoor scene synthesis systems. Existing methods directly model object joint distributions and express object relations implicitly within a scene, thereby hindering the controllability of generation. We introduce InstructScene, a novel generative framework that integrates a semantic graph prior and a layout decoder to improve controllability and fidelity for 3D scene synthesis. The proposed semantic graph prior jointly learns scene appearances and layout distributions, exhibiting versatility across various downstream tasks in a zero-shot manner. To facilitate the benchmarking for text-driven 3D scene synthesis, we curate a high-quality dataset of scene-instruction pairs with large language and multimodal models. Extensive experimental results reveal that the proposed method surpasses existing state-of-the-art approaches by a large margin. Thorough ablation studies confirm the efficacy of crucial design components. Project page: https://chenguolin.github.io/projects/InstructScene.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18845",
        "keywords": [
            "semantic graph prior",
            "3d scene synthesis",
            "scene synthesis",
            "instructscene"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"with large language and multimodal models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"with large language and multimodal models.\""
    },
    {
        "title": "LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models",
        "authors": "Yixiao Li;Yifan Yu;Chen Liang;Nikos Karampatziakis;Pengcheng He;Weizhu Chen;Tuo Zhao",
        "site": "https://iclr.cc/virtual/2024/poster/18843",
        "summary": "Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning (Dettmers et al., 2023). In this work we focus on the scenario where quantization and LoRA fine- tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrep- ancy between the quantized and full-precision model and significantly improves the generalization in downstream tasks. We evaluate our method on natural lan- guage understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and out- performs existing quantization methods, especially in the challenging 2-bit and 2/4-bit mixed precision regimes. We will release our code.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18843",
        "keywords": [
            "quantization",
            "tuning",
            "language models",
            "fine tuning",
            "natural language generation",
            "summarization",
            "question answering"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach.\"\n\nThis evidence indicates that the paper mentions a limitation of LLMs in the context of quantization and LoRA fine-tuning, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach.\"\n\nThis evidence indicates that the paper mentions a limitation of LLMs in the context of quantization and LoRA fine-tuning, but it is not the primary focus of the paper."
    },
    {
        "title": "AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents",
        "authors": "Jake Grigsby;Linxi Fan;Yuke Zhu",
        "site": "https://iclr.cc/virtual/2024/poster/18839",
        "summary": "We introduce AMAGO, an in-context Reinforcement Learning (RL) agent that uses sequence models to tackle the challenges of generalization, long-term memory, and meta-learning. Recent works have shown that off-policy learning can make in-context RL with recurrent policies viable. Nonetheless, these approaches require extensive tuning and limit scalability by creating key bottlenecks in agents' memory capacity, planning horizon, and model size. AMAGO revisits and redesigns the off-policy in-context approach to successfully train long-sequence Transformers over entire rollouts in parallel with end-to-end RL. Our agent is scalable and applicable to a wide range of problems, and we demonstrate its strong performance empirically in meta-RL and long-term memory domains. AMAGO's focus on sparse rewards and off-policy data also allows in-context learning to extend to goal-conditioned problems with challenging exploration. When combined with a multi-goal hindsight relabeling scheme, AMAGO can solve a previously difficult category of open-world domains, where agents complete many possible instructions in procedurally generated environments.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18839",
        "keywords": [
            "adaptive agents",
            "context reinforcement learning"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation",
        "authors": "Yi Wang;Yinan He;Yizhuo Li;Kunchang Li;Jiashuo Yu;Xin Ma;Xinhao Li;Guo Chen;Xinyuan Chen;Yaohui Wang;Ping Luo;Ziwei Liu;Yali Wang;Limin Wang;Yu Qiao",
        "site": "https://iclr.cc/virtual/2024/poster/18829",
        "summary": "This paper introduces InternVid, a large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodal understanding and generation. InternVid contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by detailed descriptions of total 4.1B words. Our core contribution is to develop a scalable approach to autonomously build a high-quality video-text dataset with large language models (LLM), thereby showcasing its efficacy in learning video-language representation at scale. Specifically, we utilize a multi-scale approach to generate video-related descriptions. Furthermore, we introduce ViCLIP, a video-text representation learning model based on ViT-L. Learned on InternVid via contrastive learning, this model demonstrates leading zero-shot action recognition and competitive video retrieval performance. Beyond basic video understanding tasks like recognition and retrieval, our dataset and model have broad applications. They are particularly beneficial for generating interleaved video-text data for learning a video-centric dialogue system, advancing video-to-text and text-to-video generation research. These proposed resources provide a tool for researchers and practitioners interested in multimodal video understanding and generation.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18829",
        "keywords": [
            "video text",
            "multimodal video understanding",
            "generation",
            "competitive video retrieval",
            "multimodal understanding",
            "video text dataset",
            "multimodal dataset"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our core contribution is to develop a scalable approach to autonomously build a high-quality video-text dataset with large language models (LLM), thereby showcasing its efficacy in learning video-language representation at scale.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Our core contribution is to develop a scalable approach to autonomously build a high-quality video-text dataset with large language models (LLM), thereby showcasing its efficacy in learning video-language representation at scale.\""
    },
    {
        "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
        "authors": "Angelica Chen;Ravid Shwartz-Ziv;Kyunghyun Cho;Matthew L Leavitt;Naomi Saphra",
        "site": "https://iclr.cc/virtual/2024/poster/18825",
        "summary": "Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. We present a case study of syntax acquisition in masked language models (MLMs) that demonstrates how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in pretraining when models abruptly acquire SAS, concurrent with a steep drop in loss. This breakthrough precipitates the subsequent acquisition of linguistic capabilities. We then examine the causal role of SAS by manipulating SAS during training, and demonstrate that SAS is necessary for the development of grammatical capabilities. We further find that SAS competes with other beneficial traits during training, and that briefly suppressing SAS improves model quality. These findings offer an interpretation of a real-world example of both simplicity bias and breakthrough training dynamics.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18825",
        "keywords": [
            "sudden drops",
            "phase transitions",
            "accessible",
            "syntactic attention structure"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We further find that SAS competes with other beneficial traits during training, and that briefly suppressing SAS improves model quality.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We further find that SAS competes with other beneficial traits during training, and that briefly suppressing SAS improves model quality.\""
    },
    {
        "title": "DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization",
        "authors": "Guowei Xu;Ruijie Zheng;Yongyuan Liang;Xiyao Wang;Zhecheng Yuan;Tianying Ji;Yu Luo;Xiaoyu Liu;Jiaxin Yuan;Pu Hua;Shuzhen Li;Yanjie Ze;Hal Daumé III;Furong Huang;Huazhe Xu",
        "site": "https://iclr.cc/virtual/2024/poster/18821",
        "summary": "Visual reinforcement learning (RL) has shown promise in continuous control tasks.Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds.In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents' inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks.To quantify this inactivity, we adopt dormant ratio as a metric to measure inactivity in the RL agent's network.Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent's activity level, regardless of the received reward signals.Leveraging the aforementioned insights, we introduce DrM, a method that uses three core mechanisms to guide agents' exploration-exploitation trade-offs by actively minimizing the dormant ratio. Experiments demonstrate that  DrM achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit.Most importantly, DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite as well as three dexterous hand manipulation tasks without demonstrations in Adroit, all based on pixel observations.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18821",
        "keywords": [
            "drm",
            "dormant ratio",
            "visual reinforcement learning",
            "dormant ratio minimization",
            "visual rl"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models",
        "authors": "Longhui Yu;Weisen Jiang;Han Shi;Jincheng YU;Zhengying Liu;Yu Zhang;James Kwok;Zhenguo Li;Adrian Weller;Weiyang Liu",
        "site": "https://iclr.cc/virtual/2024/poster/18796",
        "summary": "Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (\\eg, LLaMA-2) are still far away from satisfactory for solving mathematical problems due to the complex reasoning procedures. To bridge this gap, we propose \\emph{MetaMath}, a finetuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives, which results in a new dataset called MetaMathQA. Then we finetune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (\\ie, GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin.  Our MetaMath-7B model achieves $66.5\\%$ on GSM8K and $19.8\\%$ on MATH, exceeding the state-of-the-art models of the same size by $11.5\\%$ and $8.7\\%$. Particularly, MetaMath-70B achieves an accuracy of $82.3\\%$ on GSM8K, slightly better than GPT-3.5-Turbo. We release the MetaMathQA dataset, the MetaMath models with different model sizes and the training code for public use.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18796",
        "keywords": [
            "metamath",
            "metamath models",
            "metamath 70b",
            "language models",
            "large language models",
            "bootstrap",
            "bootstrapping mathematical questions",
            "mathematical reasoning",
            "finetuned"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"most existing open-source LLMs (e.g., LLaMA-2) are still far away from satisfactory for solving mathematical problems due to the complex reasoning procedures.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"most existing open-source LLMs (e.g., LLaMA-2) are still far away from satisfactory for solving mathematical problems due to the complex reasoning procedures.\""
    },
    {
        "title": "GenSim: Generating Robotic Simulation Tasks via Large Language Models",
        "authors": "Lirui Wang;Yiyang Ling;Zhecheng Yuan;Mohit Shridhar;Chen Bao;Yuzhe Qin;Bailin Wang;Huazhe Xu;Xiaolong Wang",
        "site": "https://iclr.cc/virtual/2024/poster/18747",
        "summary": "Collecting large amounts of real-world interaction data to train general robotic policies is often prohibitively expensive, thus motivating the use of simulation data. However, existing methods for data generation have generally focused on scene-level diversity (e.g., object instances and poses) rather than task-level diversity, due to the human effort required to come up with and verify novel tasks. This has made it challenging for policies trained on simulation data to demonstrate significant task-level generalization. In this paper, we propose to automatically generate rich simulation environments and expert demonstrations by exploiting a large language models' (LLM) grounding and coding ability. Our approach, dubbed GenSim, has two modes: goal-directed generation, wherein a target task is given to the LLM and the LLM proposes a task curriculum to solve the target task, and exploratory generation, wherein the LLM  bootstraps from previous tasks and iteratively proposes novel tasks that would be helpful in solving more complex tasks. We use GPT4 to expand the existing benchmark by ten times to over 100 tasks, on which we conduct supervised finetuning and evaluate several LLMs including finetuned GPTs and Code Llama on code generation for robotic simulation tasks. Furthermore, we observe that LLMs-generated simulation programs can enhance task-level generalization significantly when used for multitask policy training. We further find that with minimal sim-to-real adaptation, the multitask policies pretrained on GPT4-generated simulation tasks exhibit stronger transfer to unseen long-horizon tasks in the real world and outperform baselines by 25%. See our project website (https://gen-sim.github.io) and demo (https://huggingface.co/spaces/Gen-Sim/Gen-Sim) for visualizations and open-source models and datasets.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18747",
        "keywords": [
            "language models",
            "gensim",
            "goal directed generation",
            "visualizations",
            "robotic simulation tasks",
            "generation"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the abstract only mentions the potential of LLMs and their application in generating simulation environments and expert demonstrations, without discussing any limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the abstract only mentions the potential of LLMs and their application in generating simulation environments and expert demonstrations, without discussing any limitations."
    },
    {
        "title": "Amortizing intractable inference in large language models",
        "authors": "Edward J Hu;Moksh Jain;Eric Elmoznino;Younesse Kaddar;Guillaume Lajoie;Yoshua Bengio;Nikolay Malkin",
        "site": "https://iclr.cc/virtual/2024/poster/18725",
        "summary": "Autoregressive large language models (LLMs) compress knowledge from their training data through next-token conditional distributions. This limits tractable querying of this knowledge to start-to-end autoregressive sampling. However, many tasks of interest---including sequence continuation, infilling, and other forms of constrained generation---involve sampling from intractable posterior distributions. We address this limitation by using amortized Bayesian inference to sample from these intractable posteriors. Such amortization is algorithmically achieved by fine-tuning LLMs via diversity-seeking reinforcement learning algorithms: generative flow networks (GFlowNets). We empirically demonstrate that this distribution-matching paradigm of LLM fine-tuning can serve as an effective alternative to maximum-likelihood training and reward-maximizing policy optimization. As an important application, we interpret chain-of-thought reasoning as a latent variable modeling problem and demonstrate that our approach enables data-efficient adaptation of LLMs to tasks that require multi-step rationalization and tool use.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18725",
        "keywords": [
            "language models",
            "reinforcement learning",
            "thought reasoning",
            "large language models",
            "latent variable modeling",
            "amortized bayesian inference",
            "diversity",
            "distribution matching"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This limits tractable querying of this knowledge to start-to-end autoregressive sampling. However, many tasks of interest---including sequence continuation, infilling, and other forms of constrained generation---involve sampling from intractable posterior distributions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"This limits tractable querying of this knowledge to start-to-end autoregressive sampling. However, many tasks of interest---including sequence continuation, infilling, and other forms of constrained generation---involve sampling from intractable posterior distributions.\""
    },
    {
        "title": "Unlocking the Power of Representations in Long-term Novelty-based Exploration",
        "authors": "Alaa Saade;Steven Kapturowski;Daniele Calandriello;Charles Blundell;Pablo Sprechmann;Leopoldo Sarra;Oliver Groth;Michal Valko;Bilal Piot",
        "site": "https://iclr.cc/virtual/2024/poster/18723",
        "summary": "We introduce Robust Exploration via Clustering-based Online Density Estimation (RECODE), a non-parametric method for novelty-based exploration that estimates visitation counts for clusters of states based on their similarity in a chosen embedding space. By adapting classical clustering to the nonstationary setting of Deep RL, RECODE can efficiently track state visitation counts over thousands of episodes. We further propose a novel generalization of the inverse dynamics loss, which leverages masked transformer architectures for multi-step prediction; which in conjunction with \\DETOCS achieves a new state-of-the-art in a suite of challenging 3D-exploration tasks in DM-Hard-8. RECODE also sets new state-of-the-art in hard exploration Atari games, and is the first agent to reach the end screen in \"Pitfall!\"",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18723",
        "keywords": [
            "representations",
            "clustering",
            "power",
            "novelty"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "TD-MPC2: Scalable, Robust World Models for Continuous Control",
        "authors": "Nicklas Hansen;Hao Su;Xiaolong Wang",
        "site": "https://iclr.cc/virtual/2024/poster/18722",
        "summary": "TD-MPC is a model-based reinforcement learning (RL) algorithm that performs local trajectory optimization in the latent space of a learned implicit (decoder-free) world model. In this work, we present TD-MPC2: a series of improvements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves significantly over baselines across 104 online RL tasks spanning 4 diverse task domains, achieving consistently strong results with a single set of hyperparameters. We further show that agent capabilities increase with model and data size, and successfully train a single 317M parameter agent to perform 80 tasks across multiple task domains, embodiments, and action spaces. We conclude with an account of lessons, opportunities, and risks associated with large TD-MPC2 agents.Explore videos, models, data, code, and more at https://tdmpc2.com",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18722",
        "keywords": [
            "world models",
            "model based reinforcement learning",
            "continuous control"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI",
        "authors": "Weibang Jiang;Liming Zhao;Bao-liang Lu",
        "site": "https://iclr.cc/virtual/2024/poster/18658",
        "summary": "The current electroencephalogram (EEG) based deep learning models are typically designed for specific datasets and applications in brain-computer interaction (BCI), limiting the scale of the models and thus diminishing their perceptual capabilities and generalizability. Recently, Large Language Models (LLMs) have achieved unprecedented success in text processing, prompting us to explore the capabilities of Large EEG Models (LEMs). We hope that LEMs can break through the limitations of different task types of EEG datasets, and obtain universal perceptual capabilities of EEG signals through unsupervised pre-training. Then the models can be fine-tuned for different downstream tasks. However, compared to text data, the volume of EEG datasets is generally small and the format varies widely. For example, there can be mismatched numbers of electrodes, unequal length data samples, varied task designs, and low signal-to-noise ratio. To overcome these challenges, we propose a unified foundation model for EEG called Large Brain Model (LaBraM). LaBraM enables cross-dataset learning by segmenting the EEG signals into EEG channel patches. Vector-quantized neural spectrum prediction is used to train a semantically rich neural tokenizer that encodes continuous raw EEG channel patches into compact neural codes. We then pre-train neural Transformers by predicting the original neural codes for the masked EEG channel patches. The LaBraMs were pre-trained on about 2,500 hours of various types of EEG signals from around 20 datasets and validated on multiple different types of downstream tasks. Experiments on abnormal detection, event type classification, emotion recognition, and gait prediction show that our LaBraM outperforms all compared SOTA methods in their respective fields. Our code is available at https://github.com/935963004/LaBraM.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18658",
        "keywords": [
            "deep learning",
            "emotion recognition",
            "eeg",
            "large brain model",
            "generic representations",
            "gait prediction",
            "tremendous eeg",
            "masked eeg",
            "quantized neural spectrum prediction"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, compared to text data, the volume of EEG datasets is generally small and the format varies widely.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, compared to text data, the volume of EEG datasets is generally small and the format varies widely.\""
    },
    {
        "title": "NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers",
        "authors": "Kai Shen;Zeqian Ju;Xu Tan;Eric Liu;Yichong Leng;Lei He;Tao Qin;sheng zhao;Jiang Bian",
        "site": "https://iclr.cc/virtual/2024/poster/18637",
        "summary": "Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wild datasets is important to capture the diversity in human speech such as speaker identities, prosodies, and styles (e.g., singing). Current large TTS systems usually quantize speech into discrete tokens and use language models to generate these tokens one by one, which suffer from unstable prosody, word skipping/repeating issue, and poor voice quality. In this paper, we develop NaturalSpeech 2, a TTS system that leverages a neural audio codec with residual vector quantizers to get the quantized latent vectors and uses a diffusion model to generate these latent vectors conditioned on text input. To enhance the zero-shot capability that is important to achieve diverse speech synthesis, we design a speech prompting mechanism to facilitate in-context learning in the diffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 to large-scale datasets with 44K hours of speech and singing data and evaluate its voice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTS systems by a large margin in terms of prosody/timbre similarity, robustness, and voice quality in a zero-shot setting, and performs novel zero-shot singing synthesis with only a speech prompt. Audio samples are available at https://naturalspeech2.github.io/.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18637",
        "keywords": [
            "singing",
            "zero shot singing synthesis",
            "tts",
            "residual vector quantizers",
            "latent diffusion",
            "speech prompting"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "What's In My Big Data?",
        "authors": "Yanai Elazar;Akshita Bhagia;Ian Helgi Magnusson;Abhilasha Ravichander;Dustin Schwenk;Alane Suhr;Evan Pete Walsh;Dirk Groeneveld;Luca Soldaini;Sameer Singh;Hannaneh Hajishirzi;Noah A. Smith;Jesse Dodge",
        "site": "https://iclr.cc/virtual/2024/poster/18626",
        "summary": "Large text corpora are the backbone of language models.However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination).In this work, we propose What's In My Big Data? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities---count and search---at scale, which allows us to analyze more than 35 terabytes on a standard compute node. We apply WIMBD to ten different corpora used to train popular language models, includingC4,The Pile, andRedPajama.Our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance, we find that about 50% of the documents inRedPajamaandLAION-2B-enare duplicates. In addition, several datasets used for benchmarking models trained on such corpora are contaminated with respect to important benchmarks, including the Winograd Schema Challenge and parts of GLUE and SuperGLUE.We open-source WIMBD's code and artifacts to provide a standard set of evaluations for new text-based corpora and to encourage more analyses and transparency around them.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18626",
        "keywords": [
            "corpora",
            "text corpora",
            "text based corpora",
            "language models",
            "my big data",
            "artifacts"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination.\""
    },
    {
        "title": "Guiding Instruction-based Image Editing via Multimodal Large Language Models",
        "authors": "Tsu-Jui Fu;Wenze Hu;Xianzhi Du;William Yang Wang;Yinfei Yang;Zhe Gan",
        "site": "https://iclr.cc/virtual/2024/poster/18621",
        "summary": "Instruction-based image editing improves the controllability and flexibility of image manipulation via natural commands without elaborate descriptions or regional masks. However, human instructions are sometimes too brief for current methods to capture and follow. Multimodal large language models (MLLMs) show promising capabilities in cross-modal understanding and visual-aware response generation via LMs. We investigate how MLLMs facilitate edit instructions and present MLLM-Guided Image Editing (MGIE). MGIE learns to derive expressive instructions and provides explicit guidance. The editing model jointly captures this visual imagination and performs manipulation through end-to-end training. We evaluate various aspects of Photoshop-style modification, global photo optimization, and local editing. Extensive experimental results demonstrate that expressive instructions are crucial to instruction-based image editing, and our MGIE can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18621",
        "keywords": [
            "language models",
            "instruction",
            "multimodal",
            "controllability",
            "flexibility",
            "image editing",
            "image manipulation",
            "response generation",
            "expressive"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, human instructions are sometimes too brief for current methods to capture and follow.\"\n\nThis rating is given because the abstract mentions a limitation of current methods (which include LLMs) in capturing and following brief human instructions, but it does not elaborate on this limitation or discuss it in detail. The primary focus of the paper is on the proposed solution, MGIE.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, human instructions are sometimes too brief for current methods to capture and follow.\"\n\nThis rating is given because the abstract mentions a limitation of current methods (which include LLMs) in capturing and following brief human instructions, but it does not elaborate on this limitation or discuss it in detail. The primary focus of the paper is on the proposed solution, MGIE."
    },
    {
        "title": "Controlled Text Generation via Language Model Arithmetic",
        "authors": "Jasper Dekoninck;Marc Fischer;Luca Beurer-Kellner;Martin Vechev",
        "site": "https://iclr.cc/virtual/2024/poster/18607",
        "summary": "As Large Language Models (LLMs) are deployed more widely, customization with respect to vocabulary, style, and character becomes more important. In this work, we introduce model arithmetic, a novel inference framework for composing and biasing LLMs without the need for model (re)training or highly specific datasets. In addition, the framework allows for more precise control of generated text than direct prompting and prior controlled text generation (CTG) techniques. Using model arithmetic, we can express prior CTG techniques as simple formulas and naturally extend them to new and more effective formulations. Further, we show that speculative sampling, a technique for efficient LLM sampling, extends to our setting. This enables highly efficient text generation with multiple composed models with only marginal overhead over a single model. Our empirical evaluation demonstrates that model arithmetic allows fine-grained control of generated text while outperforming state-of-the-art on the task of toxicity reduction. We release an open source easy-to-use implementation of our framework at https://github.com/eth-sri/language-model-arithmetic.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18607",
        "keywords": [
            "text generation",
            "model arithmetic",
            "language model",
            "controlled text generation",
            "prior ctg"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"As Large Language Models (LLMs) are deployed more widely, customization with respect to vocabulary, style, and character becomes more important.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"As Large Language Models (LLMs) are deployed more widely, customization with respect to vocabulary, style, and character becomes more important.\""
    },
    {
        "title": "CABINET: Content Relevance-based Noise Reduction for Table Question Answering",
        "authors": "Sohan Patnaik;Heril Changwal;Milan Aggarwal;Sumit Bhatia;Yaman Kumar;Balaji Krishnamurthy",
        "site": "https://iclr.cc/virtual/2024/poster/18603",
        "summary": "Table understanding capability of Large Language Models (LLMs) has been extensively studied through the task of question-answering (QA) over tables. Typically, only a small part of the whole table is relevant to derive the answer for a given question. The irrelevant parts act as noise and are distracting information, resulting in sub-optimal performance due to the vulnerability of LLMs to noise. To mitigate this, we propose CABINET (Content RelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) – a framework to enable LLMs to focus on relevant tabular data by suppressing extraneous information. CABINET comprises an Unsupervised Relevance Scorer (URS), trained differentially with the QA LLM, that weighs the table content based on its relevance to the input question before feeding it to the question answering LLM (QA LLM). To further aid the relevance scorer, CABINET employs a weakly supervised module that generates a parsing statement describing the criteria of rows and columns relevant to the question and highlights the content of corresponding table cells. CABINET significantly outperforms various tabular LLM baselines, as well as GPT3-based in-context learning methods, is more robust to noise, maintains outperformance on tables of varying sizes, and establishes new SoTA performance on WikiTQ, FeTaQA, and WikiSQL datasets. We release our code and datasets here.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18603",
        "keywords": [
            "table",
            "table question answering",
            "relevance",
            "question answering",
            "cabinet",
            "noise reduction",
            "content relevance",
            "relevance scorer",
            "vulnerability",
            "language models",
            "task"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Typically, only a small part of the whole table is relevant to derive the answer for a given question. The irrelevant parts act as noise and are distracting information, resulting in sub-optimal performance due to the vulnerability of LLMs to noise.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Typically, only a small part of the whole table is relevant to derive the answer for a given question. The irrelevant parts act as noise and are distracting information, resulting in sub-optimal performance due to the vulnerability of LLMs to noise.\""
    },
    {
        "title": "Overthinking the Truth: Understanding how Language Models Process False Demonstrations",
        "authors": "Danny Halawi;Jean-Stanislas Denain;Jacob Steinhardt",
        "site": "https://iclr.cc/virtual/2024/poster/18562",
        "summary": "Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model’s internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some “critical layer”, after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces overthinking. Beyond scientific understanding, our results suggest that studying intermediate model computations could be a promising avenue for understanding and guarding against harmful model behaviors.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18562",
        "keywords": [
            "demonstrations",
            "language models",
            "imitation"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context.\""
    },
    {
        "title": "Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks",
        "authors": "Hao Chen;Jindong Wang;Ankit Shah;Ran Tao;Hongxin Wei;Xing Xie;Masashi Sugiyama;Bhiksha Raj",
        "site": "https://iclr.cc/virtual/2024/poster/18555",
        "summary": "Pre-training on large-scale datasets and then fine-tuning on downstream tasks have become a standard practice in deep learning. However, pre-training data often contain label noise that may adversely affect the generalization of the model. This paper aims to understand the nature of noise in pre-training datasets and to mitigate its impact on downstream tasks. More specifically, through extensive experiments of supervised pre-training models on synthetic noisy ImageNet-1K and YFCC15M datasets, we demonstrate that while slight noise in pre-training can benefit in-domain (ID) transfer performance, where the training and testing data share the same distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing data distribution are different. We empirically verify that the reason behind is noise in pre-training shapes the feature space differently. We then propose a light-weight black-box tuning method (NMTune) to affine the feature space to mitigate the malignant effect of noise and improve generalization on both ID and OOD tasks, considering one may not be able to fully fine-tune or even access the pre-trained models. We conduct practical experiments on popular vision and language models that are pre-trained on noisy data for evaluation of our approach. Our analysis and results show the importance of this interesting and novel research direction, which we term Noisy Model Learning.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18555",
        "keywords": [
            "label noise",
            "noisy model learning",
            "pre training",
            "tuning"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior",
        "authors": "Ashmit Khandelwal;Aditya Agrawal;Aanisha Bhattacharyya;Yaman Kumar;Somesh Singh;Uttaran Bhattacharya;Ishita Dasgupta;Stefano Petrangeli;Rajiv Ratn Shah;Changyou Chen;Balaji Krishnamurthy",
        "site": "https://iclr.cc/virtual/2024/poster/18547",
        "summary": "Shannon and Weaver's seminal information theory divides communication into three levels: technical, semantic, and effectiveness. While the technical level deals with the accurate reconstruction of transmitted symbols, the semantic and effectiveness levels deal with the inferred meaning and its effect on the receiver. Large Language Models (LLMs), with their wide generalizability, make some progress towards the second level. However, LLMs and other communication models are not conventionally designed for predicting and optimizing communication for desired receiver behaviors and intents. As a result, the effectiveness level remains largely untouched by modern communication systems. In this paper, we introduce the receivers' \"behavior tokens,\" such as shares, likes, clicks, purchases, and retweets, in the LLM's training corpora to optimize content for the receivers and predict their behaviors. Other than showing similar performance to LLMs on content understanding tasks, our trained models show generalization capabilities on the behavior dimension for behavior simulation, content simulation, behavior understanding, and behavior domain adaptation. We show results on all these capabilities using a wide range of tasks on three corpora. We call these models Large Content and Behavior Models (LCBMs). Further, to spur more research on LCBMs, we release our new Content Behavior Corpus (CBC), a repository containing communicator, message, and corresponding receiver behavior (https://behavior-in-the-wild.github.io/LCBM).",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18547",
        "keywords": [
            "behavior",
            "behavior models",
            "models",
            "content",
            "effectiveness",
            "optimize content",
            "information theory",
            "optimizing",
            "large language models",
            "behavior simulation",
            "content simulation",
            "content behavior corpus",
            "large content",
            "semantic",
            "behavior tokens",
            "communication",
            "predicting",
            "designed"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, LLMs and other communication models are not conventionally designed for predicting and optimizing communication for desired receiver behaviors and intents. As a result, the effectiveness level remains largely untouched by modern communication systems.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, LLMs and other communication models are not conventionally designed for predicting and optimizing communication for desired receiver behaviors and intents. As a result, the effectiveness level remains largely untouched by modern communication systems.\""
    },
    {
        "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback",
        "authors": "Josef Dai;Xuehai Pan;Ruiyang Sun;Jiaming Ji;Xinbo Xu;Mickel Liu;Yizhou Wang;Yaodong Yang",
        "site": "https://iclr.cc/virtual/2024/poster/18540",
        "summary": "With the development of large language models (LLMs), striking a balance between the performance and safety of AI systems has never been more critical. However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training. To address this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly decouples human preferences regarding helpfulness and harmlessness, effectively avoiding the crowd workers' confusion about the tension and allowing us to train separate reward and cost models. We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints. Leveraging the Lagrangian method to solve this constrained problem, Safe RLHF dynamically adjusts the balance between the two objectives during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we demonstrate a superior ability to mitigate harmful responses while enhancing model performance compared to existing value-aligned algorithms. Experimentally, we fine-tuned the Alpaca-7B using Safe RLHF and aligned it with collected human preferences, significantly improving its helpfulness and harmlessness according to human evaluations.Code is available at https://github.com/PKU-Alignment/safe-rlhf.Warning: This paper contains example data that may be offensive or harmful.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18540",
        "keywords": [
            "helpfulness",
            "experimentally",
            "reinforcement learning",
            "harmlessness",
            "human feedback",
            "optimization",
            "alignment"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training.\""
    },
    {
        "title": "Scaling Laws for Associative Memories",
        "authors": "Vivien Cabannes;Elvis Dohmatob;Alberto Bietti",
        "site": "https://iclr.cc/virtual/2024/poster/18538",
        "summary": "Learning arguably involves the discovery and memorization of abstract rules. The aim of this paper is to study associative memory mechanisms. Our model is based on high-dimensional matrices consisting of outer products of embeddings, which relates to the inner layers of transformer language models. We derive precise scaling laws with respect to sample size and parameter size, and discuss the statistical efficiency of different estimators, including optimization-based algorithms. We provide extensive numerical experiments to validate and interpret theoretical results, including fine-grained visualizations of the stored memory associations.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18538",
        "keywords": [
            "associative memories",
            "scaling laws",
            "scaling"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our model is based on high-dimensional matrices consisting of outer products of embeddings, which relates to the inner layers of transformer language models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Our model is based on high-dimensional matrices consisting of outer products of embeddings, which relates to the inner layers of transformer language models.\""
    },
    {
        "title": "CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents",
        "authors": "Siyuan Qi;Shuo Chen;Yexin Li;Xiangyu Kong;Junqi Wang;Bangcheng Yang;Pring Wong;Yifan Zhong;Xiaoyuan Zhang;Zhaowei Zhang;Nian Liu;Yaodong Yang;Song-Chun Zhu",
        "site": "https://iclr.cc/virtual/2024/poster/18531",
        "summary": "The generalization of decision-making agents encompasses two fundamental elements: learning from past experiences and reasoning in novel contexts. However, the predominant emphasis in most interactive environments is on learning, often at the expense of complexity in reasoning. In this paper, we introduce CivRealm, an environment inspired by the Civilization game. Civilization’s profound alignment with human society requires sophisticated learning and prior knowledge, while its ever-changing space and action space demand robust reasoning for generalization. Particularly, CivRealm sets up an imperfect-information general-sum game with a changing number of players; it presents a plethora of complex features, challenging the agent to deal with open-ended stochastic environments that require diplomacy and negotiation skills. Within CivRealm, we provide interfaces for two typical agent types: tensor-based agents that focus on learning, and language-based agents that emphasize reasoning. To catalyze further research, we present initial results for both paradigms. The canonical RL-based agents exhibit reasonable performance in mini-games, whereas both RL- and LLM-based agents struggle to make substantial progress in the full game. Overall, CivRealm stands as a unique learning and reasoning challenge for decision-making agents. The code is available at https://github.com/bigai-ai/civrealm.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18531",
        "keywords": [
            "decision making",
            "learning",
            "reasoning",
            "reasoning odyssey"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"whereas both RL- and LLM-based agents struggle to make substantial progress in the full game.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLM-based agents in the context of the CivRealm environment, but it is a brief mention and not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"whereas both RL- and LLM-based agents struggle to make substantial progress in the full game.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLM-based agents in the context of the CivRealm environment, but it is a brief mention and not the primary focus of the paper."
    },
    {
        "title": "SWE-bench: Can Language Models Resolve Real-world Github Issues?",
        "authors": "Carlos E Jimenez;John Yang;Alexander Wettig;Shunyu Yao;Kexin Pei;Ofir Press;Karthik R Narasimhan",
        "site": "https://iclr.cc/virtual/2024/poster/18505",
        "summary": "Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere 1.96% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18505",
        "keywords": [
            "codebase",
            "code generation",
            "real github issues",
            "language models",
            "proprietary models",
            "testbed"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the"
    },
    {
        "title": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework",
        "authors": "Sirui Hong;Mingchen Zhuge;Jonathan Chen;Xiawu Zheng;Yuheng Cheng;Jinlin Wang;Ceyao Zhang;Zili Wang;Steven Ka Shing Yau;Zijuan Lin;Liyang Zhou;Chenyu Ran;Lingfeng Xiao;Chenglin Wu;Jürgen Schmidhuber",
        "site": "https://iclr.cc/virtual/2024/poster/18491",
        "summary": "Recently, remarkable progress has been made on automated problem solving through societies of agents based on large language models (LLMs). Previous LLM-based multi-agent systems can already solve simple dialogue tasks. More complex tasks, however, face challenges through logic inconsistencies due to cascading hallucinations caused by naively chaining LLMs. Here we introduce MetaGPT, an innovative meta-programming framework incorporating efficient human workflows into LLM-based multi-agent collaborations. MetaGPT encodes Standardized Operating Procedures (SOPs) into prompt sequences for more streamlined workflows, thus allowing agents with human-like domain expertise to verify intermediate results and reduce errors.  MetaGPT utilizes an assembly line paradigm to assign diverse roles to various agents, efficiently breaking down complex tasks into subtasks involving many agents working together.  On collaborative software engineering benchmarks, MetaGPT generates more coherent solutions than previous chat-based multi-agent systems.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18491",
        "keywords": [
            "metagpt",
            "meta programming",
            "multi agent collaborative framework",
            "multi agent systems",
            "expertise"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"More complex tasks, however, face challenges through logic inconsistencies due to cascading hallucinations caused by naively chaining LLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"More complex tasks, however, face challenges through logic inconsistencies due to cascading hallucinations caused by naively chaining LLMs.\""
    },
    {
        "title": "Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models",
        "authors": "Shangbin Feng;Weijia Shi;Yuyang Bai;Vidhisha Balachandran;Tianxing He;Yulia Tsvetkov",
        "site": "https://iclr.cc/virtual/2024/poster/18471",
        "summary": "By design, large language models (LLMs) are static general-purpose models, expensive to retrain or update frequently. As they are increasingly adopted for knowledge-intensive tasks, it becomes evident that these design choices lead to failures to generate factual, relevant, and up-to-date knowledge. To this end, we propose Knowledge Card, a modular framework to plug in new factual and relevant knowledge into general-purpose LLMs. We first introduce knowledge cards---specialized language models trained on corpora from specific domains and sources. Knowledge cards serve as parametric repositories that are selected at inference time to generate background knowledge for the base LLM. We then propose three content selectors to dynamically select and retain information in documents generated by knowledge cards, specifically controlling for relevance, brevity, and factuality of outputs. Finally, we propose two complementary integration approaches to augment the base LLM with the (relevant, factual) knowledge curated from the specialized LMs. Through extensive experiments, we demonstrate that Knowledge Card achieves state-of-the-art performance on six benchmark datasets. Ultimately, Knowledge Card framework enables dynamic synthesis and updates of knowledge from diverse domains. Its modularity will ensure that relevant knowledge can be continuously updated through the collective efforts of the research community.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18471",
        "keywords": [
            "knowledge card",
            "language models",
            "large language models",
            "plug in specialized language",
            "plug in",
            "specialized lms"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"By design, large language models (LLMs) are static general-purpose models, expensive to retrain or update frequently. As they are increasingly adopted for knowledge-intensive tasks, it becomes evident that these design choices lead to failures to generate factual, relevant, and up-to-date knowledge.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"By design, large language models (LLMs) are static general-purpose models, expensive to retrain or update frequently. As they are increasingly adopted for knowledge-intensive tasks, it becomes evident that these design choices lead to failures to generate factual, relevant, and up-to-date knowledge.\""
    },
    {
        "title": "Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning",
        "authors": "Bingchen Zhao;Haoqin Tu;Chen Wei;Jieru Mei;Cihang Xie",
        "site": "https://iclr.cc/virtual/2024/poster/18422",
        "summary": "This paper introduces an efficient strategy to transform Large Language Models (LLMs) into Multi-Modal Large Language Models. By conceptualizing this transformation as a domain adaptation process, \\ie, transitioning from text understanding to embracing multiple modalities, we intriguingly note that, within each attention block, tuning LayerNorm suffices to yield strong performance. Moreover, when benchmarked against other tuning approaches like full parameter finetuning or LoRA, its benefits on efficiency are substantial.For example, when compared to LoRA on a 13B model scale, performance can be enhanced by an average of over 20\\% across five multi-modal tasks, and meanwhile, results in a significant reduction of trainable parameters by 41.9\\% and a decrease in GPU memory usage by 17.6\\%. On top of this LayerNorm strategy, we showcase that selectively tuning only with conversational data can improve efficiency further. Beyond these empirical outcomes, we provide a comprehensive analysis to explore the role of LayerNorm in adapting LLMs to the multi-modal domain and improving the expressive power of the model.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18422",
        "keywords": [
            "parameter finetuning",
            "domain adaptation",
            "layernorm",
            "tuning layernorm",
            "tuning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"conceptualizing this transformation as a domain adaptation process, \\ie, transitioning from text understanding to embracing multiple modalities\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2**\nEvidence: \"conceptualizing this transformation as a domain adaptation process, \\ie, transitioning from text understanding to embracing multiple modalities\""
    },
    {
        "title": "Addressing Signal Delay in Deep Reinforcement Learning",
        "authors": "Wei Wang;Dongqi Han;Xufang Luo;Dongsheng Li",
        "site": "https://iclr.cc/virtual/2024/poster/18410",
        "summary": "Despite the notable advancements in deep reinforcement learning (DRL) in recent years, a prevalent issue that is often overlooked is the impact of signal delay. Signal delay occurs when there is a lag between an agent's perception of the environment and its corresponding actions. In this paper, we first formalize delayed-observation Markov decision processes (DOMDP) by extending the standard MDP framework to incorporate signal delays. Next, we elucidate the challenges posed by the presence of signal delay in DRL, showing that trivial DRL algorithms and generic methods for partially observable tasks suffer greatly from delays. Lastly, we propose effective strategies to overcome these challenges. Our methods achieve remarkable performance in continuous robotic control tasks with large delays, yielding results comparable to those in non-delayed cases. Overall, our work contributes to a deeper understanding of DRL in the presence of signal delays and introduces novel approaches to address the associated challenges.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18410",
        "keywords": [
            "reinforcement learning",
            "delay",
            "signal delay",
            "observation markov decision processes"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "CO2: Efficient Distributed Training with Full Communication-Computation Overlap",
        "authors": "Weigao Sun;Zhen Qin;Weixuan Sun;Shidi Li;Dong Li;Xuyang Shen;Yu Qiao;Yiran Zhong",
        "site": "https://iclr.cc/virtual/2024/poster/18399",
        "summary": "The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18399",
        "keywords": [
            "distributed data parallel",
            "natural language processing",
            "full communication computation overlap"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "LLMCarbon: Modeling the End-to-End Carbon Footprint of Large Language Models",
        "authors": "Ahmad Faiz;Sotaro Kaneda;Ruhan Wang;Rita Chukwunyere Osi;Prateek Sharma;Fan Chen;Lei Jiang",
        "site": "https://iclr.cc/virtual/2024/poster/18370",
        "summary": "The carbon footprint associated with large language models (LLMs) is a significant concern, encompassing emissions from their training, inference, experimentation, and storage processes, including operational and embodied carbon emissions. An essential aspect is accurately estimating the carbon impact of emerging LLMs even before their training, which heavily relies on GPU usage. Existing studies have reported the carbon footprint of LLM training, but only one tool, mlco2, can predict the carbon footprint of new neural networks prior to physical training. However, mlco2 has several serious limitations. It cannot extend its estimation to dense or mixture-of-experts (MoE) LLMs, disregards critical architectural parameters, focuses solely on GPUs, and cannot model embodied carbon footprints. Addressing these gaps, we introduce \\textit{\\carb}, an end-to-end carbon footprint projection model designed for both dense and MoE LLMs. Compared to mlco2, \\carb~significantly enhances the accuracy of carbon footprint estimations for various LLMs. The source code is released at \\url{https://github.com/SotaroKaneda/MLCarbon}.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18370",
        "keywords": [
            "emissions",
            "footprints",
            "carbon footprint",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, mlco2 has several serious limitations. It cannot extend its estimation to dense or mixture-of-experts (MoE) LLMs, disregards critical architectural parameters, focuses solely on GPUs, and cannot model embodied carbon footprints.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, mlco2 has several serious limitations. It cannot extend its estimation to dense or mixture-of-experts (MoE) LLMs, disregards critical architectural parameters, focuses solely on GPUs, and cannot model embodied carbon footprints.\""
    },
    {
        "title": "The mechanistic basis of data dependence and abrupt learning in an in-context classification task",
        "authors": "Gautam Reddy",
        "site": "https://iclr.cc/virtual/2024/poster/18368",
        "summary": "Transformer models exhibit in-context learning: the ability to accurately predict the response to a novel query based on illustrative examples in the input sequence, which contrasts with traditional in-weights learning of query-output relationships. What aspects of the training data distribution and architecture favor in-context vs in-weights learning? Recent work has shown that specific distributional properties inherent in language, such as burstiness, large dictionaries and skewed rank-frequency distributions, control the trade-off or simultaneous appearance of these two forms of learning. We first show that these results are recapitulated in a minimal attention-only network trained on a simplified dataset. In-context learning (ICL) is driven by the abrupt emergence of an induction head, which subsequently competes with in-weights learning. By identifying progress measures that precede in-context learning and targeted experiments, we construct a two-parameter model of an induction head which emulates the full data distributional dependencies displayed by the attention-based network. A phenomenological model of induction head formation traces its abrupt emergence to the sequential learning of three nested logits enabled by an intrinsic curriculum. We propose that the sharp transitions in attention-based networks arise due to a specific chain of multi-layer operations necessary to achieve ICL, which is implemented by nested nonlinearities sequentially learned during training.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18368",
        "keywords": [
            "abrupt emergence",
            "abrupt learning",
            "context learning",
            "context classification",
            "data dependence",
            "weights learning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicitly mentioned, but the paper discusses the limitations of in-context learning in transformer models, which can be related to LLMs, specifically the abrupt emergence of an induction head competing with in-weights learning.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None explicitly mentioned, but the paper discusses the limitations of in-context learning in transformer models, which can be related to LLMs, specifically the abrupt emergence of an induction head competing with in-weights learning."
    },
    {
        "title": "Making Pre-trained Language Models Great on Tabular Prediction",
        "authors": "Jiahuan Yan;Bo Zheng;Hongxia Xu;Yiheng Zhu;Danny Chen;Jimeng Sun;Jian Wu;Jintai Chen",
        "site": "https://iclr.cc/virtual/2024/poster/18356",
        "summary": "The transferability of deep neural networks (DNNs) has made significant progress in image and language processing. However, due to the heterogeneity among tables, such DNN bonus is still far from being well exploited on tabular data prediction (e.g., regression or classification tasks). Condensing knowledge from diverse domains, language models (LMs) possess the capability to comprehend feature names from various tables, potentially serving as versatile learners in transferring knowledge across distinct tables and diverse prediction tasks, but their discrete text representation space is inherently incompatible with numerical feature values in tables. In this paper, we present TP-BERTa, a specifically pre-trained LM for tabular data prediction. Concretely, a novel relative magnitude tokenization converts scalar numerical feature values to finely discrete, high-dimensional tokens, and an intra-feature attention approach integrates feature values with the corresponding feature names. Comprehensive experiments demonstrate that our pre-trained TP-BERTa leads the performance among tabular DNNs and is competitive with Gradient Boosted Decision Tree models in typical tabular data regime.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18356",
        "keywords": [
            "transferability",
            "tabular data prediction",
            "tabular prediction",
            "deep neural networks",
            "decision tree"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"but their discrete text representation space is inherently incompatible with numerical feature values in tables.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"but their discrete text representation space is inherently incompatible with numerical feature values in tables.\""
    },
    {
        "title": "Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts",
        "authors": "Jian Xie;Kai Zhang;Jiangjie Chen;Renze Lou;Yu Su",
        "site": "https://iclr.cc/virtual/2024/poster/18352",
        "summary": "By providing external information to large language models (LLMs), tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory.However, how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory? We present the first comprehensive and controlled investigation into the behavior of LLMs when encountering knowledge conflicts.We propose a systematic framework to elicit high-quality parametric memory from LLMs and construct the corresponding counter-memory, which enables us to conduct a series of controlled experiments.Our investigation reveals seemingly contradicting behaviors of LLMs.On the one hand, different from prior wisdom, we find that LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing.On the other hand, LLMs also demonstrate a strong confirmation bias when the external evidence contains some information that is consistent with their parametric memory, despite being presented with conflicting evidence at the same time.These results pose important implications that are worth careful consideration for the further development and deployment of tool- and retrieval-augmented LLMs.Resources are available at https://github.com/OSU-NLP-Group/LLM-Knowledge-Conflict.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18352",
        "keywords": [
            "knowledge conflicts",
            "language models",
            "evidence conflicts",
            "tool augmentation"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory?\"; \"On the other hand, LLMs also demonstrate a strong confirmation bias when the external evidence contains some information that is consistent with their parametric memory, despite being presented with conflicting evidence at the same time.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory?\"; \"On the other hand, LLMs also demonstrate a strong confirmation bias when the external evidence contains some information that is consistent with their parametric memory, despite being presented with conflicting evidence at the same time.\""
    },
    {
        "title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
        "authors": "Linlu Qiu;Liwei Jiang;Ximing Lu;Melanie Sclar;Valentina Pyatkin;Chandra Bhagavatula;Bailin Wang;Yoon Kim;Yejin Choi;Nouha Dziri;Xiang Ren",
        "site": "https://iclr.cc/virtual/2024/poster/18334",
        "summary": "The ability to derive underlying principles from a handful of observations and then generalize to novel situations---known as inductive reasoning---is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through $\\textit{iterative hypothesis refinement}$, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal $\\textit{hypothesis proposers}$ (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the proposed set of rules, this hybrid approach achieves strong results across inductive reasoning benchmarks that require inducing causal relations, language-like instructions, and symbolic concepts. However, they also behave as puzzling $\\textit{inductive reasoners}$, showing notable performance gaps between rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that LMs are proposing hypotheses without being able to actually apply the rules. Through empirical and human analyses, we further reveal several discrepancies between the inductive reasoning processes of LMs and humans, shedding light on both the potentials and limitations of using LMs in inductive reasoning tasks.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18334",
        "keywords": [
            "induction",
            "inductive reasoning",
            "iterative hypothesis refinement",
            "language models",
            "human inductive process",
            "inductive reasoning benchmarks",
            "puzzling",
            "intelligence"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Prior work suggests that language models (LMs) often fall short on inductive reasoning... However, they also behave as puzzling inductive reasoners, showing notable performance gaps between rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that LMs are proposing hypotheses without being able to actually apply the rules.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Prior work suggests that language models (LMs) often fall short on inductive reasoning... However, they also behave as puzzling inductive reasoners, showing notable performance gaps between rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that LMs are proposing hypotheses without being able to actually apply the rules.\""
    },
    {
        "title": "Large Language Models are Efficient Learners of Noise-Robust Speech Recognition",
        "authors": "Yuchen Hu;CHEN CHEN;Chao-Han Huck Yang;Ruizhe Li;Chao Zhang;Pin-Yu Chen;EngSiong Chng",
        "site": "https://iclr.cc/virtual/2024/poster/18285",
        "summary": "Recent advances in large language models (LLMs) have promoted generative error correction (GER) for automatic speech recognition (ASR), which leverages the rich linguistic knowledge and powerful reasoning ability of LLMs to improve recognition results. The latest work proposes a GER benchmark with \"HyPoradise\" dataset to learn the mapping from ASR N-best hypotheses to ground-truth transcription by efficient LLM finetuning, which shows great effectiveness but lacks specificity on noise-robust ASR. In this work, we extend the benchmark to noisy conditions and investigate if we can teach LLMs to perform denoising for GER just like what robust ASR do, where one solution is introducing noise information as a conditioner into LLM. However, directly incorporating noise embeddings from audio encoder could harm the LLM tuning due to cross-modality gap. To this end, we propose to extract a language-space noise embedding from the N-best list to represent the noise conditions of source speech, which can promote the denoising process in GER. Furthermore, in order to enhance its representation ability of audio noise, we design a knowledge distillation (KD) approach via mutual information estimation to distill the real noise information in audio embeddings to our language embedding. Experiments on various latest LLMs demonstrate our approach achieves a new breakthrough with up to 53.9% correction improvement in terms of word error rate while with limited training data. Analysis shows that our language-space noise embedding can well represent the noise conditions of source speech, under which off-the-shelf LLMs show strong ability of language-space denoising.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18285",
        "keywords": [
            "knowledge distillation",
            "mutual information",
            "large language models",
            "noise robust speech recognition",
            "automatic speech recognition"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, directly incorporating noise embeddings from audio encoder could harm the LLM tuning due to cross-modality gap.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, directly incorporating noise embeddings from audio encoder could harm the LLM tuning due to cross-modality gap.\""
    },
    {
        "title": "Tool-Augmented Reward Modeling",
        "authors": "Lei Li;Yekun Chai;Shuohuan Wang;Yu Sun;Hao Tian;Ningyu Zhang;Hua Wu",
        "site": "https://iclr.cc/virtual/2024/poster/18272",
        "summary": "Reward modeling (a.k.a., preference modeling) is instrumental for aligning large language models with human preferences, particularly within the context of reinforcement learning from human feedback (RLHF). While conventional reward models (RMs) have exhibited remarkable scalability, they oft struggle with fundamental functionality such as arithmetic computation, code execution, and factual lookup. In this paper, we propose a tool-augmented preference modeling approach, named Themis, to address these limitations by empowering RMs with access to external environments, including calculators and search engines. This approach not only fosters synergy between tool utilization and reward grading but also enhances interpretive capacity and scoring reliability. Our study delves into the integration of external tools into RMs, enabling them to interact with diverse external sources and construct task-specific tool engagement and reasoning traces in an autoregressive manner. We validate our approach across a wide range of domains, incorporating seven distinct external tools. Our experimental results demonstrate a noteworthy overall improvement of 17.7% across eight tasks in preference ranking. Furthermore, our approach outperforms Gopher 280B by 7.3% on TruthfulQA task in zero-shot evaluation. In human evaluations, RLHF trained with Themis attains an average win rate of 32% when compared to baselines across four distinct tasks. Additionally, we provide a comprehensive collection of tool-related RM datasets, incorporating data from seven distinct tool APIs, totaling 15,000 instances. We have made the code, data, and model checkpoints publicly available to facilitate and inspire further research advancements (https://github.com/ernie-research/Tool-Augmented-Reward-Model).",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18272",
        "keywords": [
            "augmented reward model",
            "augmented preference modeling",
            "language models",
            "reinforcement learning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While conventional reward models (RMs) have exhibited remarkable scalability, they oft struggle with fundamental functionality such as arithmetic computation, code execution, and factual lookup.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While conventional reward models (RMs) have exhibited remarkable scalability, they oft struggle with fundamental functionality such as arithmetic computation, code execution, and factual lookup.\""
    },
    {
        "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
        "authors": "Yujia Qin;Shihao Liang;Yining Ye;Kunlun Zhu;Lan Yan;Yaxi Lu;Yankai Lin;Xin Cong;Xiangru Tang;Bill Qian;Sihan Zhao;Lauren Hong;Runchu Tian;Ruobing Xie;Jie Zhou;Mark Gerstein;dahai li;Zhiyuan Liu;Maosong Sun",
        "site": "https://iclr.cc/virtual/2024/poster/18267",
        "summary": "Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18267",
        "keywords": [
            "apis",
            "restful apis",
            "tuning",
            "toolbench",
            "tool use",
            "large language models",
            "decision tree",
            "toolllm"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions.\""
    },
    {
        "title": "PixArt-$\\alpha$: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis",
        "authors": "Junsong Chen;Jincheng YU;Chongjian GE;Lewei Yao;Enze Xie;Zhongdao Wang;James Kwok;Ping Luo;Huchuan Lu;Zhenguo Li",
        "site": "https://iclr.cc/virtual/2024/poster/18231",
        "summary": "The most advanced text-to-image (T2I) models require significant training costs (e.g., millions of GPU hours), seriously hindering the fundamental innovation for the AIGC community while increasing CO2 emissions. This paper introduces PixArt-$\\alpha$, a Transformer-based T2I diffusion model whose image generation quality is competitive with state-of-the-art image generators (e.g., Imagen, SDXL, and even Midjourney), reaching near-commercial application standards. Additionally, it supports high-resolution image synthesis up to 1024px resolution with low training cost, as shown in Figure 1 and 2. To achieve this goal, three core designs are proposed: (1) Training strategy decomposition: We devise three distinct training steps that separately optimize pixel dependency, text-image alignment, and image aesthetic quality; (2) Efficient T2I Transformer: We incorporate cross-attention modules into Diffusion Transformer (DiT) to inject text conditions and streamline the computation-intensive class-condition branch; (3) High-informative data: We emphasize the significance of concept density in text-image pairs and leverage a large Vision-Language model to auto-label dense pseudo-captions to assist text-image alignment learning. As a result, PixArt-$\\alpha$'s training speed markedly surpasses existing large-scale T2I models, e.g., PixArt-$\\alpha$ only takes 10.8% of Stable Diffusion v1.5's training time (~675 vs. ~6,250 A100 GPU days), saving nearly \\\\$300,000 (\\\\$26,000 vs. \\\\$320,000) and reducing 90% CO2 emissions. Moreover, compared with a larger SOTA model, RAPHAEL, our training cost is merely 1%. Extensive experiments demonstrate that PixArt-$\\alpha$ excels in image quality, artistry, and semantic control. We hope PixArt-$\\alpha$ will provide new insights to the AIGC community and startups to accelerate building their own high-quality yet low-cost generative models from scratch.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18231",
        "keywords": [
            "diffusion transformer",
            "photorealistic text",
            "image synthesis",
            "training",
            "density"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions",
        "authors": "Satwik Bhattamishra;Arkil Patel;Phil Blunsom;Varun Kanade",
        "site": "https://iclr.cc/virtual/2024/poster/18211",
        "summary": "In order to understand the in-context learning phenomenon, recent works have adopted a stylized experimental framework and demonstrated that Transformers can match the performance of gradient-based learning algorithms for various classes of real-valued functions. However, the limitations of Transformers in implementing learning algorithms, and their ability to learn other forms of algorithms are not well understood. Additionally, the degree to which these capabilities are confined to attention-based models is unclear. Furthermore, it remains to be seen whether the insights derived from these stylized settings can be extrapolated to pretrained Large Language Models (LLMs). In this work, we take a step towards answering these questions by demonstrating the following: (a) On a test-bed with a variety of Boolean function classes, we find that Transformers can nearly match the optimal learning algorithm for 'simpler' tasks, while their performance deteriorates on more 'complex' tasks. Additionally, we find that certain attention-free models perform (almost) identically to Transformers on a range of tasks. (b) When provided ateaching sequence, i.e. a set of examples that uniquely identifies a function in a class, we show that Transformers learn more sample-efficiently. Interestingly, our results show that Transformers can learn to implementtwo distinctalgorithms to solve asingletask, and can adaptively select the more sample-efficient algorithm depending on the sequence of in-context examples. (c) Lastly, we show that extant LLMs, e.g. LLaMA-2, GPT-4, can compete with nearest-neighbor baselines on prediction tasks that are guaranteed to not be in their training set.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18211",
        "keywords": [
            "transformers",
            "context learning",
            "llms"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the limitations of Transformers in implementing learning algorithms, and their ability to learn other forms of algorithms are not well understood.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, the limitations of Transformers in implementing learning algorithms, and their ability to learn other forms of algorithms are not well understood.\""
    },
    {
        "title": "$\\mathcal{B}$-Coder: Value-Based Deep Reinforcement Learning for Program Synthesis",
        "authors": "Zishun Yu;Yunzhe Tao;Liyu Chen;Tao Sun;Hongxia Yang",
        "site": "https://iclr.cc/virtual/2024/poster/18189",
        "summary": "Program synthesis aims to create accurate, executable programs from problem specifications, specifically from natural language descriptions in our context. Recent studies have leveraged the power of reinforcement learning (RL) in conjunction with large language models (LLMs), significantly enhancing code generation capabilities. The application of RL focuses on directly optimizing for functional correctness, offering an advantage over conventional supervised methods. Despite policy-based RL methods dominating the literature on RL for program synthesis, the nature of program synthesis tasks hints at a natural alignment with value-based methods.This stems from the rich collection of off-policy programs, including those developed by human programmers and also historical samples, coupled with the straightforward verification of generated programs through automated unit testing, meaning rewards are easy to obtain.Diverging from the dominant use of policy-based algorithms, our work explores the feasibility of value-based approaches, leading to the development of our $\\mathcal{B}$-Coder (pronounced Bellman coder).Yet, training value-based methods presents challenges due to the enormous search space inherent to program synthesis. To this end, we introduce an initialization protocol for RL agents utilizing pre-trained LMs and a conservative Bellman operator to reduce training complexities. Moreover, we demonstrate how to leverage the learned value functions as a dual strategy to post-process generated programs. Our empirical evaluations demonstrated $\\mathcal{B}$-Coder's capability in achieving state-of-the-art performance when compared to policy-based methods. Remarkably, this achievement is reached with minimal reward engineering effort, highlighting the effectiveness of value-based RL, independent of reward designs.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18189",
        "keywords": [
            "program synthesis",
            "code generation",
            "reinforcement learning",
            "deep reinforcement learning",
            "policy based rl",
            "correctness"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Yet, training value-based methods presents challenges due to the enormous search space inherent to program synthesis.\"\n\nThis paper mentions a limitation of using value-based methods with LLMs, but it is not the primary focus of the paper and is only mentioned briefly.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Yet, training value-based methods presents challenges due to the enormous search space inherent to program synthesis.\"\n\nThis paper mentions a limitation of using value-based methods with LLMs, but it is not the primary focus of the paper and is only mentioned briefly."
    },
    {
        "title": "Circuit Component Reuse Across Tasks in Transformer Language Models",
        "authors": "Jack Merullo;Carsten Eickhoff;Ellie Pavlick",
        "site": "https://iclr.cc/virtual/2024/poster/18171",
        "summary": "Recent work in mechanistic interpretability has shown that behaviors in language models can be successfully reverse-engineered through circuit analysis. A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a higher level. In this work, we present evidence that insights (both low-level findings about specific heads and higher-level findings about general algorithms) can indeed generalize across tasks. Specifically, we study the circuit discovered in (Wang, 2022) for the Indirect Object Identification (IOI) task and 1.) show that it reproduces on a larger GPT2 model, and 2.) that it is mostly reused to solve a seemingly different task: Colored Objects (Ippolito & Callison-Burch, 2023). We provide evidence that the process underlying both tasks is functionally very similar, and contains about a 78% overlap in in-circuit attention heads. We further present a proof-of-concept intervention experiment, in which we adjust four attention heads in middle layers in order to ‘repair’ the Colored Objects circuit and make it behave like the IOI circuit. In doing so, we boost accuracy from 49.6% to 93.7% on the Colored Objects task and explain most sources of error. The intervention affects downstream attention heads in specific ways predicted by their interactions in the IOI circuit, indicating that this subcircuit behavior is invariant to the different task inputs. Overall, our results provide evidence that it may yet be possible to explain large language models' behavior in terms of a relatively small number of interpretable task-general algorithmic building blocks and computational components.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18171",
        "keywords": [
            "language models",
            "transformer language models",
            "circuit component reuse",
            "circuit",
            "circuit attention",
            "experiment",
            "general algorithms"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a higher level.\"\n\nThis abstract mentions a limitation of LLMs in passing, specifically the criticism that circuit analysis is task-specific, but it does not elaborate on this limitation and instead focuses on presenting evidence that contradicts this criticism.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a higher level.\"\n\nThis abstract mentions a limitation of LLMs in passing, specifically the criticism that circuit analysis is task-specific, but it does not elaborate on this limitation and instead focuses on presenting evidence that contradicts this criticism."
    },
    {
        "title": "Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game",
        "authors": "Sam Toyer;Olivia Watkins;Ethan Adrian Mendes;Justin Svegliato;Luke Bailey;Tiffany Wang;Isaac Ong;Karim Elmaaroufi;Pieter Abbeel;Trevor Darrell;Alan Ritter;Stuart Russell",
        "site": "https://iclr.cc/virtual/2024/poster/18168",
        "summary": "While Large Language Models (LLMs) are increasingly being used in real-world applications, they remain vulnerable toprompt injection attacks: malicious third party prompts that subvert the intent of the system designer. To help researchers study this problem, we present a dataset of over 563,000 prompt injection attacks and 118,000 prompt-based \"defenses\" against prompt injection, all created by players of an online game called Tensor Trust. To the best of our knowledge, this is the first dataset that includes both human-generated attacks and defenses for instruction-following LLMs. The attacks in our dataset have easily interpretable structure, and shed light on the weaknesses of LLMs. We also use the dataset to create a benchmark for resistance to two types of prompt injection, which we refer to asprompt extractionandprompt hijacking. Our benchmark results show that many models are vulnerable to the attack strategies in the Tensor Trust dataset. Furthermore, we show that some attack strategies from the dataset generalize to deployed LLM-based applications, even though they have a very different set of constraints to the game. We release data and code attensortrust.ai/paper",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18168",
        "keywords": [
            "attacks",
            "injection attacks",
            "online game",
            "tensor trust",
            "prompt injection attacks"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While Large Language Models (LLMs) are increasingly being used in real-world applications, they remain vulnerable to prompt injection attacks: malicious third party prompts that subvert the intent of the system designer.\"; \"The attacks in our dataset have easily interpretable structure, and shed light on the weaknesses of LLMs.\"; \"Our benchmark results show that many models are vulnerable to the attack strategies",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"While Large Language Models (LLMs) are increasingly being used in real-world applications, they remain vulnerable to prompt injection attacks: malicious third party prompts that subvert the intent of the system designer.\"; \"The attacks in our dataset have easily interpretable structure, and shed light on the weaknesses of LLMs.\"; \"Our benchmark results show that many models are vulnerable to the attack strategies"
    },
    {
        "title": "Improving Generalization of Alignment with Human Preferences through Group Invariant Learning",
        "authors": "Rui Zheng;Wei Shen;Yuan Hua;Wenbin Lai;Shihan Dou;Yuhao Zhou;Zhiheng Xi;Xiao Wang;Haoran Huang;Tao Gui;Qi Zhang;Xuanjing Huang",
        "site": "https://iclr.cc/virtual/2024/poster/18165",
        "summary": "The success of AI assistants based on language models (LLMs) hinges crucially on Reinforcement Learning from Human Feedback (RLHF), which enables the generation of responses more aligned with human preferences. As universal AI assistants, there's a growing expectation for them to perform consistently across various domains. However, previous work shows that Reinforcement Learning (RL) often exploits shortcuts to attain high rewards and overlooks challenging samples.This focus on quick reward gains undermines both the stability in training and the model's ability to generalize to new, unseen data.In this work, we propose a novel approach that can learn a consistent policy via RL across various data groups or domains. Given the challenges associated with acquiring group annotations, our method automatically classifies data into different groups, deliberately maximizing performance variance.Then, we optimize the policy to perform well on challenging groups. Lastly, leveraging the established groups, our approach adaptively adjusts the exploration space, allocating more learning capacity to more challenging data and preventing the model from over-optimizing on simpler data. Experimental results indicate that our approach significantly enhances training stability and model generalization.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18165",
        "keywords": [
            "group invariant learning",
            "reinforcement learning",
            "generalization",
            "language models",
            "alignment",
            "training"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, previous work shows that Reinforcement Learning (RL) often exploits shortcuts to attain high rewards and overlooks challenging samples.This focus on quick reward gains undermines both the stability in training and the model's ability to generalize to new, unseen data.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, previous work shows that Reinforcement Learning (RL) often exploits shortcuts to attain high rewards and overlooks challenging samples.This focus on quick reward gains undermines both the stability in training and the model's ability to generalize to new, unseen data.\""
    },
    {
        "title": "Evaluating the Zero-shot Robustness of Instruction-tuned Language Models",
        "authors": "Jiuding Sun;Chantal Shaib;Byron C Wallace",
        "site": "https://iclr.cc/virtual/2024/poster/18154",
        "summary": "Instruction fine-tuning has recently emerged as a promising approach for improving the zero-shot capabilities of Large Language Models (LLMs) on new tasks. This technique has shown particular strength in improving the performance of modestly sized LLMs, sometimes inducing performance competitive with much larger model variants. In this paper, we ask two questions: (1) How sensitive are instruction-tuned models to the particular phrasings of instructions, and, (2) How can we make them more robust to such natural language variation? To answer the former, we collect a set of 319 instructions manually written by NLP practitioners for over 80 unique tasks included in widely used benchmarks, and we evaluate the variance and average performance of these instructions as compared to instruction phrasings observed during instruction fine-tuning. We find that using novel (unobserved) but appropriate instruction phrasings consistently degrades model performance, sometimes substantially so. Further, such natural instructions yield a wide variance in downstream performance, despite their semantic equivalence. Put another way, instruction-tuned models are not especially robust to instruction re-phrasings. We propose a simple method to mitigate this issue by introducing ``soft prompt'' embedding parameters and optimizing these to maximize the similarity between representations of semantically equivalent instructions. We show that this method consistently improves the robustness of instruction-tuned models.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18154",
        "keywords": [
            "instruction tuned",
            "instruction fine tuning",
            "robust",
            "zero shot robustness",
            "instruction tuned models",
            "instruction tuned language models"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Put another way, instruction-tuned models are not especially robust to instruction re-phrasings.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Put another way, instruction-tuned models are not especially robust to instruction re-phrasings.\""
    },
    {
        "title": "DyVal: Dynamic Evaluation of Large Language Models for Reasoning Tasks",
        "authors": "Kaijie Zhu;Jiaao Chen;Jindong Wang;Neil Zhenqiang Gong;Diyi Yang;Xing Xie",
        "site": "https://iclr.cc/virtual/2024/poster/18134",
        "summary": "Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns are raised about potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs. In this paper, we introduce DyVal, a general and flexible protocol for dynamic evaluation of LLMs. Based on our framework, we build graph-informed DyVal by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various LLMs ranging from Flan-T5-large to GPT-3.5-Turbo and GPT-4. Experiments show that LLMs perform worse in DyVal-generated evaluation samples with different complexities, highlighting the significance of dynamic evaluation.We also analyze the failure cases and results of different prompting methods.Moreover, DyVal-generated samples are not only evaluation sets, but also helpful data for fine-tuning to improve the performance of LLMs on existing benchmarks.We hope that DyVal can shed light on future evaluation research of LLMs. Code is available at: https://github.com/microsoft/promptbench.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18134",
        "keywords": [
            "language models",
            "large language models",
            "evaluation",
            "evaluation sets"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, concerns are raised about potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs.\"; \"Experiments show that LLMs perform worse in DyVal-generated evaluation samples with different complexities, highlighting the significance of dynamic evaluation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, concerns are raised about potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs.\"; \"Experiments show that LLMs perform worse in DyVal-generated evaluation samples with different complexities, highlighting the significance of dynamic evaluation.\""
    },
    {
        "title": "Confronting Reward Model Overoptimization with Constrained RLHF",
        "authors": "Ted Moskovitz;Aaditya K Singh;DJ Strouse;Tuomas Sandholm;Ruslan Salakhutdinov;Anca Dragan;Stephen Marcus McAleer",
        "site": "https://iclr.cc/virtual/2024/poster/18133",
        "summary": "Large language models are typically aligned with human preferences by optimizing reward models (RMs) fitted to human feedback. However, human preferences are multi-faceted, and it is increasingly common to derive reward from a composition of simpler reward models which each capture a different aspect of language quality. This itself presents a challenge, as it is difficult to appropriately weight these component RMs when combining them. Compounding this difficulty, because any RM is only a proxy for human evaluation, this process is vulnerable tooveroptimization, wherein past a certain point, accumulating higher reward is associated with worse human ratings. In this paper, we perform the first study on overoptimization in composite RMs, showing that correlation between component RMs has a significant effect on the locations of these points. We then introduce an approach to solve this issue using constrained reinforcement learning as a means of preventing the agent from exceeding each RM's threshold of usefulness. Our method addresses the problem of weighting component RMs by learning dynamic weights, naturally given by the Lagrange multipliers. As a result, each RM stays within the range at which it is an effective proxy, improving evaluation performance. Finally, we introduce an adaptive method using gradient-free optimization to identify and optimize towards these points during a single run.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18133",
        "keywords": [
            "constrained reinforcement learning",
            "reward"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Compounding this difficulty, because any RM is only a proxy for human evaluation, this process is vulnerable to overoptimization, wherein past a certain point, accumulating higher reward is associated with worse human ratings.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Compounding this difficulty, because any RM is only a proxy for human evaluation, this process is vulnerable to overoptimization, wherein past a certain point, accumulating higher reward is associated with worse human ratings.\""
    },
    {
        "title": "Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory",
        "authors": "Niloofar Mireshghallah;Hyunwoo Kim;Xuhui Zhou;Yulia Tsvetkov;Maarten Sap;Reza Shokri;Yejin Choi",
        "site": "https://iclr.cc/virtual/2024/poster/18131",
        "summary": "Existing efforts on quantifying privacy implications for large language models (LLMs) solely focus on measuring leakage of training data. In this work, we shed light on the often-overlooked interactive settings where an LLM receives information from multiple sources and generates an output to be shared with other entities, creating the potential of exposing sensitive input data in inappropriate contexts. In these scenarios, humans nat- urally uphold privacy by choosing whether or not to disclose information depending on the context. We ask the question “Can LLMs demonstrate an equivalent discernment and reasoning capability when considering privacy in context?” We propose CONFAIDE, a benchmark grounded in the theory of contextual integrity and designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. CONFAIDE consists of four tiers, gradually increasing in complexity, with the final tier evaluating contextual privacy reasoning and theory of mind capabilities. Our experiments show that even commercial models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively, highlighting the urgent need for a new direction of privacy-preserving approaches as we demonstrate a larger underlying problem stemmed in the models’ lack of reasoning capabilities.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18131",
        "keywords": [
            "privacy",
            "contextual integrity",
            "contextual privacy reasoning",
            "inappropriate",
            "context",
            "nat"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our experiments show that even commercial models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively, highlighting the urgent need for a new direction of privacy-preserving approaches as we demonstrate a larger underlying problem stemmed in the models’ lack of reasoning capabilities.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Our experiments show that even commercial models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively, highlighting the urgent need for a new direction of privacy-preserving approaches as we demonstrate a larger underlying problem stemmed in the models’ lack of reasoning capabilities.\""
    },
    {
        "title": "Solving Homogeneous and Heterogeneous Cooperative Tasks with Greedy Sequential Execution",
        "authors": "Shanqi Liu;Dong Xing;Pengjie Gu;Xinrun Wang;Bo An;Yong Liu",
        "site": "https://iclr.cc/virtual/2024/poster/18108",
        "summary": "Cooperative multi-agent reinforcement learning (MARL) is extensively used for solving complex cooperative tasks, and value decomposition methods are a prevalent approach for this domain. However, these methods have not been successful in addressing both homogeneous and heterogeneous tasks simultaneously which is a crucial aspect for the practical application of cooperative agents. On one hand, value decomposition methods demonstrate superior performance in homogeneous tasks. Nevertheless, they tend to produce agents with similar policies, which is unsuitable for heterogeneous tasks. On the other hand, solutions based on personalized observation or assigned roles are well-suited for heterogeneous tasks. However, they often lead to a trade-off situation where the agent's performance in homogeneous scenarios is negatively affected due to the aggregation of distinct policies. An alternative approach is to adopt sequential execution policies, which offer a flexible form for learning both types of tasks. However, learning sequential execution policies poses challenges in terms of credit assignment, and the limited information about subsequently executed agents can lead to sub-optimal solutions, which is known as the relative over-generalization problem. To tackle these issues, this paper proposes Greedy Sequential Execution (GSE) as a solution to learn the optimal policy that covers both scenarios. In the proposed GSE framework, we introduce an individual utility function into the framework of value decomposition to consider the complex interactions between agents. This function is capable of representing both the homogeneous and heterogeneous optimal policies. Furthermore, we utilize greedy marginal contribution calculated by the utility function as the credit value of the sequential execution policy to address the credit assignment and relative over-generalization problem. We evaluated GSE in both homogeneous and heterogeneous scenarios. The results demonstrate that GSE achieves significant improvement in performance across multiple domains, especially in scenarios involving both homogeneous and heterogeneous tasks.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18108",
        "keywords": [
            "greedy sequential execution",
            "relative over generalization problem",
            "heterogeneous cooperative tasks"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Lemur: Harmonizing Natural Language and Code for Language Agents",
        "authors": "Yiheng Xu;Hongjin SU;Chen Xing;Boyu Mi;Qian Liu;Weijia Shi;Binyuan Hui;Fan Zhou;Yitao Liu;Tianbao Xie;Zhoujun Cheng;Siheng Zhao;Lingpeng Kong;Bailin Wang;Caiming Xiong;Tao Yu",
        "site": "https://iclr.cc/virtual/2024/poster/18100",
        "summary": "We introduce Lemur and Lemur-Chat, openly accessible language models optimizedfor both natural language and coding capabilities to serve as the backboneof versatile language agents. The evolution from language chat models tofunctional language agents demands that models not only master human interaction,reasoning, and planning but also ensure grounding in the relevant environments.This calls for a harmonious blend of language and coding capabilitiesin the models. Lemur and Lemur-Chat are proposed to address this necessity,demonstrating balanced proficiencies in both domains, unlike existingopen-source models that tend to specialize in either. Through meticulous pretrainingusing a code-intensive corpus and instruction fine-tuning on text and codedata, our models achieve state-of-the-art averaged performance across diversetext and coding benchmarks. Comprehensive experiments demonstrate Lemur’ssuperiority over existing open-source models and its proficiency across variousagent tasks involving human communication, tool usage, and interaction underfully- and partially- observable environments. The harmonization between naturaland programming languages enables Lemur-Chat to significantly narrow thegap with proprietary models on agent abilities, providing key insights into developingadvanced open-source agents adept at reasoning, planning, and operatingseamlessly across environments. Our model and code have been open-sourced athttps://github.com/OpenLemur/Lemur.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18100",
        "keywords": [
            "lemur",
            "naturaland programming languages",
            "natural language",
            "lemur chat",
            "language models",
            "language agents"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"unlike existing open-source models that tend to specialize in either.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"unlike existing open-source models that tend to specialize in either.\""
    },
    {
        "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
        "authors": "Akari Asai;Zeqiu Wu;Yizhong Wang;Avirup Sil;Hannaneh Hajishirzi",
        "site": "https://iclr.cc/virtual/2024/poster/18095",
        "summary": "Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework calledSelf-Reflective Retrieval-Augmented Generation (Self-RAG)that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its generations using special tokens, called {\\it reflection} tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning, and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models. Our code and trained models are available at https://selfrag.github.io/",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18095",
        "keywords": [
            "self reflection",
            "reflective",
            "generations",
            "self rag",
            "retrieval augmented generation"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate.\""
    },
    {
        "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!",
        "authors": "Xiangyu Qi;Yi Zeng;Tinghao Xie;Pin-Yu Chen;Ruoxi Jia;Prateek Mittal;Peter Henderson",
        "site": "https://iclr.cc/virtual/2024/poster/18094",
        "summary": "Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open-source release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on customized datasets accelerate this trend. But, what are the safety costs associated with such customized fine-tuning? While existing safety alignment techniques restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent. These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing --- even if a model's initial safety alignment is impeccable, how can it be maintained after customized fine-tuning? We outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the customized fine-tuning of aligned LLMs.  (This paper contains red-teaming data and model-generated content that can be offensive in nature.)",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18094",
        "keywords": [
            "fine tuning",
            "safety alignment"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples... even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples... even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent.\""
    },
    {
        "title": "Learning Performance-Improving Code Edits",
        "authors": "Alexander G Shypula;Aman Madaan;Yimeng Zeng;Uri Alon;Jacob R. Gardner;Yiming Yang;Milad Hashemi;Graham Neubig;Parthasarathy Ranganathan;Osbert Bastani;Amir Yazdanbakhsh",
        "site": "https://iclr.cc/virtual/2024/poster/18045",
        "summary": "With the decline of Moore's law, optimizing program performance has become a major focus of software research. However, high-level optimizations such as API and algorithm changes remain elusive due to the difficulty of understanding the semantics of code. Simultaneously, pretrained large language models (LLMs) have demonstrated strong capabilities at solving a wide range of programming tasks. To that end, we introduce a framework for adapting LLMs to high-level program optimization. First, we curate a dataset of performance-improving edits made by human programmers of over 77,000 competitive C++ programming submission pairs, accompanied by extensive unit tests. A major challenge is the significant variability of measuring performance on commodity hardware, which can lead to spurious \"improvements.\" To isolate and reliably evaluate the impact of program optimizations, we design an environment based on the gem5 full system simulator, the de facto simulator used in academia and industry. Next, we propose a broad range of adaptation strategies for code optimization; for prompting, these include retrieval-based few-shot prompting and chain-of-thought, and for finetuning, these include performance-conditioned generation and synthetic data augmentation based on self-play. A combination of these techniques achieves a mean speedup of 6.86$\\times$ with eight generations, higher than average optimizations from individual programmers (3.66$\\times$). Using our model's fastest generations, we set a new upper limit on the fastest speedup possible for our dataset at 9.64$\\times$ compared to using the fastest human submissions available (9.56$\\times$).",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18045",
        "keywords": [
            "optimizations",
            "code optimization",
            "program optimization",
            "language models",
            "algorithm changes"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"A major challenge is the significant variability of measuring performance on commodity hardware, which can lead to spurious \"improvements.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"A major challenge is the significant variability of measuring performance on commodity hardware, which can lead to spurious \"improvements.\""
    },
    {
        "title": "MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning",
        "authors": "Zayne Rea Sprague;Xi Ye;Kaj Bostrom;Swarat Chaudhuri;Greg Durrett",
        "site": "https://iclr.cc/virtual/2024/poster/18015",
        "summary": "While large language models (LLMs) equipped with techniques like chain-of-thought prompting have demonstrated impressive capabilities, they still fall short in their ability to reason robustly in complex settings. However, evaluating LLM reasoning is challenging because system capabilities continue to grow while benchmark datasets for tasks like logical deduction have remained static. We introduce MuSR, a dataset for evaluating language models on multistep soft reasoning tasks specified in a natural language narrative. This dataset has two crucial features. First, it is created through a novel neurosymbolic synthetic-to-natural generation algorithm, enabling the construction of complex reasoning instances that challenge GPT-4 (e.g., murder mysteries roughly 1000 words in length) and which can be scaled further as more capable LLMs are released. Second, our data instances are free text narratives corresponding to real-world domains of reasoning; this makes it simultaneously much more challenging than other synthetically-crafted benchmarks while remaining realistic and tractable for human annotators to solve with high accuracy. We evaluate a range of LLMs and prompting techniques on this dataset and characterize the gaps that remain for techniques like chain-of-thought to perform robust reasoning.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/18015",
        "keywords": [
            "multistep soft reasoning",
            "natural generation",
            "neurosymbolic synthetic",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While large language models (LLMs) equipped with techniques like chain-of-thought prompting have demonstrated impressive capabilities, they still fall short in their ability to reason robustly in complex settings.\"; \"We evaluate a range of LLMs and prompting techniques on this dataset and characterize the gaps that remain for techniques like chain-of-thought to perform robust reasoning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"While large language models (LLMs) equipped with techniques like chain-of-thought prompting have demonstrated impressive capabilities, they still fall short in their ability to reason robustly in complex settings.\"; \"We evaluate a range of LLMs and prompting techniques on this dataset and characterize the gaps that remain for techniques like chain-of-thought to perform robust reasoning.\""
    },
    {
        "title": "Selective Visual Representations Improve Convergence and Generalization for Embodied AI",
        "authors": "Ainaz Eftekhar;Kuo-Hao Zeng;Jiafei Duan;Ali Farhadi;Aniruddha Kembhavi;Ranjay Krishna",
        "site": "https://iclr.cc/virtual/2024/poster/17987",
        "summary": "Embodied AI models often employ off the shelf vision backbones like CLIP to encode their visual observations. Although such general purpose representations encode rich syntactic and semantic information about the scene, much of this information is often irrelevant to the specific task at hand. This introduces noise within the learning process and distracts the agent's focus from task-relevant visual cues.Inspired by selective attention in humans—the process through which people filter their perception based on their experiences, knowledge, and the task at hand—we introduce a parameter-efficient approach to filter visual stimuli for embodied AI.Our approach induces a task-conditioned bottleneck using a small learnable codebook module. This codebook is trained jointly to optimize task reward and acts as a task-conditioned selective filter over the visual observation.Our experiments showcase state-of-the-art performance for object goal navigation and object displacement across $5$ benchmarks, ProcTHOR, ArchitecTHOR, RoboTHOR, AI2-iTHOR, and ManipulaTHOR. The filtered representations produced by the codebook are also able generalize better and converge faster when adapted to other simulation environments such as Habitat. Our qualitative analyses show that agents explore their environments more effectively and their representations retain task-relevant information like target object recognition while ignoring superfluous information about other objects.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17987",
        "keywords": [
            "generalize",
            "embodied",
            "selective attention",
            "selective filter",
            "learnable codebook",
            "codebook",
            "filter",
            "embodied ai",
            "filter visual",
            "visual representations",
            "parameter efficient",
            "object recognition"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Beyond Memorization: Violating Privacy via Inference with Large Language Models",
        "authors": "Robin Staab;Mark Vero;Mislav Balunovic;Martin Vechev",
        "site": "https://iclr.cc/virtual/2024/poster/17964",
        "summary": "Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models’ inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals’ privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to 85% top-1 and 95% top-3 accuracy at a fraction of the cost (100x) and time (240x) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemingly benign questions. Finally, we show that common mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against LLM inference. Our findings highlight that current LLMs can infer personal data at a previously unattainable scale. In the absence of working defenses, we advocate for a broader discussion around LLM privacy implications beyond memorization, striving for stronger and wider privacy protection.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17964",
        "keywords": [
            "language models",
            "privacy",
            "personal attributes",
            "inference",
            "infer personal",
            "text anonymization",
            "violating privacy",
            "model alignment"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Current mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against LLM inference.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Current mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against LLM inference.\""
    },
    {
        "title": "Vision-Language Foundation Models as Effective Robot Imitators",
        "authors": "Xinghang Li;Minghuan Liu;Hanbo Zhang;Cunjun Yu;Jie Xu;Hongtao Wu;Chilam Cheang;Ya Jing;Weinan Zhang;Huaping Liu;Hang Li;Tao Kong",
        "site": "https://iclr.cc/virtual/2024/poster/17943",
        "summary": "Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on robotics data.To this end, we derive a simple and novel vision-language manipulation framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo. Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step vision-language comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on language-conditioned manipulation datasets. Such a decomposition provides RoboFlamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show RoboFlamingo can be an effective and competitive alternative to adapt VLMs to robot control.Our extensive experimental results also reveal several interesting conclusions regarding the behavior of different pre-trained VLMs on manipulation tasks. We believe RoboFlamingo has the potential to be a cost-effective and easy-to-use solution for robotics manipulation, empowering everyone with the ability to fine-tune their own robotics policy. Our code will be made public upon acceptance.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17943",
        "keywords": [
            "effective",
            "benchmark",
            "robot",
            "vision language"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our extensive experimental results also reveal several interesting conclusions regarding the behavior of different pre-trained VLMs on manipulation tasks.\"\n\nThis rating is chosen because the abstract mentions some limitations or interesting conclusions about the behavior of pre-trained VLMs, but it is not the primary focus of the paper. The main focus is on introducing the RoboFlamingo framework and its effectiveness in robotics manipulation",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Our extensive experimental results also reveal several interesting conclusions regarding the behavior of different pre-trained VLMs on manipulation tasks.\"\n\nThis rating is chosen because the abstract mentions some limitations or interesting conclusions about the behavior of pre-trained VLMs, but it is not the primary focus of the paper. The main focus is on introducing the RoboFlamingo framework and its effectiveness in robotics manipulation"
    },
    {
        "title": "Sentence-level Prompts Benefit Composed Image Retrieval",
        "authors": "Yang bai;Xinxing Xu;Yong Liu;Salman Khan;Fahad Khan;Wangmeng Zuo;Rick Siow Mong Goh;Chun-Mei Feng",
        "site": "https://iclr.cc/virtual/2024/poster/17909",
        "summary": "Composed image retrieval (CIR) is the task of retrieving specific images by using a query that involves both a reference image and a relative caption. Most existing CIR models adopt the late-fusion strategy to combine visual and language features. Besides, several approaches have also been suggested to generate a pseudo-word token from the reference image, which is further integrated into the relative caption for CIR. However, these pseudo-word-based prompting methods have limitations when target image encompasses complex changes on reference image, e.g., object removal and attribute modification. In this work, we demonstrate that learning an appropriate sentence-level prompt for the relative caption (SPRC) is sufficient for achieving effective composed image retrieval. Instead of relying on pseudo- word-based prompts, we propose to leverage pretrained V-L models, e.g., BLIP-2, to generate sentence-level prompts. By concatenating the learned sentence-level prompt with the relative caption, one can readily use existing text-based image retrieval models to enhance CIR performance. Furthermore, we introduce both image-text contrastive loss and text prompt alignment loss to enforce the learning of suitable sentence-level prompts. Experiments show that our proposed method performs favorably against the state-of-the-art CIR methods on the Fashion-IQ and CIRR datasets.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17909",
        "keywords": [
            "image retrieval",
            "composed image retrieval",
            "relative caption"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these pseudo-word-based prompting methods have limitations when target image encompasses complex changes on reference image, e.g., object removal and attribute modification.\"\n\n(Note: Although the paper discusses limitations, it is not a major focus and is used to justify the proposed method. The primary focus is on the proposed solution and its performance.)",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these pseudo-word-based prompting methods have limitations when target image encompasses complex changes on reference image, e.g., object removal and attribute modification.\"\n\n(Note: Although the paper discusses limitations, it is not a major focus and is used to justify the proposed method. The primary focus is on the proposed solution and its performance.)"
    },
    {
        "title": "SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents",
        "authors": "Xuhui Zhou;Hao Zhu;Leena Mathur;Ruohong Zhang;Haofei Yu;Zhengyang Qi;Louis-Philippe Morency;Yonatan Bisk;Daniel Fried;Graham Neubig;Maarten Sap",
        "site": "https://iclr.cc/virtual/2024/poster/17897",
        "summary": "Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play andinteractunder a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17897",
        "keywords": [
            "sotopia",
            "social intelligence",
            "social commonsense reasoning",
            "artificial agents",
            "language agents",
            "interactive"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills.\""
    },
    {
        "title": "Analyzing Feed-Forward Blocks in Transformers through the Lens of Attention Maps",
        "authors": "Goro Kobayashi;Tatsuki Kuribayashi;Sho Yokoi;Kentaro Inui",
        "site": "https://iclr.cc/virtual/2024/poster/17891",
        "summary": "Transformers are ubiquitous in wide tasks.Interpreting their internals is a pivotal goal. Nevertheless, their particular components, feed-forward (FF) blocks, have typically been less analyzed despite their substantial parameter amounts.We analyze the input contextualization effects of FF blocks by rendering them in the attention maps as a human-friendly visualization scheme.Our experiments with both masked- and causal-language models reveal that FF networks modify the input contextualization to emphasize specific types of linguistic compositions. In addition, FF and its surrounding components tend to cancel out each other's effects, suggesting potential redundancy in the processing of the Transformer layer.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17891",
        "keywords": [
            "transformers",
            "contextualization",
            "attention maps",
            "ff networks"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"FF and its surrounding components tend to cancel out each other's effects, suggesting potential redundancy in the processing of the Transformer layer.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"FF and its surrounding components tend to cancel out each other's effects, suggesting potential redundancy in the processing of the Transformer layer.\""
    },
    {
        "title": "OctoPack: Instruction Tuning Code Large Language Models",
        "authors": "Niklas Muennighoff;Qian Liu;Armel Randy Zebaze;Qinkai Zheng;Binyuan Hui;Terry Yue Zhuo;Swayam Singh;Xiangru Tang;Leandro Von Werra;Shayne Longpre",
        "site": "https://iclr.cc/virtual/2024/poster/17875",
        "summary": "Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17875",
        "keywords": [
            "instruction tuning",
            "octopack",
            "language models",
            "code explanation",
            "finetuning large language models",
            "code synthesis"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "The Consensus Game: Language Model Generation via Equilibrium Search",
        "authors": "Athul Paul Jacob;Yikang Shen;Gabriele Farina;Jacob Andreas",
        "site": "https://iclr.cc/virtual/2024/poster/17870",
        "summary": "When applied to question answering and other text generation tasks, language models (LMs) may be queried generatively (by sampling answers from their output distribution) or discriminatively (by using them to score or rank a set of candidate answers). These procedures sometimes yield very different predictions. How do we reconcile mutually incompatible scoring procedures to obtain coherent LM predictions? We introduce a new, a training-free, game-theoretic procedure for language model decoding. Our approach casts language model decoding as a regularized imperfect-information sequential signaling game—which we term the concensus game—in which a generator seeks to communicate an abstract correctness parameter using natural language sentences to a discriminator. We develop computational procedures for finding approximate equilibria of this game, resulting in a decoding algorithm we call equilibrium-ranking. Applied to a large number of tasks (including reading comprehension, commonsense reasoning, mathematical problem-solving, and assistive dialog), equilibrium-ranking consistently improves performance over existing LM decoding procedures. These improvements are sometimes substantial—on multiple benchmarks, we observe that applying equilibrium-ranking to LLaMA-7B outperforms the much larger LLaMA-65B and PaLM-540B models.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17870",
        "keywords": [
            "equilibrium",
            "consensus game",
            "equilibrium search",
            "assistive dialog",
            "language model",
            "commonsense"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the abstract mentions \"sometimes yield very different predictions\" which implies some limitation, however, it is not explicitly stated as a limitation of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the abstract mentions \"sometimes yield very different predictions\" which implies some limitation, however, it is not explicitly stated as a limitation of LLMs."
    },
    {
        "title": "An Image Is Worth 1000 Lies: Transferability of Adversarial Images across Prompts on Vision-Language Models",
        "authors": "Haochen Luo;Jindong Gu;Fengyuan Liu;Philip Torr",
        "site": "https://iclr.cc/virtual/2024/poster/17850",
        "summary": "Different from traditional task-specific vision models, recent large VLMs can readily adapt to different vision tasks by simply using different textual instructions, i.e., prompts. However, a well-known concern about traditional task-specific vision models is that they can be misled by imperceptible adversarial perturbations. Furthermore, the concern is exacerbated by the phenomenon that the same adversarial perturbations can fool different task-specific models. Given that VLMs rely on prompts to adapt to different tasks, an intriguing question emerges: Can a single adversarial image mislead all predictions of VLMs when a thousand different prompts are given? This question essentially introduces a novel perspective on adversarial transferability: cross-prompt adversarial transferability. In this work, we propose the Cross-Prompt Attack (CroPA). This proposed method updates the visual adversarial perturbation with learnable textual prompts, which are designed to counteract the misleading effects of the adversarial image. By doing this, CroPA significantly improves the transferability of adversarial examples across prompts. Extensive experiments are conducted to verify the strong cross-prompt adversarial transferability of CroPA with prevalent VLMs including Flamingo, BLIP-2, and InstructBLIP in various different tasks.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17850",
        "keywords": [
            "transferability",
            "cross prompt attack"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, a well-known concern about traditional task-specific vision models is that they can be misled by imperceptible adversarial perturbations. Furthermore, the concern is exacerbated by the phenomenon that the same adversarial perturbations can fool different task-specific models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, a well-known concern about traditional task-specific vision models is that they can be misled by imperceptible adversarial perturbations. Furthermore, the concern is exacerbated by the phenomenon that the same adversarial perturbations can fool different task-specific models.\""
    },
    {
        "title": "Pre-Training Goal-based Models for Sample-Efficient Reinforcement Learning",
        "authors": "Haoqi Yuan;Zhancun Mu;Feiyang Xie;Zongqing Lu",
        "site": "https://iclr.cc/virtual/2024/poster/17838",
        "summary": "Pre-training on task-agnostic large datasets is a promising approach for enhancing the sample efficiency of reinforcement learning (RL) in solving complex tasks. We present PTGM, a novel method that pre-trains goal-based models to augment RL by providing temporal abstractions and behavior regularization. PTGM involves pre-training a low-level, goal-conditioned policy and training a high-level policy to generate goals for subsequent RL tasks. To address the challenges posed by the high-dimensional goal space, while simultaneously maintaining the agent's capability to accomplish various skills, we propose clustering goals in the dataset to form a discrete high-level action space. Additionally, we introduce a pre-trained goal prior model to regularize the behavior of the high-level policy in RL, enhancing sample efficiency and learning stability. Experimental results in a robotic simulation environment and the challenging open-world environment of Minecraft demonstrate PTGM’s superiority in sample efficiency and task performance compared to baselines. Moreover, PTGM exemplifies enhanced interpretability and generalization of the acquired low-level skills.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17838",
        "keywords": [
            "reinforcement learning",
            "sample"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis",
        "authors": "Kensen Shi;Joey Hong;Yinlin Deng;Pengcheng Yin;Manzil Zaheer;Charles Sutton",
        "site": "https://iclr.cc/virtual/2024/poster/17817",
        "summary": "When writing programs, people have the ability to tackle a new complex task by decomposing it into smaller and more familiar subtasks. While it is difficult to measure whether neural program synthesis methods have similar capabilities, we can measure whether they compositionally generalize, that is, whether a model that has been trained on the simpler subtasks is subsequently able to solve more complex tasks. In this paper, we characterize several different forms of compositional generalization that are desirable in program synthesis, forming a meta-benchmark which we use to create generalization tasks for two popular datasets, RobustFill and DeepCoder. We then propose ExeDec, a novel decomposition-based synthesis strategy that predicts execution subgoals to solve problems step-by-step informed by program execution at each step. When used with Transformer models trained from scratch, ExeDec has better synthesis performance and greatly improved compositional generalization ability compared to baselines. Finally, we use our benchmarks to demonstrate that LLMs struggle to compositionally generalize when asked to do programming-by-example in a few-shot setting, but an ExeDec-style prompting approach can improve the generalization ability and overall performance.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17817",
        "keywords": [
            "program synthesis",
            "compositional generalization",
            "generalization",
            "neural program synthesis",
            "decomposition"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"we use our benchmarks to demonstrate that LLMs struggle to compositionally generalize when asked to do programming-by-example in a few-shot setting\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"we use our benchmarks to demonstrate that LLMs struggle to compositionally generalize when asked to do programming-by-example in a few-shot setting\""
    },
    {
        "title": "ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models",
        "authors": "Seyed Iman Mirzadeh;Keivan Alizadeh-Vahid;Sachin Mehta;Carlo C del Mundo;Oncel Tuzel;Golnoosh Samei;Mohammad Rastegari;Mehrdad Farajtabar",
        "site": "https://iclr.cc/virtual/2024/poster/17805",
        "summary": "Large Language Models (LLMs) with billions of parameters have drastically transformed AI applications. However, their demanding computation during inference has raised significant challenges for deployment on resource-constrained devices. Despite recent trends favoring alternative activation functions such as GELU or SiLU, known for increased computation, this study strongly advocates for reinstating ReLU activation in LLMs. We demonstrate that using the ReLU activation function has a negligible impact on convergence and performance while significantly reducing computation and weight transfer. This reduction is particularly valuable during the memory-bound inference step, where efficiency is paramount. Exploring sparsity patterns in ReLU-based LLMs, we unveil the reutilization of activated neurons for generating new tokens and leveraging these insights, we propose practical strategies to substantially reduce LLM inference computation up to three times, using ReLU activations with minimal performance trade-offs.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17805",
        "keywords": [
            "sparsity",
            "activation sparsity",
            "language models",
            "large language models",
            "inference"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, their demanding computation during inference has raised significant challenges for deployment on resource-constrained devices.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, their demanding computation during inference has raised significant challenges for deployment on resource-constrained devices.\""
    },
    {
        "title": "Robust agents learn causal world models",
        "authors": "Jonathan Richens;Tom Everitt",
        "site": "https://iclr.cc/virtual/2024/poster/17779",
        "summary": "It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound for a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17779",
        "keywords": [
            "causal reasoning",
            "causal inference",
            "causal models"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models",
        "authors": "Erfan Shayegani;Yue Dong;Nael Abu-Ghazaleh",
        "site": "https://iclr.cc/virtual/2024/poster/17767",
        "summary": "We introduce new jailbreak attacks on vision language models (VLMs), which use aligned LLMs and are resilient to text-only jailbreak attacks. Specifically, we develop cross-modality attacks on alignment where we pair adversarial images going through the vision encoder with textual prompts to break the alignment of the language model. Our attacks employ a novel compositional strategy that combines an image, adversarially targeted towards toxic embeddings, with generic prompts to accomplish the jailbreak. Thus, the LLM draws the context to answer the generic prompt from the adversarial image. The generation of benign-appearing adversarial images leverages a novel embedding-space-based methodology, operating with no access to the LLM model. Instead, the attacks require access only to the vision encoder and utilize one of our four embedding space targeting strategies. By not requiring access to the LLM, the attacks lower the entry barrier for attackers, particularly when vision encoders such as CLIP are embedded in closed-source LLMs. The attacks achieve a high success rate across different VLMs, highlighting the risk of cross-modality alignment vulnerabilities, and the need for new alignment approaches for multi-modal models.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17767",
        "keywords": [
            "jailbreak",
            "compositional",
            "language model",
            "multi modal language models",
            "cross modality alignment",
            "cross modality attacks",
            "vision language models",
            "adversarial attacks"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"highlighting the risk of cross-modality alignment vulnerabilities, and the need for new alignment approaches for multi-modal models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"highlighting the risk of cross-modality alignment vulnerabilities, and the need for new alignment approaches for multi-modal models.\""
    },
    {
        "title": "Grounding Language Plans in Demonstrations Through Counterfactual Perturbations",
        "authors": "Yanwei Wang;Tsun-Hsuan Wang;Jiayuan Mao;Michael Hagenow;Julie Shah",
        "site": "https://iclr.cc/virtual/2024/poster/17715",
        "summary": "Grounding the common-sense reasoning of Large Language Models in physical domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior works have focused on leveraging LLMs directly for planning in symbolic spaces, this work uses LLMs to guide the search of task structures and constraints implicit in multi-step demonstrations. Specifically, we borrow from manipulation planning literature the concept of mode families, which group robot configurations by specific motion constraints, to serve as an abstraction layer between the high-level language representations of an LLM and the low-level physical trajectories of a robot. By replaying a few human demonstrations with synthetic perturbations, we generate coverage over the demonstrations' state space with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains an end-to-end differentiable neural network to predict successful trajectories from failures and as a by-product learns classifiers that ground low-level states and images in mode families without dense labeling. The learned grounding classifiers can further be used to translate language plans into reactive policies in the physical domain in an interpretable manner. We show our approach improves the interpretability and reactivity of imitation learning through 2D navigation and simulated and real robot manipulation tasks. Website: https://yanweiw.github.io/glide/",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17715",
        "keywords": [
            "demonstrations",
            "language plans",
            "language",
            "trajectories",
            "perturbations",
            "imitation",
            "manipulation planning",
            "mode families",
            "counterfactual perturbations",
            "constraints"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Grounding the common-sense reasoning of Large Language Models in physical domains remains a pivotal yet unsolved problem for embodied AI.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Grounding the common-sense reasoning of Large Language Models in physical domains remains a pivotal yet unsolved problem for embodied AI.\""
    },
    {
        "title": "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation",
        "authors": "Yangsibo Huang;Samyak Gupta;Mengzhou Xia;Kai Li;Danqi Chen",
        "site": "https://iclr.cc/virtual/2024/poster/17706",
        "summary": "The rapid progress in open-source large language models (LLMs) is significantly advancing AI development. Extensive efforts have been made before model release to align their behavior with human values, with the primary goal of ensuring their helpfulness and harmlessness. However, even carefully aligned models can be manipulated maliciously, leading to unintended behaviors, known as ``jailbreaks\". These jailbreaks are typically triggered by specific text inputs, often referred to as adversarial prompts. In this work, we propose the generation exploitation attack, an extremely simple approach that disrupts model alignment by only manipulating variations of decoding methods. By exploiting different generation strategies, including varying decoding hyper-parameters and sampling methods, we increase the attack success rate from $0\\%$ to more than $95\\%$ across 11 language models including LLaMA2, Vicuna, Falcon, and MPT families, outperforming state-of-the-art attacks with $30\\times$ lower computational cost. Finally, we propose an effective alignment method that explores diverse generation strategies, which can reasonably reduce the attack success rate under our attack. Altogether, our study underscores a major failure in current safety evaluation and alignment procedures for open-source LLMs, strongly advocating for more comprehensive red teaming and better alignment before releasing such models.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17706",
        "keywords": [
            "jailbreaks",
            "generation",
            "generation exploitation attack",
            "sampling methods",
            "language models",
            "llms",
            "alignment"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, even carefully aligned models can be manipulated maliciously, leading to unintended behaviors, known as ``jailbreaks\"\"; \"Altogether, our study underscores a major failure in current safety evaluation and alignment procedures for open-source LLMs, strongly advocating for more comprehensive red teaming and better alignment before releasing such models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, even carefully aligned models can be manipulated maliciously, leading to unintended behaviors, known as ``jailbreaks\"\"; \"Altogether, our study underscores a major failure in current safety evaluation and alignment procedures for open-source LLMs, strongly advocating for more comprehensive red teaming and better alignment before releasing such models.\""
    },
    {
        "title": "SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore",
        "authors": "Sewon Min;Suchin Gururangan;Eric Wallace;Weijia Shi;Hannaneh Hajishirzi;Noah A. Smith;Luke Zettlemoyer",
        "site": "https://iclr.cc/virtual/2024/poster/17675",
        "summary": "The legality of training language models (LMs) on copyrighted or otherwise restricted data is under intense debate. However, as we show, model performance significantly degrades if trained only on low-risk text (e.g., out-of-copyright books or government documents), due to its limited size and domain coverage. We present SILO, a new language model that manages this risk-performance tradeoff during inference. SILO is built by (1) training a parametric LM on the Open License Corpus (OLC), a new corpus we curate with 228B tokens of public domain and permissively licensed text and (2) augmenting it with a more general and easily modifiable nonparametric datastore (e.g., containing copyrighted books or news) that is only queried during inference. The datastore allows use of high-risk data without training on it, supports sentence-level data attribution, and enables data producers to opt out from the model by removing content from the store. These capabilities can foster compliance with data-use regulations such as the fair use doctrine in the United States and the GDPR in the European Union. Our experiments show that the parametric LM struggles on its own with domains not covered by OLC. However, access to the datastore greatly improves out of domain performance, closing 90% of the performance gap with an LM trained on the Pile, a more diverse corpus with mostly high-risk text. We also analyze which nonparametric approach works best, where the remaining errors lie, and how performance scales with datastore size. Our results suggest that it is possible to build high quality language models while mitigating legal risk.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17675",
        "keywords": [
            "silo",
            "silo language"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, as we show, model performance significantly degrades if trained only on low-risk text (e.g., out-of-copyright books or government documents), due to its limited size and domain coverage.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, as we show, model performance significantly degrades if trained only on low-risk text (e.g., out-of-copyright books or government documents), due to its limited size and domain coverage.\""
    },
    {
        "title": "SocioDojo: Building Lifelong Analytical Agents with Real-world Text and Time Series",
        "authors": "Junyan Cheng;Peter Chin",
        "site": "https://iclr.cc/virtual/2024/poster/17662",
        "summary": "We introduce SocioDojo, an open-ended lifelong learning environment for developing ready-to-deploy autonomous agents capable of performing human-like analysis and decision-making on societal topics such as economics, finance, politics, and culture. It consists of (1) information sources from news, social media, reports, etc., (2) a knowledge base built from books, journals, and encyclopedias, plus a toolbox of Internet and knowledge graph search interfaces, (3) 30K high-quality time series in finance, economy, society, and polls, which support a novel task called \"hyperportfolio\", that can reliably and scalably evaluate societal analysis and decision-making power of agents, inspired by portfolio optimization with time series as assets to \"invest\". We also propose a novel Analyst-Assistant-Actuator architecture for the hyperportfolio task, and a Hypothesis & Proof prompting for producing in-depth analyses on input news, articles, etc. to assist decision-making. We perform experiments and ablation studies to explore the factors that impact performance. The results show that our proposed method achieves improvements of 32.4% and 30.4% compared to the state-of-the-art method in the two experimental settings.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17662",
        "keywords": [
            "decision making",
            "hyperportfolio",
            "analytical agents",
            "finance",
            "media"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "DSPy: Compiling Declarative Language Model Calls into State-of-the-Art Pipelines",
        "authors": "Omar Khattab;Arnav Singhvi;Paridhi Maheshwari;Zhiyuan Zhang;Keshav Santhanam;Sri Vardhamanan A;Saiful Haq;Ashutosh Sharma;Thomas T. Joshi;Hanna Moazam;Heather Miller;Matei Zaharia;Christopher Potts",
        "site": "https://iclr.cc/virtual/2024/poster/17642",
        "summary": "The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks. Unfortunately, existing LM pipelines are typically implemented using hard-coded “prompt templates”, i.e. lengthy strings discovered via trial and error. Toward a more systematic approach for developing and optimizing LM pipelines, we introduce DSPy, a programming model that abstracts LM pipelines as text transformation graphs, or imperative computational graphs where LMs are invoked through declarative modules. DSPy modules are parameterized, meaning they can learn how to apply compositions of prompting, finetuning, augmentation, and reasoning techniques. We design a compiler that will optimize any DSPy pipeline to maximize a given metric, by creating and collecting demonstrations. We conduct two case studies, showing that succinct DSPy programs can express and optimize pipelines that reason about math word problems, tackle multi-hop retrieval, answer complex questions, and control agent loops. Within minutes of compiling, DSPy can automatically produce pipelines that outperform out-of-the-box few-shot prompting as well as expert-created demonstrations for GPT-3.5 and Llama2-13b-chat. On top of that, DSPy programs compiled for relatively small LMs like 770M parameter T5 and Llama2-13b-chat are competitive with many approaches that rely on large and proprietary LMs like GPT-3.5 and on expert-written prompt chains. DSPy is available at https://github.com/stanfordnlp/dspy",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17642",
        "keywords": [
            "language models",
            "declarative language model",
            "lm",
            "declarative",
            "lm pipelines",
            "pipeline",
            "programming model",
            "optimize pipelines"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Unfortunately, existing LM pipelines are typically implemented using hard-coded “prompt templates”, i.e. lengthy strings discovered via trial and error.\"\n\nThis paper discusses LLMs but does not explicitly mention any limitations of the models themselves. The limitation mentioned is related to the implementation of LM pipelines, not the LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Unfortunately, existing LM pipelines are typically implemented using hard-coded “prompt templates”, i.e. lengthy strings discovered via trial and error.\"\n\nThis paper discusses LLMs but does not explicitly mention any limitations of the models themselves. The limitation mentioned is related to the implementation of LM pipelines, not the LLMs."
    },
    {
        "title": "Large Language Models Are Not Robust Multiple Choice Selectors",
        "authors": "Chujie Zheng;Hao Zhou;Fandong Meng;Jie Zhou;Minlie Huang",
        "site": "https://iclr.cc/virtual/2024/poster/17638",
        "summary": "Multiple choice questions (MCQs) serve as a common yet important task format in the evaluation of large language models (LLMs). This work shows that modern LLMs are vulnerable to option position changes in MCQs due to their inherent “selection bias”, namely, they prefer to select specific option IDs as answers (like “Option A”). Through extensive empirical analyses with 20 LLMs on three benchmarks, we pinpoint that this behavioral bias primarily stems from LLMs’ token bias, where the model a priori assigns more probabilistic mass to specific option ID tokens (e.g., A/B/C/D) when predicting answers from the option IDs. To mitigate selection bias, we propose a label-free, inference-time debiasing method, called PriDe, which separates the model’s prior bias for option IDs from the overall prediction distribution. PriDe first estimates the prior by permutating option contents on a small number of test samples, and then applies the estimated prior to debias the remaining samples. We demonstrate that it achieves interpretable and transferable debiasing with high computational efficiency. We hope this work can draw broader research attention to the bias and robustness of modern LLMs.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17638",
        "keywords": [
            "debiasing",
            "bias",
            "mcqs",
            "selection bias",
            "multiple choice selectors",
            "behavioral bias",
            "large language models",
            "robustness",
            "option",
            "multiple choice questions",
            "token bias",
            "benchmarks"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This work shows that modern LLMs are vulnerable to option position changes in MCQs due to their inherent “selection bias”, namely, they prefer to select specific option IDs as answers (like “Option A”). Through extensive empirical analyses with 20 LLMs on three benchmarks, we pinpoint that this behavioral bias primarily stems from LLMs’ token bias, where the model",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"This work shows that modern LLMs are vulnerable to option position changes in MCQs due to their inherent “selection bias”, namely, they prefer to select specific option IDs as answers (like “Option A”). Through extensive empirical analyses with 20 LLMs on three benchmarks, we pinpoint that this behavioral bias primarily stems from LLMs’ token bias, where the model"
    },
    {
        "title": "Frozen Transformers in Language Models Are Effective Visual Encoder Layers",
        "authors": "Ziqi Pang;Ziyang Xie;Yunze Man;Yu-Xiong Wang",
        "site": "https://iclr.cc/virtual/2024/poster/17627",
        "summary": "This paper reveals that large language models (LLMs), despite being trained solely on text data, are surprisingly}strong encoders for purely visual tasks in the absence of language. Even more intriguingly, this can be achieved by a simple yet previously overlooked strategy -- employing a frozen transformer block from pre-trained LLMs as a constituent encoder layer to directly process visual tokens. Our work pushes the boundaries of leveraging LLMs for computer vision tasks, significantly departing from conventional practices that typically necessitate a multi-modal vision-language setup with associated language prompts, inputs, or outputs. We demonstrate that our approach consistently enhances performance across a diverse range of tasks} encompassing pure 2D or 3D visual recognition tasks (e.g., image and point cloud classification), temporal modeling tasks (e.g., action recognition), non-semantic tasks (e.g., motion forecasting), and multi-modal tasks (e.g., 2D/3D visual question answering and image-text retrieval). Such improvements are a general phenomenon, applicable to various types of LLMs (e.g., LLaMA and OPT) and different LLM transformer blocks. We additionally propose the information filtering hypothesis to explain the effectiveness of pre-trained LLMs in visual encoding -- the pre-trained LLM transformer blocks discern informative visual tokens and further amplify their effect. This hypothesis is empirically supported by the observation that the feature activation, after training with LLM transformer blocks, exhibits a stronger focus on relevant regions. We hope that our work inspires new perspectives on utilizing LLMs and deepening our understanding of their underlying mechanisms.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17627",
        "keywords": [
            "action recognition",
            "encoders",
            "language",
            "language models",
            "frozen transformers",
            "large language models",
            "visual encoder"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the title and abstract do not mention any limitations of LLMs, they only discuss the effectiveness of using frozen transformer blocks from pre-trained LLMs as visual encoders.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the title and abstract do not mention any limitations of LLMs, they only discuss the effectiveness of using frozen transformer blocks from pre-trained LLMs as visual encoders."
    },
    {
        "title": "Text2Reward: Reward Shaping with Language Models for Reinforcement Learning",
        "authors": "Tianbao Xie;Siheng Zhao;Chen Henry Wu;Yitao Liu;Qian Luo;Victor Zhong;Yanchao Yang;Tao Yu",
        "site": "https://iclr.cc/virtual/2024/poster/17616",
        "summary": "Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation and shaping of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates shaped dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes or unshaped dense rewards with a constant function across timesteps, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better task success rates and convergence speed than expert-written reward codes. For locomotion tasks, our method learns six novel locomotion behaviors with a success rate exceeding 94%. Furthermore, we show that the policies trained in the simulator with our method can be deployed in the real world. Finally, Text2Reward further improves the policies by refining their reward functions with human feedback. Video results are available at https://text-to-reward.github.io/",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17616",
        "keywords": [
            "reinforcement learning",
            "language models",
            "text2reward",
            "reward"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Unlike inverse RL and recent work that uses LLMs to write sparse reward codes or unshaped dense rewards with a constant function across timesteps, Text2Reward produces interpretable, free-form dense reward codes...\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Unlike inverse RL and recent work that uses LLMs to write sparse reward codes or unshaped dense rewards with a constant function across timesteps, Text2Reward produces interpretable, free-form dense reward codes...\""
    },
    {
        "title": "A Benchmark for Learning to Translate a New Language from One Grammar Book",
        "authors": "Garrett Tanzer;Mirac Suzgun;Eline Visser;Dan Jurafsky;Luke Melas-Kyriazi",
        "site": "https://iclr.cc/virtual/2024/poster/17609",
        "summary": "Large language models (LLMs) can perform impressive feats with in-context learning or lightweight finetuning. It is natural to wonder how well these models adapt to genuinely new tasks, but how does one find tasks that are unseen in internet-scale training sets? We turn to a field that is explicitly motivated and bottlenecked by a scarcity of web data: low-resource languages. In this paper, we introduce MTOB (Machine Translation from One Book), a benchmark for learning to translate between English and Kalamang—a language with less than 200 speakers and therefore virtually no presence on the web—using several hundred pages of field linguistics reference materials. This task framing is novel in that it asks a model to learn a language from a single human-readable book of grammar explanations, rather than a large mined corpus of in-domain data, more akin to L2 language learning than L1 language acquisition. We demonstrate that baselines using current LLMs are promising but fall short of human performance, achieving 44.7 chrF on Kalamang to English translation and 45.8 chrF on English to Kalamang translation, compared to 51.6 and 57.0 chrF by a human who learned Kalamang from the same reference materials. We hope that MTOB will help measure LLM capabilities along a new dimension, and that the methods developed to solve it could help expand access to language technology for underserved communities by leveraging qualitatively different kinds of data than traditional machine translation.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17609",
        "keywords": [
            "machine translation",
            "grammar book",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We demonstrate that baselines using current LLMs are promising but fall short of human performance, achieving 44.7 chrF on Kalamang to English translation and 45.8 chrF on English to Kalamang translation, compared to 51.6 and 57.0 chrF by a human who learned Kalamang from the same reference materials.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We demonstrate that baselines using current LLMs are promising but fall short of human performance, achieving 44.7 chrF on Kalamang to English translation and 45.8 chrF on English to Kalamang translation, compared to 51.6 and 57.0 chrF by a human who learned Kalamang from the same reference materials.\""
    },
    {
        "title": "Maximum Entropy Heterogeneous-Agent Reinforcement Learning",
        "authors": "Jiarong Liu;Yifan Zhong;Siyi Hu;Haobo Fu;QIANG FU;Xiaojun Chang;Yaodong Yang",
        "site": "https://iclr.cc/virtual/2024/poster/17603",
        "summary": "Multi-agent reinforcement learning(MARL) has been shown effective for cooperative games in recent years. However, existing state-of-the-art methods face challenges related to sample complexity, training instability, and the risk of converging to a suboptimal Nash Equilibrium. In this paper, we propose a unified framework for learning \\emph{stochastic} policies to resolve these issues. We embed cooperative MARL problems into probabilistic graphical models, from which we derive the maximum entropy (MaxEnt) objective for MARL. Based on the MaxEnt framework, we proposeHeterogeneous-Agent Soft Actor-Critic(HASAC) algorithm. Theoretically, we prove the monotonic improvement and convergence toquantal response equilibrium(QRE) properties of HASAC. Furthermore, we generalize a unified template for MaxEnt algorithmic design namedMaximum Entropy Heterogeneous-Agent Mirror Learning(MEHAML), which provides any induced method with the same guarantees as HASAC. We evaluate HASAC on six benchmarks: Bi-DexHands, Multi-Agent MuJoCo, StarCraft Multi-Agent Challenge, Google Research Football, Multi-Agent Particle Environment, and Light Aircraft Game. Results show that HASAC consistently outperforms strong baselines, exhibiting better sample efficiency, robustness, and sufficient exploration.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17603",
        "keywords": [
            "agent soft actor critic",
            "multi agent reinforcement learning",
            "maximum entropy",
            "mirror learning",
            "entropy heterogeneous",
            "cooperative games",
            "generalize"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Entity-Centric Reinforcement Learning for Object Manipulation from Pixels",
        "authors": "Dan Haramati;Tal Daniel;Aviv Tamar",
        "site": "https://iclr.cc/virtual/2024/poster/17585",
        "summary": "Manipulating objects is a hallmark of human intelligence, and an important task in domains such as robotics. In principle, Reinforcement Learning (RL) offers a general approach to learn object manipulation. In practice, however, domains with more than a few objects are difficult for RL agents due to the curse of dimensionality, especially when learning from raw image observations. In this work we propose a structured approach for visual RL that is suitable for representing multiple objects and their interaction, and use it to learn goal-conditioned manipulation of several objects. Key to our method is the ability to handle goals with dependencies between the objects (e.g., moving objects in a certain order). We further relate our architecture to the generalization capability of the trained agent, based on a theoretical result for compositional generalization, and demonstrate agents that learn with 3 objects but generalize to similar tasks with over 10 objects. Videos and code are available on the project website: https://sites.google.com/view/entity-centric-rl",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17585",
        "keywords": [
            "reinforcement learning",
            "object manipulation"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
        "authors": "Suyu Ge;Yunan Zhang;Liyuan Liu;Minjia Zhang;Jiawei Han;Jianfeng Gao",
        "site": "https://iclr.cc/virtual/2024/poster/17575",
        "summary": "In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss. We will release our code and the compatible CUDA kernel for reproducibility.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17575",
        "keywords": [
            "language models",
            "cache compression",
            "kv cache compression",
            "cuda kernel",
            "attention"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"reduces the memory footprint of generative inference for Large Language Models (LLMs)\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"reduces the memory footprint of generative inference for Large Language Models (LLMs)\""
    },
    {
        "title": "Unbiased Watermark for Large Language Models",
        "authors": "Zhengmian Hu;Lichang Chen;Xidong Wu;Yihan Wu;Hongyang Zhang;Heng Huang",
        "site": "https://iclr.cc/virtual/2024/poster/17572",
        "summary": "The recent advancements in large language models (LLMs) have sparked a growing apprehension regarding the potential misuse. One approach to mitigating this risk is to incorporate watermarking techniques into LLMs, allowing for the tracking and attribution of model outputs. This study examines a crucial aspect of watermarking: how significantly watermarks impact the quality of model-generated outputs. Previous studies have suggested a trade-off between watermark strength and output quality. However, our research demonstrates that it is possible to integrate watermarks without affecting the output probability distribution with appropriate implementation. We refer to this type of watermark as an unbiased watermark. This has significant implications for the use of LLMs, as it becomes impossible for users to discern whether a service provider has incorporated watermarks or not. Furthermore, the presence of watermarks does not compromise the performance of the model in downstream tasks, ensuring that the overall utility of the language model is preserved. Our findings contribute to the ongoing discussion around responsible AI development, suggesting that unbiased watermarks can serve as an effective means of tracking and attributing model outputs without sacrificing output quality.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17572",
        "keywords": [
            "watermarking",
            "language model",
            "large language models",
            "unbiased watermark"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Previous studies have suggested a trade-off between watermark strength and output quality.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Previous studies have suggested a trade-off between watermark strength and output quality.\""
    },
    {
        "title": "GROOT: Learning to Follow Instructions by Watching Gameplay Videos",
        "authors": "Shaofei Cai;Bowei Zhang;Zihao Wang;Xiaojian Ma;Anji Liu;Yitao Liang",
        "site": "https://iclr.cc/virtual/2024/poster/17564",
        "summary": "We study the problem of building a controller that can follow open-ended instructions in open-world environments. We propose to follow reference videos as instructions, which offer expressive goal specifications while eliminating the need for expensive text-gameplay annotations. A new learning framework is derived to allow learning such instruction-following controllers from gameplay videos while producing a video instruction encoder that induces a structured goal space. We implement our agent GROOT in a simple yet effective encoder-decoder architecture based on causal transformers. We evaluate GROOT against open-world counterparts and human players on a proposed Minecraft SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the human-machine gap as well as exhibiting a 70% winning rate over the best generalist agent baseline. Qualitative analysis of the induced goal space further demonstrates some interesting emergent properties, including the goal composition and complex gameplay behavior synthesis.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17564",
        "keywords": [
            "reference videos",
            "goal composition",
            "gameplay videos",
            "goal space",
            "video instruction encoder"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?",
        "authors": "Jingfeng Wu;Difan Zou;Zixiang Chen;Vladimir Braverman;Quanquan Gu;Peter Bartlett",
        "site": "https://iclr.cc/virtual/2024/poster/17535",
        "summary": "Transformers pretrained on diverse tasks exhibit remarkable in-context learning (ICL) capabilities, enabling them to solve unseen tasks solely based on input contexts without adjusting model parameters. In this paper, we study ICL in one of its simplest setups: pretraining a single-layer linear attention model for linear regression with a Gaussian prior. We establish a statistical task complexity bound for the attention model pretraining, showing that effective pretraining only requires a small number of independent tasks. Furthermore, we prove that the pretrained model closely matches the Bayes optimal algorithm, i.e., optimally tuned ridge regression, by achieving nearly Bayes optimal risk on unseen tasks under a fixed context length. These theoretical findings complement prior experimental research and shed light on the statistical foundations of ICL.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17535",
        "keywords": [
            "pretraining",
            "pretraining tasks",
            "ridge regression",
            "linear regression",
            "context learning",
            "in context learning",
            "transformers"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Towards LLM4QPE: Unsupervised Pretraining of Quantum Property Estimation and A Benchmark",
        "authors": "Yehui Tang;Hao Xiong;Nianzu Yang;Tailong Xiao;Junchi Yan",
        "site": "https://iclr.cc/virtual/2024/poster/17517",
        "summary": "Estimating the properties of quantum systems such as quantum phase has been critical in addressing the essential quantum many-body problems in physics and chemistry. Deep learning models have been recently introduced to property estimation, surpassing  conventional statistical approaches. However, these methods are tailored to the specific task and quantum data at hand. It remains an open and attractive question for devising a more universal task-agnostic pretraining model for quantum property estimation. In this paper, we propose LLM4QPE, a large language model style quantum task-agnostic pretraining and finetuning paradigm that 1) performs unsupervised pretraining on diverse quantum systems with different physical conditions; 2) uses the pretrained model for supervised finetuning and delivers high performance with limited training data, on downstream tasks. It mitigates the cost for quantum data collection and speeds up convergence. Extensive experiments show the promising efficacy of LLM4QPE in various tasks including classifying quantum phases of matter on Rydberg atom model and predicting two-body correlation function on anisotropic Heisenberg model.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17517",
        "keywords": [
            "property estimation",
            "pretraining",
            "quantum property estimation",
            "llm4qpe",
            "pretraining model",
            "benchmark"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs, but mentions \"it remains an open and attractive question for devising a more universal task-agnostic pretraining model for quantum property estimation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No evidence of discussion of limitations of LLMs, but mentions \"it remains an open and attractive question for devising a more universal task-agnostic pretraining model for quantum property estimation.\""
    },
    {
        "title": "Batched Low-Rank Adaptation of Foundation Models",
        "authors": "Yeming Wen;Swarat Chaudhuri",
        "site": "https://iclr.cc/virtual/2024/poster/17508",
        "summary": "Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning foundation models by incorporating trainable low-rank matrices, thereby reducing the number of trainable parameters. While \\lora/ offers numerous advantages, its applicability for real-time serving to a diverse and global user base is constrained by its incapability to handle multiple task-specific adapters efficiently. This imposes a performance bottleneck in scenarios requiring personalized, task-specific adaptations for each incoming request.To address this, we introduce FLoRA (Fast LoRA), a framework in which each input example in a minibatch can be associated with its unique low-rank adaptation weights, allowing for efficient batching of heterogeneous requests. We empirically demonstrate that \\flora/ retains the performance merits of \\lora/, showcasing competitive results on the MultiPL-E code generation benchmark spanning over 8 languages and a multilingual speech recognition task across 6 languages.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17508",
        "keywords": [
            "low rank adaptation",
            "speech recognition"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"its incapability to handle multiple task-specific adapters efficiently. This imposes a performance bottleneck in scenarios requiring personalized, task-specific adaptations for each incoming request.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"its incapability to handle multiple task-specific adapters efficiently. This imposes a performance bottleneck in scenarios requiring personalized, task-specific adaptations for each incoming request.\""
    },
    {
        "title": "Linearity of Relation Decoding in Transformer Language Models",
        "authors": "Evan Hernandez;Arnab Sen Sharma;Tal Haklay;Kevin Meng;Martin Wattenberg;Jacob Andreas;Yonatan Belinkov;David Bau",
        "site": "https://iclr.cc/virtual/2024/poster/17504",
        "summary": "Much of the knowledge encoded in transformer language models (LMs) may be expressed in terms of relations: relations between words and their synonyms, entities and their attributes, etc. We show that, for a subset of relations, this computation is well-approximated by a single linear transformation on the subject representation. Linear relation representations may be obtained by constructing a first-order approximation to the LM from a single prompt, and they exist for a variety of factual, commonsense, and linguistic relations. However, we also identify many cases in which LM predictions capture relational knowledge accurately, but this knowledge is not linearly encoded in their representations. Our results thus reveal a simple, interpretable, but heterogeneously deployed knowledge representation strategy in transformer LMs.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17504",
        "keywords": [
            "transformer language models",
            "knowledge representation",
            "relation decoding",
            "linear relation representations"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, we also identify many cases in which LM predictions capture relational knowledge accurately, but this knowledge is not linearly encoded in their representations.\"\n\nThis rating is given because the paper discusses a limitation of LLMs in terms of their knowledge representation strategy, specifically that relational knowledge is not always linearly encoded in their representations. This limitation is mentioned as part of the paper's main findings",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, we also identify many cases in which LM predictions capture relational knowledge accurately, but this knowledge is not linearly encoded in their representations.\"\n\nThis rating is given because the paper discusses a limitation of LLMs in terms of their knowledge representation strategy, specifically that relational knowledge is not always linearly encoded in their representations. This limitation is mentioned as part of the paper's main findings"
    },
    {
        "title": "Negative Label Guided OOD Detection with Pretrained Vision-Language Models",
        "authors": "Xue Jiang;Feng Liu;Zhen Fang;Hong Chen;Tongliang Liu;Feng Zheng;Bo Han",
        "site": "https://iclr.cc/virtual/2024/poster/17453",
        "summary": "Out-of-distribution (OOD) detection aims at identifying samples from unknown classes, playing a crucial role in trustworthy models against errors on unexpected inputs.  Extensive research has been dedicated to exploring OOD detection in the vision modality. {Vision-language models (VLMs) can leverage both textual and visual information for various multi-modal applications, whereas few OOD detection methods take into account information from the text modality. In this paper, we propose a novel post hoc OOD detection method, called NegLabel, which takes a vast number of negative labels from extensive corpus databases. We design a novel scheme for the OOD score collaborated with negative labels.Theoretical analysis helps to understand the mechanism of negative labels. Extensive experiments demonstrate that our method NegLabel achieves state-of-the-art performance on various OOD detection benchmarks and generalizes well on multiple VLM architectures. Furthermore, our method NegLabel exhibits remarkable robustness against diverse domain shifts. The codes are available at https://github.com/tmlr-group/NegLabel.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17453",
        "keywords": [
            "vision language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper discusses the limitations of OOD detection methods in general and the potential benefits of using Vision-language models (VLMs) to leverage both textual and visual information for OOD detection, which implies that current VLMs might not be effective in handling OOD detection tasks.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None, but the paper discusses the limitations of OOD detection methods in general and the potential benefits of using Vision-language models (VLMs) to leverage both textual and visual information for OOD detection, which implies that current VLMs might not be effective in handling OOD detection tasks."
    },
    {
        "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation",
        "authors": "Runpei Dong;Chunrui Han;Yuang Peng;Zekun Qi;Zheng Ge;Jinrong Yang;Liang Zhao;Jianjian Sun;Hongyu Zhou;Haoran Wei;Xiangwen Kong;Xiangyu Zhang;Kaisheng Ma;Li Yi",
        "site": "https://iclr.cc/virtual/2024/poster/17430",
        "summary": "This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17430",
        "keywords": [
            "multimodal comprehension",
            "synergistic multimodal comprehension",
            "generative modeling"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained.\"\n\nNote that the limitation is mentioned briefly, but not explored in depth, and the primary focus of the paper is on the proposed solution, DreamLLM.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained.\"\n\nNote that the limitation is mentioned briefly, but not explored in depth, and the primary focus of the paper is on the proposed solution, DreamLLM."
    },
    {
        "title": "MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning",
        "authors": "Xiang Yue;Xingwei Qu;Ge Zhang;Yao Fu;Wenhao Huang;Huan Sun;Yu Su;Wenhu Chen",
        "site": "https://iclr.cc/virtual/2024/poster/17422",
        "summary": "We introduce MAmmoTH, a series of open-source large language models (LLMs) specifically tailored for general math problem-solving. The MAmmoTH models are trained on MathInstruct, our meticulously curated instruction tuning dataset. MathInstruct is compiled from 13 math datasets with intermediate rationales, six of which have rationales newly curated by us. It presents a unique hybrid of chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also ensures extensive coverage of diverse fields in math. The hybrid of CoT and PoT not only unleashes the potential of tool use but also allows different thought processes for different math problems. As a result, the MAmmoTH series substantially outperform existing open-source models on nine mathematical reasoning datasets across all scales with an average accuracy gain between 16% and 32%. Remarkably, our MAmmoTH-7B model reaches 33% on MATH (a competition-level dataset), which exceeds the best open-source 7B model (WizardMath) by 23%, and the MAmmoTH-34B model achieves 44% accuracy on MATH, even surpassing GPT-4’s CoT result. Our work underscores the importance of diverse problem coverage and the use of hybrid rationales in developing superior math generalist models.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17422",
        "keywords": [
            "math generalist models",
            "instruction tuning",
            "hybrid instruction tuning",
            "mammoth series",
            "hybrid",
            "hybrid rationales",
            "mathinstruct"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the abstract mentions \"outperform existing open-source models\" and \"exceeds the best open-source 7B model\" which implies that the existing models have limitations that the new model overcomes, but does not explicitly state what those limitations are.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the abstract mentions \"outperform existing open-source models\" and \"exceeds the best open-source 7B model\" which implies that the existing models have limitations that the new model overcomes, but does not explicitly state what those limitations are."
    },
    {
        "title": "Provable Reward-Agnostic Preference-Based Reinforcement Learning",
        "authors": "Wenhao Zhan;Masatoshi Uehara;Wen Sun;Jason D. Lee",
        "site": "https://iclr.cc/virtual/2024/poster/17417",
        "summary": "Preference-based Reinforcement Learning (PbRL) is a paradigm in which an RL agent learns to optimize a task using pair-wise preference-based feedback over trajectories, rather than explicit reward signals. While PbRL has demonstrated practical success in fine-tuning language models, existing theoretical work focuses on regret minimization and fails to capture most of the practical frameworks. In this study, we fill in such a gap between theoretical PbRL and practical algorithms by proposing a theoretical reward-agnostic PbRL framework where exploratory trajectories that enable accurate learning of hidden reward functions are acquired before collecting any human feedback. Theoretical analysis demonstrates that our algorithm requires less human feedback for learning the optimal policy under preference-based models with linear parameterization and unknown transitions, compared to the existing theoretical literature. Specifically, our framework can incorporate linear and low-rank MDPs with efficient sample complexity. Additionally, we investigate reward-agnostic RL with action-based comparison feedback and introduce an efficient querying algorithm tailored to this scenario.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17417",
        "keywords": [
            "reinforcement learning",
            "reward agnostic rl",
            "preference based reinforcement learning",
            "regret"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Nemesis: Normalizing the Soft-prompt Vectors of Vision-Language Models",
        "authors": "Shuai Fu;Xiequn Wang;Qiushi Huang;Yu Zhang",
        "site": "https://iclr.cc/virtual/2024/poster/17371",
        "summary": "With the prevalence of large-scale pretrained vision-language models (VLMs), such as CLIP, soft-prompt tuning has become a popular method for adapting these models to various downstream tasks. However, few works delve into the inherent properties of learnable soft-prompt vectors, specifically the impact of their norms to the performance of VLMs. This motivates us to pose an unexplored research question: ``Do we need to normalize the soft prompts in VLMs?'' To fill this research gap, we first uncover a phenomenon, called the $\\textbf{Low-Norm Effect}$ by performing extensive corruption experiments, suggesting that reducing the norms of certain learned prompts occasionally enhances the performance of VLMs, while increasing them often degrades it. To harness this effect, we propose a novel method named $\\textbf{N}$ormalizing th$\\textbf{e}$ soft-pro$\\textbf{m}$pt v$\\textbf{e}$ctors of vi$\\textbf{si}$on-language model$\\textbf{s}$ ($\\textbf{Nemesis}$) to normalize soft-prompt vectors in VLMs. To the best of our knowledge, our work is the first to systematically investigate the role of norms of soft-prompt vector in VLMs, offering valuable insights for future research in soft-prompt tuning.",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17371",
        "keywords": [
            "soft prompts",
            "soft prompt vectors",
            "learnable soft prompt vectors",
            "soft pro"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, few works delve into the inherent properties of learnable soft-prompt vectors, specifically the impact of their norms to the performance of VLMs.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of VLMs (inherent properties of learnable soft-prompt vectors) but does not elaborate on it in detail, and instead focuses on the proposed solution",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, few works delve into the inherent properties of learnable soft-prompt vectors, specifically the impact of their norms to the performance of VLMs.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of VLMs (inherent properties of learnable soft-prompt vectors) but does not elaborate on it in detail, and instead focuses on the proposed solution"
    },
    {
        "title": "Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing",
        "authors": "Dujian Ding;Ankur Mallick;Chi Wang;Robert Sim;Subhabrata Mukherjee;Victor Rühle;Laks V. S. Lakshmanan;Ahmed Hassan Awadallah",
        "site": "https://iclr.cc/virtual/2024/poster/19625",
        "summary": "Large language models (LLMs) excel in most NLP tasks but also require expensive cloud servers for deployment due to their size, while smaller models that can be deployed on lower cost (e.g., edge) devices, tend to lag behind in terms of response quality. Therefore in this work we propose a hybrid inference approach which combines their respective strengths to save cost and maintain quality. Our approach uses a router that assigns queries to the small or large model based on the predicted query difficulty and the desired quality level. The desired quality level can be tuned dynamically at test time to seamlessly trade  quality for cost as per the scenario requirements. In experiments our approach allows us to make up to 40% fewer calls to the large model, with no drop in response quality.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models (LLMs) excel in most NLP tasks but also require expensive cloud servers for deployment due to their size, while smaller models that can be deployed on lower cost (e.g., edge) devices, tend to lag behind in terms of response quality.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large language models (LLMs) excel in most NLP tasks but also require expensive cloud servers for deployment due to their size, while smaller models that can be deployed on lower cost (e.g., edge) devices, tend to lag behind in terms of response quality.\""
    },
    {
        "title": "Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning",
        "authors": "Mengzhou Xia;Tianyu Gao;Zhiyuan Zeng;Danqi Chen",
        "site": "https://iclr.cc/virtual/2024/poster/19623",
        "summary": "The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged moderate-sized large language models (LLMs) highlights the potential of building smaller yet powerful LLMs. Regardless, the cost of training such models from scratch on trillions of tokens remains high. In this work, we study structured pruning as an effective means to develop smaller LLMs from pre-trained, larger models. Our approach employs two key techniques: (1) targeted structured pruning, which prunes a larger model to a specified target shape by removing layers, heads, and intermediate and hidden dimensions in an end-to-end manner, and (2) dynamic batch loading, which dynamically updates the composition of sampled data in each training batch based on varying losses across different domains. We demonstrate the efficacy of our approach by presenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B and 2.7B parameters. Sheared-LLaMA models outperform state-of-the-art open-source models of equivalent sizes, such as Pythia, INCITE, OpenLLaMA and the concurrent TinyLlama models, on a wide range of downstream and instruction tuning evaluations, while requiring only 3% of compute compared to training such models from scratch. This work provides compelling evidence that leveraging existing LLMs with structured pruning is a far more cost-effective approach for building competitive small-scale LLMs",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Regardless, the cost of training such models from scratch on trillions of tokens remains high.\"\n\nThis abstract mentions a limitation of LLMs (high training cost) in passing, but the primary focus is on the proposed solution to address this limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Regardless, the cost of training such models from scratch on trillions of tokens remains high.\"\n\nThis abstract mentions a limitation of LLMs (high training cost) in passing, but the primary focus is on the proposed solution to address this limitation."
    },
    {
        "title": "AutoLoRa: An Automated Robust Fine-Tuning Framework",
        "authors": "Xilie Xu;Jingfeng Zhang;Mohan Kankanhalli",
        "site": "https://iclr.cc/virtual/2024/poster/19622",
        "summary": "Robust Fine-Tuning (RFT) is a low-cost strategy to obtain adversarial robustness in downstream applications, without requiring a lot of computational resources and collecting significant amounts of data. This paper uncovers an issue with the existing RFT, where optimizing both adversarial and natural objectives through the feature extractor (FE) yields significantly divergent gradient directions. This divergence introduces instability in the optimization process, thereby hindering the attainment of adversarial robustness and rendering RFT highly sensitive to hyperparameters. To mitigate this issue, we propose a low-rank (LoRa) branch that disentangles RFT into two distinct components: optimizing natural objectives via the LoRa branch and adversarial objectives via the FE. Besides, we introduce heuristic strategies for automating the scheduling of the learning rate and the scalars of loss terms. Extensive empirical evaluations demonstrate that our proposed automated RFT disentangled via the LoRa branch (AutoLoRa) achieves new state-of-the-art results across a range of downstream tasks. AutoLoRa holds significant practical utility, as it automatically converts a pre-trained FE into an adversarially robust model for downstream tasks without the need for searching hyperparameters. Our source code is available atthe GitHub.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Making LLaMA SEE and Draw with SEED Tokenizer",
        "authors": "Yuying Ge;Sijie Zhao;Ziyun Zeng;Yixiao Ge;Chen Li;Xintao Wang;Ying Shan",
        "site": "https://iclr.cc/virtual/2024/poster/19618",
        "summary": "Abstract:The great success of Large Language Models (LLMs) has expanded the potential of multimodality, contributing to the gradual evolution of General Artificial Intelligence (AGI). A true AGI agent should not only possess the capability to perform predefined multi-tasks but also exhibit emergent abilities in an open-world context. However, despite the considerable advancements made by recent multimodal LLMs, they still fall short in effectively unifying comprehension and generation tasks, let alone open-world emergent abilities. We contend that the key to overcoming the present impasse lies in enabling text and images to be represented and processed interchangeably within a unified autoregressive Transformer. To this end, we introduce $\\textbf{SEED}$, an elaborate image tokenizer that empowers LLMs with the ability to $\\textbf{SEE}$ and $\\textbf{D}$raw at the same time. We identify two crucial design principles: (1) Image tokens should be independent of 2D physical patch positions and instead be produced with a $\\textit{1D causal dependency}$, exhibiting intrinsic interdependence that aligns with the left-to-right autoregressive prediction mechanism in LLMs. (2) Image tokens should capture $\\textit{high-level semantics}$ consistent with the degree of semantic abstraction in words, and be optimized for both discriminativeness and reconstruction during the tokenizer training phase. With SEED tokens, LLM is able to perform scalable multimodal autoregression under its original training recipe, i.e., next-word prediction. SEED-LLaMA is therefore produced by large-scale pretraining and instruction tuning on the interleaved textual and visual data, demonstrating impressive performance on a broad range of multimodal comprehension and generation tasks. More importantly, SEED-LLaMA has exhibited compositional emergent abilities such as multi-turn in-context multimodal generation, acting like your AI assistant. The code (training and inference) and models are released in https://github.com/AILab-CVC/SEED.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, despite the considerable advancements made by recent multimodal LLMs, they still fall short in effectively unifying comprehension and generation tasks, let alone open-world emergent abilities.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, despite the considerable advancements made by recent multimodal LLMs, they still fall short in effectively unifying comprehension and generation tasks, let alone open-world emergent abilities.\""
    },
    {
        "title": "Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages",
        "authors": "Guozheng Ma;Lu Li;Sen Zhang;Zixuan Liu;Zhen Wang;Yixin Chen;Li Shen;Xueqian Wang;Dacheng Tao",
        "site": "https://iclr.cc/virtual/2024/poster/19614",
        "summary": "Plasticity, the ability of a neural network to evolve with new data, is crucial for high-performance and sample-efficient visual reinforcement learning (VRL). Although methods like resetting and regularization can potentially mitigate plasticity loss, the influences of various components within the VRL framework on the agent's plasticity are still poorly understood. In this work, we conduct a systematic empirical exploration focusing on three primary underexplored facets and derive the following insightful conclusions: (1) data augmentation is essential in maintaining plasticity; (2) the critic's plasticity loss serves as the principal bottleneck impeding efficient training; and (3) without timely intervention to recover critic's plasticity in the early stages, its loss becomes catastrophic. These insights suggest a novel strategy to address the high replay ratio (RR) dilemma, where exacerbated plasticity loss hinders the potential improvements of sample efficiency brought by increased reuse frequency. Rather than setting a static RR for the entire training process, we propose Adaptive RR, which dynamically adjusts the RR based on the critic’s plasticity level. Extensive evaluations indicate that Adaptive RR not only avoids catastrophic plasticity loss in the early stages but also benefits from more frequent reuse in later phases, resulting in superior sample efficiency.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "NEFTune: Noisy Embeddings Improve Instruction Finetuning",
        "authors": "Neel Jain;Ping-yeh Chiang;Yuxin Wen;John Kirchenbauer;Hong-Min Chu;Gowthami Somepalli;Brian R. Bartoldson;Bhavya Kailkhura;Avi Schwarzschild;Aniruddha Saha;Micah Goldblum;Jonas Geiping;Tom Goldstein",
        "site": "https://iclr.cc/virtual/2024/poster/19612",
        "summary": "Abstract:We show that language model finetuning can be improved, sometimes dramatically, with a simple augmentation. NEFTune adds noise to the embedding vectors during training.Standard finetuning of LLaMA-2-7B using Alpaca achieves $29.79$\\% on AlpacaEval, which rises to $64.69$\\% using noisy embeddings. NEFTune also improves over strong baselines on modern instruction datasets.Models trained with Evol-Instruct see a $10$\\% improvement, with ShareGPT an $8$\\% improvement, and with OpenPlatypus an $8$\\% improvement. Even powerful models further refined with RLHF such as LLaMA-2-Chat benefit from additional training with NEFTune. Particularly, we see these improvements on the conversational abilities of the instruction model and not on traditional tasks like those on the OpenLLM Leaderboard, where performance is the same.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations of LLMs are discussed, but the paper mentions that the improvements from NEFTune are \"particularly\" seen \"on the conversational abilities of the instruction model and not on traditional tasks\", implying that LLMs may have limitations in certain tasks.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations of LLMs are discussed, but the paper mentions that the improvements from NEFTune are \"particularly\" seen \"on the conversational abilities of the instruction model and not on traditional tasks\", implying that LLMs may have limitations in certain tasks."
    },
    {
        "title": "Variance Reduced Halpern Iteration for Finite-Sum Monotone Inclusions",
        "authors": "Xufeng Cai;Ahmet Alacaoglu;Jelena Diakonikolas",
        "site": "https://iclr.cc/virtual/2024/poster/19609",
        "summary": "Abstract:Machine learning approaches relying on such criteria as adversarial robustness or multi-agent settings have raised the need for solving game-theoretic equilibrium problems. Of particular relevance to these applications are methods targeting finite-sum structure, which generically arises in empirical variants of learning problems in these contexts. Further, methods with computable approximation errors are highly desirable, as they provide verifiable exit criteria. Motivated by these applications, we study finite-sum monotone inclusion problems, which model broad classes of equilibrium problems. Our main contributions are variants of the classical Halpern iteration that employ variance reduction to obtain improved complexity guarantees in which $n$ component operators in the finite sum are ``on average'' either cocoercive or Lipschitz continuous and monotone, with parameter $L$. The resulting oracle complexity of our methods, which provide guarantees for the last iterate and for a (computable) operator norm residual, is $\\widetilde{\\mathcal{O}}( n + \\sqrt{n}L\\varepsilon^{-1})$, which improves upon existing methods by a factor up to $\\sqrt{n}$. This constitutes the first variance reduction-type result for general finite-sum monotone inclusions and for more specific problems such as convex-concave optimization when operator norm residual is the optimality measure. We further argue that, up to poly-logarithmic factors, this complexity is unimprovable in the monotone Lipschitz setting; i.e., the provided result is near-optimal.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "MINDE: Mutual Information Neural Diffusion Estimation",
        "authors": "Giulio Franzese;Mustapha BOUNOUA;Pietro Michiardi",
        "site": "https://iclr.cc/virtual/2024/poster/19605",
        "summary": "In this work we present a new method for the estimation of Mutual Information (MI) between random variables. Our approach is based on an original interpretation of the Girsanov theorem, which allows us to use score-based diffusion models to estimate the KL divergence between two densities as a difference between their score functions. As a by-product, our method also enables the estimation of the entropy of random variables. Armed with such building blocks, we present a general recipe to measure MI, which unfolds in two directions: one uses conditional diffusion process, whereas the other uses joint diffusion processes that allow  simultaneous modelling of two random variables. Our results, which derive from a thorough experimental protocol over all the variants of our approach, indicate that our method is more accurate than the main alternatives from the literature, especially for challenging distributions. Furthermore, our methods pass MI self-consistency tests, including data processing and additivity under independence, which instead are a pain-point of existing methods",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF",
        "authors": "Anand Siththaranjan;Cassidy Laidlaw;Dylan Hadfield-Menell",
        "site": "https://iclr.cc/virtual/2024/poster/19603",
        "summary": "In practice, preference learning from human feedback depends on incomplete data with hidden context. Hidden context refers to data that affects the feedback received, but which is not represented in the data used to train a preference model. This captures common issues of data collection, such as having human annotators with varied preferences, cognitive processes that result in seemingly irrational behavior, and combining data labeled according to different criteria. We prove that standard applications of preference learning, including reinforcement learning from human feedback (RLHF), implicitly aggregate over hidden contexts according to a well-known voting rule calledBorda count. We show this can produce counter-intuitive results that are very different from other methods which implicitly aggregate via expected utility. Furthermore, our analysis formalizes the way that preference learning from users with diverse values tacitly implements a social choice function. A key implication of this result is that annotators have an incentive to misreport their preferences in order to influence the learned model, leading to vulnerabilities in the deployment of RLHF. As a step towards mitigating these problems, we introduce a class of methods calleddistributional preference learning(DPL). DPL methods estimate a distribution of possible score values for each alternative in order to better account for hidden context. Experimental results indicate that applying DPL to RLHF for LLM chatbots identifies hidden context in the data and significantly reduces subsequent jailbreak vulnerability.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"A key implication of this result is that annotators have an incentive to misreport their preferences in order to influence the learned model, leading to vulnerabilities in the deployment of RLHF.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"A key implication of this result is that annotators have an incentive to misreport their preferences in order to influence the learned model, leading to vulnerabilities in the deployment of RLHF.\""
    },
    {
        "title": "SALMONN: Towards Generic Hearing Abilities for Large Language Models",
        "authors": "Changli Tang;Wenyi Yu;Guangzhi Sun;Xianzhao Chen;Tian Tan;Wei Li;Lu Lu;Zejun MA;Chao Zhang",
        "site": "https://iclr.cc/virtual/2024/poster/19600",
        "summary": "Hearing is arguably an essential ability of artificial intelligence (AI) agents in the physical world, which refers to the perception and understanding of general auditory information consisting of at least three types of sounds: speech, audio events, and music. In this paper, we propose SALMONN, a speech audio language music open neural network, built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model. SALMONN enables the LLM to directly process and understand general audio inputs and achieve competitive performances on a number of speech and audio tasks used in training, such as automatic speech recognition and translation, auditory-information-based question answering, emotion recognition, speaker verification, and music and audio captioning etc. SALMONN also has a diverse set of emergent abilities unseen in the training, which includes but is not limited to speech translation to untrained languages, speech-based slot filling, spoken-query-based question answering, audio-based storytelling, and speech audio co-reasoning etc. The presence of cross-modal emergent abilities is studied, and a novel few-shot activation tuning approach is proposed to activate such abilities. To our knowledge, SALMONN is the first model of its type and can be regarded as a step towards AI with generic hearing abilities. The source code, model checkpoints and data are available at https://github.com/bytedance/SALMONN.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model.\"\n\nThis paper discusses LLMs but does not mention any limitations of the models in the abstract. The focus is on proposing a new model that integrates LLMs with speech and audio encoders to achieve generic hearing abilities.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model.\"\n\nThis paper discusses LLMs but does not mention any limitations of the models in the abstract. The focus is on proposing a new model that integrates LLMs with speech and audio encoders to achieve generic hearing abilities."
    },
    {
        "title": "Beyond Stationarity: Convergence Analysis of Stochastic Softmax Policy Gradient Methods",
        "authors": "Sara Klein;Simon Weissmann;Leif Döring",
        "site": "https://iclr.cc/virtual/2024/poster/19585",
        "summary": "Markov Decision Processes (MDPs) are a formal framework for modeling and solving sequential decision-making problems. In finite time horizons such problems are relevant for instance for optimal stopping or specific supply chain problems, but also in the training of large language models. In contrast to infinite horizon MDPs optimal policies are not stationary, policies must be learned for every single epoch. In practice all parameters are often trained simultaneously, ignoring the inherent structure suggested by dynamic programming. This paper introduces a combination of dynamic programming and policy gradient called dynamical policy gradient, where the parameters are trained backwards in time.       For the tabular softmax parametrisation we carry out the convergence analysis for simultaneous and dynamic policy gradient towards global optima, both in the exact and sampled gradient settings without regularisation. It turns out that the use of dynamic policy gradient training much better exploits the structure of finite-time problems which is reflected in improved convergence bounds.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"In practice all parameters are often trained simultaneously, ignoring the inherent structure suggested by dynamic programming.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"In practice all parameters are often trained simultaneously, ignoring the inherent structure suggested by dynamic programming.\""
    },
    {
        "title": "Unsupervised Pretraining for Fact Verification by Language Model Distillation",
        "authors": "Adrián Bazaga;Pietro Lio;Gos Micklem",
        "site": "https://iclr.cc/virtual/2024/poster/19573",
        "summary": "Abstract:Fact verification aims to verify a claim using evidence from a trustworthy knowledge base. To address this challenge, algorithms must produce features for every claim that are both semantically meaningful, and compact enough to find a semantic alignment with the source information. In contrast to previous work, which tackled the alignment problem by learning over annotated corpora of claims and their corresponding labels, we propose SFAVEL ($\\underline{S}$elf-supervised $\\underline{Fa}$ct $\\underline{Ve}$rification via $\\underline{L}$anguage Model Distillation), a novel unsupervised pretraining framework that leverages pre-trained language models to distil self-supervised features into high-quality claim-fact alignments without the need for annotations. This is enabled by a novel contrastive loss function that encourages features to attain high-quality claim and evidence alignments whilst preserving the semantic relationships across the corpora. Notably, we present results that achieve a new state-of-the-art on FB15k-237 (+5.3\\% Hits@1) and FEVER (+8\\% accuracy) with linear evaluation.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations of LLMs, but the paper proposes a novel framework that leverages pre-trained language models, implying that existing methods may have limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations of LLMs, but the paper proposes a novel framework that leverages pre-trained language models, implying that existing methods may have limitations."
    },
    {
        "title": "Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs",
        "authors": "Yuxin Zhang;Lirui Zhao;Mingbao Lin;Sun Yunyun;Yiwu Yao;Xingjia Han;Jared Tanner;Shiwei Liu;Rongrong Ji",
        "site": "https://iclr.cc/virtual/2024/poster/19572",
        "summary": "Abstract:The ever-increasing large language models (LLMs), though opening a potential path for the upcoming artificial general intelligence, sadly drops a daunting obstacle on the way towards their on-device deployment. As one of the most well-established pre-LLMs approaches in reducing model complexity, network pruning appears to lag behind in the era of LLMs, due mostly to its costly fine-tuning (or re-training) necessity under the massive volumes of model parameter and training data. To close this industry-academia gap, we introduce Dynamic Sparse No Training ($\\texttt{DSNT}$), a training-free fine-tuning approach that slightly updates sparse LLMs without the expensive backpropagation and any weight updates. Inspired by the Dynamic Sparse Training, $\\texttt{DSNT}$ minimizes the reconstruction error between the dense and sparse LLMs, in the fashion of performing iterative weight pruning-and-growing on top of sparse LLMs. To accomplish this purpose, $\\texttt{DSNT}$ particularly takes into account the anticipated reduction in reconstruction error for pruning and growing, as well as the variance w.r.t. different input data for growing each weight. This practice can be executed efficiently in linear time since its obviates the need of backpropagation for fine-tuning LLMs. Extensive experiments on LLaMA-V1/V2, Vicuna, and OPT across various benchmarks demonstrate the effectiveness of $\\texttt{DSNT}$ in enhancing the performance of sparse LLMs, especially at high sparsity levels. For instance, $\\texttt{DSNT}$ is able to outperform the state-of-the-art Wanda by 26.79 perplexity at 70% sparsity with LLaMA-7B. Our paper offers fresh insights into how to fine-tune sparse LLMs in an efficient training-free manner and open new venues to scale the great potential of sparsity to LLMs.  Codes are available at https://github.com/zyxxmu/DSnoT.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The ever-increasing large language models (LLMs), though opening a potential path for the upcoming artificial general intelligence, sadly drops a daunting obstacle on the way towards their on-device deployment.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The ever-increasing large language models (LLMs), though opening a potential path for the upcoming artificial general intelligence, sadly drops a daunting obstacle on the way towards their on-device deployment.\""
    },
    {
        "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
        "authors": "Deyao Zhu;Jun Chen;Xiaoqian Shen;Xiang Li;Mohamed Elhoseiny",
        "site": "https://iclr.cc/virtual/2024/poster/19567",
        "summary": "The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind GPT-4 continue to remain undisclosed.We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer. Our work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, such as detailed image description generation and website creation from hand-drawn drafts.Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on. In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation). To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model's generation reliability and overall usability.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation).\""
    },
    {
        "title": "MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction Following",
        "authors": "Renze Lou;Kai Zhang;Jian Xie;Yuxuan Sun;Janice Ahn;Hanzi Xu;Yu Su;Wenpeng Yin",
        "site": "https://iclr.cc/virtual/2024/poster/19563",
        "summary": "In the realm of large language models (LLMs), enhancing instruction-following capability often involves curating expansive training data. This is achieved through two primary schemes: i) Scaling-Inputs: Amplifying (input, output) pairs per task instruction, aiming for better instruction adherence. ii) Scaling Input-Free Tasks: Enlarging tasks, each composed of an (instruction, output) pair (without requiring a separate input anymore). However, LLMs under Scaling-Inputs tend to be overly sensitive to inputs, leading to misinterpretation or non-compliance with instructions. Conversely, Scaling Input-Free Tasks demands a substantial number of tasks but is less effective in instruction following when dealing with instances in Scaling-Inputs. This work introduces MUFFIN, a new scheme of instruction-following dataset curation. Specifically, we automatically Scale Tasks per Input by diversifying these tasks with various input facets. Experimental results across four zero-shot benchmarks, spanning both Scaling-Inputs and Scaling Input-Free Tasks schemes, reveal that LLMs, at various scales, trained on MUFFIN generally demonstrate superior instruction-following capabilities compared to those trained on the two aforementioned schemes.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, LLMs under Scaling-Inputs tend to be overly sensitive to inputs, leading to misinterpretation or non-compliance with instructions. Conversely, Scaling Input-Free Tasks demands a substantial number of tasks but is less effective in instruction following when dealing with instances in Scaling-Inputs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, LLMs under Scaling-Inputs tend to be overly sensitive to inputs, leading to misinterpretation or non-compliance with instructions. Conversely, Scaling Input-Free Tasks demands a substantial number of tasks but is less effective in instruction following when dealing with instances in Scaling-Inputs.\""
    },
    {
        "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning",
        "authors": "Xi Victoria Lin;Xilun Chen;Mingda Chen;Weijia Shi;Maria Lomeli;Richard James;Pedro Rodriguez;Jacob Kahn;Gergely Szilvasy;Mike Lewis;Luke Zettlemoyer;Wen-tau Yih",
        "site": "https://iclr.cc/virtual/2024/poster/19562",
        "summary": "Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B, achieves state-of-the-art performance across a range of knowledge-intensive zero- and few-shot learning benchmarks, significantly outperforming existing in-context RALM approaches by up to +8.9% in 0-shot setting and +1.4% in 5-shot setting on average.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance.\""
    },
    {
        "title": "PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization",
        "authors": "Xinyuan Wang;Chenxi Li;Zhen Wang;Fan Bai;Haotian Luo;Jiayou Zhang;Nebojsa Jojic;Eric Xing;Zhiting Hu",
        "site": "https://iclr.cc/virtual/2024/poster/19561",
        "summary": "Expert-level prompts, carefully engineered by human experts who have a deep understanding of both large language models (LLMs) and domain knowledge, are the future of prompting and pivotal to harnessing the full power of advanced LLMs. Discovering such prompts with an automated process remains a sought-after and unresolved challenge. Existing prompt optimization techniques, though automated through iterative sampling, often fall short in injecting domain knowledge and exploring the vast prompt space for complex expert-level prompts efficiently. To address this pressing need and achieve expert-level prompting, we introduce PromptAgent, which autonomously discovers prompts equivalent in quality to those handcrafted by experts. At its core, PromptAgent views prompt optimization as a strategic planning problem and employs a principled planning algorithm (rooted in Monte Carlo Tree Search) to strategically explore the vast expert-level prompt space. PromptAgent interacts with the LLM in a human-like trial-and-error manner during the planning, and injects expert-level knowledge by reflecting on model errors and generating insightful error feedback. This novel formulation allows it to iteratively evaluate intermediate prompts, refine them based on errors, simulate future rewards, and search for high-reward paths leading to expert-level prompts. We apply PromptAgent to 12 tasks spanning three practical domains: BIG-Bench Hard (BBH), domain-expert, and general NLU tasks, showing PromptAgent consistently outperforms strong prompting and prompt optimization baselines by great margins. Our qualitative analysis further emphasizes PromptAgent's capability to distill insightful errors into expert-level prompts.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing prompt optimization techniques, though automated through iterative sampling, often fall short in injecting domain knowledge and exploring the vast prompt space for complex expert-level prompts efficiently.\"\n\nThis paper mentions a limitation of LLMs in passing, specifically the difficulty of existing prompt optimization techniques in injecting domain knowledge and exploring the vast prompt space efficiently. However, the limitation is not the primary focus of the paper,",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing prompt optimization techniques, though automated through iterative sampling, often fall short in injecting domain knowledge and exploring the vast prompt space for complex expert-level prompts efficiently.\"\n\nThis paper mentions a limitation of LLMs in passing, specifically the difficulty of existing prompt optimization techniques in injecting domain knowledge and exploring the vast prompt space efficiently. However, the limitation is not the primary focus of the paper,"
    },
    {
        "title": "Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models",
        "authors": "Zijun Wu;Yongkang Wu;Lili Mou",
        "site": "https://iclr.cc/virtual/2024/poster/19559",
        "summary": "Prompt tuning in natural language processing (NLP) has become an increasingly popular method for adapting large language models to specific tasks. However, the transferability of these prompts, especially continuous prompts, between different models remains a challenge. In this work, we propose a zero-shot continuous prompt transfer method, where source prompts are encoded into relative space and the corresponding target prompts are searched for transferring to target models. Experimental results confirm the effectiveness of our method, showing that 'task semantics' in continuous prompts can be generalized across various language models. Moreover, we find that combining 'task semantics' from multiple source models can further enhance the performance of transfer.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the transferability of these prompts, especially continuous prompts, between different models remains a challenge.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the transferability of these prompts, especially continuous prompts, between different models remains a challenge.\""
    },
    {
        "title": "Neural Neighborhood Search for Multi-agent Path Finding",
        "authors": "Zhongxia Yan;Cathy Wu",
        "site": "https://iclr.cc/virtual/2024/poster/19553",
        "summary": "Multi-agent path finding (MAPF) is the combinatorial problem of planning optimal collision-avoiding paths for multiple agents, with application to robotics, logistics, and transportation. Though many recent learning-based works have focused on large-scale combinatorial problems by guiding their decomposition into sequences of smaller subproblems, the combined spatiotemporal and time-restricted nature of MAPF poses a particular challenge for learning-based guidance of iterative approaches like large neighborhood search (LNS), which is already a state-of-the-art approach for MAPF even without learning. We address this challenge of neural-guided LNS for MAPF by designing an architecture which interleaves convolution and attention to efficiently represent MAPF subproblems, enabling practical guidance of LNS in benchmark settings. We demonstrate the speedup of our method over existing state-of-the-art LNS-based methods for MAPF as well as the robustness of our method to unseen settings. Our proposed method expands the horizon of effective deep learning-guided LNS methods into multi-path planning problems, and our proposed representation may be more broadly applicable for representing path-wise interactions.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs."
    },
    {
        "title": "PerceptionCLIP: Visual Classification by Inferring and Conditioning on Contexts",
        "authors": "Bang An;Sicheng Zhu;Michael-Andrei Panaitescu-Liess;Chaithanya Kumar Mummadi;Furong Huang",
        "site": "https://iclr.cc/virtual/2024/poster/19552",
        "summary": "Vision-language models like CLIP are widely used in zero-shot image classification due to their ability to understand various visual concepts and natural language descriptions. However, how to fully leverage CLIP's unprecedented human-like understanding capabilities to achieve better performance is still an open question. This paper draws inspiration from the human visual perception process: when classifying an object, humans first infer contextual attributes (e.g., background and orientation) which help separate the foreground object from the background, and then classify the object based on this information. Inspired by it, we observe that providing CLIP with contextual attributes improves zero-shot image classification and mitigates reliance on spurious features. We also observe that CLIP itself can reasonably infer the attributes from an image. With these observations, we propose a training-free, two-step zero-shot classification method PerceptionCLIP. Given an image, it first infers contextual attributes (e.g., background) and then performs object classification conditioning on them. Our experiments show that PerceptionCLIP achieves better generalization, group robustness, and interpretability.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, how to fully leverage CLIP's unprecedented human-like understanding capabilities to achieve better performance is still an open question.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, how to fully leverage CLIP's unprecedented human-like understanding capabilities to achieve better performance is still an open question.\""
    },
    {
        "title": "Towards Offline Opponent Modeling with In-context Learning",
        "authors": "Yuheng Jing;Kai Li;Bingyun Liu;Yifan Zang;Haobo Fu;QIANG FU;Junliang Xing;Jian Cheng",
        "site": "https://iclr.cc/virtual/2024/poster/19549",
        "summary": "Opponent modeling aims at learning the opponent's behaviors, goals, or beliefs to reduce the uncertainty of the competitive environment and assist decision-making. Existing work has mostly focused on learning opponent models online, which is impractical and inefficient in practical scenarios. To this end, we formalize an Offline Opponent Modeling (OOM) problem with the objective of utilizing pre-collected offline datasets to learn opponent models that characterize the opponent from the viewpoint of the controlled agent, which aids in adapting to the unknown fixed policies of the opponent. Drawing on the promises of the Transformers for decision-making, we introduce a general approach, Transformer Against Opponent (TAO), for OOM. Essentially, TAO tackles the problem by harnessing the full potential of the supervised pre-trained Transformers' in-context learning capabilities. The foundation of TAO lies in three stages: an innovative offline policy embedding learning stage, an offline opponent-aware response policy training stage, and a deployment stage for opponent adaptation with in-context learning. Theoretical analysis establishes TAO's equivalence to Bayesian posterior sampling in opponent modeling and guarantees TAO's convergence in opponent policy recognition. Extensive experiments and ablation studies on competitive environments with sparse and dense rewards demonstrate the impressive performance of TAO. Our approach manifests remarkable prowess for fast adaptation, especially in the face of unseen opponent policies, confirming its in-context learning potency.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Drawing on the promises of the Transformers for decision-making, we introduce a general approach, Transformer Against Opponent (TAO), for OOM.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Drawing on the promises of the Transformers for decision-making, we introduce a general approach, Transformer Against Opponent (TAO), for OOM.\""
    },
    {
        "title": "MEND: Meta Demonstration Distillation for Efficient and Effective In-Context Learning",
        "authors": "Yichuan Li;Xiyao Ma;Sixing Lu;Kyumin Lee;Xiaohu Liu;Chenlei Guo",
        "site": "https://iclr.cc/virtual/2024/poster/19544",
        "summary": "Large Language models (LLMs) have demonstrated impressive in-context learning (ICL) capabilities, where a LLM makes predictions for a given test input together with a few input-output pairs (demonstrations).Nevertheless, the inclusion of demonstrations poses a challenge, leading to a quadratic increase in the computational overhead of the self-attention mechanism.Existing solutions attempt to condense lengthy demonstrations into compact vectors. However, they often require task-specific retraining or compromise LLM's in-context learning performance. To mitigate these challenges, we present Meta Demonstration Distillation (MEND), where a language model learns to distill any lengthy demonstrations into vectors without retraining for a new downstream task. We exploit the knowledge distillation to enhance alignment between MEND and MEND, achieving both efficiency and effectiveness concurrently. MEND is endowed with the meta-knowledge of distilling demonstrations through a two-stage training process, which includes meta-distillation pretraining and fine-tuning.Comprehensive evaluations across seven diverse ICL settings using decoder-only (GPT-2) and encoder-decoder (T5) attest to MEND's prowess.It not only matches but often outperforms the Vanilla ICL as well as other state-of-the-art distillation models, while significantly reducing the computational demands. This innovation promises enhanced scalability and efficiency for the practical deployment of large language models.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Nevertheless, the inclusion of demonstrations poses a challenge, leading to a quadratic increase in the computational overhead of the self-attention mechanism.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Nevertheless, the inclusion of demonstrations poses a challenge, leading to a quadratic increase in the computational overhead of the self-attention mechanism.\""
    },
    {
        "title": "Deep SE(3)-Equivariant Geometric Reasoning for Precise Placement Tasks",
        "authors": "Ben Eisner;Yi Yang;Todor Davchev;Mel Vecerik;Jonathan Scholz;David Held",
        "site": "https://iclr.cc/virtual/2024/poster/19539",
        "summary": "Many robot manipulation tasks can be framed as geometric reasoning tasks, where an agent must be able to precisely manipulate an object into a position that satisfies the task from a set of initial conditions. Often, task success is defined based on the relationship between two objects - for instance, hanging a mug on a rack.  In such cases, the solution should be equivariant to the initial position of the objects as well as the agent, and invariant to the pose of the camera. This poses a challenge for learning systems which attempt to solve this task by learning directly from high-dimensional demonstrations: the agent must learn to be both equivariant as well as precise, which can be challenging without any inductive biases about the problem. In this work, we propose a method for precise relative pose prediction which is provably SE(3)-equivariant, can be learned from only a few demonstrations, and can generalize across variations in a class of objects. We accomplish this by factoring the problem into learning an SE(3) invariant task-specific representation of the scene and then interpreting this representation with novel geometric reasoning layers which are provably SE(3) equivariant. We demonstrate that our method can yield substantially more precise placement predictions in simulated placement tasks than previous methods trained with the same amount of data, and can accurately represent relative placement relationships data collected from real-world demonstrations. Supplementary information and videos can be found at https://sites.google.com/view/reldist-iclr-2023.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Enabling Lanuguage Models to Implicitly Learn Self-Improvement",
        "authors": "Ziqi Wang;Le Hou;Tianjian Lu;Yuexin Wu;Yunxuan Li;Hongkun Yu;Heng Ji",
        "site": "https://iclr.cc/virtual/2024/poster/19534",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in open-ended text generation tasks. However, the inherent open-ended nature of these tasks implies that there is always room for improvement in the quality of model responses. To address this challenge, various approaches have been proposed to enhance the performance of LLMs. There has been a growing focus on enabling LLMs to self-improve their response quality, thereby reducing the reliance on extensive human annotation efforts for collecting diverse and high-quality training data. Recently, prompting-based methods have been widely explored among self-improvement methods owing to their effectiveness, efficiency, and convenience. However, those methods usually require explicitly and thoroughly written rubrics as inputs to LLMs. It is expensive and challenging to manually derive and provide all necessary rubrics with a real-world complex goal for improvement (e.g., being more helpfulness and less harmful). To this end, we propose an imPlicit self-ImprovemenT (PIT) framework that implicitly learns the improvement goal from human preference data. PIT only requires preference data that are used to train reward models with no extra human efforts. Specifically, we reformulate the training objective of reinforcement learning from human feedback (RLHF) -- instead of maximizing response quality for a given input, we maximize the quality gap of the response conditioned on a reference response. In this way, PIT is implicitly trained with the improvement goal of better aligning with human preferences. Experiments on two real-world datasets and one synthetic dataset show that our method significantly outperforms prompting-based methods.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the inherent open-ended nature of these tasks implies that there is always room for improvement in the quality of model responses.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the inherent open-ended nature of these tasks implies that there is always room for improvement in the quality of model responses.\""
    },
    {
        "title": "Language Model Self-improvement by Reinforcement Learning Contemplation",
        "authors": "Jing-Cheng Pang;Pengyuan Wang;Kaiyuan Li;Xiong-Hui Chen;Jiacheng Xu;Zongzhang Zhang;Yang Yu",
        "site": "https://iclr.cc/virtual/2024/poster/19526",
        "summary": "Language model self-improvement (LMSI) techniques have recently gained significant attention as they improve language models without requiring external supervision. A common approach is reinforcement learning from AI feedback (RLAIF), which trains a reward model based on AI preference data and employs a reinforcement learning algorithm to train the language model. However, RLAIF relies on the heuristic assumption that an AI model can provide effective feedback and correct wrong answers, requiring a solid capability of the language model. This paper presents a novel LMSI method, Reinforcement Learning Contemplation (RLC). We disclose that it is simpler for language models to evaluate a sentence than to generate it, even for small language models. Leveraging the gap between the evaluation and generation, RLC evaluates generated answers and updates language model parameters using reinforcement learning to maximize evaluation scores. Through testing on various challenging reasoning tasks and text summarization task, our experiments show that RLC effectively improves language model performance without external supervision, resulting in an answering accuracy increase (from 31.23% to 37.09%) for BigBench-hard reasoning tasks, and a rise in BERTScore for CNN/Daily Mail summarization tasks. Furthermore, RLC can be applied to models of different sizes, showcasing its broad applicability.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, RLAIF relies on the heuristic assumption that an AI model can provide effective feedback and correct wrong answers, requiring a solid capability of the language model.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, RLAIF relies on the heuristic assumption that an AI model can provide effective feedback and correct wrong answers, requiring a solid capability of the language model.\""
    },
    {
        "title": "Chain of Thought Empowers Transformers to Solve Inherently Serial Problems",
        "authors": "Zhiyuan Li;Hong Liu;Denny Zhou;Tengyu Ma",
        "site": "https://iclr.cc/virtual/2024/poster/19524",
        "summary": "Abstract:Generating a sequence of intermediate steps, \\emph{a.k.a.}, a chain of thought (CoT), is a highly effective method to improve the accuracy of large language models (LLMs) on arithmetics and symbolic reasoning tasks. However, the mechanism behind CoT remains unclear. This work provides a theoretical understanding of the power of CoT for decoder-only transformers through the lens of expressiveness. Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low. Given input length $n$, previous works have constant-depth transformers with finite precision $\\mathsf{poly}(n)$ embedding size can only solve problems in $\\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in $\\mathsf{AC}^0$, a proper subset of $ \\mathsf{TC}^0$. However, with $T$ steps of CoT, constant-depth transformers using constant-bit precision and $O(\\log n)$ embedding size can solve any problem solvable by boolean circuits of size $T$. Empirically, enabling CoT dramatically improves the accuracy for tasks that are hard for parallel computation, including the composition of permutation groups, iterated squaring, and circuit value problems, especially for low-depth transformers.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the mechanism behind CoT remains unclear... Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, the mechanism behind CoT remains unclear... Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low.\""
    },
    {
        "title": "The Marginal Value of Momentum for Small Learning Rate SGD",
        "authors": "Runzhe Wang;Sadhika Malladi;Tianhao Wang;Kaifeng Lyu;Zhiyuan Li",
        "site": "https://iclr.cc/virtual/2024/poster/19521",
        "summary": "Momentum is known to accelerate the convergence of gradient descent in strongly convex settings without stochastic gradient noise. In stochastic optimization, such as training neural networks, folklore suggests that momentum may help deep learning optimization by reducing the variance of the stochastic gradient update, but previous theoretical analyses do not find momentum to offer any provable acceleration. Theoretical results in this paper clarify the role of momentum in stochastic settings where the learning rate is small and gradient noise is the dominant source of instability, suggesting that SGD with and without momentum behave similarly in the short and long time horizons. Experiments show that momentum indeed has limited benefits for both optimization and generalization in practical training regimes where the optimal learning rate is not very large, including small- to medium-batch training from scratch on ImageNet and fine-tuning language models on downstream tasks.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Experiments show that momentum indeed has limited benefits for both optimization and generalization in practical training regimes... including fine-tuning language models on downstream tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Experiments show that momentum indeed has limited benefits for both optimization and generalization in practical training regimes... including fine-tuning language models on downstream tasks.\""
    },
    {
        "title": "The Reasonableness Behind Unreasonable Translation Capability of Large Language Model",
        "authors": "Tingchen Fu;Lemao Liu;Deng Cai;Guoping Huang;Shuming Shi;Rui Yan",
        "site": "https://iclr.cc/virtual/2024/poster/19519",
        "summary": "Multilingual large language models trained on non-parallel data yield impressive translation capabilities. Existing studies demonstrate that incidental sentence-level bilingualism within pre-training data contributes to the LLM's translation abilities. However, it has also been observed that LLM's translation capabilities persist even when incidental sentence-level bilingualism are excluded from the training corpus.In this study, we comprehensively investigate the unreasonable effectiveness and the underlying mechanism for LLM's translation abilities, specifically addressing the question why large language models learn to translate without parallel data, using the BLOOM model series as a representative example. Through extensive experiments, our findings suggest the existence of unintentional bilingualism in the pre-training corpus, especially word alignment data significantly contributes to the large language model's acquisition of translation ability. Moreover, the translation signal derived from word alignment data is comparable to that from sentence-level bilingualism. Additionally, we study the effects of monolingual data and parameter-sharing in assisting large language model to learn to translate. Together, these findings present another piece of the broader puzzle of trying to understand how large language models acquire translation capability.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it has also been observed that LLM's translation capabilities persist even when incidental sentence-level bilingualism are excluded from the training corpus.\"\n\nThis paper mentions a limitation of LLMs in passing, but does not explore it in depth. The primary focus of the paper is on understanding the underlying mechanism of LLM's translation abilities.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, it has also been observed that LLM's translation capabilities persist even when incidental sentence-level bilingualism are excluded from the training corpus.\"\n\nThis paper mentions a limitation of LLMs in passing, but does not explore it in depth. The primary focus of the paper is on understanding the underlying mechanism of LLM's translation abilities."
    },
    {
        "title": "PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training",
        "authors": "Dawei Zhu;Nan Yang;Liang Wang;Yifan Song;Wenhao Wu;Furu Wei;Sujian Li",
        "site": "https://iclr.cc/virtual/2024/poster/19506",
        "summary": "Large Language Models (LLMs) are trained with a pre-defined context length, restricting their use in scenarios requiring long inputs. Previous efforts for adapting LLMs to a longer length usually requires fine-tuning with this target length (Full-length fine-tuning), suffering intensive training cost. To decouple train length from target length for efficient context window extension, we propose Positional Skip-wisE (PoSE) training that smartly simulates long inputs using a fixed context window. This is achieved by first dividing the original context window into several chunks, then designing distinct skipping bias terms to manipulate the position indices of each chunk. These bias terms and the lengths of each chunk are altered for every training example, allowing the model to adapt to all positions within target length. Experimental results show that PoSE greatly reduces memory and time overhead compared with Full-length fine-tuning, with minimal impact on performance. Leveraging this advantage, we have successfully extended the LLaMA model to 128k tokens using a 2k training context window. Furthermore, we empirically confirm that PoSE is compatible with all RoPE-based LLMs and position interpolation strategies. Notably, our method can potentially support infinite length, limited only by memory usage in inference. With ongoing progress for efficient inference, we believe PoSE can further scale the context window beyond 128k.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large Language Models (LLMs) are trained with a pre-defined context length, restricting their use in scenarios requiring long inputs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large Language Models (LLMs) are trained with a pre-defined context length, restricting their use in scenarios requiring long inputs.\""
    },
    {
        "title": "Evaluating Language Model Agency Through Negotiations",
        "authors": "Tim Ruben Davidson;Veniamin Veselovsky;Michal Kosinski;Robert West",
        "site": "https://iclr.cc/virtual/2024/poster/19505",
        "summary": "We introduce an approach to evaluate language model (LM) agency using negotiation games. This approach better reflects real-world use cases and addresses some of the shortcomings of alternative LM benchmarks. Negotiation games enable us to study multi-turn, and cross-model interactions, modulate complexity, and side-step accidental evaluation data leakage. We use our approach to test six widely used and publicly accessible LMs, evaluating performance and alignment in both self-play and cross-play settings. Noteworthy findings include: (i) only closed-source models tested here were able to complete these tasks; (ii) cooperative bargaining games proved to be most challenging to the models; and (iii) even the most powerful models sometimes \"lose\" to weaker opponents.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Noteworthy findings include: (i) only closed-source models tested here were able to complete these tasks; (ii) cooperative bargaining games proved to be most challenging to the models; and (iii) even the most powerful models sometimes \"lose\" to weaker opponents.\"\n\nThis rating is chosen because the paper mentions specific limitations and challenges of LLMs, such as their inability to complete",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Noteworthy findings include: (i) only closed-source models tested here were able to complete these tasks; (ii) cooperative bargaining games proved to be most challenging to the models; and (iii) even the most powerful models sometimes \"lose\" to weaker opponents.\"\n\nThis rating is chosen because the paper mentions specific limitations and challenges of LLMs, such as their inability to complete"
    },
    {
        "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models",
        "authors": "Huaixiu Steven Zheng;Swaroop Mishra;Xinyun Chen;Heng-Tze Cheng;Ed H. Chi;Quoc V Le;Denny Zhou",
        "site": "https://iclr.cc/virtual/2024/poster/19503",
        "summary": "We present STEP-BACK PROMPTING, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide reasoning, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of STEP-BACK PROMPTING with PaLM-2L, GPT-4 and Llama2-70B models, and observe substantial performance gains on various challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, STEP-BACK PROMPTING improves PaLM-2L performance on MMLU (Physics and Chemistry) by 7% and 11% respectively, TimeQA by 27%, and MuSiQue by 7%.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations, but implies that LLMs need improvement in reasoning abilities, as the paper aims to \"enable\" them to do abstractions.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations, but implies that LLMs need improvement in reasoning abilities, as the paper aims to \"enable\" them to do abstractions."
    },
    {
        "title": "Privately Aligning Language Models with Reinforcement Learning",
        "authors": "Fan Wu;Huseyin A Inan;Arturs Backurs;Varun Chandrasekaran;Janardhan Kulkarni;Robert Sim",
        "site": "https://iclr.cc/virtual/2024/poster/19501",
        "summary": "Positioned between pre-training and user deployment, aligning large language models (LLMs) through reinforcement learning (RL) has emerged as a prevailing strategy for training instruction following-models such as ChatGPT. In this work, we initiate the study of privacy-preserving alignment of LLMs through Differential Privacy (DP) in conjunction with RL. Following the influential work of Ziegler et al. (2020), we study two dominant paradigms: (i) alignment via RL without human in the loop (e.g., positive review generation) and (ii) alignment via RL from human feedback (RLHF) (e.g., summarization in a human-preferred way). We give a new DP framework to achieve alignment via RL, and prove its correctness. Our experimental results validate the effectiveness of our approach, offering competitive utility while ensuring strong privacy protections.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs, only mentions the strategy for training instruction following-models.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No evidence of discussion of limitations of LLMs, only mentions the strategy for training instruction following-models."
    },
    {
        "title": "Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy",
        "authors": "Shuhai Zhang;Yiliao Song;Jiahao Yang;Yuanqing Li;Bo Han;Mingkui Tan",
        "site": "https://iclr.cc/virtual/2024/poster/19498",
        "summary": "Large language models (LLMs) such as ChatGPT have exhibited remarkable performance in generating human-like texts. However, machine-generated texts (MGTs) may carry critical risks, such as plagiarism issues and hallucination information. Therefore, it is very urgent and important to detect MGTs in many situations. Unfortunately, it is challenging to distinguish MGTs and human-written texts because the distributional discrepancy between them is often very subtle due to the remarkable performance of LLMS. In this paper, we seek to exploit \\textit{maximum mean discrepancy} (MMD) to address this issue in the sense that MMD can well identify distributional discrepancies. However,  directly training a detector with MMD using diverse MGTs will incur a significantly increased variance of MMD since MGTs may contain \\textit{multiple text populations} due to various LLMs. This will severely impair MMD's ability to measure the difference between two samples. To tackle this, we propose a novel \\textit{multi-population} aware optimization method for MMD called MMD-MP, which can \\textit{avoid variance increases} and thus improve the stability to measure the distributional discrepancy. Relying on MMD-MP, we develop two methods for paragraph-based and sentence-based detection, respectively. Extensive experiments on various LLMs, \\eg,  GPT2 and ChatGPT, show superior detection performance of our MMD-MP.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, machine-generated texts (MGTs) may carry critical risks, such as plagiarism issues and hallucination information.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, machine-generated texts (MGTs) may carry critical risks, such as plagiarism issues and hallucination information.\""
    },
    {
        "title": "The Generalization Gap in Offline Reinforcement Learning",
        "authors": "Ishita Mediratta;Qingfei You;Minqi Jiang;Roberta Raileanu",
        "site": "https://iclr.cc/virtual/2024/poster/19490",
        "summary": "Despite recent progress in offline learning, these methods are still trained and tested on the same environment. In this paper, we compare the generalization abilities of widely used online and offline learning methods such as online reinforcement learning (RL), offline RL, sequence modeling, and behavioral cloning. Our experiments show that offline learning algorithms perform worse on new environments than online learning ones. We also introduce the first benchmark for evaluating generalization in offline learning, collecting datasets of varying sizes and skill-levels from Procgen (2D video games) and WebShop (e-commerce websites). The datasets contain trajectories for a limited number of game levels or natural language instructions and at test time, the agent has to generalize to new levels or instructions. Our experiments reveal that existing offline learning algorithms struggle to match the performance of online RL on both train and test environments. Behavioral cloning is a strong baseline, outperforming state-of-the-art offline RL and sequence modeling approaches when trained on data from multiple environments and tested on new ones. Finally, we find that increasing the diversity of the data, rather than its size, improves performance on new environments for all offline learning algorithms. Our study demonstrates the limited generalization of current offline learning algorithms highlighting the need for more research in this area.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training",
        "authors": "Hong Liu;Zhiyuan Li;David Leo Wright Hall;Percy Liang;Tengyu Ma",
        "site": "https://iclr.cc/virtual/2024/poster/19488",
        "summary": "Given the massive cost of language model pre-training, a non-trivial improvement of the optimization algorithm would lead to a material reduction on the time and cost of training. Adam and its variants have been state-of-the-art for years, and more sophisticated second-order (Hessian-based) optimizers often incur too much per-step overhead. In this paper, we propose Sophia, a simple scalable second-order optimizer that uses a light-weight estimate of the diagonal Hessian as the pre-conditioner. The update is the moving average of the gradients divided by the moving average of the estimated Hessian, followed by element-wise clipping. The clipping controls the worst-case update size and tames the negative impact of non-convexity and rapid change of Hessian along the trajectory. Sophia only estimates the diagonal Hessian every handful of iterations, which has negligible average per-step time and memory overhead. On language modeling with GPT models of sizes ranging from 125M to 1.5B, Sophia achieves a 2x speed-up compared to Adam in the number of steps, total compute, and wall-clock time, achieving the same perplexity with 50\\% fewer steps, less total compute, and reduced wall-clock time.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"more sophisticated second-order (Hessian-based) optimizers often incur too much per-step overhead.\"\n\nThis paper discusses LLMs, specifically the optimization algorithms used for pre-training, but only briefly mentions a limitation of more sophisticated second-order optimizers, which is not a direct limitation of LLMs themselves.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"more sophisticated second-order (Hessian-based) optimizers often incur too much per-step overhead.\"\n\nThis paper discusses LLMs, specifically the optimization algorithms used for pre-training, but only briefly mentions a limitation of more sophisticated second-order optimizers, which is not a direct limitation of LLMs themselves."
    },
    {
        "title": "On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes",
        "authors": "Rishabh Agarwal;Nino Vieillard;Yongchao Zhou;Piotr Stanczyk;Sabela Ramos Garea;Matthieu Geist;Olivier Bachem",
        "site": "https://iclr.cc/virtual/2024/poster/19484",
        "summary": "Knowledge distillation (KD) is widely used for compressing a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, current KD methods for auto-regressive sequence models suffer from distribution mismatch between output sequences seen during training and those generated by the student during inference. To address this issue, we introduce Generalized Knowledge Distillation (GKD). Instead of solely relying on a fixed set of output sequences, GKD trains the student on its self-generated output sequences by leveraging feedback from the teacher on such sequences. Unlike supervised KD approaches, GKD also offers the flexibility to employ alternative loss functions between the student and teacher, which can be useful when the student lacks the expressivity to mimic the teacher's distribution. Furthermore, GKD facilitates the seamless integration of distillation with RL fine-tuning (RLHF). We demonstrate the efficacy of GKD for distilling auto-regressive T5 language models on summarization, translation, and arithmetic reasoning tasks.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, current KD methods for auto-regressive sequence models suffer from distribution mismatch between output sequences seen during training and those generated by the student during inference.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, current KD methods for auto-regressive sequence models suffer from distribution mismatch between output sequences seen during training and those generated by the student during inference.\""
    },
    {
        "title": "Language Model Decoding as Direct Metrics Optimization",
        "authors": "Haozhe Ji;Pei Ke;Hongning Wang;Minlie Huang",
        "site": "https://iclr.cc/virtual/2024/poster/19478",
        "summary": "Despite the remarkable advances in language modeling, current mainstream decoding methods still struggle to generate texts that align with human texts across different aspects. In particular, sampling-based methods produce less-repetitive texts which are often disjunctive in discourse, while search-based methods maintain topic coherence at the cost of increased repetition. Overall, these methods fall short in achieving holistic alignment across a broad range of aspects. In this work, we frame decoding from a language model as an optimization problem with the goal of strictly matching the expected performance with human texts measured by multiple metrics of desired aspects simultaneously. The resulting decoding distribution enjoys an analytical solution that scales the input language model distribution via a sequence-level energy function defined by these metrics. And most importantly, we prove that this induced distribution is guaranteed to improve the perplexity on human texts, which suggests a better approximation to the underlying distribution of human texts. To facilitate tractable sampling from this globally normalized distribution, we adopt the Sampling-Importance-Resampling technique. Experiments on various domains and model scales demonstrate the superiority of our method in metrics alignment with human texts and human evaluation over strong baselines.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the remarkable advances in language modeling, current mainstream decoding methods still struggle to generate texts that align with human texts across different aspects.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite the remarkable advances in language modeling, current mainstream decoding methods still struggle to generate texts that align with human texts across different aspects.\""
    },
    {
        "title": "LCOT: Linear Circular Optimal Transport",
        "authors": "Rocio P Diaz Martin;Ivan Vladimir Medri;Yikun Bai;Xinran Liu;Kangbai Yan;Gustavo Rohde;Soheil Kolouri",
        "site": "https://iclr.cc/virtual/2024/poster/19477",
        "summary": "The optimal transport problem for measures supported on non-Euclidean spaces has recently gained ample interest in diverse applications involving representation learning. In this paper, we focus on circular probability measures, i.e., probability measures supported on the unit circle, and introduce a new computationally efficient metric for these measures, denoted as Linear Circular Optimal Transport (LCOT). The proposed metric comes with an explicit linear embedding that allows one to apply Machine Learning (ML) algorithms to the embedded measures and seamlessly modify the underlying metric for the ML algorithm to LCOT. We show that the proposed metric is rooted in the Circular Optimal Transport (COT) and can be considered the linearization of the COT metric with respect to a fixed reference measure. We provide a theoretical analysis of the proposed metric and derive the computational complexities for pairwise comparison of circular probability measures. Lastly, through a set of numerical experiments, we demonstrate the benefits of LCOT in learning representations from circular measures.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Curiosity-driven Red-teaming for Large Language Models",
        "authors": "Zhang-Wei Hong;Idan Shenfeld;Tsun-Hsuan Wang;Yung-Sung Chuang;Aldo Pareja;James R. Glass;Akash Srivastava;Pulkit Agrawal",
        "site": "https://iclr.cc/virtual/2024/poster/19471",
        "summary": "Abstract:Large language models (LLMs) hold great potential for many natural language applications but risk generating incorrect or toxic content. To probe when an LLM generates unwanted content, the current paradigm is to recruit a $\\textit{red team}$ of human testers to design input prompts (i.e., test cases) that elicit undesirable responses from LLMs. However, relying solely on human testers is expensive and time-consuming. Recent works automate red teaming by training a separate red team LLM with reinforcement learning (RL) to generate test cases that maximize the chance of eliciting undesirable responses from the target LLM. However, current RL methods are only able to generate a small number of effective test cases resulting in a low coverage of the span of prompts that elicit undesirable responses from the target LLM.To overcome this limitation, we draw a connection between the problem of increasing the coverage of generated test cases and the well-studied approach of curiosity-driven exploration that optimizes for novelty. Our method of curiosity-driven red teaming (CRT) achieves greater coverage of test cases while mantaining or increasing their effectiveness compared to existing methods.Our method, CRT successfully provokes toxic responses from LLaMA2 model that has been heavily fine-tuned using human preferences to avoid toxic outputs. Code is available at https://github.com/Improbable-AI/curiosity_redteam.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, relying solely on human testers is expensive and time-consuming....However, current RL methods are only able to generate a small number of effective test cases resulting in a low coverage of the span of prompts that elicit undesirable responses from the target LLM.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, relying solely on human testers is expensive and time-consuming....However, current RL methods are only able to generate a small number of effective test cases resulting in a low coverage of the span of prompts that elicit undesirable responses from the target LLM.\""
    },
    {
        "title": "Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding",
        "authors": "Zilong Wang;Hao Zhang;Chun-Liang Li;Julian Martin Eisenschlos;Vincent Perot;Zifeng Wang;Lesly Miculicich;Yasuhisa Fujii;Jingbo Shang;Chen-Yu Lee;Tomas Pfister",
        "site": "https://iclr.cc/virtual/2024/poster/19470",
        "summary": "Table-based reasoning with large language models (LLMs) is a promising direction to tackle many table understanding tasks, such as table-based question answering and fact verification. Compared with generic reasoning, table-based reasoning requires the extraction of underlying semantics from both free-form questions and semi-structured tabular data. Chain-of-Thought and its similar approaches incorporate the reasoning chain in the form of textual context, but it is still an open question how to effectively leverage tabular data in the reasoning chain. We propose the Chain-of-Table framework, where tabular data is explicitly used in the reasoning chain as a proxy for intermediate thoughts. Specifically, we guide LLMs using in-context learning to iteratively generate operations and update the table to represent a tabular reasoning chain. LLMs can therefore dynamically plan the next operation based on the results of the previous ones. This continuous evolution of the table forms a chain, showing the reasoning process for a given tabular problem. The chain carries structured information of the intermediate results, enabling more accurate and reliable predictions. Chain-of-Table achieves new state-of-the-art performance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM choices.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"it is still an open question how to effectively leverage tabular data in the reasoning chain.\"\n\nThis abstract mentions a limitation of LLMs in table-based reasoning, but it is a minor detail and the focus of the paper is on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"it is still an open question how to effectively leverage tabular data in the reasoning chain.\"\n\nThis abstract mentions a limitation of LLMs in table-based reasoning, but it is a minor detail and the focus of the paper is on the proposed solution."
    },
    {
        "title": "Llemma: An Open Language Model for Mathematics",
        "authors": "Zhangir Azerbayev;Hailey Schoelkopf;Keiran Paster;Marco Dos Santos;Stephen Marcus McAleer;Albert Q. Jiang;Jia Deng;Stella Biderman;Sean Welleck",
        "site": "https://iclr.cc/virtual/2024/poster/19459",
        "summary": "We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known openly released models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations, but the presentation of a new model and its performance suggests addressing limitations of previous models, e.g., \"outperforms all known openly released models\".",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations, but the presentation of a new model and its performance suggests addressing limitations of previous models, e.g., \"outperforms all known openly released models\"."
    },
    {
        "title": "EMO: EARTH MOVER DISTANCE OPTIMIZATION FOR AUTO-REGRESSIVE LANGUAGE MODELING",
        "authors": "Siyu Ren;Zhiyong Wu;Kenny Q. Zhu",
        "site": "https://iclr.cc/virtual/2024/poster/19455",
        "summary": "Neural language models are probabilistic models of human text. They are predominantly trained using maximum likelihood estimation (MLE), which is equivalent to minimizing the forward cross-entropy between the empirical data distribution and the model distribution. However, various degeneration phenomena are still widely observed when decoding from the distributions learned by such models. We establish that the forward cross-entropy is suboptimal as a distance metric for aligning human and model distribution due to its (1) recall-prioritization (2) negative diversity ignorance and (3) train-test mismatch. In this paper, we propose Earth Mover Distance Optimization (EMO) for auto-regressive language modeling. EMO capitalizes on the inherent properties of earth mover distance to address the aforementioned challenges. Due to the high complexity of direct computation, we further introduce a feasible upper bound for EMO to ease end-to-end training. Upon extensive evaluation of language models trained using EMO and MLE. We find that EMO demonstrates a consistently better language modeling performance than MLE across domains. Moreover, EMO demonstrates noteworthy enhancements in downstream performance with minimal fine-tuning on merely 25,000 sentences. This highlights the tremendous potential of EMO as a lightweight calibration method for enhancing large-scale pre-trained language models.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, various degeneration phenomena are still widely observed when decoding from the distributions learned by such models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, various degeneration phenomena are still widely observed when decoding from the distributions learned by such models.\""
    },
    {
        "title": "Language Model Detectors Are Easily Optimized Against",
        "authors": "Charlotte Nicks;Eric Mitchell;Rafael Rafailov;Archit Sharma;Christopher D Manning;Chelsea Finn;Stefano Ermon",
        "site": "https://iclr.cc/virtual/2024/poster/19453",
        "summary": "The fluency and general applicability of large language models (LLMs) has motivated significant interest in detecting whether a piece of text was written by a language model. While both academic and commercial detectors have been deployed in some settings, particularly education, other research has highlighted the fragility of these systems. In this paper, we demonstrate a data-efficient attack that fine-tunes language models to confuse existing detectors, leveraging recent developments in reinforcement learning of language models. We use the `human-ness' score (often just a log probability) of various open-source and commercial detectors as a reward function for reinforcement learning, subject to a KL-divergence constraint that the resulting model does not differ significantly from the original. For a 7B parameter Llama-2 model, fine-tuning for under a day reduces the AUROC of the OpenAI RoBERTa-Large detector from 0.84 to 0.63, while perplexity on OpenWebText increases from 8.7 to only 9.0; with a larger perplexity budget, we can drive AUROC to 0.30 (worse than random). Similar to traditional adversarial attacks, we find that this increase in 'detector evasion' generalizes to other detectors not used during training. In light of our empirical results, we advise against continued reliance on LLM-generated text detectors. Models, datasets, and selected experiment code will be released at https://github.com/charlottttee/llm-detector-evasion.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While both academic and commercial detectors have been deployed in some settings, particularly education, other research has highlighted the fragility of these systems.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"While both academic and commercial detectors have been deployed in some settings, particularly education, other research has highlighted the fragility of these systems.\""
    },
    {
        "title": "The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry",
        "authors": "Michael Zhang;Kush Bhatia;Hermann Kumbong;Christopher Re",
        "site": "https://iclr.cc/virtual/2024/poster/19452",
        "summary": "Linear attentions have shown promise for improving Transformer efficiency, reducing attention's quadratic complexity to linear in sequence length. This holds exciting promise for (1) training linear Transformers from scratch, (2) `inetuned-conversion of task-specific Transformers into linear versions that recover task performance, and (3) pretrained-conversion of Transformers, such as language models, into linear versions readily finetunable on downstream tasks. However, linear attentions often underperform compared to standard softmax attention. To close this performance gap, we study the behaviors of softmax and linear attentions in various train-from-scratch and finetuned-conversion settings. We find prior linear attentions lack key properties of softmax attention tied to good performance: low-entropy (or spiky) weights and dot-product monotonicity. We further observe surprisingly simple feature maps that retain these properties match softmax performance, but are inefficient to compute in linear attention. We thus propose Hedgehog, a learnable linear attention that retains the spiky and monotonic properties of softmax attention while maintaining linear complexity. Hedgehog uses simple, trainable MLPs to produce attention weights mimicking softmax attention. Experiments show Hedgehog recovers over 99\\% of standard Transformer performance in train-from-scratch and finetuned-conversion settings, outperforming prior linear attentions by up to 6 perplexity points on WikiText-103 when training causal GPT models from scratch, and up to 8.7 GLUE score points when converting finetuned bidirectional BERT models. Hedgehog also enables pretrained-conversion. Converting a pretrained GPT-2 into a linear attention variant achieves state-of-the-art 16.7 perplexity on WikiText-103 for 125M subquadratic decoder models. We finally turn a pretrained Llama-2 7B into a viable linear attention Llama. With low-rank adaptation, Hedgehog-Llama-2 7B achieves 28.1 higher ROUGE-1 points over the base standard attention model, where prior linear attentions lead to 16.5 point drops.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, linear attentions often underperform compared to standard softmax attention.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, linear attentions often underperform compared to standard softmax attention.\""
    },
    {
        "title": "Pessimistic Nonlinear Least-Squares Value Iteration for Offline Reinforcement Learning",
        "authors": "Qiwei Di;Heyang Zhao;Jiafan He;Quanquan Gu",
        "site": "https://iclr.cc/virtual/2024/poster/19449",
        "summary": "Offline reinforcement learning (RL), where the agent aims to learn the optimal policy based on the data collected by a behavior policy, has attracted increasing attention in recent years. While offline RL with linear function approximation has been extensively studied with optimal results achieved under certain assumptions, many works shift their interest to offline RL with non-linear function approximation.However, limited works on offline RL with non-linear function approximation have instance-dependent regret guarantees.    In this paper, we propose an oracle-efficient algorithm, dubbed Pessimistic Nonlinear Least-Square Value Iteration (PNLSVI), for offline RL with non-linear function approximation. Our algorithmic design comprises three innovative components: (1) a variance-based weighted regression scheme that can be applied to a wide range of function classes, (2) a subroutine for variance estimation, and (3) a planning phase that utilizes a pessimistic value iteration approach. Our algorithm enjoys a regret bound that has a tight dependency on the function class complexity and achieves minimax optimal instance-dependent regret when specialized to linear function approximation. Our work extends the previous instance-dependent results within simpler function classes, such as linear and differentiable function to a more general framework. To the best of our knowledge, this is the first statistically optimal algorithm for nonlinear offline RL.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Habitat 3.0: A Co-Habitat for Humans, Avatars, and Robots",
        "authors": "Xavier Puig;Eric Undersander;Andrew Szot;Mikael Dallaire Cote;Tsung-Yen Yang;Ruslan Partsey;Ruta Desai;Alexander Clegg;Michal Hlavac;So Yeon Min;Vladimír Vondruš;Theophile Gervet;Vincent-Pierre Berges;John M Turner;Oleksandr Maksymets;Zsolt Kira;Mrinal Kalakrishnan;Jitendra Malik;Devendra Singh Chaplot;Unnat Jain;Dhruv Batra;Akshara Rai;Roozbeh Mottaghi",
        "site": "https://iclr.cc/virtual/2024/poster/19442",
        "summary": "We present Habitat 3.0: a simulation platform for studying collaborative human-robot tasks in home environments. Habitat 3.0 offers contributions across three dimensions: (1) Accurate humanoid simulation: addressing challenges in modeling complex deformable bodies and diversity in appearance and motion, all while ensuring high simulation speed. (2) Human-in-the-loop infrastructure: enabling real human interaction with simulated robots via mouse/keyboard or a VR interface, facilitating evaluation of robot policies with human input. (3) Collaborative tasks: studying two collaborative tasks, Social Navigation and Social Rearrangement. Social Navigation investigates a robot's ability to locate and follow humanoid avatars in unseen environments, whereas Social Rearrangement addresses collaboration between a humanoid and robot while rearranging a scene. These contributions allow us to study end-to-end learned and heuristic baselines for human-robot collaboration in-depth, as well as evaluate them with humans in the loop. Our experiments demonstrate that learned robot policies lead to efficient task completion when collaborating with unseen humanoid agents and human partners that might exhibit behaviors that the robot has not seen before. Additionally, we observe emergent behaviors during collaborative task execution, such as the robot yielding space when obstructing a humanoid agent, thereby allowing the effective completion of the task by the humanoid agent. Furthermore, our experiments using the human-in-the-loop tool demonstrate that our automated evaluation with humanoids can provide an indication of the relative ordering of different policies when evaluated with real human collaborators. Habitat 3.0 unlocks interesting new features in simulators for Embodied AI, and we hope it paves the way for a new frontier of embodied human-AI interaction capabilities. For more details and visualizations, visit: https://aihabitat.org/habitat3.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions",
        "authors": "Lorenzo Pacchiardi;Alex James Chan;Sören Mindermann;Ilan Moscovitz;Alexa Yue Pan;Yarin Gal;Owain Evans;Jan M. Brauner",
        "site": "https://iclr.cc/virtual/2024/poster/19439",
        "summary": "Large language models (LLMs) can “lie”, which we define as outputting false statements when incentivised to, despite “knowing” the truth in a demonstrable sense. LLMs might “lie”, for example, when instructed to output misinformation. Here, we develop a simple lie detector that requires neither access to the LLM’s activations (black-box) nor ground-truth knowledge of the fact in question. The detector works by asking a predefined set of unrelated follow-up questions after a suspected lie, and feeding the LLM’s yes/no answers into a logistic regression classifier. Despite its simplicity, this lie detector is highly accurate and surprisingly general. When trained on examples from a single setting—prompting GPT-3.5 to lie about factual questions—the detector generalises out-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie, (3) sycophantic lies, and (4) lies emerging in real-life scenarios such as sales. These results indicate that LLMs have distinctive lie-related behavioural patterns, consistent across architectures and contexts, which could enable general-purpose lie detection.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"LLMs can “lie”, which we define as outputting false statements when incentivised to, despite “knowing” the truth in a demonstrable sense.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"LLMs can “lie”, which we define as outputting false statements when incentivised to, despite “knowing” the truth in a demonstrable sense.\""
    },
    {
        "title": "When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method",
        "authors": "Biao Zhang;Zhongtao Liu;Colin Cherry;Orhan Firat",
        "site": "https://iclr.cc/virtual/2024/poster/19432",
        "summary": "While large language models (LLMs) often adopt finetuning to unlock their capabilities for downstream applications, our understanding on the inductive biases (especially the scaling properties) of different finetuning methods is still limited. To fill this gap, we conduct systematic experiments studying whether and how different scaling factors, including LLM model size, pretraining data size, new finetuning parameter size and finetuning data size, affect the finetuning performance. We consider two types of finetuning – full-model tuning (FMT) and parameter efficient tuning (PET, including prompt tuning and LoRA), and explore their scaling behaviors in the data-limited regime where the LLM model size substantially outweighs the finetuning data size. Based on two sets of pretrained bilingual LLMs from 1B to 16B and experiments on bilingual machine translation and multilingual summarization benchmarks, we find that 1) LLM finetuning follows a powerbased multiplicative joint scaling law between finetuning data size and each other scaling factor; 2) LLM finetuning benefits more from LLM model scaling than pretraining data scaling, and PET parameter scaling is generally ineffective; and 3) the optimal finetuning method is highly task- and finetuning data-dependent. We hope our findings could shed light on understanding, selecting and developing LLM finetuning methods.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"our understanding on the inductive biases (especially the scaling properties) of different finetuning methods is still limited\"; \"PET parameter scaling is generally ineffective\"; \"the optimal finetuning method is highly task- and finetuning data-dependent.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"our understanding on the inductive biases (especially the scaling properties) of different finetuning methods is still limited\"; \"PET parameter scaling is generally ineffective\"; \"the optimal finetuning method is highly task- and finetuning data-dependent.\""
    },
    {
        "title": "MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning",
        "authors": "Haozhe Zhao;Zefan Cai;Shuzheng Si;Xiaojian Ma;Kaikai An;Liang Chen;Zixuan Liu;Sheng Wang;Wenjuan Han;Baobao Chang",
        "site": "https://iclr.cc/virtual/2024/poster/19429",
        "summary": "Since the resurgence of deep learning, vision-language models (VLMs) enhanced by large language models (LLMs) have grown exponentially in popularity. However, while LLMs can utilize extensive background knowledge and task information with in-context learning, most VLMs still struggle with understanding complex multi-modal prompts with multiple images, making VLMs less effective in downstream vision-language tasks.In this paper, we address the limitation above by 1) introducing vision-language Model withMulti-ModalIn-ContextLearning(MMICL), a new approach to allow the VLM to deal with multi-modal inputs efficiently; 2) proposing a novel context scheme to augment the in-context learning ability of the VLM; 3) constructing the Multi-modal In-Context Learning (MIC) dataset, designed to enhance the VLM's ability to understand complex multi-modal prompts.Our experiments confirm that MMICL achieves new state-of-the-art zero-shot performance on a wide range of general vision-language tasks, especially for complex benchmarks, including MME and MMBench. Our analysis demonstrates that MMICL effectively tackles the challenge of complex multi-modal prompt understanding and emerges the impressive ICL ability. Furthermore, we observe that MMICL successfully alleviates language bias in VLMs, a common issue for VLMs that often leads to hallucination when faced with extensive textual context.Our code, dataset, dataset tool, and model are available at https://github.com/PKUnlp-icler/MIC.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, while LLMs can utilize extensive background knowledge and task information with in-context learning, most VLMs still struggle with understanding complex multi-modal prompts with multiple images, making VLMs less effective in downstream vision-language tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, while LLMs can utilize extensive background knowledge and task information with in-context learning, most VLMs still struggle with understanding complex multi-modal prompts with multiple images, making VLMs less effective in downstream vision-language tasks.\""
    },
    {
        "title": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization",
        "authors": "Yidong Wang;Zhuohao Yu;Wenjin Yao;Zhengran Zeng;Linyi Yang;Cunxiang Wang;Hao Chen;Chaoya Jiang;Rui Xie;Jindong Wang;Xing Xie;Wei Ye;Shikun Zhang;Yue Zhang",
        "site": "https://iclr.cc/virtual/2024/poster/19427",
        "summary": "Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models. To determine the optimal hyperparameters, an automatic, robust, and reliable evaluation benchmark is essential. However, establishing such a benchmark is not a trivial task due to the challenges associated with evaluation accuracy and privacy protection. In response to these challenges, we introduce a judge large language model, named PandaLM, which is trained to distinguish the superior model given several LLMs. PandaLM's focus extends beyond just the objective correctness of responses, which is the main focus of traditional evaluation datasets. It addresses vital subjective factors such as relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality. To ensure the reliability of PandaLM, we collect a diverse human-annotated test dataset, where all contexts are generated by humans and labels are aligned with human preferences. Our findings reveal that PandaLM-7B offers a performance comparable to both GPT-3.5 and GPT-4. Impressively, PandaLM-70B surpasses their performance. PandaLM enables the evaluation of LLM to be fairer but with less cost, evidenced by significant improvements achieved by models tuned through PandaLM compared to their counterparts trained with default Alpaca's hyperparameters. In addition, PandaLM does not depend on API-based evaluations, thus avoiding potential data leakage.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models.\""
    },
    {
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": "Yuxian Gu;Li Dong;Furu Wei;Minlie Huang",
        "site": "https://iclr.cc/virtual/2024/poster/19420",
        "summary": "Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge of white-box LLMs into small models is still under-explored, which becomes more important with the prosperity of open-source LLMs. In this work, we propose a KD approach that distills LLMs into smaller language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective optimization approach to learn this objective. The student models are named MiniLLM. Extensive experiments in the instruction-following setting show that MiniLLM generates more precise responses with higher overall quality, lower exposure bias, better calibration, and higher long-text generation performance than the baselines. Our method is scalable for different model familieswith 120M to 13B parameters. Our code, data, and model checkpoints can be found in https://github.com/microsoft/LMOps/tree/main/minillm.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge of white-box LLMs into small models is still under-explored, which becomes more important with the prosperity of open-source LLMs.\"\n\nThis paper mentions a limitation of previous knowledge distillation",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge of white-box LLMs into small models is still under-explored, which becomes more important with the prosperity of open-source LLMs.\"\n\nThis paper mentions a limitation of previous knowledge distillation"
    },
    {
        "title": "INViTE: INterpret and Control Vision-Language Models with Text Explanations",
        "authors": "Haozhe Chen;Junfeng Yang;Carl Vondrick;Chengzhi Mao",
        "site": "https://iclr.cc/virtual/2024/poster/19418",
        "summary": "Large-scale pre-trained vision foundation models, such as CLIP, have become de facto backbones for various vision tasks. However, due to their black-box nature, understanding the underlying rules behind these models’ predictions and controlling model behaviors have remained open challenges. We present INViTE: a framework for INterpreting Vision Transformer’s latent tokens with Text Explanations. Given a latent token, INViTE retains its semantic information to the final layer using transformer’s local operations and retrieves the closest text for explanation. INViTE enables understanding of model visual reasoning procedure without needing additional model training or data collection. Based on the obtained interpretations, INViTE allows for model editing that controls model reasoning behaviors and improves model robustness against biases and spurious correlations. Our code is available at https://github.com/tonychenxyz/vit-interpret.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, due to their black-box nature, understanding the underlying rules behind these models’ predictions and controlling model behaviors have remained open challenges.\"\n\nNote that the paper does not specifically discuss limitations of LLMs, but rather the limitations of large-scale pre-trained vision foundation models, which can be considered as a part of multimodal models integrating language with other modalities.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, due to their black-box nature, understanding the underlying rules behind these models’ predictions and controlling model behaviors have remained open challenges.\"\n\nNote that the paper does not specifically discuss limitations of LLMs, but rather the limitations of large-scale pre-trained vision foundation models, which can be considered as a part of multimodal models integrating language with other modalities."
    },
    {
        "title": "Jointly Training Large Autoregressive Multimodal Models",
        "authors": "Emanuele Aiello;LILI YU;Yixin Nie;Armen Aghajanyan;Barlas Oguz",
        "site": "https://iclr.cc/virtual/2024/poster/19416",
        "summary": "In recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models. We also introduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned model demonstrates unparalleled performance in generating high-quality multimodal outputs and represents the first model explicitly designed for this purpose.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge.\"\n\nThis paper talks about LLMs, but only mentions one limitation in passing, which is the challenge of integrating language and image models into a single robust model. The primary focus of the paper is on the proposed solution, the Joint Autoregressive Mixture",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge.\"\n\nThis paper talks about LLMs, but only mentions one limitation in passing, which is the challenge of integrating language and image models into a single robust model. The primary focus of the paper is on the proposed solution, the Joint Autoregressive Mixture"
    },
    {
        "title": "Whittle Index with Multiple Actions and State Constraint for Inventory Management",
        "authors": "Chuheng Zhang;Xiangsen Wang;Wei Jiang;Xianliang Yang;Siwei Wang;Lei Song;Jiang Bian",
        "site": "https://iclr.cc/virtual/2024/poster/19412",
        "summary": "Whittle index is a heuristic tool that leads to good performance for the restless bandits problem. In this paper, we extend Whittle index to a new multi-agent reinforcement learning (MARL) setting with multiple discrete actions and a possibly changing constraint on the state space, resulting in WIMS (Whittle Index with Multiple actions and State constraint). This setting is common for inventory management where each agent chooses a replenishing quantity level for the corresponding stock-keeping-unit (SKU) such that the total profit is maximized while the total inventory does not exceed a certain limit. Accordingly, we propose a deep MARL algorithm based on WIMS for inventory management. Empirically, our algorithm is evaluated on real large-scale inventory management problems with up to 2307 SKUs and outperforms operation-research-based methods and baseline MARL algorithms.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs."
    },
    {
        "title": "Prompt Risk Control: A Rigorous Framework for Responsible Deployment of Large Language Models",
        "authors": "Thomas P Zollo;Todd Morrill;Zhun Deng;Jake Snell;Toniann Pitassi;Richard Zemel",
        "site": "https://iclr.cc/virtual/2024/poster/19408",
        "summary": "With the explosion of the zero-shot capabilities of (and thus interest in) pre-trained large language models, there has come accompanying interest in how best to prompt a language model to perform a given task. While it may be tempting to choose a prompt based on empirical results on a validation set, this can lead to a deployment where an unexpectedly high loss occurs. To mitigate this prospect, we propose a lightweight framework, Prompt Risk Control, for selecting a prompt based on rigorous upper bounds on families of informative risk measures. We provide and compare different methods for producing bounds on a diverse set of risk metrics like mean, CVaR, and the Gini coefficient of the loss distribution. In addition, we extend the underlying statistical bounding techniques to accommodate the possibility of distribution shifts in deployment. Extensive experiments on high-impact applications like chatbots, medical question answering, and news summarization highlight why such a framework is necessary to reduce exposure to the worst outcomes.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While it may be tempting to choose a prompt based on empirical results on a validation set, this can lead to a deployment where an unexpectedly high loss occurs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While it may be tempting to choose a prompt based on empirical results on a validation set, this can lead to a deployment where an unexpectedly high loss occurs.\""
    },
    {
        "title": "In-Context Learning Dynamics with Random Binary Sequences",
        "authors": "Eric J Bigelow;Ekdeep Singh Lubana;Robert P. Dick;Hidenori Tanaka;Tomer Ullman",
        "site": "https://iclr.cc/virtual/2024/poster/19405",
        "summary": "Large language models (LLMs) trained on huge text datasets demonstrate intriguing capabilities, achieving state-of-the-art performance on tasks they were not explicitly trained for. The precise nature of LLM capabilities is often mysterious, and different prompts can elicit different capabilities through in-context learning. We propose a framework that enables us to analyze in-context learning dynamics to understand latent concepts underlying LLMs’ behavioral patterns. This provides a more nuanced understanding than success-or-failure evaluation benchmarks, but does not require observing internal activations as a mechanistic interpretation of circuits would. Inspired by the cognitive science of human randomness perception, we use random binary sequences as context and study dynamics of in-context learning by manipulating properties of context data, such as sequence length. In the latest GPT-3.5+ models, we find emergent abilities to generate seemingly random numbers and learn basic formal languages, with striking in-context learning dynamics where model outputs transition sharply from seemingly random behaviors to deterministic repetition.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The precise nature of LLM capabilities is often mysterious, and different prompts can elicit different capabilities through in-context learning.\"\n\nThis rating is chosen because the abstract mentions the mysterious nature of LLM capabilities and the fact that different prompts can elicit different capabilities, which can be seen as a limitation in understanding and predicting LLM behavior. However, this limitation is not the primary focus of",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The precise nature of LLM capabilities is often mysterious, and different prompts can elicit different capabilities through in-context learning.\"\n\nThis rating is chosen because the abstract mentions the mysterious nature of LLM capabilities and the fact that different prompts can elicit different capabilities, which can be seen as a limitation in understanding and predicting LLM behavior. However, this limitation is not the primary focus of"
    },
    {
        "title": "Compressed Context Memory for Online Language Model Interaction",
        "authors": "Jang-Hyun Kim;Junyoung Yeom;Sangdoo Yun;Hyun Oh Song",
        "site": "https://iclr.cc/virtual/2024/poster/19404",
        "summary": "Abstract:This paper presents a context key/value compression method for Transformer language models in online scenarios, where the context continually expands. As the context lengthens, the attention process demands increasing memory and computations, which in turn reduces the throughput of the language model. To address this challenge, we propose a compressed context memory system that continually compresses the accumulating attention key/value pairs into a compact memory space, facilitating language model inference in a limited memory space of computing environments. Our compression process involves integrating a lightweight conditional LoRA into the language model's forward pass during inference, without the need for fine-tuning the model's entire set of weights. We achieve efficient training by modeling the recursive compression process as a single parallelized forward computation. Through evaluations on conversation, personalization, and multi-task learning, we demonstrate that our approach achieves the performance level of a full context model with $5\\times$ smaller context memory size. We further demonstrate the applicability of our approach in a streaming setting with an unlimited context length, outperforming the sliding window approach. Codes are available at https://github.com/snu-mllab/context-memory.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"As the context lengthens, the attention process demands increasing memory and computations, which in turn reduces the throughput of the language model.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"As the context lengthens, the attention process demands increasing memory and computations, which in turn reduces the throughput of the language model.\""
    },
    {
        "title": "Personalize Segment Anything Model with One Shot",
        "authors": "Renrui Zhang;Zhengkai Jiang;Ziyu Guo;Shilin Yan;Junting Pan;Hao Dong;Yu Qiao;Peng Gao;Hongsheng Li",
        "site": "https://iclr.cc/virtual/2024/poster/19398",
        "summary": "Driven by large-data pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful promptable framework, revolutionizing the segmentation field. Despite the generality, customizing SAM for specific visual concepts without man-powered prompting is under-explored, e.g., automatically segmenting your pet dog in numerous images. In this paper, we introduce a training-free Personalization approach for SAM, termed PerSAM. Given only one-shot data, i.e., a single image with a reference mask, we first obtain a positive-negative location prior for the target concept in new images. Then, aided by target visual semantics, we empower SAM for personalized object segmentation via two proposed techniques: target-guided attention and target-semantic prompting. In this way, we can effectively customize the general-purpose SAM for private use without any training. To further alleviate the ambiguity of segmentation scales, we present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we introduce a scale-aware fine-tuning to aggregate multi-scale masks, which only tunes 2 parameters within 10 seconds for improved performance. To demonstrate our efficacy, we construct a new dataset, PerSeg, for the evaluation of personalized object segmentation, and also test our methods on various one-shot image and video segmentation benchmarks. Besides, we propose to leverage PerSAM to improve DreamBooth for personalized text-to-image synthesis. By mitigating the disturbance of training-set backgrounds, our approach showcases better target appearance generation and higher fidelity to the input text prompt. Code is released at https://github.com/ZrrSkywalker/Personalize-SAM.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "IceFormer: Accelerated Inference with Long-Sequence Transformers on CPUs",
        "authors": "Yuzhen Mao;Martin Ester;Ke Li",
        "site": "https://iclr.cc/virtual/2024/poster/19389",
        "summary": "Abstract:One limitation of existing Transformer-based models is that they cannot handle very long sequences as input since their self-attention operations exhibit quadratic time and space complexity. This problem becomes especially acute when Transformers are deployed on hardware platforms equipped only with CPUs. To address this issue, we propose a novel method for accelerating self-attention at inference time that works with pretrained Transformer models out-of-the-box without requiring retraining. We experiment using our method to accelerate various long-sequence Transformers, including a leading LLaMA 2-based LLM, on various benchmarks and demonstrate a greater speedup of $2.73\\times$ - $7.63\\times$ while retaining $98.6$% - $99.6$% of the accuracy of the original pretrained models. The code is available on our project website at https://yuzhenmao.github.io/IceFormer/.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"One limitation of existing Transformer-based models is that they cannot handle very long sequences as input since their self-attention operations exhibit quadratic time and space complexity.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"One limitation of existing Transformer-based models is that they cannot handle very long sequences as input since their self-attention operations exhibit quadratic time and space complexity.\""
    },
    {
        "title": "Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models",
        "authors": "Zhaowei Zhu;Jialu Wang;Hao Cheng;Yang Liu",
        "site": "https://iclr.cc/virtual/2024/poster/19388",
        "summary": "Language models have shown promise in various tasks but can be affected by undesired data during training, fine-tuning, or alignment. For example, if some unsafe conversations are wrongly annotated as safe ones, the model fine-tuned on these samples may be harmful. Therefore, the correctness of annotations, i.e., the credibility of the dataset, is important. This study focuses on the credibility of real-world datasets, including the popular benchmarks Jigsaw Civil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that can be used for training a harmless language model. Given the cost and difficulty of cleaning these datasets by humans, we introduce a systematic framework for evaluating the credibility of datasets, identifying label errors, and evaluating the influence of noisy labels in the curated language data, specifically focusing on unsafe comments and conversation classification. With the framework, we find and fix an average of6.16\\%label errors in11datasets constructed from the above benchmarks. The data credibility and downstream learning performance can be remarkably improved by directly fixing label errors, indicating the significance of cleaning existing real-world datasets. Code is available athttps://github.com/Docta-ai/docta.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Language models have shown promise in various tasks but can be affected by undesired data during training, fine-tuning, or alignment.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Language models have shown promise in various tasks but can be affected by undesired data during training, fine-tuning, or alignment.\""
    },
    {
        "title": "Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models",
        "authors": "Sheng Shen;Le Hou;Yanqi Zhou;Nan Du;Shayne Longpre;Jason Wei;Hyung Won Chung;Barret Zoph;William Fedus;Xinyun Chen;Tu Vu;Yuexin Wu;Wuyang Chen;Albert Webson;Yunxuan Li;Vincent Y Zhao;Hongkun Yu;Kurt Keutzer;Trevor Darrell;Denny Zhou",
        "site": "https://iclr.cc/virtual/2024/poster/19384",
        "summary": "Sparse Mixture-of-Experts (MoE) is a neural architecture design that adds learnable parameters to Large Language Models (LLMs) without increasing computational complexity (FLOPs). Instruction tuning is a technique for training LLMs to follow instructions. We advocate combining these two approaches, as we find that MoE models benefit more from instruction tuning than dense models. In particular, we conduct empirical studies across three experimental setups: (i) Direct finetuning on individual downstream tasks devoid of instruction tuning; (ii) Instruction tuning followed by in-context few-shot or zero-shot generalization on downstream tasks; and (iii) Instruction tuning supplemented by further finetuning on individual downstream tasks. In the first scenario, MoE models overall underperform dense models of identical computational capacity. This narrative, however, dramatically changes with the introduction of instruction tuning (in the second and third scenarios), used independently or in conjunction with task-specific finetuning. Our most powerful model, FLAN-MoE-32B, surpasses the performance of Flan-PaLM-62B on four benchmark tasks, while using only a third of the FLOPs. The advancements embodied by FLAN-MoE inspire a reevaluation of the design principles of large-scale, high-performance language models in the framework of task-agnostic learning.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"In the first scenario, MoE models overall underperform dense models of identical computational capacity.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"In the first scenario, MoE models overall underperform dense models of identical computational capacity.\""
    },
    {
        "title": "Large Language Model Cascades with Mixture of Thought Representations for Cost-Efficient Reasoning",
        "authors": "Murong Yue;Jie Zhao;Min Zhang;Liang Du;Ziyu Yao",
        "site": "https://iclr.cc/virtual/2024/poster/19383",
        "summary": "Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM \"cascade\" to save the cost of using LLMs, particularly for performing (e.g., mathematical, causal) reasoning tasks. Our cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the most challenging questions necessitate the stronger and more expensive LLM. To realize this decision-making, we consider the \"answer consistency\" of the weaker LLM as a signal of the question difficulty and propose several methods for answering sampling and consistency checking, including one leveraging a mixture of two thought representations (i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and stronger LLMs, respectively, our cascade pipeline demonstrates comparable performance but reduces about 60% of the cost compared with fully using the stronger LLM.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"but this strong performance often comes with the high expense of using paid API services.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"but this strong performance often comes with the high expense of using paid API services.\""
    },
    {
        "title": "A Semantic Invariant Robust Watermark for Large Language Models",
        "authors": "Aiwei Liu;Leyi Pan;Xuming Hu;Shiao Meng;Lijie Wen",
        "site": "https://iclr.cc/virtual/2024/poster/19382",
        "summary": "Watermark algorithms for large language models (LLMs) have achieved extremely high accuracy in detecting text generated by LLMs. Such algorithms typically involve adding extra watermark logits to the LLM's logits at each generation step. However, prior algorithms face a trade-off between attack robustness and security robustness. This is because the watermark logits for a token are determined by a certain number of preceding tokens; a small number leads to low security robustness, while a large number results in insufficient attack robustness. In this work, we propose a semantic invariant watermarking method for LLMs that provides both attack robustness and security robustness. The watermark logits in our work are determined by the semantics of all preceding tokens. Specifically, we utilize another embedding LLM to generate semantic embeddings for all preceding tokens, and then these semantic embeddings are transformed into the watermark logits through our trained watermark model.Subsequent analyses and experiments demonstrated the attack robustness of our method in semantically invariant settings: synonym substitution and text paraphrasing settings. Finally, we also show that our watermark possesses adequate security robustness.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, prior algorithms face a trade-off between attack robustness and security robustness.\"\n\nNote that the limitation mentioned is related to prior watermarking algorithms for LLMs, not the LLMs themselves.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, prior algorithms face a trade-off between attack robustness and security robustness.\"\n\nNote that the limitation mentioned is related to prior watermarking algorithms for LLMs, not the LLMs themselves."
    },
    {
        "title": "Chain of Hindsight aligns Language Models with Feedback",
        "authors": "Hao Liu;Carmelo Sferrazza;Pieter Abbeel",
        "site": "https://iclr.cc/virtual/2024/poster/19378",
        "summary": "Learning from human preferences is important for language models to match human needs and to align with human and social values. Prior works have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them inefficient in terms of data utilization and challenging to apply in general, or they depend on reinforcement learning, which often suffers from imperfect reward functions and relies on extremely challenging optimizations. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sequences of sentences, which are then used to fine-tune the model, allowing us to take advantage of the language comprehension capabilities of language models.We condition the model on a sequence of model generations paired with feedback. By doing so, the model is trained to generate outputs based on feedback, while learning to identify and correct negative attributes or errors.  Applying our method to large language models, we observed that Chain of Hindsight significantly surpasses previous methods in aligning language models with human preferences. We report significant improvements on summarization and dialogue benchmarks, with our approach markedly preferred in human evaluations.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them inefficient in terms of data utilization and challenging to apply in general, or they depend on reinforcement learning, which often suffers from imperfect reward functions and relies on extremely challenging optimizations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them inefficient in terms of data utilization and challenging to apply in general, or they depend on reinforcement learning, which often suffers from imperfect reward functions and relies on extremely challenging optimizations.\""
    },
    {
        "title": "SLiMe: Segment Like Me",
        "authors": "Aliasghar Khani;Saeid Asgari;Aditya Sanghi;Ali Mahdavi Amiri;Ghassan Hamarneh",
        "site": "https://iclr.cc/virtual/2024/poster/19368",
        "summary": "Significant strides have been made using large vision-language models, like Stable Diffusion (SD), for a variety of downstream tasks, including image generation, image editing, and 3D shape generation. Inspired by these advancements, we explore leveraging these vision-language models for segmenting images at any desired granularity using as few as one annotated sample. We propose SLiMe, which frames this problem as an optimization task. Specifically, given a single image and its segmentation mask, we first extract our novel “weighted accumulated self-attention map” along with cross-attention map from the SD prior. Then, using these extracted maps, the text embeddings of SD are optimized to highlight the segmented region in these attention maps, which in turn can be used to derive new segmentation results. Moreover, leveraging additional training data when available, i.e. few-shot, improves the performance of SLiMe. We performed comprehensive experiments examining various design factors and showed that SLiMe outperforms other existing one-shot and few-shot segmentation methods.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models",
        "authors": "Xiaogeng Liu;Nan Xu;Muhao Chen;Chaowei Xiao",
        "site": "https://iclr.cc/virtual/2024/poster/19366",
        "summary": "The aligned Large Language Models (LLMs) are powerful language understanding and decision-making tools that are created through extensive alignment with human feedback. However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs. Investigating jailbreak prompts can lead us to delve into the limitations of LLMs and further guide us to secure them. Unfortunately, existing jailbreak techniques suffer from either (1) scalability issues, where attacks heavily rely on manual crafting of prompts, or (2) stealthiness problems, as attacks depend on token-based algorithms to generate prompts that are often semantically meaningless, making them susceptible to detection through basic perplexity testing. In light of these challenges, we intend to answer this question: Can we develop an approach that can automatically generate stealthy jailbreak prompts? In this paper, we introduce AutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can automatically generate stealthy jailbreak prompts by the carefully designed hierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDAN not only automates the process while preserving semantic meaningfulness, but also demonstrates superior attack strength in cross-model transferability, and cross-sample universality compared with the baseline. Moreover, we also compare AutoDAN with perplexity-based defense methods and show that AutoDAN can bypass them effectively. Code is available at https://github.com/SheltonLiu-N/AutoDAN.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs. Investigating jailbreak prompts can lead us to delve into the limitations of LLMs and further guide us to secure them.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs. Investigating jailbreak prompts can lead us to delve into the limitations of LLMs and further guide us to secure them.\""
    },
    {
        "title": "Online Continual Learning for Interactive Instruction Following Agents",
        "authors": "Byeonghwi Kim;Minhyuk Seo;Jonghyun Choi",
        "site": "https://iclr.cc/virtual/2024/poster/19364",
        "summary": "In learning an embodied agent executing daily tasks via language directives, the literature largely assumes that the agent learns all training data at the beginning. We argue that such a learning scenario is less realistic, since a robotic agent is supposed to learn the world continuously as it explores and perceives it. To take a step towards a more realistic embodied agent learning scenario, we propose two continual learning setups for embodied agents; learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new environments (Environment Incremental Learning, Environment-IL) For the tasks, previous ‘data prior’ based continual learning methods maintain logits for the past tasks. However, the stored information is often insufficiently learned information and requires task boundary information, which might not always be available. Here, we propose to update them based on confidence scores without task boundary information (i.e., task-free) in a moving average fashion, named Confidence-Aware Moving Average (CAMA). In the proposed challenging Behavior-IL and Environment-IL setups, our simple CAMA outperforms prior arts in our empirical validations by noticeable margins.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization",
        "authors": "Weiyang Liu;Zeju Qiu;Yao Feng;Yuliang Xiu;Yuxuan Xue;Longhui Yu;Haiwen Feng;Zhen Liu;Juyeon Heo;Songyou Peng;Yandong Wen;Michael J. Black;Adrian Weller;Bernhard Schölkopf",
        "site": "https://iclr.cc/virtual/2024/poster/19363",
        "summary": "Large foundation models are becoming ubiquitous, but training them from scratch is prohibitively expensive. Thus, efficiently adapting these powerful models to downstream tasks is increasingly important. In this paper, we study a principled finetuning paradigm -- Orthogonal Finetuning (OFT) -- for downstream task adaptation. Despite demonstrating good generalizability, OFT still uses a fairly large number of trainable parameters due to the high dimensionality of orthogonal matrices. To address this, we start by examining OFT from an information transmission perspective, and then identify a few key desiderata that enable better parameter-efficiency. Inspired by how the Cooley-Tukey fast Fourier transform algorithm enables efficient information transmission, we propose an efficient orthogonal parameterization using  butterfly structures. We apply this parameterization to OFT, creating  a novel parameter-efficient finetuning method, called Orthogonal Butterfly (BOFT). By subsuming OFT as a special case, BOFT introduces a generalized orthogonal finetuning framework. Finally, we conduct an extensive empirical study of adapting large vision transformers, large language models, and text-to-image diffusion models to various downstream tasks in computer vision and natural language. The results validate the effectiveness of BOFT as a generic finetuning method.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large foundation models are becoming ubiquitous, but training them from scratch is prohibitively expensive.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Large foundation models are becoming ubiquitous, but training them from scratch is prohibitively expensive.\""
    },
    {
        "title": "Human Feedback is not Gold Standard",
        "authors": "Tom Hosking;Phil Blunsom;Max Bartolo",
        "site": "https://iclr.cc/virtual/2024/poster/19356",
        "summary": "Human feedback has become the de facto standard for evaluating the performance of Large Language Models, and is increasingly being used as a training objective. However, it is not clear which properties of a generated output this single `preference' score captures. We hypothesise that preference scores are subjective and open to undesirable biases. We critically analyse the use of human feedback for both training and evaluation, to verify whether it fully captures a range of crucial error criteria. We find that while preference scores have fairly good coverage, they under-represent important aspects like factuality. We further hypothesise that both preference scores and error annotation may be affected by confounders, and leverage instruction-tuned models to generate outputs that vary along two possible confounding dimensions: assertiveness and complexity. We find that the assertiveness of an output skews the perceived rate of factuality errors, indicating that human annotations are not a fully reliable evaluation metric or training objective. Finally, we offer preliminary evidence that using human feedback as a training objective disproportionately increases the assertiveness of model outputs. We encourage future work to carefully consider whether preference scores are well aligned with the desired objective.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that while preference scores have fairly good coverage, they under-represent important aspects like factuality.\"; \"We find that the assertiveness of an output skews the perceived rate of factuality errors, indicating that human annotations are not a fully reliable evaluation metric or training objective.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We find that while preference scores have fairly good coverage, they under-represent important aspects like factuality.\"; \"We find that the assertiveness of an output skews the perceived rate of factuality errors, indicating that human annotations are not a fully reliable evaluation metric or training objective.\""
    },
    {
        "title": "Belief-Enriched Pessimistic Q-Learning against Adversarial State Perturbations",
        "authors": "Xiaolin Sun;Zizhan Zheng",
        "site": "https://iclr.cc/virtual/2024/poster/19351",
        "summary": "Reinforcement learning (RL) has achieved phenomenal success in various domains. However, its data-driven nature also introduces new vulnerabilities that can be exploited by malicious opponents. Recent work shows that a well-trained RL agent can be easily manipulated by strategically perturbing its state observations at the test stage. Existing solutions either introduce a regularization term to improve the smoothness of the trained policy against perturbations or alternatively train the agent's policy and the attacker's policy. However, the former does not provide sufficient protection against strong attacks, while the latter is computationally prohibitive for large environments. In this work, we propose a new robust RL algorithm for deriving a pessimistic policy to safeguard against an agent's uncertainty about true states. This approach is further enhanced with belief state inference and diffusion-based state purification to reduce uncertainty. Empirical results show that our approach obtains superb performance under strong attacks and has a comparable training overhead with regularization-based methods. Our code is available at https://github.com/SliencerX/Belief-enriched-robust-Q-learning.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models",
        "authors": "Sreyan Ghosh;Ashish Seth;Sonal Kumar;Utkarsh Tyagi;Chandra Kiran Reddy Evuru;Ramaneswaran S;S Sakshi;Oriol Nieto;Ramani Duraiswami;Dinesh Manocha",
        "site": "https://iclr.cc/virtual/2024/poster/19339",
        "summary": "A fundamental characteristic of audio is its compositional nature. Audio-language models (ALMs) trained using a contrastive approach (e.g., CLAP) that learns a shared representation between audio and language modalities have improved performance in many downstream applications, including zero-shot audio classification, audio retrieval, etc. However, the ability of these models to effectively perform compositional reasoning remains largely unexplored and necessitates additional research. In this paper, we propose CompA, a collection of two expert-annotated benchmarks with a majority of real-world audio samples, to evaluate compositional reasoning in ALMs. Our proposed CompA-order evaluates how well an ALM understands the order or occurrence of acoustic events in audio, and CompA-attribute evaluates attribute-binding of acoustic events. An instance from either benchmark consists of two audio-caption pairs, where both audios have the same acoustic events but with different compositions. An ALM is evaluated on how well it matches the right audio to the right caption. Using this benchmark, we first show that current ALMs perform only marginally better than random chance, thereby struggling with compositional reasoning. Next, we propose CompA-CLAP, where we fine-tune CLAP using a novel learning method to improve its compositional reasoning abilities. To train CompA-CLAP, we first propose improvements to contrastive training with composition-aware hard negatives, allowing for more focused training. Next, we propose a novel modular contrastive loss that helps the model learn fine-grained compositional understanding and overcomes the acute scarcity of openly available compositional audios. CompA-CLAP significantly improves over all our baseline models on the CompA benchmark, indicating its superior compositional reasoning capabilities.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the ability of these models to effectively perform compositional reasoning remains largely unexplored and necessitates additional research.\"; \"Using this benchmark, we first show that current ALMs perform only marginally better than random chance, thereby struggling with compositional reasoning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, the ability of these models to effectively perform compositional reasoning remains largely unexplored and necessitates additional research.\"; \"Using this benchmark, we first show that current ALMs perform only marginally better than random chance, thereby struggling with compositional reasoning.\""
    },
    {
        "title": "CPPO: Continual Learning for Reinforcement Learning with Human Feedback",
        "authors": "Han Zhang;Yu Lei;Lin Gui;Min Yang;Yulan He;Hui Wang;Ruifeng Xu",
        "site": "https://iclr.cc/virtual/2024/poster/19338",
        "summary": "The approach of Reinforcement Learning from Human Feedback (RLHF) is widely used for enhancing pre-trained Language Models (LM), enabling them to better align with human preferences. Existing RLHF-based LMs however require complete retraining whenever new queries or feedback are introduced, as human preferences may differ across different domains or topics. LM retraining is often impracticable in most real-world scenarios, due to the substantial time and computational costs involved, as well as data privacy concerns. To address this limitation, we propose Continual Proximal Policy Optimization (CPPO), a novel method that is able to continually align LM with dynamic human preferences. Specifically, CPPO adopts a weighting strategy to decide which samples should be utilized for enhancing policy learning and which should be used for solidifying past experiences. This seeks a good trade-off between policy learning and knowledge retention. Our experimental results show that CPPO outperforms strong Continuous learning (CL) baselines when it comes to consistently aligning with human preferences. Furthermore, compared to PPO, CPPO offers more efficient and stable learning in non-continual scenarios.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing RLHF-based LMs however require complete retraining whenever new queries or feedback are introduced, as human preferences may differ across different domains or topics. LM retraining is often impracticable in most real-world scenarios, due to the substantial time and computational costs involved, as well as data privacy concerns.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Existing RLHF-based LMs however require complete retraining whenever new queries or feedback are introduced, as human preferences may differ across different domains or topics. LM retraining is often impracticable in most real-world scenarios, due to the substantial time and computational costs involved, as well as data privacy concerns.\""
    },
    {
        "title": "Language-Interfaced Tabular Oversampling via Progressive Imputation and Self-Authentication",
        "authors": "June Yong Yang;Geondo Park;Joowon Kim;Hyeongwon Jang;Eunho Yang",
        "site": "https://iclr.cc/virtual/2024/poster/19334",
        "summary": "Tabular data in the wild are frequently afflicted with class-imbalance, biasing machine learning model predictions towards major classes. A data-centric solution to this problem is oversampling - where the classes are balanced by adding synthetic minority samples via generative methods. However, although tabular generative models are capable of generating synthetic samples under a balanced distribution, their integrity suffers when the number of minority samples is low. To this end, pre-trained generative language models with rich prior knowledge are a fitting candidate for the task at hand. Nevertheless, an oversampling strategy tailored for tabular data that utilizes the extensive capabilities of such language models is yet to emerge. In this paper, we propose a novel oversampling framework for tabular data to channel the abilities of generative language models. By leveraging its conditional sampling capabilities, we synthesize minority samples by progressively masking the important features of the majority class samples and imputing them towards the minority distribution. To reduce the inclusion of imperfectly converted samples, we utilize the power of the language model itself to self-authenticate the labels of the samples generated by itself, sifting out ill-converted samples. Extensive experiments on a variety of datasets and imbalance ratios reveal that the proposed method successfully generates reliable minority samples to boost the performance of machine learning classifiers, even under heavy imbalance ratios.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"although tabular generative models are capable of generating synthetic samples under a balanced distribution, their integrity suffers when the number of minority samples is low.\"\n\nThis paper mentions a limitation of generative language models, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"although tabular generative models are capable of generating synthetic samples under a balanced distribution, their integrity suffers when the number of minority samples is low.\"\n\nThis paper mentions a limitation of generative language models, but it is not the primary focus of the paper."
    },
    {
        "title": "Learning with a Mole: Transferable latent spatial representations for navigation without reconstruction",
        "authors": "Guillaume Bono;Leonid Antsfeld;Assem Sadek;Gianluca Monaci;Christian Wolf",
        "site": "https://iclr.cc/virtual/2024/poster/19332",
        "summary": "Agents navigating in 3D environments require some form of memory, which should hold a compact and actionable representation of the history of observations useful for decision taking and planning. In most end-to-end learning approaches the representation is latent and usually does not have a clearly defined interpretation, whereas classical robotics addresses this with scene reconstruction resulting in some form of map, usually estimated with geometry and sensor models and/or learning. In this work we propose to learn an actionable representation of the scene independently of the targeted downstream task and without explicitly optimizing reconstruction. The learned representation is optimized by a blind auxiliary agent trained to navigate with it on multiple short sub episodes branching out from a waypoint and, most importantly, without any direct visual observation. We argue and show that the blindness property is important and forces the (trained) latent representation to be the only means for planning. With probing experiments we show that the learned representation optimizes navigability and not reconstruction. On downstream tasks we show that it is robust to changes in distribution, in particular the sim2real gap, which we evaluate with a real physical robot in a real office building, significantly improving performance.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Fast-ELECTRA for Efficient Pre-training",
        "authors": "Chengyu Dong;Liyuan Liu;Hao Cheng;Jingbo Shang;Jianfeng Gao;Xiaodong Liu",
        "site": "https://iclr.cc/virtual/2024/poster/19329",
        "summary": "ELECTRA pre-trains language models by detecting tokens in a sequence that have been replaced by an auxiliary model. Although ELECTRA offers a significant boost in efficiency, its potential is constrained by the training cost brought by the auxiliary model. Notably, this model, which is jointly trained with the main model, only serves to assist the training of the main model and is discarded post-training. This results in a substantial amount of training cost being expended in vain. To mitigate this issue, we propose Fast-ELECTRA, which leverages an existing language model as the auxiliary model. To construct a learning curriculum for the main model, we smooth its output distribution via temperature scaling following a descending schedule. Our approach rivals the performance of state-of-the-art ELECTRA-style pre-training methods, while significantly eliminating the computation and memory cost brought by the joint training of the auxiliary model. Our method also reduces the sensitivity to hyper-parameters and enhances the pre-training stability.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although ELECTRA offers a significant boost in efficiency, its potential is constrained by the training cost brought by the auxiliary model.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although ELECTRA offers a significant boost in efficiency, its potential is constrained by the training cost brought by the auxiliary model.\""
    },
    {
        "title": "Context is Environment",
        "authors": "Sharut Gupta;Stefanie Jegelka;David Lopez-Paz;Kartik Ahuja",
        "site": "https://iclr.cc/virtual/2024/poster/19324",
        "summary": "Abstract:Two lines of work are taking the central stage in AI research. On the one hand, the community is making increasing efforts to build models that discard spurious correlations and generalize better in novel test environments. Unfortunately, the hard lesson so far is that no proposal convincingly outperforms a simple empirical risk minimization baseline. On the other hand, large language models (LLMs) have erupted as algorithms able to learn in-context, generalizing on-the-fly to eclectic contextual circumstances that users enforce by means of prompting. In this paper, we argue that context is environment, and posit that in-context learning holds the key to better domain generalization. Via extensive theory and experiments, we show that paying attention to context$\\unicode{x2013}\\unicode{x2013}$unlabeled examples as they arrive$\\unicode{x2013}\\unicode{x2013}$allows our proposed In-Context Risk Minimization (ICRM) algorithm to zoom-in on the test environment risk minimizer, leading to significant out-of-distribution performance improvements. Furthermore, training with context helps the model learn a better featurizer. From all of this, two messages are worth taking home. Researchers in domain generalization should consider environment as context, and harness the adaptive power of in-context learning. Researchers in LLMs should consider context as environment, to better structure data towards generalization. Code is available at https://github.com/facebookresearch/ICRM.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Researchers in LLMs should consider context as environment, to better structure data towards generalization.\"\n\nThis evidence suggests that the paper mentions a limitation of LLMs, specifically their need for better data structuring to achieve generalization, but does not explore this limitation in detail.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Researchers in LLMs should consider context as environment, to better structure data towards generalization.\"\n\nThis evidence suggests that the paper mentions a limitation of LLMs, specifically their need for better data structuring to achieve generalization, but does not explore this limitation in detail."
    },
    {
        "title": "Prometheus: Inducing Fine-Grained Evaluation Capability in Language Models",
        "authors": "Seungone Kim;Jamin Shin;Yejin Cho;Joel Jang;Shayne Longpre;Hwaran Lee;Sangdoo Yun;Seongjin Shin;Sungdong Kim;James Thorne;Minjoon Seo",
        "site": "https://iclr.cc/virtual/2024/poster/19321",
        "summary": "Recently, GPT-4 has become the de facto evaluator for long-form text generated by large language models (LLMs). However, for practitioners and researchers with large and custom evaluation tasks, GPT-4 is unreliable due to its closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose PROMETHEUS a fully open-source LLM that is on par with GPT-4’s evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. For this purpose, we construct a new dataset – FEEDBACK COLLECTION – that consists of 1K fine-grained score rubrics, 20K instructions, and 100K natural language feedback generated by GPT-4. Using the FEEDBACK COLLECTION, we train PROMETHEUS, a 13B evaluation-specific LLM that can assess any given response based on novel and unseen score rubrics and reference materials provided by the user. Our dataset’s versatility and diversity make our model generalize to challenging real-world criteria, such as prioritizing conciseness, child-readability, or varying levels of formality. We show that PROMETHEUS shows a stronger correlation with GPT-4 evaluation compared to ChatGPT on seven evaluation benchmarks (Two Feedback Collection testsets, MT Bench, Vicuna Bench, Flask Eval, MT Bench Human Judgment, and HHH Alignment), showing the efficacy of our model and dataset design. During human evaluation with hand-crafted score rubrics, PROMETHEUS shows a Pearson correlation of 0.897 with human evaluators, which is on par with GPT-4-0613 (0.882), and greatly outperforms ChatGPT (0.392). Remarkably, when assessing the quality of the generated feedback, PROMETHEUS demonstrates a win rate of 58.62% when compared to GPT-4 evaluation and a win rate of 79.57% when compared to ChatGPT evaluation. Our findings suggests that by adding reference materials and training on GPT-4 feedback, we can obtain effective open-source evaluator LMs.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, for practitioners and researchers with large and custom evaluation tasks, GPT-4 is unreliable due to its closed-source nature, uncontrolled versioning, and prohibitive costs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, for practitioners and researchers with large and custom evaluation tasks, GPT-4 is unreliable due to its closed-source nature, uncontrolled versioning, and prohibitive costs.\""
    },
    {
        "title": "Combining Axes Preconditioners through Kronecker Approximation for Deep Learning",
        "authors": "Sai Surya Duvvuri;Fnu Devvrit;Rohan Anil;Cho-Jui Hsieh;Inderjit S Dhillon",
        "site": "https://iclr.cc/virtual/2024/poster/19316",
        "summary": "Adaptive regularization based optimization methods such as full-matrix Adagrad which use gradient second-moment information hold significant potential for fast convergence in deep neural network (DNN) training, but are memory intensive and computationally demanding for large neural nets. We develop a technique called Combining AxeS PReconditioners (CASPR), which optimizes matrix-shaped DNN parameters by finding different preconditioners for each mode/axis of the parameter and combining them using a Kronecker-sum based approximation. We show tighter convergence guarantees in stochastic optimization compared to a Kronecker product based preconditioner, Shampoo, which arises as a special case of CASPR. Furthermore, our experiments demonstrates that CASPR approximates the gradient second-moment matrix in full-matrix Adagrad more accurately, and shows significant improvement in training and generalization performance compared to existing practical adaptive regularization based methods such as Shampoo and Adam in a variety of tasks including graph neural network on OGBG-molpcba, Transformer on a universal dependencies dataset and auto-regressive large language modeling on C4 dataset.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"auto-regressive large language modeling on C4 dataset.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"auto-regressive large language modeling on C4 dataset.\""
    },
    {
        "title": "One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention",
        "authors": "Arvind V. Mahankali;Tatsunori Hashimoto;Tengyu Ma",
        "site": "https://iclr.cc/virtual/2024/poster/19314",
        "summary": "Abstract:Recent works have empirically analyzed in-context learning and shown that transformers trained on synthetic linear regression tasks can learn to implement ridge regression, which is the Bayes-optimal predictor, given sufficient capacity (Akyurek et al., 2023), while one-layer transformers with linear self-attention and no MLP layer will learn to implement one step of gradient descent (GD) on a least-squares linear regression objective (von Oswald et al., 2022). However, the theory behind these observations remains poorly understood. We theoretically study transformers with a single layer of linear self-attention, trained on synthetic noisy linear regression data. First, we mathematically show that when the covariates are drawn from a standard Gaussian distribution, the one-layer transformer which minimizes the pre-training loss will implement a single step of GD on the least-squares linear regression objective. Then, we find that changing the distribution of the covariates and weight vector to a non-isotropic Gaussian distribution has a strong impact on the learned algorithm: the global minimizer of the pre-training loss now implements a single step of $\\textit{pre-conditioned}$ GD. However, if only the distribution of the responses is changed, then this does not have a large effect on the learned algorithm: even when the response comes from a more general family of $\\textit{nonlinear}$ functions, the global minimizer of the pre-training loss still implements a single step of GD on a least-squares linear regression objective.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking",
        "authors": "Nikhil Prakash;Tamar Rott Shaham;Tal Haklay;Yonatan Belinkov;David Bau",
        "site": "https://iclr.cc/virtual/2024/poster/19313",
        "summary": "Fine-tuning on generalized tasks such as instruction following, code generation, and mathematics has been shown to enhance language models' performance on a range of tasks. Nevertheless, explanations of how such fine-tuning influences the internal computations in these models remain elusive. We study how fine-tuning affects the internal mechanisms implemented in language models. As a case study, we explore the property of entity tracking, a crucial facet of language comprehension, where models fine-tuned on mathematics have substantial performance gains. We identify a mechanism that enables entity tracking and show that (i) both the original model and its fine-tuned version implement entity tracking with the same circuit. In fact, the entity tracking circuit of the fine-tuned version performs better than the full original model. (ii) The circuits of all the models implement roughly the same functionality, that is entity tracking is performed by tracking the position of the correct entity in both the original model and its fine-tuned version. (iii) Performance boost in the fine-tuned model is primarily attributed to its improved ability to handle positional information. To uncover these findings, we employ two methods: DCM, which automatically detects model components responsible for specific semantics, and CMAP, a new approach for patching activations across models to reveal improved mechanisms. Our findings suggest that fine-tuning enhances, rather than fundamentally alters, the mechanistic operation of the model.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the paper discusses fine-tuning and its effects on internal mechanisms in language models, but does not mention any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the paper discusses fine-tuning and its effects on internal mechanisms in language models, but does not mention any limitations of LLMs."
    },
    {
        "title": "Learning Nash Equilibria in Rank-1 Games",
        "authors": "Nikolas Patris;Ioannis Panageas",
        "site": "https://iclr.cc/virtual/2024/poster/19312",
        "summary": "Learning Nash equilibria (NE) in games has garnered significant attention, particularly in the context of training Generative Adversarial Networks (GANs) and multi-agent Reinforcement Learning. The current state-of-the-art in efficiently learning games focuses on landscapes that meet the (weak) Minty property or games characterized by a unique function, often referred to as potential games. A significant challenge in this domain is that computing Nash equilibria is a computationally intractable task [Daskalakis et al. 2009]. In this paper we focus on bimatrix games (A,B) called rank-1. These are games in which the sum of the payoff matrices A+B is a rank 1 matrix; note that standard zero-sum games are rank 0. We show that optimistic gradient descent/ascent converges to an \\epsilon-approximate NE after 1/\\epsilon^2 log(1/\\epsilon) iterates in rank-1 games. We achieve this by leveraging structural results about the NE landscape of rank-1 games Adsul et al. 2021. Notably, our approach bypasses the fact that these games do not satisfy the MVI property.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Learning Decentralized Partially Observable Mean Field Control for Artificial Collective Behavior",
        "authors": "Kai Cui;Sascha H. Hauck;Christian Fabian;Heinz Koeppl",
        "site": "https://iclr.cc/virtual/2024/poster/19304",
        "summary": "Recent reinforcement learning (RL) methods have achieved success in various domains. However, multi-agent RL (MARL) remains a challenge in terms of decentralization, partial observability and scalability to many agents. Meanwhile, collective behavior requires resolution of the aforementioned challenges, and remains of importance to many state-of-the-art applications such as active matter physics, self-organizing systems, opinion dynamics, and biological or robotic swarms. Here, MARL via mean field control (MFC) offers a potential solution to scalability, but fails to consider decentralized and partially observable systems. In this paper, we enable decentralized behavior of agents under partial information by proposing novel models for decentralized partially observable MFC (Dec-POMFC), a broad class of problems with permutation-invariant agents allowing for reduction to tractable single-agent Markov decision processes (MDP) with single-agent RL solution. We provide rigorous theoretical results, including a dynamic programming principle, together with optimality guarantees for Dec-POMFC solutions applied to finite swarms of interest. Algorithmically, we propose Dec-POMFC-based policy gradient methods for MARL via centralized training and decentralized execution, together with policy gradient approximation guarantees. In addition, we improve upon state-of-the-art histogram-based MFC by kernel methods, which is of separate interest also for fully observable MFC. We evaluate numerically on representative collective behavior tasks such as adapted Kuramoto and Vicsek swarming models, being on par with state-of-the-art MARL. Overall, our framework takes a step towards RL-based engineering of artificial collective behavior via MFC.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs or language models.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs or language models."
    },
    {
        "title": "Bootstrapping Variational Information Pursuit with Large Language and Vision Models for Interpretable Image Classification",
        "authors": "Aditya Chattopadhyay;Kwan Ho Ryan Chan;Rene Vidal",
        "site": "https://iclr.cc/virtual/2024/poster/19290",
        "summary": "Variational Information Pursuit (V-IP) is an interpretable-by-design framework that makes predictions by sequentially selecting a short chain of user-defined, interpretable queries about the data that are most informative for the task. The prediction is based solely on the obtained query answers, which also serve as a faithful explanation for the prediction. Applying the framework to any task requires (i) specification of a query set, and (ii) densely annotated data with query answers to train classifiers to answer queries at test time. This limits V-IP's application to small-scale tasks where manual data annotation is feasible. In this work, we focus on image classification tasks and propose to relieve this bottleneck by leveraging pretrained language and vision models. Specifically, following recent work, we propose to use GPT, a Large Language Model, to propose semantic concepts as queries for a given classification task. To answer these queries, we propose a light-weight Concept Question-Answering network (Concept-QA) which learns to answer binary queries about semantic concepts in images. We design pseudo-labels to train our Concept-QA model using GPT and CLIP (a Vision-Language Model). Empirically, we find our Concept-QA model to be competitive with state-of-the-art VQA models in terms of answering accuracy but with an order of magnitude fewer parameters. This allows for seamless integration of Concept-QA into the V-IP framework as a fast-answering mechanism. We name this method Concept-QA+V-IP. Finally, we show on several datasets that Concept-QA+V-IP produces shorter, interpretable query chains which are more accurate than V-IP trained with CLIP-based answering systems. Code available at https://github.com/adityac94/conceptqa_vip.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation is mentioned, but the paper proposes a method to \"relieve this bottleneck\" of requiring densely annotated data, which might be a limitation of the V-IP framework when used with LLMs, however, it is not a direct limitation of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation is mentioned, but the paper proposes a method to \"relieve this bottleneck\" of requiring densely annotated data, which might be a limitation of the V-IP framework when used with LLMs, however, it is not a direct limitation of LLMs."
    },
    {
        "title": "On the Learnability of Watermarks for Language Models",
        "authors": "Chenchen Gu;Xiang Lisa Li;Percy Liang;Tatsunori Hashimoto",
        "site": "https://iclr.cc/virtual/2024/poster/19286",
        "summary": "Watermarking of language model outputs enables statistical detection of model-generated text, which can mitigate harms and misuses of language models. Existing watermarking strategies operate by altering the decoder of an existing language model. In this paper, we ask whether language models can directly learn to generate watermarked text, which would have significant implications for the real-world deployment of watermarks. First, learned watermarks could be used to build open models that naturally generate watermarked text, enabling watermarking for open models, where users can control the decoding procedure. Second, if watermarking is used to determine the provenance of generated text, an adversary can hurt the reputation of a victim model by spoofing its watermark and generating damaging watermarked text. To investigate the learnability of watermarks, we propose watermark distillation, which trains a student model to behave like a teacher model that uses decoding-based watermarking. We test our approach on three decoding-based watermarking strategies and various hyperparameter settings, finding that models can learn to generate watermarked text with high detectability. We also find limitations to learnability, including the loss of watermarking capabilities under fine-tuning on normal text and high sample complexity when learning low-distortion watermarks.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We also find limitations to learnability, including the loss of watermarking capabilities under fine-tuning on normal text and high sample complexity when learning low-distortion watermarks.\"\n\nThis rating is given because the paper discusses limitations of learnability of watermarks for LLMs, but it is not the primary focus of the paper. The limitations are mentioned and analyzed, but the paper also",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We also find limitations to learnability, including the loss of watermarking capabilities under fine-tuning on normal text and high sample complexity when learning low-distortion watermarks.\"\n\nThis rating is given because the paper discusses limitations of learnability of watermarks for LLMs, but it is not the primary focus of the paper. The limitations are mentioned and analyzed, but the paper also"
    },
    {
        "title": "DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models",
        "authors": "Yongchan Kwon;Eric Wu;Kevin Wu;James Zou",
        "site": "https://iclr.cc/virtual/2024/poster/19284",
        "summary": "Quantifying the impact of training data points is crucial for understanding the outputs of machine learning models and for improving the transparency of the AI pipeline. The influence function is a principled and popular data attribution method, but its computational cost often makes it challenging to use. This issue becomes more pronounced in the setting of large language models and text-to-image models. In this work, we propose DataInf, an efficient influence approximation method that is practical for large-scale generative AI models. Leveraging an easy-to-compute closed-form expression, DataInf outperforms existing influence computation algorithms in terms of computational and memory efficiency. Our theoretical analysis shows that DataInf is particularly well-suited for parameter-efficient fine-tuning techniques such as LoRA. Through systematic empirical evaluations, we show that DataInf accurately approximates influence scores and is orders of magnitude faster than existing methods. In applications to RoBERTa-large, Llama-2-13B-chat, and stable-diffusion-v1.5 models, DataInf effectively identifies the most influential fine-tuning examples better than other approximate influence scores. Moreover, it can help to identify which data points are mislabeled.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The influence function is a principled and popular data attribution method, but its computational cost often makes it challenging to use. This issue becomes more pronounced in the setting of large language models and text-to-image models.\"\n\nNote that the limitation mentioned is related to the computational cost of the influence function in the context of large language models, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The influence function is a principled and popular data attribution method, but its computational cost often makes it challenging to use. This issue becomes more pronounced in the setting of large language models and text-to-image models.\"\n\nNote that the limitation mentioned is related to the computational cost of the influence function in the context of large language models, but it is not the primary focus of the paper."
    },
    {
        "title": "Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks",
        "authors": "Yixuan Weng;Minjun Zhu;Fei Xia;Bin Li;Shizhu He;Kang Liu;Jun Zhao",
        "site": "https://iclr.cc/virtual/2024/poster/19283",
        "summary": "Language models' (LMs) proficiency in handling deterministic symbolic reasoning and rule-based tasks remains limited due to their dependency implicit learning on textual data. To endow LMs with genuine rule comprehension abilities, we propose \"Neural Comprehension\" - a framework that synergistically integrates compiled neural networks (CoNNs) into the standard transformer architecture. CoNNs are neural modules designed to explicitly encode rules through artificially generated attention weights. By incorporating CoNN modules, the Neural Comprehension framework enables LMs to accurately and robustly execute rule-intensive symbolic tasks. Extensive experiments demonstrate the superiority of our approach over existing techniques in terms of length generalization, efficiency, and interpretability for symbolic operations. Furthermore, it can be applied to LMs across different model scales, outperforming tool-calling methods in arithmetic reasoning tasks while maintaining superior inference efficiency. Our work highlights the potential of seamlessly unifying explicit rule learning via CoNNs and implicit pattern learning in LMs, paving the way for true symbolic comprehension capabilities. The code is released at: \\url{https://github.com/wengsyx/Neural-Comprehension}.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Language models' (LMs) proficiency in handling deterministic symbolic reasoning and rule-based tasks remains limited due to their dependency implicit learning on textual data.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Language models' (LMs) proficiency in handling deterministic symbolic reasoning and rule-based tasks remains limited due to their dependency implicit learning on textual data.\""
    },
    {
        "title": "Video Language Planning",
        "authors": "Yilun Du;Sherry Yang;Pete Florence;Fei Xia;Ayzaan Wahid;brian ichter;Pierre Sermanet;Tianhe Yu;Pieter Abbeel;Joshua B. Tenenbaum;Leslie Pack Kaelbling;Andy Zeng;Jonathan Tompson",
        "site": "https://iclr.cc/virtual/2024/poster/19282",
        "summary": "We are interested in enabling visual planning for complex long-horizon tasks in the space of generated videos and language, leveraging recent advances in large generative models pretrained on Internet-scale data.  To this end, we present video language planning (VLP), an algorithm that consists of a tree search procedure, where we train (i) vision-language models to serve as both policies and value functions, and (ii) text-to-video models as dynamics models. VLP takes as input a long-horizon task instruction and current image observation, and outputs a long video plan that provides detailed multimodal (video and language) specifications that describe how to complete the final task. VLP scales with increasing computation budget where more computation time results in improved video plans, and is able to synthesize long-horizon video plans across different robotics domains -- from multi-object rearrangement, to multi-camera bi-arm dexterous manipulation. Generated video plans can be translated into real robot actions via goal-conditioned policies, conditioned on each intermediate frame of the generated video. Experiments show that VLP substantially improves long-horizon task success rates compared to prior methods on both simulated and real robots (across 3 hardware platforms).",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"leveraging recent advances in large generative models pretrained on Internet-scale data.\"\n\nThis paper mentions large generative models, which are related to LLMs, but does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"leveraging recent advances in large generative models pretrained on Internet-scale data.\"\n\nThis paper mentions large generative models, which are related to LLMs, but does not discuss any limitations of LLMs."
    },
    {
        "title": "Domain-Agnostic Molecular Generation with Chemical Feedback",
        "authors": "Yin Fang;Ningyu Zhang;Zhuo Chen;Lingbing Guo;Xiaohui Fan;Huajun Chen",
        "site": "https://iclr.cc/virtual/2024/poster/19281",
        "summary": "The generation of molecules with desired properties has become increasingly popular, revolutionizing the way scientists design molecular structures and providing valuable support for chemical and drug design. However, despite the potential of language models in molecule generation, they face challenges such as generating syntactically or chemically flawed molecules, having narrow domain focus, and struggling to create diverse and feasible molecules due to limited annotated data or external molecular databases.To tackle these challenges, we introduce MolGen, a pre-trained molecular language model tailored specifically for molecule generation. Through the reconstruction of over 100 million molecular SELFIES, MolGen internalizes structural and grammatical insights. This is further enhanced by domain-agnostic molecular prefix tuning, fostering robust knowledge transfer across diverse domains. Importantly, our chemical feedback paradigm steers the model away from \"molecular hallucinations\", ensuring alignment between the model's estimated probabilities and real-world chemical preferences. Extensive experiments on well-known benchmarks underscore MolGen's optimization capabilities in properties such as penalized logP, QED, and molecular docking. Additional analyses confirm its proficiency in accurately capturing molecule distributions, discerning intricate structural patterns, and efficiently exploring the chemical space (https://github.com/zjunlp/MolGen).",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, despite the potential of language models in molecule generation, they face challenges such as generating syntactically or chemically flawed molecules, having narrow domain focus, and struggling to create diverse and feasible molecules due to limited annotated data or external molecular databases.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, despite the potential of language models in molecule generation, they face challenges such as generating syntactically or chemically flawed molecules, having narrow domain focus, and struggling to create diverse and feasible molecules due to limited annotated data or external molecular databases.\""
    },
    {
        "title": "Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks",
        "authors": "Samyak Jain;Robert Kirk;Ekdeep Singh Lubana;Robert P. Dick;Hidenori Tanaka;Tim Rocktäschel;Edward Grefenstette;David Krueger",
        "site": "https://iclr.cc/virtual/2024/poster/19277",
        "summary": "Fine-tuning large pre-trained models has become the de facto strategy for developing both task-specific and general-purpose machine learning systems, including developing models that are safe to deploy. Despite its clear importance, there has been minimal work that explains how fine-tuning alters the underlying capabilities learned by a model during pretraining: does fine-tuning yield entirely novel capabilities or does it just modulate existing ones? We address this question empirically in synthetic, controlled settings where we can use mechanistic interpretability tools (e.g., network pruning and probing) to understand how the model's underlying capabilities are changing. We perform an extensive analysis of the effects of fine-tuning in these settings, and show that: (i) fine-tuning rarely alters the underlying model capabilities; (ii) a minimal transformation, which we call a `wrapper', is typically learned on top of the underlying model capabilities, creating the illusion that they have been modified; and (iii) further fine-tuning on a task where such ``wrapped capabilities'' are relevant leads to sample-efficient revival of the capability, i.e., the model begins reusing these capabilities after only a few gradient steps. This indicates that practitioners can unintentionally remove a model's safety wrapper merely by fine-tuning it on a, e.g., superficially unrelated, downstream task. We additionally perform analysis on language models trained on the TinyStories dataset to support our claims in a more realistic setup.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This indicates that practitioners can unintentionally remove a model's safety wrapper merely by fine-tuning it on a, e.g., superficially unrelated, downstream task.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"This indicates that practitioners can unintentionally remove a model's safety wrapper merely by fine-tuning it on a, e.g., superficially unrelated, downstream task.\""
    },
    {
        "title": "LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents",
        "authors": "Jae-Woo Choi;Youngwoo Yoon;Hyobin Ong;Jaehong Kim;Minsu Jang",
        "site": "https://iclr.cc/virtual/2024/poster/19271",
        "summary": "Large language models (LLMs) have recently received considerable attention as alternative solutions for task planning. However, comparing the performance of language-oriented task planners becomes difficult, and there exists a dearth of detailed exploration regarding the effects of various factors such as pre-trained model selection and prompt construction. To address this, we propose a benchmark system for automatically quantifying performance of task planning for home-service embodied agents. Task planners are tested on two pairs of datasets and simulators: 1) ALFRED and AI2-THOR, 2) an extension of Watch-And-Help and VirtualHome. Using the proposed benchmark system, we perform extensive experiments with LLMs and prompts, and explore several enhancements of the baseline planner. We expect that the proposed benchmark tool would accelerate the development of language-oriented task planners.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, comparing the performance of language-oriented task planners becomes difficult, and there exists a dearth of detailed exploration regarding the effects of various factors such as pre-trained model selection and prompt construction.\"\n\n(Note: The abstract mentions a limitation of LLMs in the context of task planning, but it is not the primary focus of the paper. The limitation is mentioned briefly to justify the need",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, comparing the performance of language-oriented task planners becomes difficult, and there exists a dearth of detailed exploration regarding the effects of various factors such as pre-trained model selection and prompt construction.\"\n\n(Note: The abstract mentions a limitation of LLMs in the context of task planning, but it is not the primary focus of the paper. The limitation is mentioned briefly to justify the need"
    },
    {
        "title": "SpeechTokenizer: Unified Speech Tokenizer for Speech Language Models",
        "authors": "Xin Zhang;Dong Zhang;Shimin Li;Yaqian Zhou;Xipeng Qiu",
        "site": "https://iclr.cc/virtual/2024/poster/19270",
        "summary": "Current speech large language models build upon discrete speech representations,which can be categorized into semantic tokens and acoustic tokens. However,existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech languagemodels, we established the first benchmark, SLMTokBench. Our results indicatethat neither semantic nor acoustic tokens are ideal for this purpose. Therefore, wepropose SpeechTokenizer, a unified speech tokenizer for speech large languagemodels. SpeechTokenizer adopts the Encoder-Decoder architecture with residualvector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically acrossdifferent RVQ layers. Furthermore, We construct a Unified Speech LanguageModel (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstratesstrong performance on the SLMTokBench benchmark. Also, USLM outperformsVALL-E in zero-shot Text-to-Speech tasks. Code and models are available athttps://github.com/ZhangXInFD/SpeechTokenizer/.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our results indicatethat neither semantic nor acoustic tokens are ideal for this purpose.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Our results indicatethat neither semantic nor acoustic tokens are ideal for this purpose.\""
    },
    {
        "title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data",
        "authors": "Guan Wang;Sijie Cheng;Xianyuan Zhan;Xiangang Li;Sen Song;Yang Liu",
        "site": "https://iclr.cc/virtual/2024/poster/19263",
        "summary": "Nowadays, open-source large language models like LLaMA have emerged. Recent developments have incorporated supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to align these models with human goals. However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data. In this study, we present a novel framework, named OpenChat, to advance open-source language models with mixed-quality data. Specifically, we consider the general SFT training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels. We propose the C(onditioned)-RLFT, which regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. Interestingly, the optimal policy in C-RLFT can be easily solved through single-stage, RL-free supervised learning, which is lightweight and avoids costly human preference labeling.Through extensive experiments on three standard benchmarks, our openchat-13b fine-tuned with C-RLFT achieves the highest average performance among all 13b open-source language models. Moreover, we use AGIEval to validate the model generalization performance, in which only openchat-13b surpasses the base model. Finally, we conduct a series of analyses to shed light on the effectiveness and robustness of OpenChat. Our code, data, and models are publicly available at https://github.com/imoneoi/openchat and https://huggingface.co/openchat.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data.\"\n\nThis rating is given because the paper mentions a limitation of SFT and RLFT methods in the context of training open-source language models with mixed-quality data, but it does not delve deeper into the limitations and instead focuses on proposing a novel framework",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data.\"\n\nThis rating is given because the paper mentions a limitation of SFT and RLFT methods in the context of training open-source language models with mixed-quality data, but it does not delve deeper into the limitations and instead focuses on proposing a novel framework"
    },
    {
        "title": "Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning",
        "authors": "Ruizhe Shi;Yuyao Liu;Yanjie Ze;Simon Shaolei Du;Huazhe Xu",
        "site": "https://iclr.cc/virtual/2024/poster/19257",
        "summary": "Abstract:Offline reinforcement learning (RL) aims to find a near-optimal policy using pre-collected datasets. Given recent advances in Large Language Models (LLMs) and their few-shot learning prowess, this paper introduces $\\textbf{La}$nguage Models for $\\textbf{Mo}$tion Control ($\\textbf{LaMo}$), a general framework based on Decision Transformers to effectively use pre-trained Language Models (LMs) for offline RL. Our framework highlights four crucial components: (1)  Initializing Decision Transformers with sequentially pre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast to full-weight fine-tuning, to combine the pre-trained knowledge from LMs and in-domain knowledge effectively, (3) using the non-linear MLP transformation instead of linear projections, to generate embeddings, and (4) integrating an auxiliary language prediction loss during fine-tuning to stabilize the LMs and retain their original abilities on languages. Empirical results indicate $\\textbf{LaMo}$ achieves state-of-the-art performance in sparse-reward tasks and closes the gap between value-based offline RL methods and decision transformers in dense-reward tasks. In particular, our method demonstrates superior performance in scenarios with limited data samples.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations, but the framework aims to effectively use pre-trained LMs, implying potential challenges in utilizing them.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations, but the framework aims to effectively use pre-trained LMs, implying potential challenges in utilizing them."
    },
    {
        "title": "Rethinking Model Ensemble in Transfer-based Adversarial Attacks",
        "authors": "Huanran Chen;Yichi Zhang;Yinpeng Dong;Xiao Yang;Hang Su;Jun Zhu",
        "site": "https://iclr.cc/virtual/2024/poster/19251",
        "summary": "It is widely recognized that deep learning models lack robustness to adversarial examples. An intriguing property of adversarial examples is that they can transfer across different models, which enables black-box attacks without any knowledge of the victim model. An effective strategy to improve the transferability is attacking an ensemble of models. However, previous works simply average the outputs of different models, lacking an in-depth analysis on how and why model ensemble methods can strongly improve the transferability. In this paper, we rethink the ensemble in adversarial attacks and define the common weakness of model ensemble with two properties: 1) the flatness of loss landscape; and 2) the closeness to the local optimum of each model. We empirically and theoretically show that both properties are strongly correlated with the transferability and propose a Common Weakness Attack (CWA) to generate more transferable adversarial examples by promoting these two properties. Experimental results on both image classification and object detection tasks validate the effectiveness of our approach to improving the adversarial transferability, especially when attacking adversarially trained models. We also successfully apply our method to attack a black-box large vision-language model -- Google's Bard, showing the practical effectiveness. Code is available at \\url{https://github.com/huanranchen/AdversarialAttacks}.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We also successfully apply our method to attack a black-box large vision-language model -- Google's Bard, showing the practical effectiveness.\"\n\nNote: Although the paper mentions a large vision-language model (Google's Bard), the primary focus is on adversarial attacks and transferability, not specifically on limitations of LLMs. The mention of the large vision-language model is brief and used to demonstrate",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We also successfully apply our method to attack a black-box large vision-language model -- Google's Bard, showing the practical effectiveness.\"\n\nNote: Although the paper mentions a large vision-language model (Google's Bard), the primary focus is on adversarial attacks and transferability, not specifically on limitations of LLMs. The mention of the large vision-language model is brief and used to demonstrate"
    },
    {
        "title": "General Stability Analysis for Zeroth-Order Optimization Algorithms",
        "authors": "Xinyue Liu;Hualin Zhang;Bin Gu;Hong Chen",
        "site": "https://iclr.cc/virtual/2024/poster/19245",
        "summary": "Zeroth-order optimization algorithms are widely used for black-box optimization problems, such as those in machine learning and prompt engineering, where the gradients are approximated using function evaluations. Recently, a generalization result was provided for zeroth-order stochastic gradient descent (SGD) algorithms through stability analysis. However, this result was limited to the vanilla 2-point zeroth-order estimate of Gaussian distribution used in SGD algorithms. To address these limitations, we propose a general proof framework for stability analysis that applies to convex, strongly convex, and non-convex conditions, and yields results for popular zeroth-order optimization algorithms, including SGD, GD, and SVRG, as well as various zeroth-order estimates, such as 1-point and 2-point with different distributions and coordinate estimates. Our general analysis shows that coordinate estimation can lead to tighter generalization bounds for SGD, GD, and SVRG versions of zeroth-order optimization algorithms, due to the smaller expansion brought by coordinate estimates to stability analysis.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Large Language Models as Analogical Reasoners",
        "authors": "Michihiro Yasunaga;Xinyun Chen;Yujia Li;Panupong Pasupat;Jure Leskovec;Percy Liang;Ed H. Chi;Denny Zhou",
        "site": "https://iclr.cc/virtual/2024/poster/19243",
        "summary": "Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process. In this work, we introduce a new prompting approach, analogical prompting, designed to automatically guide the reasoning process of large language models. Inspired by analogical reasoning, a cognitive process in which humans draw from relevant past experiences to tackle new problems, our approach prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem. This method presents several advantages: it obviates the need for labeling or retrieving exemplars, offering generality and convenience; it can also tailor the generated exemplars and knowledge to each problem, offering adaptability. Experimental results show that our approach outperforms 0-shot CoT and manual few-shot CoT in a variety of reasoning tasks, including math problem solving in GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None (the abstract does not mention any limitations of LLMs, but rather presents a new prompting approach to improve their performance).",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None (the abstract does not mention any limitations of LLMs, but rather presents a new prompting approach to improve their performance)."
    },
    {
        "title": "OMNI: Open-endedness via Models of human Notions of Interestingness",
        "authors": "Jenny Zhang;Joel Lehman;Kenneth Stanley;Jeff Clune",
        "site": "https://iclr.cc/virtual/2024/poster/19242",
        "summary": "Abstract:Open-ended algorithms aim to learn new, interesting behaviors forever. That requires a vast environment search space, but there are thus infinitely many possible tasks. Even after filtering for tasks the current agent can learn (i.e., learning progress), countless learnable yet uninteresting tasks remain (e.g., minor variations of previously learned tasks). An Achilles Heel of open-endedness research is the inability to quantify (and thus prioritize) tasks that are not just learnable, but also $\\textit{interesting}$ (e.g., worthwhile and novel). We propose solving this problem by $\\textit{Open-endedness via Models of human Notions of Interestingness}$ (OMNI). The insight is that we can utilize foundation models (FMs) as a model of interestingness (MoI), because they $\\textit{already}$ internalize human concepts of interestingness from training on vast amounts of human-generated data, where humans naturally write about what they find interesting or boring. We show that FM-based MoIs improve open-ended learning by focusing on tasks that are both learnable $\\textit{and interesting}$, outperforming baselines based on uniform task sampling or learning progress alone. This approach has the potential to dramatically advance the ability to intelligently select which tasks to focus on next (i.e., auto-curricula), and could be seen as AI selecting its own next task to learn, facilitating self-improving AI and AI-Generating Algorithms.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"because they internalize human concepts of interestingness from training on vast amounts of human-generated data\"\n\nNote that the paper does not explicitly discuss limitations of LLMs but mentions their capabilities and how they can be utilized to improve open-ended learning.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"because they internalize human concepts of interestingness from training on vast amounts of human-generated data\"\n\nNote that the paper does not explicitly discuss limitations of LLMs but mentions their capabilities and how they can be utilized to improve open-ended learning."
    },
    {
        "title": "BatchPrompt: Accomplish more with less",
        "authors": "Jianzhe Lin;Maurice Diesendruck;Liang Du;Robin Abraham",
        "site": "https://iclr.cc/virtual/2024/poster/19241",
        "summary": "The ever-increasing token limits of large language models (LLMs) have enabled long context as input. Many LLMs are trained and fine-tuned to perform zero/few-shot inference using instruction-based prompts. Prompts typically include a detailed task instruction, several examples, and a single data point for inference. This baseline is referred to as “SinglePrompt” in this paper. In terms of token count, when the data input is small compared to instructions and examples, this results in lower token utilization, compared with encoder-based models like fine-tuned BERT. This cost inefficiency, affecting inference speed and compute budget, counteracts many of the benefits that LLMs offer. This paper aims to alleviate this problem by batching multiple data points in each prompt, a strategy we refer to as “BatchPrompt”. We improve token utilization by increasing the “density” of data points, however, this cannot be done naively. Simple batching can degrade performance, especially as batch size increases, and data points can yield different answers depending on their position within a prompt. To address the quality issue while retaining high token utilization, we introduce Batch Permutation and Ensembling (BPE) for BatchPrompt – a simple majority vote over repeated permutations of data, that recovers label quality at the cost of more token usage. To counterbalance this cost, we further propose Self-reflection-guided EArly Stopping (SEAS), which can terminate the voting process early for data points that the LLM handles confidently. Our comprehensive experimental evaluation demonstrates that BPE + SEAS can boost the performance of BatchPrompt by a striking margin on a range of popular NLP tasks, including question answering (Boolq), textual entailment (RTE), and duplicate questions identification (QQP). This performance is even competitive with/higher than single-data prompting (SinglePrompt), while using far fewer LLM calls and input tokens. At batch size 32, our BatchPrompt + BPE + SEAS uses 15.7% the number of LLM calls, and achieves: Boolq accuracy 90.6% → 90.9% with 27.4% tokens, QQP accuracy 87.2% → 88.4% with 18.6% tokens, RTE accuracy 91.5% → 91.1% with 30.8% tokens. We hope our simple yet effective approach will shed light on the future research of large language models. Code: github.com/microsoft/BatchPrompt",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This cost inefficiency, affecting inference speed and compute budget, counteracts many of the benefits that LLMs offer.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"This cost inefficiency, affecting inference speed and compute budget, counteracts many of the benefits that LLMs offer.\""
    },
    {
        "title": "KoLA: Carefully Benchmarking World Knowledge of Large Language Models",
        "authors": "Jifan Yu;Xiaozhi Wang;Shangqing Tu;Shulin Cao;Daniel Zhang-Li;Xin Lv;Hao Peng;Zijun Yao;Xiaohan Zhang;Hanming Li;Chunyang Li;Zheyuan Zhang;Yushi Bai;Yantao Liu;Amy Xin;Kaifeng Yun;Linlu GONG;Nianyi Lin;Jianhui Chen;Zhili Wu;Yunjia Qi;Weikai Li;Yong Guan;Kaisheng Zeng;Ji Qi;Hailong Jin;Jinxin Liu;Yu Gu;Yuan Yao;Ning Ding;Lei Hou;Zhiyuan Liu;Xu Bin;Jie Tang;Juanzi Li",
        "site": "https://iclr.cc/virtual/2024/poster/19238",
        "summary": "The unprecedented performance of large language models (LLMs) necessitates improvements in evaluations. Rather than merely exploring the breadth of LLM abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations. Given the importance of world knowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark (KoLA), in which we carefully design three crucial factors: (1) For ability modeling, we mimic human cognition to form a four-level taxonomy of knowledge-related abilities, covering 19 tasks. (2) For data, to ensure fair comparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs, along with continuously collected emerging corpora, aiming to evaluate the capacity to handle unseen data and evolving knowledge. (3) For evaluation criteria, we adopt a contrastive system, including overall standard scores for better numerical comparability across tasks and models, and a unique self-contrast metric for automatically evaluating knowledge-creating ability. We evaluate 21 open-source and commercial LLMs and obtain some intriguing findings. The KoLA dataset will be updated every three months to provide timely references for developing LLMs and knowledge-related systems.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Rather than merely exploring the breadth of LLM abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations.\"\n\nThis abstract mentions the need for improved evaluations of LLMs, implying that current evaluations may have limitations, but it does not elaborate on specific limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Rather than merely exploring the breadth of LLM abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations.\"\n\nThis abstract mentions the need for improved evaluations of LLMs, implying that current evaluations may have limitations, but it does not elaborate on specific limitations of LLMs."
    },
    {
        "title": "What Algorithms can Transformers Learn? A Study in Length Generalization",
        "authors": "Hattie Zhou;Arwen Bradley;Etai Littwin;Noam Razin;Omid Saremi;Joshua M. Susskind;Samy Bengio;Preetum Nakkiran",
        "site": "https://iclr.cc/virtual/2024/poster/19236",
        "summary": "Large language models exhibit surprising emergent generalization properties, yet also struggle on many simple reasoning tasks such as arithmetic and parity. In this work, we focus on length generalization, and we propose a unifying framework to understand when and how Transformers can be expected to length generalize on a given task. First, we show that there exist algorithmic tasks for which standarddecoder-only Transformers trained from scratch naturally exhibit strong length generalization. For these tasks, we leverage the RASP programming language (Weiss et al., 2021) to show that the correct algorithmic solution which solves the task can be represented by a simple Transformer. We thus propose the RASP-Generalization Conjecture: Transformers tend to learn a length-generalizing solution if there exists a short RASP-L program that works for all input lengths. We present empirical evidence to support the correlation between RASP-simplicity and generalization. We leverage our insights to give new scratchpad formats which yield strong length generalization on traditionally hard tasks (such as parity and addition), and we illustrate how scratchpad can hinder generalization when it increases the complexity of the corresponding RASP-L program. Overall, our work provides a novel perspective on the mechanisms of length generalization and the algorithmic capabilities of Transformers.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models exhibit surprising emergent generalization properties, yet also struggle on many simple reasoning tasks such as arithmetic and parity.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Large language models exhibit surprising emergent generalization properties, yet also struggle on many simple reasoning tasks such as arithmetic and parity.\""
    },
    {
        "title": "Function Vectors in Large Language Models",
        "authors": "Eric Todd;Millicent Li;Arnab Sen Sharma;Aaron Mueller;Byron C Wallace;David Bau",
        "site": "https://iclr.cc/virtual/2024/poster/19235",
        "summary": "We report the presence of a simple neural mechanism that represents an input-output function as a vector within autoregressive transformer language models (LMs). Using causal mediation analysis on a diverse range of in-context-learning (ICL) tasks, we find that a small number attention heads transport a compact representation of the demonstrated task, which we call a function vector (FV).  FVs are robust to changes in context, i.e., they trigger execution of the task on inputs such as zero-shot and natural text settings that do not resemble the ICL contexts from which they are collected. We test FVs across a range of tasks, models, and layers and find strong causal effects across settings in middle layers. We investigate the internal structure of FVs and find while that they often contain information that encodes the output space of the function, this information alone is not sufficient to reconstruct an FV. Finally, we test semantic vector composition in FVs, and find that to some extent they can be summed to create vectors that trigger new complex tasks. Our findings show that compact, causal internal vector representations of function abstractions can be explicitly extracted from LLMs.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search",
        "authors": "Yuchen Zhuang;Xiang Chen;Tong Yu;Saayan Mitra;Victor Bursztyn;Ryan A. Rossi;Somdeb Sarkhel;Chao Zhang",
        "site": "https://iclr.cc/virtual/2024/poster/19230",
        "summary": "Abstract:Large language models (LLMs) have demonstrated powerful decision-making and planning capabilities in solving complicated real-world problems. LLM-based autonomous agents can interact with diverse tools (e.g., functional APIs) and generate solution plans that execute a series of API function calls in a step-by-step manner. The multitude of candidate API function calls significantly expands the action space, amplifying the critical need for efficient action space navigation. However, existing methods either struggle with unidirectional exploration in expansive action spaces, trapped into a locally optimal solution, or suffer from exhaustively traversing all potential actions, causing inefficient navigation. To address these issues, we propose ToolChain*, an efficient tree search-based planning algorithm for LLM-based agents. It formulates the entire action space as a decision tree, where each node represents a possible API function call involved in a solution plan. By incorporating the A$^*$ search algorithm with task-specific cost function design, it efficiently prunes high-cost branches that may involve incorrect actions, identifying the most low-cost valid path as the solution. Extensive experiments on multiple tool-use and reasoning tasks demonstrate that ToolChain* efficiently balances exploration and exploitation within an expansive action space. It outperforms state-of-the-art baselines on planning and reasoning tasks by 3.1% and 3.5% on average while requiring 7.35x and 2.31x less time, respectively.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing methods either struggle with unidirectional exploration in expansive action spaces, trapped into a locally optimal solution, or suffer from exhaustively traversing all potential actions, causing inefficient navigation.\"\n\nThis evidence briefly mentions limitations of existing methods for action space navigation in LLMs, but does not elaborate on the limitations of LLMs themselves.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, existing methods either struggle with unidirectional exploration in expansive action spaces, trapped into a locally optimal solution, or suffer from exhaustively traversing all potential actions, causing inefficient navigation.\"\n\nThis evidence briefly mentions limitations of existing methods for action space navigation in LLMs, but does not elaborate on the limitations of LLMs themselves."
    },
    {
        "title": "Compressing LLMs: The Truth is Rarely Pure and Never Simple",
        "authors": "AJAY KUMAR JAISWAL;Zhe Gan;Xianzhi Du;Bowen Zhang;Zhangyang Wang;Yinfei Yang",
        "site": "https://iclr.cc/virtual/2024/poster/19229",
        "summary": "Abstract:Despite their remarkable achievements, modern Large Language Models (LLMs) encounter exorbitant computational and memory footprints. Recently, several works have shown significant success in *training-free* and  *data-free* compression (pruning and quantization) of LLMs achieving 50-60\\% sparsity and reducing the bit-width down to 3 or 4 bits per weight, with negligible perplexity degradation over the uncompressed baseline. As recent research efforts are focused on developing increasingly sophisticated compression methods, our work takes a step back, and re-evaluates the effectiveness of existing SoTA compression methods, which rely on a fairly simple and widely questioned metric, perplexity (even for dense LLMs). We introduce **K**nowledge-**I**ntensive **C**ompressed LLM Benchmar**K** **(LLM-KICK)**, a collection of carefully-curated tasks to re-define the evaluation protocol for compressed LLMs, which have significant alignment with their dense counterparts, and perplexity fail to capture subtle change in their true capabilities. LLM-KICK unveils many favorable merits and unfortunate plights of current SoTA compression methods: all pruning methods suffer significant performance degradation, sometimes at trivial sparsity ratios (*e.g.*, 25-30\\%), and fail for N:M sparsity on knowledge-intensive tasks; current quantization methods are more successful than pruning; yet, pruned LLMs even at $\\geq 50$\\% sparsity are robust in-context retrieval and summarization systems; among others. LLM-KICK is designed to holistically access compressed LLMs' ability for language understanding, reasoning, generation, in-context retrieval, in-context summarization, *etc.* We hope our study can foster the development of better LLM compression methods. The reproduced codes are available at https://github.com/VITA-Group/llm-kick.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"all pruning methods suffer significant performance degradation, sometimes at trivial sparsity ratios (*e.g.*, 25-30\\%), and fail for N:M sparsity on knowledge-intensive tasks; current quantization methods are more successful than pruning; yet, pruned LLMs even at $\\geq$50$\\% sparsity are robust in-context retrieval and summarization systems; among others",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"all pruning methods suffer significant performance degradation, sometimes at trivial sparsity ratios (*e.g.*, 25-30\\%), and fail for N:M sparsity on knowledge-intensive tasks; current quantization methods are more successful than pruning; yet, pruned LLMs even at $\\geq$50$\\% sparsity are robust in-context retrieval and summarization systems; among others"
    },
    {
        "title": "A Multi-Level Framework for Accelerating Training Transformer Models",
        "authors": "Longwei Zou;Han Zhang;Yangdong Deng",
        "site": "https://iclr.cc/virtual/2024/poster/19223",
        "summary": "The fast growing capabilities of large-scale deep learning models, such as Bert, GPT and ViT, are revolutionizing the landscape of NLP, CV and many other domains. Training such models, however, poses an unprecedented demand for computing power, which incurs exponentially increasing energy cost and carbon dioxide emissions. It is thus critical to develop efficient training solutions to reduce the training costs. Motivated by a set of key observations of inter- and intra-layer similarities among feature maps and attentions that can be identified from typical training processes, we propose a multi-level framework for training acceleration. Specifically, the framework is based on three basic operators, Coalescing, De-coalescing and Interpolation, which can be orchestrated to build a multi-level training framework. The framework consists of a V-cycle training process, which progressively down- and up-scales the model size and projects the parameters between adjacent levels of models via coalescing and de-coalescing. The key idea is that a smaller model that can be trained for fast convergence and the trained parameters provides high-qualities intermediate solutions for the next level larger network. The interpolation operator is designed to break the symmetry of neurons incurred by de-coalescing for better convergence performance. Our experiments on transformer-based language models (e.g. Bert, GPT) as well as a vision model (e.g. DeiT) prove that the proposed framework reduces the computational cost by about 20% on training BERT/GPT-Base models and up to 51.6% on training the BERT-Large model while preserving the performance.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Training such models, however, poses an unprecedented demand for computing power, which incurs exponentially increasing energy cost and carbon dioxide emissions.\"\n\nThis paper discusses LLMs, but only mentions one limitation (high computational cost) in passing, without elaborating or analyzing it further.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Training such models, however, poses an unprecedented demand for computing power, which incurs exponentially increasing energy cost and carbon dioxide emissions.\"\n\nThis paper discusses LLMs, but only mentions one limitation (high computational cost) in passing, without elaborating or analyzing it further."
    },
    {
        "title": "CellPLM: Pre-training of Cell Language Model Beyond Single Cells",
        "authors": "Hongzhi Wen;Wenzhuo Tang;Xinnan Dai;Jiayuan Ding;Wei Jin;Yuying Xie;Jiliang Tang",
        "site": "https://iclr.cc/virtual/2024/poster/19221",
        "summary": "Abstract:The current state-of-the-art single-cell pre-trained models are greatly inspired by the success of large language models. They trained transformers by treating genes as tokens and cells as sentences. However, three fundamental differences between single-cell data and natural language data are overlooked: (1) scRNA-seq data are presented as bag-of-genes instead of sequences of RNAs; (2) Cell-cell relations are more intricate and important than inter-sentence relations; and (3) The quantity of single-cell data is considerably inferior to text data, and they are very noisy. In light of these characteristics, we propose a new pre-trained model, $\\textit{CellPLM}$, which takes cells as tokens and tissues as sentences. In addition, we leverage spatially-resolved transcriptomic data in pre-training to facilitate learning cell-cell relationships and introduce a Gaussian prior distribution as an additional inductive bias to overcome data limitations. $\\textit{CellPLM}$ is the first single-cell pre-trained transformer that encodes cell-cell relations and it consistently outperforms existing pre-trained and non-pre-trained models in diverse downstream tasks, with 100 times higher inference speed on generating cell embeddings than previous pre-trained models.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, three fundamental differences between single-cell data and natural language data are overlooked: (1) scRNA-seq data are presented as bag-of-genes instead of sequences of RNAs; (2) Cell-cell relations are more intricate and important than inter-sentence relations; and (3) The quantity of single-cell data is considerably inferior to text data, and they are very",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, three fundamental differences between single-cell data and natural language data are overlooked: (1) scRNA-seq data are presented as bag-of-genes instead of sequences of RNAs; (2) Cell-cell relations are more intricate and important than inter-sentence relations; and (3) The quantity of single-cell data is considerably inferior to text data, and they are very"
    },
    {
        "title": "Demystifying Poisoning Backdoor Attacks from a Statistical Perspective",
        "authors": "Ganghua Wang;Xun Xian;Ashish Kundu;Jayanth Srinivasa;Xuan Bi;Mingyi Hong;Jie Ding",
        "site": "https://iclr.cc/virtual/2024/poster/19218",
        "summary": "Backdoor attacks pose a significant security risk to machine learning applications due to their stealthy nature and potentially serious consequences. Such attacks involve embedding triggers within a learning model with the intention of causing malicious behavior when an active trigger is present while maintaining regular functionality without it. This paper derives a fundamental understanding of backdoor attacks that applies to both discriminative and generative models, including diffusion models and large language models. We evaluate the effectiveness of any backdoor attack incorporating a constant trigger, by establishing tight lower and upper boundaries for the performance of the compromised model on both clean and backdoor test data. The developed theory answers a series of fundamental but previously underexplored problems, including (1) what are the determining factors for a backdoor attack's success, (2) what is the direction of the most effective backdoor attack, and (3) when will a human-imperceptible trigger succeed. We demonstrate the theory by conducting experiments using benchmark datasets and state-of-the-art backdoor attack scenarios. Our code is available \\href{https://github.com/KeyWgh/DemystifyBackdoor}{here}.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This paper derives a fundamental understanding of backdoor attacks that applies to both discriminative and generative models, including diffusion models and large language models.\"\n\nNote that the paper does not exclusively focus on LLMs, but it does discuss a significant limitation (security risk) of LLMs, which is a major part of the paper's contribution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"This paper derives a fundamental understanding of backdoor attacks that applies to both discriminative and generative models, including diffusion models and large language models.\"\n\nNote that the paper does not exclusively focus on LLMs, but it does discuss a significant limitation (security risk) of LLMs, which is a major part of the paper's contribution."
    },
    {
        "title": "What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning",
        "authors": "Wei Liu;Weihao Zeng;Keqing He;Yong Jiang;Junxian He",
        "site": "https://iclr.cc/virtual/2024/poster/19215",
        "summary": "Instruction tuning is a standard technique employed to align large language models to end tasks and user preferences after the initial pretraining phase. Recent research indicates the critical role of data engineering in instruction tuning -- when appropriately selected, only limited data is necessary to achieve superior performance. However, we still lack a principled understanding of what makes good instruction tuning data for alignment, and how we should select data automatically and effectively. In this work, we delve deeply into automatic data selection strategies for alignment. We start with controlled studies to measure data across three dimensions: complexity, quality, and diversity, along which we examine existing methods and introduce novel techniques for enhanced data measurement. Subsequently, we propose a simple strategy to select data samples based on the measurement. We present Deita (short for Data-Efficient Instruction Tuning for Alignment), a series of models fine-tuned from LLaMA models using data samples automatically selected with our proposed approach.  When assessed through both automatic metrics and human evaluation, Deita performs better or on par with the state-of-the-art open-source alignment models such as Vicuna and WizardLM with only 6K training data samples -- 10x less than the data used in the baselines. We anticipate this work to provide clear guidelines and tools on automatic data selection, aiding researchers and practitioners in achieving data-efficient alignment.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent research indicates the critical role of data engineering in instruction tuning -- when appropriately selected, only limited data is necessary to achieve superior performance.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs in the context of instruction tuning, specifically the need for careful data selection to achieve superior performance. However, this limitation is not explored in depth, and the primary",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Recent research indicates the critical role of data engineering in instruction tuning -- when appropriately selected, only limited data is necessary to achieve superior performance.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs in the context of instruction tuning, specifically the need for careful data selection to achieve superior performance. However, this limitation is not explored in depth, and the primary"
    },
    {
        "title": "AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?",
        "authors": "Qi Zhao;Shijie Wang;Ce Zhang;Changcheng Fu;Minh Quan Do;Nakul Agarwal;Kwonjoon Lee;Chen Sun",
        "site": "https://iclr.cc/virtual/2024/poster/19210",
        "summary": "Can we better anticipate an actor’s future actions (e.g. mix eggs) by knowing what commonly happens after the current action (e.g. crack eggs)? What if the actor also shares the goal (e.g. make fried rice) with us? The long-term action anticipation (LTA) task aims to predict an actor’s future behavior from video observations in the form of verb and noun sequences, and it is crucial for human-machine interaction.We propose to formulate the LTA task from two perspectives: a bottom-up approach that predicts the next actions autoregressively by modeling temporal dynamics; and a top-down approach that infers the goal of the actor and plans the needed procedure to accomplish the goal. We hypothesize that large language models (LLMs), which have been pretrained on procedure text data (e.g. recipes, how-tos),have the potential to help LTA from both perspectives. It can help provide the prior knowledge on the possible next actions, and infer the goal given the observed part of a procedure, respectively. We propose AntGPT, which represents video observations as sequences of human actions, and uses the action representation for an LLM to infer the goals and model temporal dynamics. AntGPT achieves state-of-the-art performance on Ego4D LTA v1 and v2, EPIC-Kitchens-55, as well as EGTEA GAZE+, thanks to LLMs’ goal inference and temporal dynamics modeling capabilities. We further demonstrate that these capabilities can be effectively distilled into a compact neural network 1.3% of the original LLM model size. Code and model will be released upon acceptance.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We hypothesize that large language models (LLMs), which have been pretrained on procedure text data (e.g. recipes, how-tos), have the potential to help LTA from both perspectives.\"\n\nThis paper discusses LLMs but does not mention any limitations of the models in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"We hypothesize that large language models (LLMs), which have been pretrained on procedure text data (e.g. recipes, how-tos), have the potential to help LTA from both perspectives.\"\n\nThis paper discusses LLMs but does not mention any limitations of the models in the abstract."
    },
    {
        "title": "Large Language Models as Optimizers",
        "authors": "Chengrun Yang;Xuezhi Wang;Yifeng Lu;Hanxiao Liu;Quoc V Le;Denny Zhou;Xinyun Chen",
        "site": "https://iclr.cc/virtual/2024/poster/19209",
        "summary": "Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to our main application in prompt optimization, where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks. Code at https://github.com/google-deepmind/opro.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the abstract only discusses the application and effectiveness of LLMs as optimizers, without mentioning any limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the abstract only discusses the application and effectiveness of LLMs as optimizers, without mentioning any limitations."
    },
    {
        "title": "PB-LLM: Partially Binarized Large Language Models",
        "authors": "Zhihang Yuan;Yuzhang Shang;Zhen Dong",
        "site": "https://iclr.cc/virtual/2024/poster/19207",
        "summary": "This paper explores network binarization, a radical form of quantization, compressing model weights to a single bit, specifically for Large Language Models (LLMs) compression. Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized LLMs. Specifically, our exploration first uncovers the ineffectiveness of naïve applications of existing binarization algorithms and highlights the imperative role of salient weights in achieving low-bit quantization. Thus, PB-LLM filters a small ratio of salient weights during binarization, allocating them to higher-bit storage, i.e., partially-binarization. PB-LLM is extended to recover the capacities of quantized LMMs, by analyzing from the perspective of post-training quantization (PTQ) and quantization-aware training (QAT). Under PTQ, combining the concepts from GPTQ, we reconstruct the binarized weight matrix guided by the Hessian matrix and successfully recover the reasoning capacity of PB-LLM in low-bit. Under QAT, we freeze the salient weights during training, explore the derivation of optimal scaling factors crucial for minimizing the quantization error, and propose a scaling mechanism based on this derived scaling strategy for residual binarized weights. Those explorations and the developed methodologies significantly contribute to rejuvenating the performance of low-bit quantized LLMs and present substantial advancements in the field of network binarization for LLMs. Code is available at https://github.com/hahnyuan/PB-LLM.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized LLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized LLMs.\""
    },
    {
        "title": "Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature",
        "authors": "Guangsheng Bao;Yanbin Zhao;Zhiyang Teng;Linyi Yang;Yue Zhang",
        "site": "https://iclr.cc/virtual/2024/poster/19201",
        "summary": "Large language models (LLMs) have shown the ability to produce fluent and cogent content, presenting both productivity opportunities and societal risks. To build trustworthy AI systems, it is imperative to distinguish between machine-generated and human-authored content. The leading zero-shot detector, DetectGPT, showcases commendable performance but is marred by its intensive computational  costs. In this paper, we introduce the concept ofconditional probability curvatureto elucidate discrepancies in word choices between LLMs and humans within a given context. Utilizing this curvature as a foundational metric, we presentFast-DetectGPT, an optimized zero-shot detector, which substitutes DetectGPT's perturbation step with a more efficient sampling step. Our evaluations on various datasets, source models, and test conditions indicate that Fast-DetectGPT not only surpasses DetectGPT by a relative around 75\\% in both the white-box and black-box settings but also accelerates the detection process by a factor of 340, as detailed in Table 1.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models (LLMs) have shown the ability to produce fluent and cogent content, presenting both productivity opportunities and societal risks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large language models (LLMs) have shown the ability to produce fluent and cogent content, presenting both productivity opportunities and societal risks.\""
    },
    {
        "title": "Enhancing Human Experience in Human-Agent Collaboration: A Human-Centered Modeling Approach Based on Positive Human Gain",
        "authors": "Yiming Gao;Feiyu Liu;Liang Wang;Dehua Zheng;Zhenjie Lian;Weixuan Wang;Wenjin Yang;Siqin Li;Xianliang Wang;Wenhui Chen;Jing Dai;QIANG FU;Yang Wei;Lanxiao Huang;Wei Liu",
        "site": "https://iclr.cc/virtual/2024/poster/19199",
        "summary": "Existing game AI research mainly focuses on enhancing agents' abilities to win games, but this does not inherently make humans have a better experience when collaborating with these agents. For example, agents may dominate the collaboration and exhibit unintended or detrimental behaviors, leading to poor experiences for their human partners. In other words, most game AI agents are modeled in a \"self-centered\" manner. In this paper, we propose a \"human-centered\" modeling scheme for collaborative agents that aims to enhance the experience of humans. Specifically, we model the experience of humans as the goals they expect to achieve during the task. We expect that agents should learn to enhance the extent to which humans achieve these goals while maintaining agents' original abilities (e.g., winning games). To achieve this, we propose the Reinforcement Learning from Human Gain (RLHG) approach. The RLHG approach introduces a \"baseline\", which corresponds to the extent to which humans primitively achieve their goals, and encourages agents to learn behaviors that can effectively enhance humans in achieving their goals better. We evaluate the RLHG agent in the popular Multi-player Online Battle Arena (MOBA) game, Honor of Kings, by conducting real-world human-agent tests. Both objective performance and subjective preference results show that the RLHG agent provides participants better gaming experience.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation",
        "authors": "Suhyeon Lee;Won Jun Kim;Jinho Chang;Jong Chul Ye",
        "site": "https://iclr.cc/virtual/2024/poster/19198",
        "summary": "Following the impressive development of LLMs, vision-language alignment in LLMs is actively being researched to enable multimodal reasoning and visual input/output. This direction of research is particularly relevant to medical imaging because accurate medical image analysis and generation consist of a combination of reasoning based on visual features and prior knowledge. Many recent works have focused on training adapter networks that serve as an information bridge between image processing (encoding or generating) networks and LLMs; but presumably, in order to achieve maximum reasoning potential of LLMs on visual information as well, visual and language features should be allowed to interact more freely. This is especially important in the medical domain because understanding and generating medical images such as chest X-rays (CXR) require not only accurate visual and language-based reasoning but also a more intimate mapping between the two modalities. Thus, taking inspiration from previous work on the transformer and VQ-GAN combination for bidirectional image and text generation, we build upon this approach and develop a method for instruction-tuning an LLM pre-trained only on text to gain vision-language capabilities for medical images. Specifically, we leverage a pretrained LLM’s existing question-answering and instruction-following abilities to teach it to understand visual inputs by instructing it to answer questions about image inputs and, symmetrically, output both text and image responses appropriate to a given query by tuning the LLM with diverse tasks that encompass image-based text-generation and text-based image-generation. We show that our LLM-CXR trained in this approach shows better image-text alignment in both CXR understanding and generation tasks while being smaller in size compared to previously developed models that perform a narrower range of tasks.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Many recent works have focused on training adapter networks that serve as an information bridge between image processing (encoding or generating) networks and LLMs; but presumably, in order to achieve maximum reasoning potential of LLMs on visual information as well, visual and language features should be allowed to interact more freely.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Many recent works have focused on training adapter networks that serve as an information bridge between image processing (encoding or generating) networks and LLMs; but presumably, in order to achieve maximum reasoning potential of LLMs on visual information as well, visual and language features should be allowed to interact more freely.\""
    },
    {
        "title": "Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images",
        "authors": "Kuofeng Gao;Yang Bai;Jindong Gu;Shu-Tao Xia;Philip Torr;Zhifeng Li;Wei Liu",
        "site": "https://iclr.cc/virtual/2024/poster/19194",
        "summary": "Large vision-language models (VLMs) such as GPT-4 have achieved exceptional performance across various multi-modal tasks. However, the deployment of VLMs necessitates substantial energy consumption and computational resources. Once attackers maliciously induce high energy consumption and latency time (energy-latency cost) during inference of VLMs, it will exhaust computational resources. In this paper, we explore this attack surface about availability of VLMs and aim to induce high energy-latency cost during inference of VLMs. We find that high energy-latency cost during inference of VLMs can be manipulated by maximizing the length of generated sequences. To this end, we propose verbose images, with the goal of crafting an imperceptible perturbation to induce VLMs to generate long sentences during inference. Concretely, we design three loss objectives. First, a loss is proposed to delay the occurrence of end-of-sequence (EOS) token, where EOS token is a signal for VLMs to stop generating further tokens. Moreover, an uncertainty loss and a token diversity loss are proposed to increase the uncertainty over each generated token and the diversity among all tokens of the whole generated sequence, respectively, which can break output dependency at token-level and sequence-level. Furthermore, a temporal weight adjustment algorithm is proposed, which can effectively balance these losses. Extensive experiments demonstrate that our verbose images can increase the length of generated sequences by 7.87× and 8.56× compared to original images on MS-COCO and ImageNet datasets, which presents potential challenges for various applications.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Once attackers maliciously induce high energy consumption and latency time (energy-latency cost) during inference of VLMs, it will exhaust computational resources.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Once attackers maliciously induce high energy consumption and latency time (energy-latency cost) during inference of VLMs, it will exhaust computational resources.\""
    },
    {
        "title": "Test-Time Training on Nearest Neighbors for Large Language Models",
        "authors": "Moritz Hardt;Yu Sun",
        "site": "https://iclr.cc/virtual/2024/poster/19176",
        "summary": "Many recent efforts augment language models with retrieval, by adding retrieved data to the input context. For this approach to succeed, the retrieved data must be added at both training and test time. Moreover, as input length grows linearly with the size of retrieved data, cost in computation and memory grows quadratically for modern Transformers. To avoid these complications, we simply fine-tune the model on retrieved data at test time, using its standard training setup. We build a large-scale distributed index based on text embeddings of the Pile dataset. For each test input, our system retrieves its neighbors and fine-tunes the model on their text. Surprisingly, retrieving and training on as few as 20 neighbors, each for only one gradient iteration, drastically improves performance across more than 20 language modeling tasks in the Pile. For example, test-time training with nearest neighbors significantly narrows the performance gap between a small GPT-2 and a GPT-Neo model more than 10 times larger. Sufficient index quality and size, however, are necessary. Our work establishes a first baseline of test-time training for language modeling.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Sufficient index quality and size, however, are necessary.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Sufficient index quality and size, however, are necessary.\""
    },
    {
        "title": "AutoCast++: Enhancing World Event Prediction with Zero-shot Ranking-based Context Retrieval",
        "authors": "Qi Yan;Raihan Seraj;Jiawei He;Lili Meng;Tristan Sylvain",
        "site": "https://iclr.cc/virtual/2024/poster/19174",
        "summary": "Machine-based prediction of real-world events is garnering attention due to its potential for informed decision-making. Whereas traditional forecasting predominantly hinges on structured data like time-series, recent breakthroughs in language models enable predictions using unstructured text. In particular, (Zou et al., 2022) unveils AutoCast, a new benchmark that employs news articles for answering forecasting queries. Nevertheless, existing methods still trail behind human performance. The cornerstone of accurate forecasting, we argue, lies in identifying a concise, yet rich subset of news snippets from a vast corpus. With this motivation, we introduce AutoCast++, a zero-shot ranking-based context retrieval system, tailored to sift through expansive news document collections for event forecasting. Our approach first re-ranks articles based on zero-shot question-passage relevance, honing in on semantically pertinent news. Following this, the chosen articles are subjected to zero-shot summarization to attain succinct context. Leveraging a pre-trained language model, we conduct both the relevance evaluation and article summarization without needing domain-specific training. Notably, recent articles can sometimes be at odds with preceding ones due to new facts or unanticipated incidents, leading to fluctuating temporal dynamics. To tackle this, our re-ranking mechanism gives preference to more recent articles, and we further regularize the multi-passage representation learning to align with human forecaster responses made on different dates. Empirical results underscore marked improvements across multiple metrics, improving the performance for multiple-choice questions (MCQ) by 48% and true/false (TF) questions by up to 8%. Code is available at https://github.com/BorealisAI/Autocast-plus-plus.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Nevertheless, existing methods still trail behind human performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Nevertheless, existing methods still trail behind human performance.\""
    },
    {
        "title": "WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions",
        "authors": "Can Xu;Qingfeng Sun;Kai Zheng;Xiubo Geng;Pu Zhao;Jiazhan Feng;Chongyang Tao;Qingwei Lin;Daxin Jiang",
        "site": "https://iclr.cc/virtual/2024/poster/19164",
        "summary": "Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Both automatic and human evaluations consistently indicate that WizardLM outperforms baselines such as Alpaca (trained from Self-Instruct) and Vicuna (trained from human-created instructions). The experimental results demonstrate that the quality of instruction-following dataset crafted by Evol-Instruct can significantly improve the performance of LLMs.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs in passing, specifically the challenge of creating instruction data for training, but it is not the primary focus of the paper. The paper focuses on proposing a solution to this limitation rather",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs in passing, specifically the challenge of creating instruction data for training, but it is not the primary focus of the paper. The paper focuses on proposing a solution to this limitation rather"
    },
    {
        "title": "Efficient Multi-agent Reinforcement Learning by Planning",
        "authors": "Qihan Liu;Jianing Ye;Xiaoteng Ma;Jun Yang;Bin Liang;Chongjie Zhang",
        "site": "https://iclr.cc/virtual/2024/poster/19160",
        "summary": "Abstract:Multi-agent reinforcement learning (MARL) algorithms have accomplished remarkable breakthroughs in solving large-scale decision-making tasks. Nonetheless, most existing MARL algorithms are model-free, limiting sample efficiency and hindering their applicability in more challenging scenarios. In contrast, model-based reinforcement learning (MBRL), particularly algorithms integrating planning, such as MuZero, has demonstrated superhuman performance with limited data in many tasks. Hence, we aim to boost the sample efficiency of MARL by adopting model-based approaches. However, incorporating planning and search methods into multi-agent systems poses significant challenges. The expansive action space of multi-agent systems often necessitates leveraging the nearly-independent property of agents to accelerate learning. To tackle this issue, we propose the MAZero algorithm, which combines a centralized model with Monte Carlo Tree Search (MCTS) for policy search. We design an ingenious network structure to facilitate distributed execution and parameter sharing. To enhance search efficiency in deterministic environments with sizable action spaces, we introduce two novel techniques: Optimistic Search Lambda (OS($\\lambda$)) and Advantage-Weighted Policy Optimization (AWPO). Extensive experiments on the SMAC benchmark demonstrate that MAZero outperforms model-free approaches in terms of sample efficiency and provides comparable or better performance than existing model-based methods in terms of both sample and computational efficiency.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "LLCP: Learning Latent Causal Processes for Reasoning-based Video Question Answer",
        "authors": "Guangyi Chen;Yuke Li;Xiao Liu;Zijian Li;Eman Al Suradi;Donglai Wei;Kun Zhang",
        "site": "https://iclr.cc/virtual/2024/poster/19158",
        "summary": "Current approaches to Video Question Answering (VideoQA) primarily focus on cross-modality matching, which is limited by the requirement for extensive data annotations and the insufficient capacity for causal reasoning (e.g. attributing accidents). To address these challenges, we introduce a causal framework for video reasoning, termed Learning Latent Causal Processes (LLCP). At the heart of LLCP lies a multivariate generative model designed to analyze the spatial-temporal dynamics of objects within events. Leveraging the inherent modularity of causal mechanisms, we train the model through self-supervised local auto-regression eliminating the need for annotated question-answer pairs. During inference, the model is applied to answer two types of reasoning questions: accident attribution, which infers the cause from observed effects, and counterfactual prediction, which predicts the effects of counterfactual conditions given the factual evidence. In the first scenario, we identify variables that deviate from the established distribution by the learned model, signifying the root cause of accidents. In the second scenario, we replace embeddings of previous variables with counterfactual ones, enabling us to forecast potential developments. Once we have identified these cause/effect variables, natural language answers are derived through a combination of grammatical parsing and a pre-trained vision-language model. We assess the efficacy of LLCP on both synthetic and real-world data, demonstrating comparable performance to supervised methods despite our framework using no paired textual annotations.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Current approaches to Video Question Answering (VideoQA) primarily focus on cross-modality matching, which is limited by the requirement for extensive data annotations and the insufficient capacity for causal reasoning (e.g. attributing accidents).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Current approaches to Video Question Answering (VideoQA) primarily focus on cross-modality matching, which is limited by the requirement for extensive data annotations and the insufficient capacity for causal reasoning (e.g. attributing accidents).\""
    },
    {
        "title": "Finite-Time Analysis of On-Policy Heterogeneous Federated Reinforcement Learning",
        "authors": "Chenyu Zhang;Han Wang;Aritra Mitra;James Anderson",
        "site": "https://iclr.cc/virtual/2024/poster/19155",
        "summary": "Federated reinforcement learning (FRL) has emerged as a promising paradigm for reducing the sample complexity of reinforcement learning tasks by exploiting information from different agents. However, when each agent interacts with a potentially different environment, little to nothing is known theoretically about the non-asymptotic performance of FRL algorithms. The lack of such results can be attributed to various technical challenges and their intricate interplay: Markovian sampling, linear function approximation, multiple local updates to save communication, heterogeneity in the reward functions and transition kernels of the agents' MDPs, and continuous state-action spaces.  Moreover, in the on-policy setting, the behavior policies vary with time, further complicating the analysis. In response, we introduce FedSARSA, a novel federated on-policy reinforcement learning scheme, equipped with linear function approximation, to address these challenges and provide a comprehensive finite-time error analysis. Notably, we establish that FedSARSA converges to a policy that is near-optimal for all agents, with the extent of near-optimality proportional to the level of heterogeneity. Furthermore, we prove that FedSARSA leverages agent collaboration to enable linear speedups as the number of agents increases, which holds for both fixed and adaptive step-size configurations.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Model Merging by Uncertainty-Based Gradient Matching",
        "authors": "Nico Daheim;Thomas Möllenhoff;Edoardo Ponti;Iryna Gurevych;Mohammad Emtiyaz Khan",
        "site": "https://iclr.cc/virtual/2024/poster/19152",
        "summary": "Models trained on different datasets can be merged by a weighted-averaging of their parameters, but why does it work and when can it fail? Here, we connect the inaccuracy of weighted-averaging to mismatches in the gradients and propose a new uncertainty-based scheme to improve the performance by reducing the mismatch. The connection also reveals implicit assumptions in other schemes such as averaging, task arithmetic, and Fisher-weighted averaging. Our new method gives consistent improvements for large language models and vision transformers, both in terms of performance and robustness to hyperparameters.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicitly mentioned, but \"why does it work and when can it fail?\" implies limitations, however, it's not about LLMs specifically.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None explicitly mentioned, but \"why does it work and when can it fail?\" implies limitations, however, it's not about LLMs specifically."
    },
    {
        "title": "On the Reliability of Watermarks for Large Language Models",
        "authors": "John Kirchenbauer;Jonas Geiping;Yuxin Wen;Manli Shu;Khalid Saifullah;Kezhi Kong;Kasun Fernando;Aniruddha Saha;Micah Goldblum;Tom Goldstein",
        "site": "https://iclr.cc/virtual/2024/poster/19147",
        "summary": "Abstract:As LLMs become commonplace, machine-generated text has the potential to flood the internet with spam, social media bots, and valueless content. _Watermarking_ is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet a crucial question remains: How reliable is watermarking in realistic settings in the wild? There, watermarked text may be modified to suit a user's needs, or entirely rewritten to avoid detection. We study the robustness of watermarked text after it is re-written by humans, paraphrased by a non-watermarked LLM, or mixed into a longer hand-written document. We find that watermarks remain detectable even after human and machine paraphrasing. While these attacks dilute the strength of the watermark, paraphrases are statistically likely to leak n-grams or even longer fragments of the original text, resulting in high-confidence detections when enough tokens are observed.  For example, after strong human paraphrasing the watermark is detectable after observing 800 tokens on average, when setting a $1\\mathrm{e}{-5}$ false positive rate. We also consider a range of new detection schemes that are sensitive to short spans of watermarked text embedded inside a large document, and we compare the robustness of watermarking to other kinds of detectors.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Yet a crucial question remains: How reliable is watermarking in realistic settings in the wild?\"; \"We study the robustness of watermarked text after it is re-written by humans, paraphrased by a non-watermarked LLM, or mixed into a longer hand-written document.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Yet a crucial question remains: How reliable is watermarking in realistic settings in the wild?\"; \"We study the robustness of watermarked text after it is re-written by humans, paraphrased by a non-watermarked LLM, or mixed into a longer hand-written document.\""
    },
    {
        "title": "ReLoRA: High-Rank Training Through Low-Rank Updates",
        "authors": "Vladislav Lialin;Sherin Muckatira;Namrata Shivagunde;Anna Rumshisky",
        "site": "https://iclr.cc/virtual/2024/poster/19143",
        "summary": "Despite the dominance and effectiveness of scaling, resulting in large networks with hundreds of billions of parameters, the necessity to train overparameterized models remains poorly understood, while training costs grow exponentially. In this paper, we explore parameter-efficient training techniques as an approach to training large neural networks. We introduce a novel method called ReLoRA, which utilizes low-rank updates to train high-rank networks. We apply ReLoRA to training transformer language models with up to 1.3B parameters and demonstrate comparable performance to regular neural network training. ReLoRA saves up to 5.5Gb of RAM per GPU and improves training speed by 9-40% depending on the model size and hardware setup. Our findings show the potential of parameter- efficient techniques for large-scale pre-training. Our code is available on GitHub.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "The LLM Surgeon",
        "authors": "Tycho F. A. van der Ouderaa;Markus Nagel;Mart Van Baalen;Tijmen Blankevoort",
        "site": "https://iclr.cc/virtual/2024/poster/19140",
        "summary": "State-of-the-art language models are becoming increasingly large in an effort to achieve the highest performance on large corpora of available textual data. However, the sheer size of the Transformer architectures makes it difficult to deploy models within computational, environmental or device-specific constraints. We explore data-driven compression of existing pretrained models as an alternative to training smaller models from scratch. To do so, we scale Kronecker-factored curvature approximations of the target loss landscape to large language models. In doing so, we can compute both the dynamic allocation of structures that can be removed as well as updates of remaining weights that account for the removal. We provide a general framework for unstructured, semi-structured and structured pruning and improve upon weight updates to capture more correlations between weights, while remaining computationally efficient. Experimentally, our method can prune rows and columns from a range of OPT models and Llamav2-7B by 20\\%-30\\%, with a negligible loss in performance, and achieve state-of-the-art results in unstructured and semi-structured pruning of large language models. We will open source our code on GitHub upon acceptance.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the sheer size of the Transformer architectures makes it difficult to deploy models within computational, environmental or device-specific constraints.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the sheer size of the Transformer architectures makes it difficult to deploy models within computational, environmental or device-specific constraints.\""
    },
    {
        "title": "GraphPulse: Topological representations for temporal graph property prediction",
        "authors": "Kiarash Shamsi;Farimah Poursafaei;Shenyang Huang;Bao Tran Gia Ngo;Baris Coskunuzer;Cuneyt Gurcan Akcora",
        "site": "https://iclr.cc/virtual/2024/poster/19138",
        "summary": "Many real-world networks evolve over time, and predicting the evolution of such networks remains a challenging task. Graph Neural Networks (GNNs) have shown empirical success for learning on static graphs, but they lack the ability to effectively learn from nodes and edges with different timestamps. Consequently, the prediction of future properties in temporal graphs remains a relatively under-explored area.In this paper, we aim to bridge this gap by introducing a principled framework, named GraphPulse. The framework combines two important techniques for the analysis of temporal graphs within a Newtonian framework. First, we employ the Mapper method, a key tool in topological data analysis, to extract essential clustering information from graph nodes. Next, we harness the sequential modeling capabilities of Recurrent Neural Networks (RNNs) for temporal reasoning regarding the graph's evolution. Through extensive experimentation, we demonstrate that our model enhances the ROC-AUC metric by 10.2\\% in comparison to the top-performing state-of-the-art method across various temporal networks. We provide the implementation of GraphPulse at https://github.com/kiarashamsi/GraphPulse.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Constrained Decoding for Cross-lingual Label Projection",
        "authors": "Duong Minh Le;Yang Chen;Alan Ritter;Wei Xu",
        "site": "https://iclr.cc/virtual/2024/poster/19137",
        "summary": "Zero-shot cross-lingual transfer utilizing multilingual LLMs has become a popular learning paradigm for low-resource languages with no labeled training data. However, for NLP tasks that involve fine-grained predictions on words and phrases, the performance of zero-shot cross-lingual transfer learning lags far behind supervised fine-tuning methods. Therefore, it is common to exploit translation and label projection to further improve the performance by (1) translating training data that is available in a high-resource language (e.g., English) together with the gold labels into low-resource languages, and/or (2) translating test data in low-resource languages to a high-source language to run inference on, then projecting the predicted span-level labels back onto the original test data. However, state-of-the-art marker-based label projection methods suffer from translation quality degradation due to the extra label markers injected in the input to the translation model. In this work, we explore a new direction that leverages constrained decoding for label projection to overcome the aforementioned issues. Our new method not only can preserve the quality of translated texts but also has the versatility of being applicable to both translating training and translating test data strategies. This versatility is crucial as our experiments reveal that translating test data can lead to a considerable boost in performance compared to translating only training data. We evaluate on two cross-lingual transfer tasks, namely Named Entity Recognition and Event Argument Extraction, spanning 20 languages. The results demonstrate that our approach outperforms the state-of-the-art marker-based method by a large margin and also shows better performance than other label projection methods that rely on external word alignment.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, for NLP tasks that involve fine-grained predictions on words and phrases, the performance of zero-shot cross-lingual transfer learning lags far behind supervised fine-tuning methods.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, for NLP tasks that involve fine-grained predictions on words and phrases, the performance of zero-shot cross-lingual transfer learning lags far behind supervised fine-tuning methods.\""
    },
    {
        "title": "MetaCoCo: A New Few-Shot Classification Benchmark with Spurious Correlation",
        "authors": "Min Zhang;Haoxuan Li;Fei Wu;Kun Kuang",
        "site": "https://iclr.cc/virtual/2024/poster/19133",
        "summary": "Out-of-distribution (OOD) problems in few-shot classification (FSC) occur when novel classes sampled from testing distributions differ from base classes drawn from training distributions, which considerably degrades the performance of deep learning models deployed in real-world applications. Recent studies suggest that the OOD problems in FSC mainly including: (a) cross-domain few-shot classification (CD-FSC) and (b) spurious-correlation few-shot classification (SC-FSC). Specifically, CD-FSC occurs when a classifier learns transferring knowledge from base classes drawn from \\underline{seen} training distributions but recognizes novel classes sampled from unseen testing distributions. In contrast, SC-FSC arises when a classifier relies on non-causal features (or contexts) that happen to be correlated with the labels (or concepts) in base classes but such relationships no longer hold during the model deployment. Despite CD-FSC has been extensively studied, SC-FSC remains understudied due to lack of the corresponding evaluation benchmarks. To this end, we present Meta Concept Context (MetaCoCo), a benchmark with spurious-correlation shifts collected from real-world scenarios. Moreover, to quantify the extent of spurious-correlation shifts of the presented MetaCoCo, we further propose a metric by using CLIP as a pre-trained vision-language model. Extensive experiments on the proposed benchmark are performed to evaluate the state-of-the-art methods in FSC, cross-domain shifts, and self-supervised learning. The experimental results show that the performance of the existing methods degrades significantly in the presence of spurious-correlation shifts. We open-source all codes of our benchmark and hope that the proposed MetaCoCo can facilitate future research on spurious-correlation shifts problems in FSC.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction",
        "authors": "Jiangmeng Li;Fei Song;Yifan Jin;Wenwen Qiang;Changwen Zheng;Fuchun Sun;Hui Xiong",
        "site": "https://iclr.cc/virtual/2024/poster/19127",
        "summary": "As a novel and effective fine-tuning paradigm based on large-scale pre-trained language models (PLMs), prompt-tuning aims to reduce the gap between downstream tasks and pre-training objectives. While prompt-tuning has yielded continuous advancements in various tasks, such an approach still remains a persistent defect: prompt-tuning methods fail to generalize to specific few-shot patterns. From the perspective of distribution analyses, we disclose that the intrinsic issues behind the phenomenon are the over-multitudinous conceptual knowledge contained in PLMs and the abridged knowledge for target downstream domains, which jointly result in that PLMs mis-locate the knowledge distributions corresponding to the target domains in the universal knowledge embedding space. To this end, we intuitively explore to approximate the unabridged target domains of downstream tasks in a debiased manner, and then abstract such domains to generate discriminative prompts, thereby providing the de-ambiguous guidance for PLMs. Guided by such an intuition, we propose a simple yet effective approach, namely BayesPrompt, to learn prompts that contain the domain discriminative information against the interference from domain-irrelevant knowledge. BayesPrompt primitively leverages known distributions to approximate the debiased factual distributions of target domains and further uniformly samples certain representative features from the approximated distributions to generate the ultimate prompts for PLMs. We provide theoretical insights with the connection to domain adaptation. Empirically, our method achieves state-of-the-art performance on benchmarks.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While prompt-tuning has yielded continuous advancements in various tasks, such an approach still remains a persistent defect: prompt-tuning methods fail to generalize to specific few-shot patterns.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"While prompt-tuning has yielded continuous advancements in various tasks, such an approach still remains a persistent defect: prompt-tuning methods fail to generalize to specific few-shot patterns.\""
    },
    {
        "title": "Group Preference Optimization: Few-Shot Alignment of Large Language Models",
        "authors": "Siyan Zhao;John Dang;Aditya Grover",
        "site": "https://iclr.cc/virtual/2024/poster/19125",
        "summary": "Many applications of large language models (LLMs), ranging from chatbots tocreative writing, require nuanced subjective judgments that can differ significantlyacross different groups. Existing alignment algorithms can be expensive to alignfor each group, requiring prohibitive amounts of group-specific preference dataand computation for real-world use cases. We introduce Group Preference Optimization (GPO), an alignment framework that steers language models to preferences of individual groups in a few-shot manner. In GPO, we augment the baseLLM with an independent transformer module trained to predict the preferencesof a group for the LLM generations. For few-shot learning, we parameterize thismodule as an in-context autoregressive transformer and train it via meta-learningon several groups. We empirically validate the efficacy of GPO through rigorous evaluations using LLMs with varied sizes on three human opinion adaptation tasks. These tasks involve adapting to the preferences of US demographicgroups, global countries, and individual users. Our results demonstrate that GPOnot only aligns models more accurately but also requires fewer group-specificpreferences and less training and inference computing resources, outperformingexisting strategies such as in-context steering and fine-tuning methods.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing alignment algorithms can be expensive to align for each group, requiring prohibitive amounts of group-specific preference data and computation for real-world use cases.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (the need for expensive alignment algorithms and large amounts of group-specific data) but does not explore it in depth and focuses on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing alignment algorithms can be expensive to align for each group, requiring prohibitive amounts of group-specific preference data and computation for real-world use cases.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (the need for expensive alignment algorithms and large amounts of group-specific data) but does not explore it in depth and focuses on the proposed solution."
    },
    {
        "title": "Vision-by-Language for Training-Free Compositional Image Retrieval",
        "authors": "Shyamgopal Karthik;Karsten Roth;Massimiliano Mancini;Zeynep Akata",
        "site": "https://iclr.cc/virtual/2024/poster/19114",
        "summary": "Given an image and a target modification (e.g an image of the Eiffel tower and the text “without people and at night-time”), Compositional Image Retrieval (CIR) aims to retrieve the relevant target image in a database. While supervised approaches rely on annotating triplets that is costly (i.e. query image, textual modification, and target image), recent research sidesteps this need by using large-scale vision-language models (VLMs), performing Zero-Shot CIR (ZS-CIR). However, state-of-the-art approaches in ZS-CIR still require training task-specific, customized models over large amounts of image-text pairs. In this work, we proposeto tackle CIR in a training-free manner via our Compositional Image Retrieval through Vision-by-Language (CIReVL), a simple, yet human-understandable and scalable pipeline that effectively recombines large-scale VLMs with large language models (LLMs). By captioning the reference image using a pre-trained generative VLM and asking a LLM to recompose the caption based on the textual target modification for subsequent retrieval via e.g. CLIP, we achieve modular language reasoning. In four ZS-CIR benchmarks, we find competitive, in-part state-of-the-art performance - improving over supervised methods Moreover, the modularity of CIReVL offers simple scalability without re-training, allowing us to both investigate scaling laws and bottlenecks for ZS-CIR while easily scaling up to in parts more than double of previously reported results. Finally, we show that CIReVL makes CIR human-understandable by composing image and text in a modular fashion in the language domain, thereby making it intervenable, allowing to post-hoc re-align failure cases. Code will be released upon acceptance.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions \"However, state-of-the-art approaches in ZS-CIR still require training task-specific, customized models over large amounts of image-text pairs\" which is a limitation of the state-of-the-art approaches, not directly of the LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions \"However, state-of-the-art approaches in ZS-CIR still require training task-specific, customized models over large amounts of image-text pairs\" which is a limitation of the state-of-the-art approaches, not directly of the LLMs."
    },
    {
        "title": "AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors",
        "authors": "Weize Chen;Yusheng Su;Jingwei Zuo;Cheng Yang;Chenfei Yuan;Chi-Min Chan;Heyang Yu;Yaxi Lu;Yi-Hsin Hung;Chen Qian;Yujia Qin;Xin Cong;Ruobing Xie;Zhiyuan Liu;Maosong Sun;Jie Zhou",
        "site": "https://iclr.cc/virtual/2024/poster/19109",
        "summary": "Autonomous agents empowered by Large Language Models (LLMs) have undergone significant improvements, enabling them to generalize across a broad spectrum of tasks. However, in real-world scenarios, cooperation among individuals is often required to enhance the efficiency and effectiveness of task accomplishment. Hence, inspired by human group dynamics, we propose a multi-agent framework AgentVerse that can effectively orchestrate a collaborative group of expert agents as a greater-than-the-sum-of-its-parts system. Our experiments demonstrate that AgentVerse can proficiently deploy multi-agent groups that outperform a single agent. Extensive experiments on text understanding, reasoning, coding, tool utilization, and embodied AI confirm the effectiveness of AgentVerse. Moreover, our analysis of agent interactions within AgentVerse reveals the emergence of specific collaborative behaviors, contributing to heightened group efficiency. We will release our codebase, AgentVerse, to further facilitate multi-agent research.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the paper mentions LLMs but does not discuss any limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the paper mentions LLMs but does not discuss any limitations."
    },
    {
        "title": "HyperAttention: Long-context Attention in Near-Linear Time",
        "authors": "Insu Han;Rajesh Jayaram;Amin Karbasi;Vahab Mirrokni;David Woodruff;Amir Zandieh",
        "site": "https://iclr.cc/virtual/2024/poster/19098",
        "summary": "We present an approximate attention mechanism namedHyperAttentionto address the computational challenges posed by the growing complexity of long contexts used in Large Language Models (LLMs). Recent work suggests that in the worst-case scenario, the quadratic time is necessary unless the entries of the attention matrix are bounded or the matrix has low stable rank. We introduce two parameters which measure: (1) the max column norm in the normalized attention matrix, and (2) the ratio of row norms in the unnormalized attention matrix after detecting and removing large entries. We use these fine-grained parameters to capture the hardness of the problem. Despite previous lower bounds, we are able to achieve a linear time sampling algorithm even when the matrix has unbounded entries or a large stable rank, provided the above parameters are small.HyperAttention features a modular design that easily accommodates integration of other fast low-level implementations, particularly FlashAttention. Empirically, employing Locality Sensitive Hashing (LSH) to identify large entries, HyperAttention outperforms existing methods, giving significant speed improvements compared to state-of-the-art solutions like FlashAttention. This development presents substantial implications for enabling LLMs to handle significantly larger contexts.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent work suggests that in the worst-case scenario, the quadratic time is necessary unless the entries of the attention matrix are bounded or the matrix has low stable rank.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Recent work suggests that in the worst-case scenario, the quadratic time is necessary unless the entries of the attention matrix are bounded or the matrix has low stable rank.\""
    },
    {
        "title": "L2MAC: Large Language Model Automatic Computer for Extensive Code Generation",
        "authors": "Samuel Holt;Max Ruiz Luyten;Mihaela van der Schaar",
        "site": "https://iclr.cc/virtual/2024/poster/19096",
        "summary": "Transformer-based large language models (LLMs) are constrained by the fixed context window of the underlying transformer architecture, hindering their ability to produce long and coherent outputs. Memory-augmented LLMs are a promising solution, but current approaches cannot handle long output generation tasks since they (1) only focus on reading memory and reduce its evolution to the concatenation of new memories or (2) use very specialized memories that cannot adapt to other domains. This paper presents L2MAC, the first practical LLM-based general-purpose stored-program automatic computer (von Neumann architecture) framework, an LLM-based multi-agent system, for long and consistent output generation. Its memory has two components: the instruction registry, which is populated with a prompt program to solve the user-given task, and a file store, which will contain the final and intermediate outputs. Each instruction in turn is executed by a separate LLM agent, whose context is managed by a control unit capable of precise memory reading and writing to ensure effective interaction with the entire file store. These components enable L2MAC to generate extensive outputs, bypassing the constraints of the finite context window while producing outputs that fulfill a complex user-specified task. We empirically demonstrate that L2MAC achieves state-of-the-art performance in generating large codebases for system design tasks, significantly outperforming other coding methods in implementing the detailed user-specified task; we show that L2MAC works for general-purpose extensive text-based tasks, such as writing an entire book; and we provide valuable insights into L2MAC's performance improvement over existing methods.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Transformer-based large language models (LLMs) are constrained by the fixed context window of the underlying transformer architecture, hindering their ability to produce long and coherent outputs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Transformer-based large language models (LLMs) are constrained by the fixed context window of the underlying transformer architecture, hindering their ability to produce long and coherent outputs.\""
    },
    {
        "title": "Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation",
        "authors": "Niels Mündler;Jingxuan He;Slobodan Jenko;Martin Vechev",
        "site": "https://iclr.cc/virtual/2024/poster/19094",
        "summary": "Large language models (large LMs) are susceptible to producing text that contains hallucinated content. An important instance of this problem is self-contradiction, where the LM generates two contradictory sentences within the same context. In this work, we present a comprehensive investigation into self-contradiction for various instruction-tuned LMs, covering evaluation, detection, and mitigation. Our primary evaluation task is open-domain text generation, but we also demonstrate the applicability of our approach to shorter question answering. Our analysis reveals the prevalence of self-contradictions, e.g., in 17.7% of all sentences produced by ChatGPT. We then propose a novel prompting-based framework designed to effectively detect and mitigate self-contradictions. Our detector achieves high accuracy, e.g., around 80% F1 score when prompting ChatGPT. The mitigation algorithm iteratively refines the generated text to remove contradictory information while preserving text fluency and informativeness. Importantly, our entire framework is applicable to black-box LMs and does not require retrieval of external knowledge. Rather, our method complements retrieval-based methods, as a large portion of self-contradictions (e.g., 35.2% for ChatGPT) cannot be verified using online text. Our approach is practically effective and has been released as a push-button tool to benefit the public at https://chatprotect.ai/.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models (large LMs) are susceptible to producing text that contains hallucinated content... Our analysis reveals the prevalence of self-contradictions, e.g., in 17.7% of all sentences produced by ChatGPT.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Large language models (large LMs) are susceptible to producing text that contains hallucinated content... Our analysis reveals the prevalence of self-contradictions, e.g., in 17.7% of all sentences produced by ChatGPT.\""
    },
    {
        "title": "Building Cooperative Embodied Agents Modularly with Large Language Models",
        "authors": "Hongxin Zhang;Weihua Du;Jiaming Shan;Qinhong Zhou;Yilun Du;Joshua B. Tenenbaum;Tianmin Shu;Chuang Gan",
        "site": "https://iclr.cc/virtual/2024/poster/19093",
        "summary": "In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent CoELA, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current Open LMs like LLAMA-2 still underperform, we fine-tune a CoELA with data collected with our agents and show how they can achieve promising performance. We also conducted a user study for human-agent interaction and discovered that CoELA communicating in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Though current Open LMs like LLAMA-2 still underperform\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Though current Open LMs like LLAMA-2 still underperform\""
    },
    {
        "title": "An Emulator for Fine-tuning Large Language Models using Small Language Models",
        "authors": "Eric Mitchell;Rafael Rafailov;Archit Sharma;Chelsea Finn;Christopher D Manning",
        "site": "https://iclr.cc/virtual/2024/poster/19092",
        "summary": "Widely used language models (LMs) are typically built by scaling up a two-stage training pipeline: a pre-training stage that uses a very large, diverse dataset of text and a fine-tuning (sometimes, 'alignment') stage that uses targeted examples or other specifications of desired behaviors. While it has been hypothesized that knowledge and skills come from pre-training, and fine-tuning mostly filters this knowledge and skillset, this intuition has not been extensively tested. To aid in doing so, we introduce a novel technique for decoupling the knowledge and skills gained in these two stages, enabling a direct answer to the question,What would happen if we combined the knowledge learned by a large model during pre-training with the knowledge learned by a small model during fine-tuning (or vice versa)?Using an RL-based framework derived from recent developments in learning from human preferences, we introduceemulated fine-tuning (EFT), a principled and practical method for sampling from a distribution that approximates (or 'emulates') the result of pre-training and fine-tuning at different scales. Our experiments with EFT show that scaling up fine-tuning tends to improve helpfulness, while scaling up pre-training tends to improve factuality. Beyond decoupling scale, we show that EFT enables test-time adjustment of competing behavioral traits like helpfulness and harmlessness without additional training. Finally, a special case of emulated fine-tuning, which we call LMup-scaling, avoids resource-intensive fine-tuning of large pre-trained models by ensembling them with small fine-tuned models, essentially emulating the result of fine-tuning the large pre-trained model. Up-scaling consistently improves helpfulness and factuality of instruction-following models in the Llama, Llama-2, and Falcon families, without additional hyperparameters or training. For reference implementation, seehttps://github.com/eric-mitchell/emulated-fine-tuning.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While it has been hypothesized that knowledge and skills come from pre-training, and fine-tuning mostly filters this knowledge and skillset, this intuition has not been extensively tested.\"\n\nThis rating is chosen because the abstract mentions a limitation of LLMs, specifically the lack of understanding of how pre-training and fine-tuning interact, but does not explore this limitation in depth and instead focuses",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While it has been hypothesized that knowledge and skills come from pre-training, and fine-tuning mostly filters this knowledge and skillset, this intuition has not been extensively tested.\"\n\nThis rating is chosen because the abstract mentions a limitation of LLMs, specifically the lack of understanding of how pre-training and fine-tuning interact, but does not explore this limitation in depth and instead focuses"
    },
    {
        "title": "ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving",
        "authors": "Zhibin Gou;Zhihong Shao;Yeyun Gong;yelong shen;Yujiu Yang;Minlie Huang;Nan Duan;Weizhu Chen",
        "site": "https://iclr.cc/virtual/2024/poster/19091",
        "summary": "Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, ToRA models significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with 13%-19% absolute improvements on average. Notably, ToRA-7B reaches 44.6% on the competition-level dataset MATH, surpassing the best open-source model WizardMath-70B by 22% absolute. ToRA-34B is also the first open-source model that achieves an accuracy exceeding 50% on MATH, which significantly outperforms GPT-4's CoT result, and is competitive with GPT-4 solving problems with programs. Additionally, we conduct a comprehensive analysis of the benefits and remaining challenges of tool interaction for mathematical reasoning, providing valuable insights for future research.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics.\""
    },
    {
        "title": "Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning",
        "authors": "Ted Zadouri;Ahmet Üstün;Arash Ahmadian;Beyza Ermis;Acyr Locatelli;Sara Hooker",
        "site": "https://iclr.cc/virtual/2024/poster/19086",
        "summary": "The Mixture of Experts (MoE) is a widely known neural architecture where  an ensemble of specialized sub-models optimizes overall performance with a constant computational cost. However, conventional MoEs pose challenges at scale due to the need to store all experts in memory. In this paper, we push MoE to the limit. We propose extremely parameter-efficient MoE by uniquely combining MoE architecture with lightweight experts.Our MoE architecture outperforms standard parameter-efficient fine-tuning (PEFT) methods and is on par with full fine-tuning by only updating the lightweight experts -- less than 1\\% of an 11B parameters model. Furthermore, our method generalizes to unseen tasks as it does not depend on any prior task knowledge. Our research underscores the versatility of the mixture of experts architecture, showcasing its ability to deliver robust performance even when subjected to rigorous parameter constraints.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Sparse Autoencoders Find Highly Interpretable Features in Language Models",
        "authors": "Robert Huben;Hoagy Cunningham;Logan Riggs Smith;Aidan Ewart;Lee Sharkey",
        "site": "https://iclr.cc/virtual/2024/poster/19081",
        "summary": "One of the roadblocks to a better understanding of neural networks' internals is \\textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \\textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task \\citep{wang2022interpretability} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"One of the roadblocks to a better understanding of neural networks' internals is polysemanticity, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"One of the roadblocks to a better understanding of neural networks' internals is polysemanticity, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally.\""
    },
    {
        "title": "LOQA: Learning with Opponent Q-Learning Awareness",
        "authors": "Milad Aghajohari;Juan Agustin Duque;Tim Cooijmans;Aaron Courville",
        "site": "https://iclr.cc/virtual/2024/poster/19078",
        "summary": "In various real-world scenarios, interactions among agents often resemble the dynamics of general-sum games, where each agent strives to optimize its own utility. Despite the ubiquitous relevance of such settings, decentralized machine learning algorithms have struggled to find equilibria that maximize individual utility while preserving social welfare. In this paper we introduce Learning with Opponent Q-Learning Awareness (LOQA) , a novel reinforcement learning algorithm tailored to optimizing an agent's individual utility while fostering cooperation among adversaries in partially competitive environments. LOQA assumes that each agent samples actions proportionally to their action-value function Q. Experimental results demonstrate the effectiveness of LOQA at achieving state-of-the-art performance in benchmark scenarios such as the Iterated Prisoner's Dilemma and the Coin Game. LOQA achieves these outcomes with a significantly reduced computational footprint compared to previous works, making it a promising approach for practical multi-agent applications.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models",
        "authors": "Jing Liu;Ruihao Gong;Xiuying Wei;Zhiwei Dong;Jianfei Cai;Bohan Zhuang",
        "site": "https://iclr.cc/virtual/2024/poster/19073",
        "summary": "Large Language Models (LLMs) have demonstrated unparalleled efficacy in natural language processing. However, their high computational demands and memory overheads hinder their broad deployment. To address this, two quantization strategies emerge, including Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ). For LLMs, the billions of parameters make the QAT impractical due to the prohibitive training cost and thus PTQ becomes more prevalent. In existing studies, activation outliers in particular channels are identified as the biggest challenge to PTQ accuracy. They propose to transform the magnitudes from activations to weights, which however offers limited alleviation or suffers from unstable gradients, resulting in a severe performance drop at low-bitwidth. In this paper, we propose QLLM, an accurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM introduces an adaptive channel reassembly technique that reallocates the magnitude of outliers to other channels, thereby mitigating their impact on the quantization range. This is achieved by channel disassembly and channel assembly, which first breaks down the outlier channels into several sub-channels to ensure a more balanced distribution of activation magnitudes. Then similar channels are merged to maintain the original channel number for efficiency. Additionally, an adaptive strategy is designed to autonomously determine the optimal number of sub-channels for channel disassembly. To further compensate for the performance loss caused by quantization, we propose an efficient tuning method that only learns a small number of low-rank weights while freezing the pre-trained quantized model. After training, these low-rank parameters can be fused into the frozen weights without affecting inference. Extensive experiments on LLaMA-1 and LLaMA-2 show that QLLM is able to obtain accurate quantized models efficiently. For example, QLLM quantizes the 4-bit LLaMA-2-70B within 10 hours on a single A100-80G GPU, outperforming the previous state-of-the-art method by 7.89% on the average accuracy across five zero-shot tasks. Code is available atZIP LabandModelTC.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, their high computational demands and memory overheads hinder their broad deployment.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, their high computational demands and memory overheads hinder their broad deployment.\""
    },
    {
        "title": "SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking",
        "authors": "Chris Cundy;Stefano Ermon",
        "site": "https://iclr.cc/virtual/2024/poster/19072",
        "summary": "In many domains, autoregressive models can attain high likelihood on the task of predicting the next observation. However, this maximum-likelihood (MLE) objective does not necessarily match a downstream use-case of autoregressively generating high-quality sequences. The MLE objective weights sequences proportionally to their frequency under the data distribution, with no guidance for the model's behaviour out of distribution (OOD): leading to compounding error during autoregressive generation. In order to address this compounding error problem, we formulate sequence generation as an imitation learning (IL) problem. This allows us to minimize a variety of divergences between the distribution of sequences generated by an autoregressive model and sequences from a dataset, including divergences with weight on OOD generated sequences. The IL framework also allows us to incorporate backtracking by introducing a backspace action into the generation process. This further mitigates the compounding error problem by allowing the model to revert a sampled token if it takes the sequence OOD. Our resulting method, SequenceMatch, can be implemented without adversarial training or major architectural changes. We identify the SequenceMatch-χ2 divergence as a more suitable training objective for autoregressive models which are used for generation. We show that empirically, SequenceMatch training leads to improvements over MLE on text generation with language models and arithmetic",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, this maximum-likelihood (MLE) objective does not necessarily match a downstream use-case of autoregressively generating high-quality sequences... leading to compounding error during autoregressive generation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, this maximum-likelihood (MLE) objective does not necessarily match a downstream use-case of autoregressively generating high-quality sequences... leading to compounding error during autoregressive generation.\""
    },
    {
        "title": "Bayesian Low-rank Adaptation for Large Language Models",
        "authors": "Adam X. Yang;Maxime Robeyns;Xi Wang;Laurence Aitchison",
        "site": "https://iclr.cc/virtual/2024/poster/19071",
        "summary": "Parameter-efficient fine-tuning (PEFT) has emerged as a new paradigm for cost-efficient fine-tuning of large language models (LLMs), with low-rank adaptation (LoRA) being a widely adopted choice. However, fine-tuned LLMs often become overconfident especially when fine-tuned on small datasets. Bayesian methods, with their inherent ability to estimate uncertainty, serve as potent tools to mitigate overconfidence and enhance calibration. In this work, we introduce Laplace-LoRA, a straightforward yet effective Bayesian method, which applies the Laplace approximation to the LoRA parameters and, considerably boosts the calibration of fine-tuned LLMs.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, fine-tuned LLMs often become overconfident especially when fine-tuned on small datasets.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, fine-tuned LLMs often become overconfident especially when fine-tuned on small datasets.\""
    },
    {
        "title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate",
        "authors": "Chi-Min Chan;Weize Chen;Yusheng Su;Jianxuan Yu;Wei Xue;Shanghang Zhang;Jie Fu;Zhiyuan Liu",
        "site": "https://iclr.cc/virtual/2024/poster/19065",
        "summary": "Abstract:Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality.Recognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies.In this paper, we construct a multi-agent referee team called $\\textbf{ChatEval}$ to autonomously discuss and evaluate the quality of different texts. Our experiments on two benchmarks illustrate that ChatEval delivers superior accuracy and correlation in alignment with human assessment. Furthermore, we find that the diverse role prompts (different personas) are essential in the multi-agent debate process; that is, utilizing the same role description in the prompts can lead to a degradation in performance. Our qualitative analysis also shows that ChatEval transcends mere textual scoring, offering a human-mimicking evaluation process for reliable assessments.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality.\""
    },
    {
        "title": "AlpaGasus: Training a Better Alpaca with Fewer Data",
        "authors": "Lichang Chen;Shiyang Li;Jun Yan;Hai Wang;Kalpa Gunaratna;Vikas Yadav;Zheng Tang;Vijay Srinivasan;Tianyi Zhou;Heng Huang;Hongxia Jin",
        "site": "https://iclr.cc/virtual/2024/poster/19058",
        "summary": "Abstract:Large language models~(LLMs) strengthen instruction-following capability through instruction-finetuning (IFT) on supervised instruction/response data. However, widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly contain many low-quality instances with incorrect or irrelevant responses, which are misleading and detrimental to IFT.  In this paper, we propose a simple and effective data selection strategy that automatically identifies and removes low-quality data using a strong LLM (e.g., ChatGPT). To this end, we introduce Alpagasus, which is finetuned on only 9k high-quality data filtered from the 52k Alpaca data. Alpagasus significantly outperforms the original Alpaca as evaluated by GPT-4 on multiple test sets and the controlled human study. Its 13B variant matches $>90\\%$ performance of its teacher LLM (i.e., Text-Davinci-003) on test tasks. It also provides 5.7x faster training, reducing the training time for a 7B variant from 80 minutes (for Alpaca) to 14 minutes \\footnote{We apply IFT for the same number of epochs as Alpaca(7B) but on fewer data, using 4$\\times$NVIDIA A100 (80GB) GPUs and following the original Alpaca setting and hyperparameters.}.  In the experiment, we also demonstrate that our method can work not only for machine-generated datasets but also for human-written datasets. Overall, Alpagasus demonstrates a novel data-centric IFT paradigm that can be generally applied to instruction-tuning data, leading to faster training and better instruction-following models.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly contain many low-quality instances with incorrect or irrelevant responses, which are misleading and detrimental to IFT.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly contain many low-quality instances with incorrect or irrelevant responses, which are misleading and detrimental to IFT.\""
    },
    {
        "title": "Does Writing with Language Models Reduce Content Diversity?",
        "authors": "Vishakh Padmakumar;He He",
        "site": "https://iclr.cc/virtual/2024/poster/19056",
        "summary": "Large language models (LLMs) have led to a surge in collaborative writing with model assistance. As different users incorporate suggestions from the same model, there is a risk of decreased diversity in the produced content, potentially limiting diverse perspectives in public discourse. In this work, we measure the impact of co-writing on diversity via a controlled experiment, where users write argumentative essays in three setups---using a base LLM (GPT3), a feedback-tuned LLM (InstructGPT), and writing without model help. We develop a set of diversity metrics and find that writing with InstructGPT (but not the GPT3) results in a statistically significant reduction in diversity. Specifically, it increases the similarity between the writings of different authors and reduces the overall lexical and content diversity. We additionally find that this effect is mainly attributable to InstructGPT contributing less diverse text to co-written essays. In contrast, the user-contributed text remains unaffected by model collaboration. This suggests that the recent improvement in generation quality from adapting models to human feedback might come at the cost of more homogeneous and less diverse content.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"As different users incorporate suggestions from the same model, there is a risk of decreased diversity in the produced content, potentially limiting diverse perspectives in public discourse.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"As different users incorporate suggestions from the same model, there is a risk of decreased diversity in the produced content, potentially limiting diverse perspectives in public discourse.\""
    },
    {
        "title": "Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization",
        "authors": "Yang Jin;Kun Xu;Kun Xu;Liwei Chen;Chao Liao;Jianchao Tan;Quzhe Huang;Bin CHEN;Chengru Song;dai meng;Di ZHANG;Wenwu Ou;Kun Gai;Yadong MU",
        "site": "https://iclr.cc/virtual/2024/poster/19049",
        "summary": "Recently, the remarkable advance of the Large Language Model (LLM) has inspired researchers to transfer its extraordinary reasoning capability to both vision and language data. However, the prevailing approaches primarily regard the visual input as a prompt and focus exclusively on optimizing the text generation process conditioned upon vision content by a frozen LLM. Such an inequitable treatment of vision and language heavily constrains the model's potential. In this paper, we break through this limitation by representing both vision and language in a unified form. Specifically, we introduce a well-designed visual tokenizer to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read. The resulting visual tokens encompass high-level semantics worthy of a word and also support dynamic sequence length varying from the image. Coped with this tokenizer, the presented foundation model called LaVIT can handle both image and text indiscriminately under the same generative learning paradigm. This unification empowers LaVIT to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously. Extensive experiments further showcase that it outperforms the existing models by a large margin on massive vision-language tasks. Our code and models are available at https://github.com/jy0205/LaVIT.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the prevailing approaches primarily regard the visual input as a prompt and focus exclusively on optimizing the text generation process conditioned upon vision content by a frozen LLM. Such an inequitable treatment of vision and language heavily constrains the model's potential.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the prevailing approaches primarily regard the visual input as a prompt and focus exclusively on optimizing the text generation process conditioned upon vision content by a frozen LLM. Such an inequitable treatment of vision and language heavily constrains the model's potential.\""
    },
    {
        "title": "CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets",
        "authors": "Lifan Yuan;Yangyi Chen;Xingyao Wang;Yi Fung;Hao Peng;Heng Ji",
        "site": "https://iclr.cc/virtual/2024/poster/19043",
        "summary": "Large language models (LLMs) are often augmented with tools to solve complex tasks. By generating code snippets and executing them through task-specific Application Programming Interfaces (APIs), they can offload certain functions to dedicated external modules, such as image encoding and performing calculations. However, most existing approaches to augment LLMs with tools are constrainedby general-purpose APIs and lack the flexibility for tailoring them to specific tasks. In this work, we present CRAFT, a general tool creation and retrieval framework for LLMs. It creates toolsets specifically curated for the tasks and equips LLMs with a component that retrieves tools from these sets to enhance their capability to solve complex tasks. For each task, we collect specific code solutions by promptingGPT-4 to solve the training examples. Following a validation step ensuring the correctness, these solutions are abstracted into code snippets to enhance reusability, and deduplicated for higher quality. At inference time, the language model retrieves snippets from the toolsets and then executes them or generates the output conditioning on the retrieved snippets. Our method is designed to be flexible andoffers a plug-and-play approach to adapt off-the-shelf LLMs to unseen domains and modalities, without any finetuning. Experiments on vision-language, tabular processing, and mathematical reasoning tasks show that our approach achieves substantial improvements compared to strong baselines. In addition, our in-depth analysis reveals that: (1) consistent performance improvement can be achieved byscaling up the number of tools and the capability of the backbone models; (2) each component of our approach contributes to the performance gains; (3) the created tools are well-structured and reliable with low complexity  and atomicity.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, most existing approaches to augment LLMs with tools are constrained by general-purpose APIs and lack the flexibility for tailoring them to specific tasks.\"\n\nThis paper is rated with 1 since it talks about LLMs but does not mention any explicit limitation of the models in the abstract. The mentioned limitation is related to the approaches to augment LLMs rather than LLMs",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, most existing approaches to augment LLMs with tools are constrained by general-purpose APIs and lack the flexibility for tailoring them to specific tasks.\"\n\nThis paper is rated with 1 since it talks about LLMs but does not mention any explicit limitation of the models in the abstract. The mentioned limitation is related to the approaches to augment LLMs rather than LLMs"
    },
    {
        "title": "Image Clustering Conditioned on Text Criteria",
        "authors": "Sehyun Kwon;Jaeseung Park;Minkyu Kim;Jaewoong Cho;Ernest K. Ryu;Kangwook Lee",
        "site": "https://iclr.cc/virtual/2024/poster/19041",
        "summary": "Abstract:Classical clustering methods do not provide users with direct control of the clustering results, and the clustering results may not be consistent with the relevant criterion that a user has in mind. In this work, we present a new methodology for performing image clustering based on user-specified criteria in the form of text by leveraging modern Vision-Language Models and Large Language Models. We call our method Image Clustering Conditioned on Text Criteria (IC$|$TC), and it represents a different paradigm of image clustering. IC$|$TC requires a minimal and practical degree of human intervention and grants the user significant control over the clustering results in return. Our experiments show that IC$|$TC can effectively cluster images with various criteria, such as human action, physical location, or the person's mood, significantly outperforming baselines.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"leveraging modern Vision-Language Models and Large Language Models.\"\n\nThis paper does not discuss any limitations of LLMs, but rather uses them as a tool for the proposed methodology.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"leveraging modern Vision-Language Models and Large Language Models.\"\n\nThis paper does not discuss any limitations of LLMs, but rather uses them as a tool for the proposed methodology."
    },
    {
        "title": "Hypothesis Search: Inductive Reasoning with Language Models",
        "authors": "Ruocheng Wang;Eric Zelikman;Gabriel Poesia;Yewen Pu;Nick Haber;Noah Goodman",
        "site": "https://iclr.cc/virtual/2024/poster/19039",
        "summary": "Inductive reasoning is a core problem-solving capacity: humans can identify underlying principles from a few examples, which can then be robustly generalized to novel scenarios. Recent work has evaluated large language models (LLMs) on inductive reasoning tasks by directly prompting them yielding \"in context learning.\" This can work well for straightforward inductive tasks, but performs very poorly on more complex tasks such as the Abstraction and Reasoning Corpus (ARC). In this work, we propose to improve the inductive reasoning ability of LLMs by generating explicit hypotheses at multiple levels of abstraction: we prompt the LLM to propose multiple abstract hypotheses about the problem, in natural language, then implement the natural language hypotheses as concrete Python programs. These programs can be directly verified by running on the observed examples and generalized to novel inputs. To reduce the hypothesis search space, we explore steps to filter the set of hypotheses to be implemented as programs: we either ask the LLM to summarize them into a smaller set of hypotheses, or ask human annotators to select a subset. We verify our pipeline's effectiveness on the ARC visual inductive reasoning benchmark, its variant 1D-ARC, and string transformation dataset SyGuS. On a random 40-problem subset of ARC, our automated pipeline using LLM summaries achieves 27.5% accuracy, significantly outperforming the direct prompting baseline (accuracy of 12.5%). With the minimal human input of selecting from LLM-generated candidates, the performance is boosted to 37.5%. Our ablation studies show that abstract hypothesis generation and concrete program representations are both beneficial for LLMs to perform inductive reasoning tasks.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This can work well for straightforward inductive tasks, but performs very poorly on more complex tasks such as the Abstraction and Reasoning Corpus (ARC).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"This can work well for straightforward inductive tasks, but performs very poorly on more complex tasks such as the Abstraction and Reasoning Corpus (ARC).\""
    },
    {
        "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval",
        "authors": "Parth Sarthi;Salman Abdullah;Aditi Tuli;Shubh Khanna;Anna Goldie;Christopher D Manning",
        "site": "https://iclr.cc/virtual/2024/poster/19034",
        "summary": "Retrieval-augmented language models can better adapt to changes in world state and incorporate long-tail knowledge.  However, most existing methods retrieve only short contiguous chunks from a retrieval corpus, limiting holistic understanding of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of summarization from the bottom up. At inference time, our RAPTOR model retrieves from this tree, integrating information across lengthy documents at different levels of abstraction. Controlled experiments show that retrieval with recursive summaries offers significant improvements over traditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; for example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve the best performance on the QuALITY benchmark by 20\\% in absolute accuracy.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, most existing methods retrieve only short contiguous chunks from a retrieval corpus, limiting holistic understanding of the overall document context.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, most existing methods retrieve only short contiguous chunks from a retrieval corpus, limiting holistic understanding of the overall document context.\""
    },
    {
        "title": "The Reversal Curse: LLMs trained on “A is B” fail to learn “B is A”",
        "authors": "Lukas Berglund;Meg Tong;Maximilian Kaufmann;Mikita Balesni;Asa Cooper Stickland;Tomasz Korbak;Owain Evans",
        "site": "https://iclr.cc/virtual/2024/poster/19033",
        "summary": "We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form ''AisB'', it will not automatically generalize to the reverse direction ''BisA''. This is theReversal Curse. For instance, if a model is trained on ''Valentina Tereshkova was the first woman to travel to space'', it will not automatically be able to answer the question, ''Who was the first woman to travel to space?''. Moreover, the likelihood of the correct answer (''Valentina Tershkova'') will not be higher than for a random name. Thus, models do not generalize a prevalent pattern in their training set: if ''AisB'' occurs, ''BisA'' is more likely to occur. It is worth noting, however, that if ''AisB'' appearsin-context, models can deduce the reverse relationship. We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as ''Uriah Hawthorne is the composer ofAbyssal Melodies'' and showing that they fail to correctly answer ''Who composedAbyssal Melodies?''. The Reversal Curse is robust across model sizes and model families and is not alleviated by data augmentation.We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about real-world celebrities, such as ''Who is Tom Cruise's mother? [A: Mary Lee Pfeiffer]'' and the reverse ''Who is Mary Lee Pfeiffer's son?''. GPT-4 correctly answers questions like the former 79\\% of the time, compared to 33\\% for the latter. Code available at: https://github.com/lukasberglund/reversal_curse.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "5",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form ''AisB'', it will not automatically generalize to the reverse direction ''BisA''. This is theReversal Curse.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 5\nEvidence: \"We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form ''AisB'', it will not automatically generalize to the reverse direction ''BisA''. This is theReversal Curse.\""
    },
    {
        "title": "Duolando: Follower GPT with Off-Policy Reinforcement Learning for Dance Accompaniment",
        "authors": "Li Siyao;Tianpei Gu;Zhitao Yang;Zhengyu Lin;Ziwei Liu;Henghui Ding;Lei Yang;Chen Change Loy",
        "site": "https://iclr.cc/virtual/2024/poster/19027",
        "summary": "We introduce a novel task within the field of human motion generation, termed dance accompaniment, which necessitates the generation of responsive movements from a dance partner, the \"follower\", synchronized with the lead dancer’s movements and the underlying musical rhythm. Unlike existing solo or group dance generation tasks, a duet dance scenario entails a heightened degree of interaction between the two participants, requiring delicate coordination in both pose and position. To support this task, we first build a large-scale and diverse duet interactive dance dataset, DD100, by recording about 117 minutes of professional dancers’ performances. To address the challenges inherent in this task, we propose a GPT based model, Duolando, which autoregressively predicts the subsequent tokenized motion conditioned on the coordinated information of the music, the leader’s and the follower’s movements. To further enhance the GPT’s capabilities of generating stable results on unseen conditions (music and leader motions), we devise an off-policy reinforcement learning strategy that allows the model to explore viable trajectories from out-of-distribution samplings, guided by human-defined rewards. Based on the collected dataset and proposed method, we establish a benchmark with several carefully designed metrics.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"To further enhance the GPT’s capabilities of generating stable results on unseen conditions (music and leader motions)\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"To further enhance the GPT’s capabilities of generating stable results on unseen conditions (music and leader motions)\""
    },
    {
        "title": "Domain Randomization via Entropy Maximization",
        "authors": "Gabriele Tiboni;Pascal Klink;Jan Peters;Tatiana Tommasi;Carlo D'Eramo;Georgia Chalvatzaki",
        "site": "https://iclr.cc/virtual/2024/poster/19025",
        "summary": "Varying dynamics parameters in simulation is a popular Domain Randomization (DR) approach for overcoming the reality gap in Reinforcement Learning (RL). Nevertheless, DR heavily hinges on the choice of the sampling distribution of the dynamics parameters, since high variability is crucial to regularize the agent's behavior but notoriously leads to overly conservative policies when randomizing excessively. In this paper, we propose a novel approach to address sim-to-real transfer, which automatically shapes dynamics distributions during training in simulation without requiring real-world data. We introduce DOmain RAndomization via Entropy MaximizatiON (DORAEMON), a constrained optimization problem that directly maximizes the entropy of the training distribution while retaining generalization capabilities. In achieving this, DORAEMON gradually increases the diversity of sampled dynamics parameters as long as the probability of success of the current policy is sufficiently high. We empirically validate the consistent benefits of DORAEMON in obtaining highly adaptive and generalizable policies, i.e. solving the task at hand across the widest range of dynamics parameters, as opposed to representative baselines from the DR literature. Notably, we also demonstrate the Sim2Real applicability of DORAEMON through its successful zero-shot transfer in a robotic manipulation setup under unknown real-world parameters.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "REValueD: Regularised Ensemble Value-Decomposition for Factorisable Markov Decision Processes",
        "authors": "David Ireland;Giovanni Montana",
        "site": "https://iclr.cc/virtual/2024/poster/19022",
        "summary": "Discrete-action reinforcement learning algorithms often falter in tasks with high-dimensional discrete action spaces due to the vast number of possible actions. A recent advancement leverages value-decomposition, a concept from multi-agent reinforcement learning, to tackle this challenge. This study delves deep into the effects of this value-decomposition, revealing that whilst it curtails the over-estimation bias inherent to Q-learning algorithms, it amplifies target variance. To counteract this, we present an ensemble of critics to mitigate target variance. Moreover, we introduce a regularisation loss that helps to mitigate the effects that exploratory actions in one dimension can have on the value of optimal actions in other dimensions. Our novel algorithm, REValueD, tested on discretised versions of the DeepMind Control Suite tasks, showcases superior performance, especially in the challenging humanoid and dog tasks. We further dissect the factors influencing REValueD's performance, evaluating the significance of the regularisation loss and the scalability of REValueD with increasing sub-actions per dimension.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Tree-Planner: Efficient Close-loop Task Planning with Large Language Models",
        "authors": "Mengkang Hu;Yao Mu;Xinmiao Chelsey Yu;Mingyu Ding;Shiguang Wu;Wenqi Shao;Qiguang Chen;Bin Wang;Yu Qiao;Ping Luo",
        "site": "https://iclr.cc/virtual/2024/poster/19017",
        "summary": "This paper studies close-loop task planning, which refers to the process of generating a sequence of skills (a plan) to accomplish a specific goal while adapting the plan based on real-time observations.Recently, prompting Large Language Models (LLMs) to generate actions iteratively has become a prevalent paradigm due to its superior performance and user-friendliness.However, this paradigm is plagued by two inefficiencies: high token consumption and redundant error correction, both of which hinder its scalability for large-scale testing and applications.To address these issues, we propose Tree-Planner, which reframes task planning with LLMs into three distinct phases: plan sampling,  action tree construction, and grounded deciding.Tree-Planner starts by using an LLM to sample a set of potential plans before execution, followed by the aggregation of them to form an action tree.Finally, the LLM performs a top-down decision-making process on the tree, taking into account real-time environmental information.Experiments show that Tree-Planner achieves state-of-the-art performance while maintaining high efficiency.By decomposing LLM queries into a single plan-sampling call and multiple grounded-deciding calls,a considerable partof the prompt are less likely to be repeatedly consumed. As a result, token consumption is reduced by 92.2\\% compared to the previously best-performing model.Additionally, by enabling backtracking on the action tree as needed, the correction process becomes more flexible, leading to a 40.5\\% decrease in error corrections.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, this paradigm is plagued by two inefficiencies: high token consumption and redundant error correction, both of which hinder its scalability for large-scale testing and applications.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, this paradigm is plagued by two inefficiencies: high token consumption and redundant error correction, both of which hinder its scalability for large-scale testing and applications.\""
    },
    {
        "title": "Universal Jailbreak Backdoors from Poisoned Human Feedback",
        "authors": "Javier Rando;Florian Tramèr",
        "site": "https://iclr.cc/virtual/2024/poster/19013",
        "summary": "Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an attacker poisons the RLHF data to embed a jailbreak trigger into the model as a backdoor. The trigger then acts like a universal sudo command, enabling arbitrary harmful responses without  the need to search for an adversarial prompt. Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. We investigate the design decisions in RLHF that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak backdoors.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior.\""
    },
    {
        "title": "Increasing Model Capacity for Free: A Simple Strategy for Parameter Efficient Fine-tuning",
        "authors": "Haobo SONG;Hao Zhao;Soumajit Majumder;Tao Lin",
        "site": "https://iclr.cc/virtual/2024/poster/19009",
        "summary": "Fine-tuning large pre-trained foundation models, such as the 175B GPT-3, has become the prevailing approach for downstream tasks. While parameter-efficient fine-tuning methods have been proposed and proven effective without retraining all model parameters, their performance is limited by the capacity of incremental modules, especially under constrained parameter budgets.To overcome this challenge, we propose CAPABOOST, a simple yet effective strategy that enhances model capacity by leveraging low-rank updates through parallel weight modules in target layers. By applying static random masks to the shared weight matrix, CAPABOOST constructs a diverse set of weight matrices, effectively increasing the rank of incremental weights without adding parameters. Notably, our approach can be seamlessly integrated into various existing parameter-efficient fine-tuning methods. We extensively validate the efficacy of CAPABOOST through experiments on diverse downstream tasks, including natural language understanding, question answering, and image classification. Our results demonstrate significant improvements over baselines, without incurring additional computationor storage costs. We will make our code and benchmark publicly available.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"their performance is limited by the capacity of incremental modules, especially under constrained parameter budgets.\"\n\nThis abstract mentions a limitation of LLMs (performance limitation due to capacity of incremental modules) but does not explore it in depth, and the primary focus is on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"their performance is limited by the capacity of incremental modules, especially under constrained parameter budgets.\"\n\nThis abstract mentions a limitation of LLMs (performance limitation due to capacity of incremental modules) but does not explore it in depth, and the primary focus is on the proposed solution."
    },
    {
        "title": "Learning to Compose: Improving Object Centric Learning by Injecting Compositionality",
        "authors": "Whie Jung;Jaehoon Yoo;Sungjin Ahn;Seunghoon Hong",
        "site": "https://iclr.cc/virtual/2024/poster/18993",
        "summary": "Learning compositional representation is a key aspect of object-centric learning as it enables flexible systematic generalization and supports complex visual reasoning. However, most of the existing approaches rely on auto-encoding objective, while the compositionality is implicitly imposed by the architectural or algorithmic bias in the encoder. This misalignment between auto-encoding objective and learning compositionality often results in failure of capturing meaningful object representations. In this study, we propose a novel objective that explicitly encourages compositionality of the representations. Built upon the existing object-centric learning framework (e.g., slot attention), our method incorporates additional constraints that an arbitrary mixture of object representations from two images should be valid by maximizing the likelihood of the composite data. We demonstrate that incorporating our objective to the existing framework consistently improves the objective-centric learning and enhances the robustness to the architectural choices.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "In-Context Learning through the Bayesian Prism",
        "authors": "Madhur Panwar;Kabir Ahuja;Navin Goyal",
        "site": "https://iclr.cc/virtual/2024/poster/18992",
        "summary": "Abstract:In-context learning (ICL) is one of the surprising and useful features of large language models and subject of intense research. Recently, stylized meta-learning-like ICL setups have been devised that train transformers on sequences of input-output pairs $(x, f(x))$. The function $f$ comes from a function class and generalization is checked by evaluating on sequences generated from unseen functions from the same class. One of the main discoveries in this line of research has been that for several function classes, such as linear regression, transformers successfully generalize to new functions in the class. However, the inductive biases of these models resulting in this behavior are not clearly understood. A model with unlimited training data and compute is a Bayesian predictor: it learns the pretraining distribution.In this paper we empirically examine how far this Bayesian perspective can help us understand ICL. To this end, we generalize the previous meta-ICL setup to hierarchical meta-ICL setup which involve unions of multiple task families. We instantiate this setup on a diverse range of linear and nonlinear function families and find that transformers can do ICL in this setting as well. Where Bayesian inference is tractable, we find evidence that high-capacity transformers mimic the Bayesian predictor. The Bayesian perspective provides insights into the inductive bias of ICL and how transformers perform a particular task when they are trained on multiple tasks. We also find that transformers can learn to generalize to new function classes that were not seen during pretraining. This involves deviation from the Bayesian predictor. We examine these deviations in more depth offering new insights and hypotheses.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the inductive biases of these models resulting in this behavior are not clearly understood.\"; \"We also find that transformers can learn to generalize to new function classes that were not seen during pretraining. This involves deviation from the Bayesian predictor.\"\n\nThis paper discusses the limitations of LLMs in the context of in-context learning and inductive biases, but the primary focus is on understanding",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, the inductive biases of these models resulting in this behavior are not clearly understood.\"; \"We also find that transformers can learn to generalize to new function classes that were not seen during pretraining. This involves deviation from the Bayesian predictor.\"\n\nThis paper discusses the limitations of LLMs in the context of in-context learning and inductive biases, but the primary focus is on understanding"
    },
    {
        "title": "FairerCLIP: Debiasing CLIP's Zero-Shot Predictions using Functions in RKHSs",
        "authors": "Sepehr Dehdashtian;Lan Wang;Vishnu Boddeti",
        "site": "https://iclr.cc/virtual/2024/poster/18989",
        "summary": "Large pre-trained vision-language models such as CLIP provide compact and general-purpose representations of text and images that are demonstrably effective across multiple downstream zero-shot prediction tasks. However, owing to the nature of their training process, these models have the potential to 1) propagate or amplify societal biases in the training data and 2) learn to rely on spurious features. This paper proposes FairerCLIP, a general approach for making zero-shot predictions of CLIP more fair and robust to spurious correlations. We formulate the problem of jointly debiasing CLIP’s image and text representations in reproducing kernel Hilbert spaces (RKHSs), which affords multiple benefits: 1) Flexibility: Unlike existing approaches, which are specialized to either learn with or without ground-truth labels, FairerCLIP is adaptable to learning in both scenarios. 2) Ease of Optimization: FairerCLIP lends itself to an iterative optimization involving closed-form solvers, which leads to 4×-10× faster training than the existing methods. 3) Sample Efficiency: Under sample-limited conditions, FairerCLIP significantly outperforms baselines when they fail entirely. And, 4) Performance: Empirically, FairerCLIP achieves appreciable accuracy gains on benchmark fairness and spurious correlation datasets over their respective baselines.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"owing to the nature of their training process, these models have the potential to 1) propagate or amplify societal biases in the training data and 2) learn to rely on spurious features.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"owing to the nature of their training process, these models have the potential to 1) propagate or amplify societal biases in the training data and 2) learn to rely on spurious features.\""
    },
    {
        "title": "Neural Language of Thought Models",
        "authors": "Yi-Fu Wu;Minseung Lee;Sungjin Ahn",
        "site": "https://iclr.cc/virtual/2024/poster/18988",
        "summary": "The Language of Thought Hypothesis suggests that human cognition operates on a structured, language-like system of mental representations. While neural language models can naturally benefit from the compositional structure inherently and explicitly expressed in language data, learning such representations from non-linguistic general observations, like images, remains a challenge. In this work, we introduce the Neural Language of Thought Model (NLoTM), a novel approach for unsupervised learning of LoTH-inspired representation and generation. NLoTM comprises two key components: (1) the Semantic Vector-Quantized Variational Autoencoder, which learns hierarchical, composable discrete representations aligned with objects and their properties, and (2) the Autoregressive LoT Prior, an autoregressive transformer that learns to generate semantic concept tokens compositionally, capturing the underlying data distribution. We evaluate NLoTM on several 2D and 3D image datasets, demonstrating superior performance in downstream tasks, out-of-distribution generalization, and image generation quality compared to patch-based VQ-VAE and continuous object-centric representations. Our work presents a significant step towards creating neural networks exhibiting more human-like understanding by developing LoT-like representations and offers insights into the intersection of cognitive science and machine learning.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Towards Best Practices of Activation Patching in Language Models: Metrics and Methods",
        "authors": "Fred Zhang;Neel Nanda",
        "site": "https://iclr.cc/virtual/2024/poster/18984",
        "summary": "Mechanistic interpretability seeks to understand the internal mechanisms ofmachine learning models, where localization—identifying the important modelcomponents—is a key step. Activation patching, also known as causal tracing orinterchange intervention, is a standard technique for this task (Vig et al., 2020), butthe literature contains many variants with little consensus on the choice of hyperparameters or methodology. In this work, we systematically examine the impactof methodological details in activation patching, including evaluation metrics andcorruption methods. In several settings of localization and circuit discovery in language models, we find that varying these hyperparameters could lead to disparateinterpretability results. Backed by empirical observations, we give conceptual arguments for why certain metrics or methods may be preferred. Finally, we providerecommendations for the best practices of activation patching going forwards.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"In several settings of localization and circuit discovery in language models, we find that varying these hyperparameters could lead to disparate interpretability results.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"In several settings of localization and circuit discovery in language models, we find that varying these hyperparameters could lead to disparate interpretability results.\""
    },
    {
        "title": "Chain-of-Experts: When LLMs Meet Complex Operations Research Problems",
        "authors": "Ziyang Xiao;Dongxiang Zhang;Yangjun Wu;Lilin Xu;Yuan Jessica Wang;Xiongwei Han;Xiaojin Fu;Tao Zhong;Jia Zeng;Mingli Song;Gang Chen",
        "site": "https://iclr.cc/virtual/2024/poster/18977",
        "summary": "Large language models (LLMs) have emerged as powerful techniques for various NLP tasks, such as mathematical reasoning and plan generation. In this paper, we study automatic modeling and programming for complex operation research (OR) problems, so as to alleviate the heavy dependence on domain experts and benefit a spectrum of industry sectors. We present the first LLM-based solution, namely Chain-of-Experts (CoE), a novel multi-agent cooperative framework to enhance reasoning capabilities. Specifically, each agent is assigned a specific role and endowed with domain knowledge related to OR. We also introduce a conductor to orchestrate these agents via forward thought construction and backward reflection mechanism. Furthermore, we release a benchmark dataset (ComplexOR) of complex OR problems to facilitate OR research and community development. Experimental results show that CoE significantly outperforms the state-of-the-art LLM-based approaches both on LPWP and ComplexOR.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the paper discusses the capabilities of LLMs and proposes a novel framework to enhance their reasoning capabilities, but does not mention any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the paper discusses the capabilities of LLMs and proposes a novel framework to enhance their reasoning capabilities, but does not mention any limitations of LLMs."
    },
    {
        "title": "Eureka: Human-Level Reward Design via Coding Large Language Models",
        "authors": "Yecheng Jason Ma;William Liang;Guanzhi Wang;De-An Huang;Osbert Bastani;Dinesh Jayaraman;Yuke Zhu;Linxi Fan;Anima Anandkumar",
        "site": "https://iclr.cc/virtual/2024/poster/18971",
        "summary": "Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks. However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem. We bridge this fundamental gap and present Eureka, a human-level reward design algorithm powered by LLMs. Eureka exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over reward code. The resulting rewards can then be used to acquire complex skills via reinforcement learning. Without any task-specific prompting or pre-defined reward templates, Eureka generates reward functions that outperform expert human-engineered rewards. In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies, Eureka outperforms human experts on 83% of the tasks, leading to an average normalized improvement of 52%. The generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF), readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating. Finally, using Eureka rewards in a curriculum learning setting, we demonstrate for the first time, a simulated Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a pen in circles at rapid speed.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem.\"\n\nThis paper discusses LLMs and mentions a limitation of LLMs, but only briefly, without going into detail, and focuses on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem.\"\n\nThis paper discusses LLMs and mentions a limitation of LLMs, but only briefly, without going into detail, and focuses on the proposed solution."
    },
    {
        "title": "Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-to-Image Generation",
        "authors": "Jaemin Cho;Yushi Hu;Jason Michael Baldridge;Roopal Garg;Peter Anderson;Ranjay Krishna;Mohit Bansal;Jordi Pont-Tuset;Su Wang",
        "site": "https://iclr.cc/virtual/2024/poster/18963",
        "summary": "Evaluating text-to-image models is notoriously difficult. A strong recent approach for assessing text-image faithfulness is based on QG/A (question generation and answering), which uses pre-trained foundational models to automatically generate a set of questions and answers from the prompt, and output images are scored based on whether these answers extracted with a visual question answering model are consistent with the prompt-based answers. This kind of evaluation is naturally dependent on the quality of the underlying QG and VQA models. We identify and address several reliability challenges in existing QG/A work: (a) QG questions should respect the prompt (avoiding hallucinations, duplications, and omissions) and (b) VQA answers should be consistent (not asserting that there is no motorcycle in an image while also claiming the motorcycle is blue). We address these issues with Davidsonian Scene Graph (DSG), an empirically grounded evaluation framework inspired by formal semantics, which is adaptable to any QG/A frameworks. DSG produces atomic and unique questions organized in dependency graphs, which (i) ensure appropriate semantic coverage and (ii) sidestep inconsistent answers. With extensive experimentation and human evaluation on a range of model configurations (LLM, VQA, and T2I), we empirically demonstrate that DSG addresses the challenges noted above. Finally, we present DSG-1k, an open-sourced evaluation benchmark that includes 1,060 prompts, covering a wide range of fine-grained semantic categories with a balanced distribution. We release the DSG-1k prompts and the corresponding DSG questions.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No direct evidence, but \"(a) QG questions should respect the prompt (avoiding hallucinations, duplications, and omissions)\" implies potential limitations of LLMs in generating questions, and \"(b) VQA answers should be consistent\" implies potential limitations of LLMs in providing consistent answers.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: No direct evidence, but \"(a) QG questions should respect the prompt (avoiding hallucinations, duplications, and omissions)\" implies potential limitations of LLMs in generating questions, and \"(b) VQA answers should be consistent\" implies potential limitations of LLMs in providing consistent answers."
    },
    {
        "title": "Towards Generative Abstract Reasoning: Completing Raven’s Progressive Matrix via Rule Abstraction and Selection",
        "authors": "Fan Shi;Bin Li;Xiangyang Xue",
        "site": "https://iclr.cc/virtual/2024/poster/18960",
        "summary": "Endowing machines with abstract reasoning ability has been a long-term research topic in artificial intelligence. Raven's Progressive Matrix (RPM) is widely used to probe abstract visual reasoning in machine intelligence, where models will analyze the underlying rules and select one image from candidates to complete the image matrix. Participators of RPM tests can show powerful reasoning ability by inferring and combining attribute-changing rules and imagining the missing images at arbitrary positions of a matrix. However, existing solvers can hardly manifest such an ability in realistic RPM tests. In this paper, we propose a deep latent variable model for answer generation problems through Rule AbstractIon and SElection (RAISE). RAISE can encode image attributes into latent concepts and abstract atomic rules that act on the latent concepts. When generating answers, RAISE selects one atomic rule out of the global knowledge set for each latent concept to constitute the underlying rule of an RPM. In the experiments of bottom-right and arbitrary-position answer generation, RAISE outperforms the compared solvers in most configurations of realistic RPM datasets. In the odd-one-out task and two held-out configurations, RAISE can leverage acquired latent concepts and atomic rules to find the rule-breaking image in a matrix and handle problems with unseen combinations of rules and attributes.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Vanishing Gradients in Reinforcement Finetuning of Language Models",
        "authors": "Noam Razin;Hattie Zhou;Omid Saremi;Vimal Thilak;Arwen Bradley;Preetum Nakkiran;Joshua M. Susskind;Etai Littwin",
        "site": "https://iclr.cc/virtual/2024/poster/18959",
        "summary": "Pretrained language models are commonly aligned with human preferences and downstream tasks via reinforcement finetuning (RFT), which refers to maximizing a (possibly learned) reward function using policy gradient algorithms. This work identifies a fundamental optimization obstacle in RFT: we prove that the expected gradient for an input vanishes when its reward standard deviation under the model is small, even if the expected reward is far from optimal. Through experiments on an RFT benchmark and controlled environments, as well as a theoretical analysis, we then demonstrate that vanishing gradients due to small reward standard deviation are prevalent and detrimental, leading to extremely slow reward maximization. Lastly, we explore ways to overcome vanishing gradients in RFT. We find the common practice of an initial supervised finetuning (SFT) phase to be the most promising candidate, which sheds light on its importance in an RFT pipeline. Moreover, we show that a relatively small number of SFT optimization steps on as few as 1% of the input samples can suffice, indicating that the initial SFT phase need not be expensive in terms of compute and data labeling efforts. Overall, our results emphasize that being mindful for inputs whose expected gradient vanishes, as measured by the reward standard deviation, is crucial for successful execution of RFT.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"we prove that the expected gradient for an input vanishes when its reward standard deviation under the model is small, even if the expected reward is far from optimal. Through experiments on an RFT benchmark and controlled environments, as well as a theoretical analysis, we then demonstrate that vanishing gradients due to small reward standard deviation are prevalent and detrimental, leading to extremely slow reward maximization.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"we prove that the expected gradient for an input vanishes when its reward standard deviation under the model is small, even if the expected reward is far from optimal. Through experiments on an RFT benchmark and controlled environments, as well as a theoretical analysis, we then demonstrate that vanishing gradients due to small reward standard deviation are prevalent and detrimental, leading to extremely slow reward maximization.\""
    },
    {
        "title": "Large Language Models Cannot Self-Correct Reasoning Yet",
        "authors": "Jie Huang;Xinyun Chen;Swaroop Mishra;Huaixiu Steven Zheng;Adams Wei Yu;Xinying Song;Denny Zhou",
        "site": "https://iclr.cc/virtual/2024/poster/18956",
        "summary": "Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance even degrades after self-correction. Drawing from these insights, we offer suggestions for future research and practical applications in this field.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "5",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"A contemporary methodology, self-correction, has been proposed as a remedy to these issues... our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance even degrades after self-correction.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 5\nEvidence: \"A contemporary methodology, self-correction, has been proposed as a remedy to these issues... our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance even degrades after self-correction.\""
    },
    {
        "title": "Talk like a Graph: Encoding Graphs for Large Language Models",
        "authors": "Bahare Fatemi;Jonathan Halcrow;Bryan Perozzi",
        "site": "https://iclr.cc/virtual/2024/poster/18954",
        "summary": "Graphs are a powerful tool for representing and analyzing complex relationships in real-world applications such as social networks, recommender systems, and computational finance. Reasoning on graphs is essential for drawing inferences about the relationships between entities in a complex system, and to identify hidden patterns and trends. Despite the remarkable progress in automated reasoning with natural text, reasoning on graphs with large language models (LLMs) remains an understudied problem. In this work, we perform the first comprehensive study of encoding graph-structured data as text for consumption by LLMs. We show that LLM performance on graph reasoning tasks varies on three fundamental levels: (1) the graph encoding method, (2) the nature of the graph task itself, and (3) interestingly, the very structure of the graph considered. These novel results provide valuable insight on strategies for encoding graphs as text. Using these insights we illustrate how the correct choice of encoders can boost performance on graph reasoning tasks inside LLMs by 4.8% to 61.8%, depending on the task.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the remarkable progress in automated reasoning with natural text, reasoning on graphs with large language models (LLMs) remains an understudied problem.\"\n\nThis rating is chosen because the abstract mentions a limitation of LLMs (reasoning on graphs) in passing, but does not elaborate on it and focuses more on the proposed solution and results.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite the remarkable progress in automated reasoning with natural text, reasoning on graphs with large language models (LLMs) remains an understudied problem.\"\n\nThis rating is chosen because the abstract mentions a limitation of LLMs (reasoning on graphs) in passing, but does not elaborate on it and focuses more on the proposed solution and results."
    },
    {
        "title": "A Good Learner can Teach Better: Teacher-Student Collaborative Knowledge Distillation",
        "authors": "Ayan Sengupta;Shantanu Dixit;Md Shad Akhtar;Tanmoy Chakraborty",
        "site": "https://iclr.cc/virtual/2024/poster/18953",
        "summary": "Abstract:Knowledge distillation (KD) is a technique used to transfer knowledge from a larger ''teacher'' model into a smaller ''student'' model. Recent advancements in meta-learning-based knowledge distillation (MetaKD) emphasize that the fine-tuning of teacher models should be aware of the student's need to achieve better knowledge distillation. However, existing MetaKD methods often lack incentives for the teacher model to improve itself. In this study, we introduce MPDistil, a meta-policy distillation technique, that utilizes novel optimization strategies to foster both *collaboration* and *competition* during the fine-tuning of the teacher model in the meta-learning step. Additionally, we propose a curriculum learning framework for the student model in a competitive setup, in which the student model aims to outperform the teacher model by self-training on various tasks. Exhaustive experiments on SuperGLUE and GLUE benchmarks demonstrate the efficacy of MPDistil compared to $20$ conventional KD and advanced MetaKD baselines, showing significant performance enhancements in the student model -- e.g., a distilled 6-layer BERT model outperforms a 12-layer BERT model on five out of six SuperGLUE tasks. Furthermore, MPDistil, while applied to a large language teacher model (DeBERTa-v2-xxlarge), significantly narrows the performance gap of its smaller student counterpart (DeBERTa-12) by just $4.6$% on SuperGLUE. We further demonstrate how higher rewards and customized training curricula strengthen the student model and enhance generalizability.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation of LLMs is mentioned in the abstract, but it discusses knowledge distillation from a larger teacher model into a smaller student model, which can be related to the limitations of LLMs in terms of knowledge transfer and size.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation of LLMs is mentioned in the abstract, but it discusses knowledge distillation from a larger teacher model into a smaller student model, which can be related to the limitations of LLMs in terms of knowledge transfer and size."
    },
    {
        "title": "Toward Optimal Policy Population Growth in Two-Player Zero-Sum Games",
        "authors": "Stephen Marcus McAleer;JB Lanier;Kevin A. Wang;Pierre Baldi;Tuomas Sandholm;Roy Fox",
        "site": "https://iclr.cc/virtual/2024/poster/18948",
        "summary": "In competitive two-agent environments, deep reinforcement learning (RL) methods like Policy Space Response Oracles (PSRO) often increase exploitability between iterations, which is problematic when training in large games. To address this issue, we introduce anytime double oracle (ADO), an algorithm that ensures exploitability does not increase between iterations, and its approximate extensive-form version, anytime PSRO (APSRO). ADO converges to a Nash equilibrium while iteratively reducing exploitability. However, convergence in these algorithms may require adding all of a game's deterministic policies. To improve this, we propose Self-Play PSRO (SP-PSRO), which incorporates an approximately optimal stochastic policy into the population in each iteration. APSRO and SP-PSRO demonstrate lower exploitability and near-monotonic exploitability reduction in games like Leduc poker and Liar's Dice. Empirically, SP-PSRO often converges much faster than APSRO and PSRO, requiring only a few iterations in many games.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning",
        "authors": "Fuxiao Liu;Kevin Lin;Linjie Li;Jianfeng Wang;Yaser Yacoob;Lijuan Wang",
        "site": "https://iclr.cc/virtual/2024/poster/18947",
        "summary": "Despite the promising progress in multi-modal tasks, current large multi-modal models (LMMs) are prone to hallucinating inconsistent descriptions with respect to the associated image and human instructions. This paper addresses this issue by introducing the first large and diverse visual instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction. Our dataset comprises 400k visualinstructions generated by GPT4, covering 16 vision-and-language tasks with open-ended instructions and answers. Unlike existing studies that primarily focus on positive instruction samples, we design LRV-Instruction to include both positive and negative instructions for more robust visual instruction tuning. Our negative instructions are designed at three semantic levels: (i) Nonexistent Object Manipulation, (ii) Existent Object Manipulation and (iii) Knowledge Manipulation. To efficiently measure the hallucination generated by LMMs, we propose GPT4-Assisted Visual Instruction Evaluation (GAVIE), a stable approach to evaluate visual instruction tuning like human experts. GAVIE does not require human-annotated groundtruth answers and can adapt to diverse instruction formats. We conduct comprehensive experiments to investigate the hallucination of LMMs. Our results demonstrate existing LMMs exhibit significant hallucinations when presented with our negative instructions, particularly Existent Object and Knowledge Manipulation instructions. Moreover, we successfully mitigate hallucination by finetuning MiniGPT4 and mPLUG-Owl on LRV-Instruction while improving performance on several publicdatasets compared to state-of-the-art methods. Additionally, we observed that a balanced ratio of positive and negative instances in the training data leads to a more robust model. Code and data will be released upon publication.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our results demonstrate existing LMMs exhibit significant hallucinations when presented with our negative instructions, particularly Existent Object and Knowledge Manipulation instructions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Our results demonstrate existing LMMs exhibit significant hallucinations when presented with our negative instructions, particularly Existent Object and Knowledge Manipulation instructions.\""
    },
    {
        "title": "Scaling Laws of RoPE-based Extrapolation",
        "authors": "Xiaoran Liu;Hang Yan;Chenxin An;Xipeng Qiu;Dahua Lin",
        "site": "https://iclr.cc/virtual/2024/poster/18943",
        "summary": "Abstract:The extrapolation capability of Large Language Models (LLMs) based on Rotary Position Embedding \\citep{su2021roformer} is currently a topic of considerable interest. The mainstream approach to addressing extrapolation with LLMs involves modifying RoPE by replacing 10000, the rotary base of $\\theta_n={10000}^{-2n/d}$ in the original RoPE, with a larger value and providing longer fine-tuning text. In this work, we first observe that fine-tuning a RoPE-based LLM with either a smaller or larger base in pre-training context length could significantly enhance its extrapolation performance. After that, we propose \\textbf{\\textit{Scaling Laws of RoPE-based Extrapolation}}, a unified framework from the periodic perspective, to describe the relationship between the extrapolation performance and base value as well as tuning context length. In this process, we also explain the origin of the RoPE-based extrapolation issue by \\textbf{\\textit{critical dimension for extrapolation}}. Besides these observations and analyses, we achieve extrapolation up to 1 million context length within only 16K training length on LLaMA2 7B and 13B \\citep{touvron2023llama2}.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"explain the origin of the RoPE-based extrapolation issue by \\textbf{\\textit{critical dimension for extrapolation}}.\"\n\nThis paper briefly mentions a limitation of LLMs related to extrapolation, but the primary focus is on proposing a framework and achieving better extrapolation performance.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"explain the origin of the RoPE-based extrapolation issue by \\textbf{\\textit{critical dimension for extrapolation}}.\"\n\nThis paper briefly mentions a limitation of LLMs related to extrapolation, but the primary focus is on proposing a framework and achieving better extrapolation performance."
    },
    {
        "title": "Towards Codable Watermarking for Injecting Multi-Bits Information to LLMs",
        "authors": "Lean Wang;Wenkai Yang;Deli Chen;Hao Zhou;Yankai Lin;Fandong Meng;Jie Zhou;Xu Sun",
        "site": "https://iclr.cc/virtual/2024/poster/18937",
        "summary": "As large language models (LLMs) generate texts with increasing fluency and realism, there is a growing need to identify the source of texts to prevent the abuse of LLMs. Text watermarking techniques have proven reliable in distinguishing whether a text is generated by LLMs by injecting hidden patterns. However, we argue that existing LLM watermarking methods are encoding-inefficient and cannot flexibly meet the diverse information encoding needs (such as encoding model version, generation time, user id, etc.). In this work, we conduct the first systematic study on the topic ofCodable Text Watermarking for LLMs(CTWL) that allows text watermarks to carry multi-bit customizable information. First of all, we study the taxonomy of LLM watermarking technologies and give a mathematical formulation for CTWL. Additionally, we provide a comprehensive evaluation system for CTWL: (1) watermarking success rate, (2) robustness against various corruptions, (3) coding rate of payload information, (4) encoding and decoding efficiency, (5) impacts on the quality of the generated text. To meet the requirements of these non-Pareto-improving metrics, we follow the most prominent vocabulary partition-based watermarking direction, and devise an advanced CTWL method namedBalance-Marking. The core idea of our method is to use a proxy language model to split the vocabulary into probability-balanced parts, thereby effectively maintaining the quality of the watermarked text. Our code is available at https://github.com/lancopku/codable-watermarking-for-llm.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"As large language models (LLMs) generate texts with increasing fluency and realism, there is a growing need to identify the source of texts to prevent the abuse of LLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"As large language models (LLMs) generate texts with increasing fluency and realism, there is a growing need to identify the source of texts to prevent the abuse of LLMs.\""
    },
    {
        "title": "When Do Prompting and Prefix-Tuning Work? A Theory of Capabilities and Limitations",
        "authors": "Aleksandar Petrov;Philip Torr;Adel Bibi",
        "site": "https://iclr.cc/virtual/2024/poster/18932",
        "summary": "Context-based fine-tuning methods, including prompting, in-context learning, soft prompting (also known as prompt tuning), and prefix-tuning, have gained popularity due to their ability to often match the performance of full fine-tuning with a fraction of the parameters. Despite their empirical successes, there is little theoretical understanding of how these techniques influence the internal computation of the model and their expressiveness limitations. We show that despite the continuous embedding space being more expressive than the discrete token space, soft-prompting and prefix-tuning are potentially less expressive than full fine-tuning, even with the same number of learnable parameters. Concretely, context-based fine-tuning cannot change the relative attention pattern over the content and can only bias the outputs of an attention layer in a fixed direction. This suggests that while techniques like prompting, in-context learning, soft prompting, and prefix-tuning can effectively elicit skills present in the pretrained model, they may not be able to learn novel tasks that require new attention patterns.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite their empirical successes, there is little theoretical understanding of how these techniques influence the internal computation of the model and their expressiveness limitations... This suggests that while techniques like prompting, in-context learning, soft prompting, and prefix-tuning can effectively elicit skills present in the pretrained model, they may not be able to learn novel tasks that require new attention patterns.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Despite their empirical successes, there is little theoretical understanding of how these techniques influence the internal computation of the model and their expressiveness limitations... This suggests that while techniques like prompting, in-context learning, soft prompting, and prefix-tuning can effectively elicit skills present in the pretrained model, they may not be able to learn novel tasks that require new attention patterns.\""
    },
    {
        "title": "SKILL-MIX: a Flexible and Expandable Family of Evaluations for AI Models",
        "authors": "Dingli Yu;Simran Kaur;Arushi Gupta;Jonah Brown-Cohen;Anirudh Goyal;Sanjeev Arora",
        "site": "https://iclr.cc/virtual/2024/poster/18931",
        "summary": "Abstract:With LLMs shifting their role from statistical modeling of language to serving as general-purpose AI agents, how should LLM evaluations change? Arguably, a key ability of an AI agent is to flexibly combine, as needed, the basic skills it has learned. The capability to combine skills plays an important role in (human) pedagogy and also in a paper on emergence phenomena (Arora & Goyal, 2023).This work introduces SKILL-MIX, a new evaluation to measure ability to combine skills. Using a list of $N$  skills the evaluator repeatedly picks random subsets of $k$ skills and asks the LLM to produce text combining that subset of skills. Since the number of subsets grows like $N^k$, for even modest $k$ this evaluation will, with high probability, require the LLM to produce text significantly different from any text in the training set. The paper develops a methodology for (a) designing and administering such an evaluation, and (b) automatic grading (plus spot-checking by humans) of the results using GPT-4 as well as the open LLaMA-2 70B model. Administering a version of SKILL-MIX to popular chatbots gave results that,  while generally in line with prior expectations, contained surprises. Sizeable differences exist among model capabilities that are not captured by their ranking on popular LLM leaderboards (\"cramming for the leaderboard\"). Furthermore, simple probability calculations indicate that GPT-4's reasonable performance on $k=5$ is suggestive of going beyond \"stochastic parrot\" behavior (Bender et al., 2021), i.e., it combines skills in ways that it had not seen during training.We sketch how the methodology can lead to a SKILL-MIX based eco-system of open evaluations for AI capabilities of future models. We maintain a leaderboard of SKILL-MIX at [https://skill-mix.github.io](https://skill-mix.github.io).",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Sizeable differences exist among model capabilities that are not captured by their ranking on popular LLM leaderboards (\"cramming for the leaderboard\")\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Sizeable differences exist among model capabilities that are not captured by their ranking on popular LLM leaderboards (\"cramming for the leaderboard\")\""
    },
    {
        "title": "ReFusion: Improving Natural Language Understanding with Computation-Efficient Retrieval Representation Fusion",
        "authors": "Shangyu Wu;Ying Xiong;Yufei CUI;Xue Liu;Buzhou Tang;Tei-Wei Kuo;Chun Jason Xue",
        "site": "https://iclr.cc/virtual/2024/poster/18922",
        "summary": "Retrieval-based augmentations (RA) incorporating knowledge from an external database into language models have greatly succeeded in various knowledge-intensive (KI) tasks. However, integrating retrievals in non-knowledge-intensive (NKI) tasks is still challenging.Existing works focus on concatenating retrievals with inputs to improve model performance. Unfortunately, the use of retrieval concatenation-based augmentations causes an increase in the input length, substantially raising the computational demands of attention mechanisms.This paper proposes a new paradigm of RA named \\textbf{ReFusion}, a computation-efficient \\textbf{Re}trieval representation \\textbf{Fusion} with bi-level optimization. Unlike previous works, ReFusion directly fuses the retrieval representations into the hidden states of models.Specifically, ReFusion leverages an adaptive retrieval integrator to seek the optimal combination of the proposed ranking schemes across different model layers. Experimental results demonstrate that the proposed ReFusion can achieve superior and robust performance in various NKI tasks.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Unfortunately, the use of retrieval concatenation-based augmentations causes an increase in the input length, substantially raising the computational demands of attention mechanisms.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Unfortunately, the use of retrieval concatenation-based augmentations causes an increase in the input length, substantially raising the computational demands of attention mechanisms.\""
    },
    {
        "title": "Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models",
        "authors": "Jung Hwan Heo;Jeonghoon Kim;Beomseok Kwon;Byeongwook Kim;Se Jung Kwon;Dongsoo Lee",
        "site": "https://iclr.cc/virtual/2024/poster/18921",
        "summary": "Abstract:Large Language Models (LLMs) have recently demonstrated a remarkable success across various tasks. However, efficiently serving LLMs has been a challenge due to its large memory bottleneck, specifically in small batch inference settings (e.g. mobile devices). Weight-only quantization can be a promising approach, but sub-4 bit quantization remains a challenge due to large-magnitude activation outliers. To mitigate the undesirable outlier effect, we first propose per-IC quantization, a simple yet effective method that creates quantization groups within each input channel (IC) rather than the conventional per-output channel (OC). Our method is motivated by the observation that activation outliers affect the input dimension of the weight matrix, so similarly grouping the weights in the IC direction can $\\textit{isolate outliers to be within a group}$. We also find that activation outliers do not dictate quantization difficulty, and inherent weight sensitivities also exist. With per-IC quantization as a new outlier-friendly scheme, we then propose Adaptive Dimensions ($\\textbf{AdaDim}$), a versatile quantization framework that can adapt to various weight sensitivity patterns. We demonstrate the effectiveness of AdaDim by augmenting prior methods such as Round-To-Nearest and GPTQ, showing significant improvements across various language modeling benchmarks for both base (up to $+4.7\\%$ on MMLU) and instruction-tuned (up to $+10\\%$ on HumanEval) LLMs.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, efficiently serving LLMs has been a challenge due to its large memory bottleneck, specifically in small batch inference settings (e.g. mobile devices). Weight-only quantization can be a promising approach, but sub-4 bit quantization remains a challenge due to large-magnitude activation outliers.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, efficiently serving LLMs has been a challenge due to its large memory bottleneck, specifically in small batch inference settings (e.g. mobile devices). Weight-only quantization can be a promising approach, but sub-4 bit quantization remains a challenge due to large-magnitude activation outliers.\""
    },
    {
        "title": "Towards Robust Multi-Modal Reasoning via Model Selection",
        "authors": "Xiangyan Liu;Rongxue LI;Wei Ji;Tao Lin",
        "site": "https://iclr.cc/virtual/2024/poster/18902",
        "summary": "Abstract:The reasoning capabilities of LLM (Large Language Model) are widely acknowledged in recent research, inspiring studies on tool learning and autonomous agents. LLM serves as the ``brain'' of the agent, orchestrating multiple tools for collaborative multi-step task solving. Unlike methods invoking tools like calculators or weather APIs for straightforward tasks, multi-modal agents excel by integrating diverse AI models for complex challenges. However, current multi-modal agents neglect the significance of model selection: they primarily focus on the planning and execution phases, and will only invoke predefined task-specific models for each subtask, making the execution fragile. Meanwhile, other traditional model selection methods are either incompatible with or suboptimal for the multi-modal agent scenarios, due to ignorance of dependencies among subtasks arising by multi-step reasoning.To this end, we identify the key challenges therein and propose the $\\textbf{\\textit{M}}^\\textbf{\\textit{3}}$ framework as a plug-in with negligible runtime overhead at test-time. This framework improves model selection and bolsters the robustness of multi-modal agents in multi-step reasoning. In the absence of suitable benchmarks, we create MS-GQA, a new dataset specifically designed to investigate the model selection challenge in multi-modal agents. Our experiments reveal that our framework enables dynamic model selection, considering both user inputs and subtask dependencies, thereby robustifying the overall reasoning process. Our code and benchmark: https://github.com/LINs-lab/M3.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, current multi-modal agents neglect the significance of model selection: they primarily focus on the planning and execution phases, and will only invoke predefined task-specific models for each subtask, making the execution fragile.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, current multi-modal agents neglect the significance of model selection: they primarily focus on the planning and execution phases, and will only invoke predefined task-specific models for each subtask, making the execution fragile.\""
    },
    {
        "title": "Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit",
        "authors": "Blake Bordelon;Lorenzo Noci;Mufan Bill Li;Boris Hanin;Cengiz Pehlevan",
        "site": "https://iclr.cc/virtual/2024/poster/18897",
        "summary": "Abstract:The cost of hyperparameter tuning in deep learning has been rising with model sizes, prompting practitioners to find new tuning methods using a proxy of smaller networks. One such proposal uses $\\mu$P parameterized networks, where the optimal hyperparameters for small width networks *transfer* to networks with arbitrarily large width. However, in this scheme, hyperparameters do not transfer across depths. As a remedy, we study residual networks with a residual branch scale of $1/\\sqrt{\\text{depth}}$ in combination with the $\\mu$P parameterization. We provide experiments demonstrating that residual architectures including convolutional ResNets and vision transformers trained with this parameterization exhibit transfer of optimal hyperparameters across width and depth on CIFAR-10 and ImageNet.  Furthermore, our empirical findings are supported and motivated by theory. Using recent developments in the dynamical mean field theory (DMFT) description of neural network learning dynamics, we show that this parameterization of ResNets admits a well-defined feature learning joint infinite-width and infinite-depth limit and show convergence of finite-size network dynamics towards this limit.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Language Model Cascades: Token-Level Uncertainty And Beyond",
        "authors": "Neha Gupta;Harikrishna Narasimhan;Wittawat Jitkrittum;Ankit Singh Rawat;Aditya Krishna Menon;Sanjiv Kumar",
        "site": "https://iclr.cc/virtual/2024/poster/18893",
        "summary": "Recent advances in language models (LMs) have led to significant improvements in quality on complex NLP tasks, but at the expense of increased inference costs. A simple strategy to achieve more favorable cost-quality tradeoffs is cascading: here, a small model is invoked for most “easy” instances, while a few “hard” instances are deferred to the large model. While the principles underpinning effective cascading are well-studied for classification tasks — with deferral based on predicted class uncertainty favored theoretically and practically — a similar understanding is lacking for generative LM tasks. In this work, we initiate a systematic study of deferral rules for LM cascades. We begin by examining the natural extension of predicted class uncertainty to generative LM tasks, namely, the predicted sequence uncertainty. We show that this measure suffers from the length bias problem, either over- or under-emphasizing outputs based on their lengths. This is because LMs produce a sequence of uncertainty values, one for each output token; and moreover, the number of output tokens is variable across different examples. To mitigate the length bias, we propose to exploit the richer token-level uncertainty information implicit in generative LMs. We argue that naive predicted sequence uncertainty corresponds to a simple aggregation of these uncertainties. By contrast, we show that incorporating token-level uncertainty through learned post-hoc deferral rules can significantly outperform such simple aggregation strategies, via experiments on a range of natural language benchmarks with FLAN-T5 models. We further show that incorporating embeddings from the smaller model and intermediate layers of the larger model can give an additional boost in the overall cost-quality tradeoff.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We show that this measure suffers from the length bias problem, either over- or under-emphasizing outputs based on their lengths. This is because LMs produce a sequence of uncertainty values, one for each output token; and moreover, the number of output tokens is variable across different examples.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We show that this measure suffers from the length bias problem, either over- or under-emphasizing outputs based on their lengths. This is because LMs produce a sequence of uncertainty values, one for each output token; and moreover, the number of output tokens is variable across different examples.\""
    },
    {
        "title": "DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning",
        "authors": "Zhengxiang Shi;Aldo Lipani",
        "site": "https://iclr.cc/virtual/2024/poster/18890",
        "summary": "Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it maintains competitive performance with fewer trainable parameters and does not drastically scale up its parameters as the model size expands. However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying. To address this issue, we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt into a shorter soft prompt and a pair of low-rank matrices that are then optimised with two different learning rates. This allows DePT to achieve better performance while saving substantial memory and time costs compared to vanilla PT and its variants, without changing trainable parameter sizes. Through extensive experiments on 23 natural language processing (NLP) and vision-language (VL) tasks, we demonstrate that DePT outperforms state-of-the-art PEFT approaches, including the full fine-tuning baseline, in some scenarios. Additionally, we empirically show that DEPT grows more efficient as the model size increases. Our further study reveals that DePT integrates seamlessly with parameter-efficient transfer learning in the few-shot learning setting and highlights its adaptability to various model architectures and sizes.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying.\""
    },
    {
        "title": "An Extensible Framework for Open Heterogeneous Collaborative Perception",
        "authors": "Yifan Lu;Yue Hu;Yiqi Zhong;Dequan Wang;Yanfeng Wang;Siheng Chen",
        "site": "https://iclr.cc/virtual/2024/poster/18889",
        "summary": "Collaborative perception aims to mitigate the limitations of single-agent perception, such as occlusions, by facilitating data exchange among multiple agents. However, most current works consider a homogeneous scenario where all agents use identity sensors and perception models. In reality, heterogeneous agent types may continually emerge and inevitably face a domain gap when collaborating with existing agents. In this paper, we introduce a new open heterogeneous problem: how to accommodate continually emerging new heterogeneous agent types into collaborative perception, while ensuring high perception performance and low integration cost? To address this problem, we propose HEterogeneous ALliance (HEAL), a novel extensible collaborative perception framework. HEAL first establishes a unified feature space with initial agents via a novel multi-scale foreground-aware Pyramid Fusion network. When heterogeneous new agents emerge with previously unseen modalities or models, we align them to the established unified space with an innovative backward alignment. This step only involves individual training on the new agent type, thus presenting extremely low training costs and high extensibility. To enrich agents' data heterogeneity, we bring OPV2V-H, a new large-scale dataset with more diverse sensor types. Extensive experiments on OPV2V-H and DAIR-V2X datasets show that HEAL surpasses SOTA methods in performance while reducing the training parameters by 91.5\\% when integrating 3 new agent types. We further implement a comprehensive codebase at: https://github.com/yifanlu0227/HEAL",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs."
    },
    {
        "title": "Teaching Large Language Models to Self-Debug",
        "authors": "Xinyun Chen;Maxwell Lin;Nathanael Schärli;Denny Zhou",
        "site": "https://iclr.cc/virtual/2024/poster/18880",
        "summary": "Abstract:Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose self-debugging, which teaches a large language model to debug its predicted program. In particular, we demonstrate that self-debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by leveraging code execution and explaining the generated code in natural language. Self-debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, self-debugging with code explanation consistently improves the baseline by 2-3%, and improves the prediction accuracy on problems of the hardest level by 9%. On TransCoder and MBPP where unit tests are available, self-debugging improves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback messages and reusing failed predictions, self-debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10$\\times$ candidate programs.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, for complex programming tasks, generating the correct solution in one go becomes challenging\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, for complex programming tasks, generating the correct solution in one go becomes challenging\""
    },
    {
        "title": "Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering",
        "authors": "Han Zhou;Xingchen Wan;Lev Proleev;Diana Mincu;Jilin Chen;Katherine A Heller;Subhrajit Roy",
        "site": "https://iclr.cc/virtual/2024/poster/18875",
        "summary": "Prompting and in-context learning (ICL) have become efficient learning paradigms for large language models (LLMs). However, LLMs suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the ICL examples. To address this problem that results in unexpected performance degradation, calibration methods have been developed to mitigate the effects of these biases while recovering LLM performance. In this work, we first conduct a systematic analysis of the existing calibration methods, where we both provide a unified view and reveal the failure cases. Inspired by these analyses, we propose Batch Calibration (BC), a simple yet intuitive method that controls the contextual bias from the batched input, unifies various prior approaches and effectively addresses the aforementioned issues. BC is zero-shot, inference-only, and incurs negligible additional costs. In the few-shot setup, we further extend BC to allow it to learn the contextual bias from labeled data. We validate the effectiveness of BC with PaLM 2-(S, M, L) and CLIP models and demonstrate state-of-the-art performance over previous calibration baselines across more than 10 natural language understanding and image classification tasks.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, LLMs suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the ICL examples.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, LLMs suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the ICL examples.\""
    },
    {
        "title": "Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models",
        "authors": "Archiki Prasad;Elias Stengel-Eskin;Mohit Bansal",
        "site": "https://iclr.cc/virtual/2024/poster/18874",
        "summary": "An increasing number of vision-language tasks can be handled with little to no training, i.e., in a zero and few-shot manner, by marrying large language models (LLMs) to vision encoders, resulting in large vision-language models (LVLMs). While this has huge upsides, such as not requiring training data or custom architectures, how an input is presented to an LVLM can have a major impact on zero-shot model performance. In particular, inputs phrased in an underspecified way can result in incorrect answers due to factors like missing visual information, complex implicit reasoning, or linguistic ambiguity. Therefore, adding visually-grounded information to the input as a preemptive clarification should improve model performance by reducing underspecification, e.g., by localizing objects and disambiguating references. Similarly, in the VQA setting, changing the way questions are framed can make them easier for models to answer. To this end, we presentRephrase,Augment andReason (RepARe), a gradient-free framework that extracts salient details about the image using the underlying LVLM as a captioner and reasoner, in order to propose modifications to the original question. We then use the LVLM’s confidence over a generated answer as an unsupervised scoring function to select the rephrased question most likely to improve zero-shot performance. Focusing on three visual question answering tasks, we show that RepARe can result in a 3.85% (absolute) increase in zero-shot accuracy on VQAv2, 6.41%, and 7.94% points increase on A-OKVQA, and VizWiz respectively. Additionally, we find that using gold answers for oracle question candidate selection achieves a substantial gain in VQA accuracy by up to 14.41%. Through extensive analysis, we demonstrate that outputs from RepARe increase syntactic complexity, and effectively utilize vision-language interaction and the frozen LLM.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"inputs phrased in an underspecified way can result in incorrect answers due to factors like missing visual information, complex implicit reasoning, or linguistic ambiguity.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"inputs phrased in an underspecified way can result in incorrect answers due to factors like missing visual information, complex implicit reasoning, or linguistic ambiguity.\""
    },
    {
        "title": "Massive Editing for Large Language Models via Meta Learning",
        "authors": "Chenmien Tan;Ge Zhang;Jie Fu",
        "site": "https://iclr.cc/virtual/2024/poster/18873",
        "summary": "While large language models (LLMs) have enabled learning knowledge from the pre-training corpora, the acquired knowledge may be fundamentally incorrect or outdated over time, which necessitates rectifying the knowledge of the language model (LM) after the training. A promising approach involves employing a hyper-network to generate parameter shift, whereas existing hyper-networks suffer from inferior scalability in synchronous editing operation amount (Hase et al., 2023b; Huang et al., 2023). For instance, Mitchell et al. (2022) mimics gradient accumulation to sum the parameter shifts together, which lacks statistical significance and is prone to cancellation effect. To mitigate the problem, we propose the MAssive Language Model Editing Network (MALMEN), which formulates the parameter shift aggregation as the least square problem, subsequently updating the LM parameter using the normal equation. To accommodate editing multiple facts simultaneously with limited memory budgets, we separate the computation on the hyper-network and LM, enabling arbitrary batch size on both neural networks. Our method is evaluated by editing up to thousands of facts on LMs with different architectures, i.e., BERT-base, GPT-2, and GPT-J (6B), across various knowledge-intensive NLP tasks, i.e., closed book fact-checking and question answering. Remarkably, MALMEN is capable of editing hundreds of times more facts than MEND (Mitchell et al., 2022) with the identical hyper-network architecture and outperforms editor specifically designed for GPT, i.e., MEMIT (Meng et al., 2023).",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the acquired knowledge may be fundamentally incorrect or outdated over time, which necessitates rectifying the knowledge of the language model (LM) after the training.\"\n\nThis abstract mentions a limitation of LLMs (acquired knowledge being incorrect or outdated) but does not explore it in depth and focuses on the proposed solution to mitigate this limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the acquired knowledge may be fundamentally incorrect or outdated over time, which necessitates rectifying the knowledge of the language model (LM) after the training.\"\n\nThis abstract mentions a limitation of LLMs (acquired knowledge being incorrect or outdated) but does not explore it in depth and focuses on the proposed solution to mitigate this limitation."
    },
    {
        "title": "Select to Perfect: Imitating desired behavior from large multi-agent data",
        "authors": "Tim Franzmeyer;Edith Elkind;Philip Torr;Jakob Nicolaus Foerster;Joao F. Henriques",
        "site": "https://iclr.cc/virtual/2024/poster/18872",
        "summary": "AI agents are commonly trained with large datasets of demonstrations of human behavior.However, not all behaviors are equally safe or desirable.Desired characteristics for an AI agent can be expressed by assigning desirability scores, which we assume are not assigned to individual behaviors but to collective trajectories.For example, in a dataset of vehicle interactions, these scores might relate to the number of incidents that occurred. We first assess the effect of each individual agent's behavior on the collective desirability score, e.g., assessing how likely an agent is to cause incidents.This allows us to selectively imitate agents with a positive effect, e.g., only imitating agents that are unlikely to cause incidents. To enable this, we propose the concept of an agent's \\textit{Exchange Value}, which quantifies an individual agent's contribution to the collective desirability score. The Exchange Value is the expected change in desirability score when substituting the agent for a randomly selected agent.We propose additional methods for estimating Exchange Values from real-world datasets, enabling us to learn desired imitation policies that outperform relevant baselines. The project website can be found at https://tinyurl.com/select-to-perfect.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment",
        "authors": "Geyang Guo;Ranchi Zhao;Tianyi Tang;Xin Zhao;Ji-Rong Wen",
        "site": "https://iclr.cc/virtual/2024/poster/18867",
        "summary": "Abstract:Alignment with human preference is a desired property of large language models (LLMs). Currently, the main alignment approach is based on reinforcement learning from human feedback (RLHF). Despite the effectiveness of RLHF, it is intricate to implement and train, thus recent studies explore how to develop alternative alignment approaches based on supervised fine-tuning (SFT). A major limitation of SFT is that it essentially does imitation learning, which can't fully understand what are the expected behaviors. To address this issue, we propose an improved alignment approach named $\\textbf{FIGA}$. Different from prior methods, we incorporate fine-grained (i.e., token or phrase level) quality signals that are derived by contrasting good and bad responses. Our approach has made two major contributions. Firstly, we curate a refined alignment dataset that pairs initial responses and the corresponding revised ones. Secondly, we devise a new loss function can leverage fine-grained quailty signals to instruct the learning of LLMs for alignment. Extensive experiments have demonstrated the effectiveness of our approaches by comparing a number of competitive baselines.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"A major limitation of SFT is that it essentially does imitation learning, which can't fully understand what are the expected behaviors.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"A major limitation of SFT is that it essentially does imitation learning, which can't fully understand what are the expected behaviors.\""
    },
    {
        "title": "CCIL: Continuity-Based Data Augmentation for Corrective Imitation Learning",
        "authors": "Liyiming Ke;Yunchu Zhang;Abhay Deshpande;Siddhartha Srinivasa;Abhishek Gupta",
        "site": "https://iclr.cc/virtual/2024/poster/18866",
        "summary": "We present a new technique to enhance the robustness of imitation learning methods by generating corrective data to account for compounding error and disturbances. While existing methods rely on interactive expert labeling, additional offline datasets, or domain-specific invariances, our approach requires minimal additional assumptions beyond expert data. The key insight is to leverage local continuity in the environment dynamics. Our method first constructs a dynamics model from the expert demonstration, enforcing local Lipschitz continuity while skipping the discontinuous regions. In the locally continuous regions, this model allows us to generate corrective labels within the neighborhood of the demonstrations but beyond the actual set of states and actions in the dataset. Training on this augmented data enhances the agent's ability to recover from perturbations and deal with compounding error. We demonstrate the effectiveness of our generated labels through experiments in a variety of robotics domains that have distinct forms of continuity and discontinuity, including classic control, drone flying, high-dimensional navigation, locomotion, and tabletop manipulation.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Attention-Guided Contrastive Role Representations for Multi-agent Reinforcement Learning",
        "authors": "Zican Hu;Zongzhang Zhang;Huaxiong Li;Chunlin Chen;Hongyu Ding;Zhi Wang",
        "site": "https://iclr.cc/virtual/2024/poster/18863",
        "summary": "Real-world multi-agent tasks usually involve dynamic team composition with the emergence of roles, which should also be a key to efficient cooperation in multi-agent reinforcement learning (MARL). Drawing inspiration from the correlation between roles and agent's behavior patterns, we propose a novel framework ofAttention-guidedCOntrastiveRole representation learning forMARL (ACORM) to promote behavior heterogeneity, knowledge transfer, and skillful coordination across agents. First, we introduce mutual information maximization to formalize role representation learning, derive a contrastive learning objective, and concisely approximate the distribution of negative pairs. Second, we leverage an attention mechanism to prompt the global state to attend to learned role representations in value decomposition, implicitly guiding agent coordination in a skillful role space to yield more expressive credit assignment. Experiments on challenging StarCraft II micromanagement and Google research football tasks demonstrate the state-of-the-art performance of our method and its advantages over existing approaches. Our code is available athttps://github.com/NJU-RL/ACORM.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "A Black-box Approach for Non-stationary Multi-agent Reinforcement Learning",
        "authors": "Haozhe Jiang;Qiwen Cui;Zhihan Xiong;Maryam Fazel;Simon Shaolei Du",
        "site": "https://iclr.cc/virtual/2024/poster/18862",
        "summary": "Abstract:We investigate learning the equilibria in non-stationary multi-agent systems and address the challenges that differentiate multi-agent learning from single-agent learning. Specifically, we focus on games with bandit feedback, where testing an equilibrium can result in substantial regret even when the gap to be tested is small, and the existence of multiple optimal solutions (equilibria) in stationary games poses extra challenges. To overcome these obstacles, we propose a versatile black-box approach applicable to a broad spectrum of problems, such as general-sum games, potential games, and Markov games, when equipped with appropriate learning and testing oracles for stationary environments. Our algorithms can achieve $\\widetilde{O}\\left(\\Delta^{1/4}T^{3/4}\\right)$ regret when the degree of nonstationarity, as measured by total variation $\\Delta$, is known, and $\\widetilde{O}\\left(\\Delta^{1/5}T^{4/5}\\right)$ regret when $\\Delta$ is unknown, where $T$ is the number of rounds. Meanwhile, our algorithm inherits the favorable dependence on number of agents from the oracles. As a side contribution that may be independent of interest, we show how to test for various types of equilibria by a black-box reduction to single-agent learning, which includes Nash equilibria, correlated equilibria, and coarse correlated equilibria.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Zoology: Measuring and Improving Recall in Efficient Language Models",
        "authors": "Simran Arora;Sabri Eyuboglu;Aman Timalsina;Isys Johnson;Michael Poli;James Zou;Atri Rudra;Christopher Re",
        "site": "https://iclr.cc/virtual/2024/poster/18860",
        "summary": "Attention-free language models that combine gating and convolutions are growing in popularity due to their efficiency and increasingly competitive performance. To better understand these architectures, we pretrain a suite of 17 attention and gated-convolution language models, finding that SoTA gated-convolution architectures still underperform attention by up to 2.1 perplexity points on the Pile. In fine-grained analysis, we find 82% of the gap is explained by each model's ability to recall information that is previously mentioned in-context, e.g. \"Hakuna Matata means no worries Hakuna Matata it means no\" -> ??. On this task, termed \"associative recall\", we find that attention outperforms gated-convolutions by a large margin: a 70M parameter attention model outperforms a 1.4 billion parameter gated-convolution model on associative recall. This is surprising because prior work shows gated convolutions can perfectly solve synthetic tests for AR capability.  To close the gap between synthetics and real language, we develop a new formalization of the task called multi-query associative recall (MQAR) that better reflects actual language. We perform an empirical and theoretical study of MQAR that elucidates differences in the parameter-efficiency of attention and gated-convolution recall. Informed by our analysis, we evaluate simple convolution-attention hybrids and show that hybrids with input-dependent sparse attention patterns can close 97.4% of the gap to attention, while maintaining sub-quadratic scaling. Code is at: https://github.com/HazyResearch/zoology.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"On this task, termed \"associative recall\", we find that attention outperforms gated-convolutions by a large margin: a 70M parameter attention model outperforms a 1.4 billion parameter gated-convolution model on associative recall.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"On this task, termed \"associative recall\", we find that attention outperforms gated-convolutions by a large margin: a 70M parameter attention model outperforms a 1.4 billion parameter gated-convolution model on associative recall.\""
    },
    {
        "title": "Fast Value Tracking for Deep Reinforcement Learning",
        "authors": "Frank Shih;Faming Liang",
        "site": "https://iclr.cc/virtual/2024/poster/18858",
        "summary": "Reinforcement learning (RL) tackles sequential decision-making problems by creatingagents that interacts with their environment. However, existing algorithms often view these problem as static, focusing on point estimates for model parameters to maximize expected rewards, neglecting the stochastic dynamics of agent-environment interactions and the critical role of uncertainty quantification.Our research leverages the Kalman filtering paradigm to introduce a novel and scalable sampling algorithm called Langevinized Kalman Temporal-Difference (LKTD) for deep reinforcement learning. This algorithm, grounded in Stochastic Gradient Markov Chain Monte Carlo (SGMCMC), efficiently draws samples from the posterior distribution of deep neural network parameters. Under mild conditions, we prove that the posterior samples generated by the LKTD algorithm converge to a stationary distribution. This convergence not only enables us to quantify uncertainties associated with the value function and model parameters but also allows us to monitor these uncertainties during policy updates throughout the training phase. The LKTD algorithm paves the way for more robust and adaptable reinforcement learning approaches.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Delta-AI: Local objectives for amortized inference in sparse graphical models",
        "authors": "Jean-Pierre René Falet;Hae Beom Lee;Nikolay Malkin;Chen Sun;Dragos Secrieru;Dinghuai Zhang;Guillaume Lajoie;Yoshua Bengio",
        "site": "https://iclr.cc/virtual/2024/poster/18855",
        "summary": "Abstract:We present a new algorithm for amortized inference in sparse probabilistic graphical models (PGMs), which we call $\\Delta$-amortized inference ($\\Delta$-AI). Our approach is based on the observation that when the sampling of variables in a PGM is seen as a sequence of actions taken by an agent, sparsity of the PGM enables local credit assignment in the agent's policy learning objective. This yields a local constraint that can be turned into a local loss in the style of generative flow networks (GFlowNets) that enables off-policy training but avoids the need to instantiate all the random variables for each parameter update, thus speeding up training considerably. The $\\Delta$-AI objective matches the conditional distribution of a variable given its Markov blanket in a tractable learned sampler, which has the structure of a Bayesian network, with the same conditional distribution under the target PGM. As such, the trained sampler recovers marginals and conditional distributions of interest and enables inference of partial subsets of variables. We illustrate $\\Delta$-AI's effectiveness for sampling from synthetic PGMs and training latent variable models with sparse factor structure. Code: https://github.com/GFNOrg/Delta-AI.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Sign2GPT: Leveraging Large Language Models for Gloss-Free Sign Language Translation",
        "authors": "Ryan Wong;Necati Cihan Camgoz;Richard Bowden",
        "site": "https://iclr.cc/virtual/2024/poster/18847",
        "summary": "Automatic Sign Language Translation requires the integration of both computer vision and natural language processing to effectively bridge the communication gap between sign and spoken languages. However, the deficiency in large-scale training data to support sign language translation means we need to leverage resources from spoken language. We introduce, Sign2GPT, a novel framework for sign language translation that utilizes large-scale pretrained vision and language models via lightweight adapters for gloss-free sign language translation. The lightweight adapters are crucial for sign language translation, due to the constraints imposed by limited dataset sizes and the computational requirements when training with long sign videos.We also propose a novel pretraining strategy that directs our encoder to learn sign representations from automatically extracted pseudo-glosses without requiring gloss order information or annotations.We evaluate our approach on two public benchmark sign language translation datasets, namely RWTH-PHOENIX-Weather 2014T and CSL-Daily, and improve on state-of-the-art gloss-free translation performance with a significant margin.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the deficiency in large-scale training data to support sign language translation means we need to leverage resources from spoken language.\"\n\nThis paper discusses LLMs but does not mention any explicit limitation of the models in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, the deficiency in large-scale training data to support sign language translation means we need to leverage resources from spoken language.\"\n\nThis paper discusses LLMs but does not mention any explicit limitation of the models in the abstract."
    },
    {
        "title": "Ins-DetCLIP: Aligning Detection Model to Follow Human-Language Instruction",
        "authors": "Renjie Pi;Lewei Yao;Jianhua Han;Xiaodan Liang;Wei Zhang;Hang Xu",
        "site": "https://iclr.cc/virtual/2024/poster/18842",
        "summary": "This paper introduces Instruction-oriented Object Detection (IOD), a new task that enhances human-computer interaction by enabling object detectors to understand user instructions and locate relevant objects. Unlike traditional open-vocabulary object detection tasks that rely on users providing a list of required category names, IOD requires models to comprehend natural-language instructions, contextual reasoning, and output the name and location of the desired categories. This poses fresh challenges for modern object detection systems. To develop an IOD system, we create a dataset called IOD-Bench, which consists of instruction-guided detections, along with specialized evaluation metrics. We leverage large-scale language models (LLMs) to generate a diverse set of instructions (8k+) based on existing public object detection datasets, covering a wide range of real-world scenarios. As an initial approach to the IOD task, we propose a model called Ins-DetCLIP. It harnesses the extensive knowledge within LLMs to empower the detector with instruction-following capabilities. Specifically, our Ins-DetCLIP employs a visual encoder (i.e., DetCLIP, an open-vocabulary detector) to extract object-level features. These features are then aligned with the input instructions using a cross-modal fusion module integrated into a pre-trained LLM. Experimental results conducted on IOD-Bench demonstrate that our model consistently outperforms baseline methods that directly combine LLMs with detection models. This research aims to pave the way for a more adaptable and versatile interaction paradigm in modern object detection systems, making a significant contribution to the field.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This poses fresh challenges for modern object detection systems.\"\n\nHowever, a more accurate rating could be 2 as it mentions a limitation of modern object detection systems which include LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"This poses fresh challenges for modern object detection systems.\"\n\nHowever, a more accurate rating could be 2 as it mentions a limitation of modern object detection systems which include LLMs."
    },
    {
        "title": "The Curse of Diversity in Ensemble-Based Exploration",
        "authors": "Zhixuan Lin;Pierluca D'Oro;Evgenii Nikishin;Aaron Courville",
        "site": "https://iclr.cc/virtual/2024/poster/18840",
        "summary": "We uncover a surprising phenomenon in deep reinforcement learning: training a diverse ensemble of data-sharing agents -- a well-established exploration strategy -- can significantly impair the performance of the individual ensemble members when compared to standard single-agent training. Through careful analysis, we attribute the degradation in performance to the low proportion of self-generated data in the shared training data for each ensemble member, as well as the inefficiency of the individual ensemble members to learn from such highly off-policy data. We thus name this phenomenonthe curse of diversity. We find that several intuitive solutions -- such as a larger replay buffer or a smaller ensemble size -- either fail to consistently mitigate the performance loss or undermine the advantages of ensembling. Finally, we demonstrate the potential of representation learning to counteract the curse of diversity with a novel method named Cross-Ensemble Representation Learning (CERL) in both discrete and continuous control domains. Our work offers valuable insights into an unexpected pitfall in ensemble-based exploration and raises important caveats for future applications of similar approaches.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents",
        "authors": "Yang Deng;Wenxuan Zhang;Wai Lam;See-Kiong Ng;Tat-Seng Chua",
        "site": "https://iclr.cc/virtual/2024/poster/18838",
        "summary": "Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases. In this work, we introduce a new dialogue policy planning paradigm to strategize LLMs for proactive dialogue problems with a tunable language model plug-in as a plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a novel training framework to facilitate supervised fine-tuning over available human-annotated data as well as reinforcement learning from goal-oriented AI feedback with dynamic interaction data collected by the LLM-based self-play simulation. In this manner, the LLM-powered dialogue agent can not only be generalized to different cases after the training, but also be applicable to different applications by just substituting the learned plug-in. In addition, we propose to evaluate the policy planning capability of dialogue systems under the interactive setting. Experimental results demonstrate that PPDPP consistently and substantially outperforms existing approaches on three different proactive dialogue applications, including negotiation, emotional support, and tutoring dialogues.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases.\"\n\nThis rating is given because the paper mentions a limitation of LLMs (bounded policy",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases.\"\n\nThis rating is given because the paper mentions a limitation of LLMs (bounded policy"
    },
    {
        "title": "Discovering Temporally-Aware Reinforcement Learning Algorithms",
        "authors": "Matthew Thomas Jackson;Chris Lu;Louis Kirsch;Robert Tjarko Lange;Shimon Whiteson;Jakob Nicolaus Foerster",
        "site": "https://iclr.cc/virtual/2024/poster/18831",
        "summary": "Recent advancements in meta-learning have enabled the automatic discovery of novel reinforcement learning algorithms parameterized by surrogate objective functions. To improve upon manually designed algorithms, the parameterization of this learned objective function must be expressive enough to represent novel principles of learning (instead of merely recovering already established ones) while still generalizing to a wide range of settings outside of its meta-training distribution. However, existing methods focus on discovering objective functions that, like many widely used objective functions in reinforcement learning, do not take into account the total number of steps allowed for training, or “training horizon”. In contrast, humans use a plethora of different learning objectives across the course of acquiring a new ability. For instance, students may alter their studying techniques based on the proximity to exam deadlines and their self-assessed capabilities. This paper contends that ignoring the optimization time horizon significantly restricts the expressive potential of discovered learning algorithms. We propose a simple augmentation to two existing objective discovery approaches that allows the discovered algorithm to dynamically update its objective function throughout the agent’s training procedure, resulting in expressive schedules and increased generalization across different training horizons. In the process, we find that commonly used meta-gradient approaches fail to discover such adaptive objective functions while evolution strategies discover highly dynamic learning rules. We demonstrate the effectiveness of our approach on a wide range of tasks and analyze the resulting learned algorithms, which we find effectively balance exploration and exploitation by modifying the structure of their learning rules throughout the agent’s lifetime.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "GENOME: Generative Neuro-Symbolic Visual Reasoning by Growing and Reusing Modules",
        "authors": "Zhenfang Chen;Rui Sun;Wenjun Liu;Yining Hong;Chuang Gan",
        "site": "https://iclr.cc/virtual/2024/poster/18827",
        "summary": "Recent works have shown that Large Language Models (LLMs) could empower traditional neuro-symbolic models via programming capabilities to translate languages into module descriptions, thus achieving strong visual reasoning results while maintaining the model’s transparency and efficiency. However, these models usually exhaustively generate the entire code snippet given each new instance of a task, which is extremely ineffective. On the contrary, human beings gradually acquire knowledge that can be reused and grow into more profound skills for fast generalization to new tasks since we are an infant. Inspired by this, we propose generative neuro-symbolic visual reasoning by growing and reusing modules. Specifically, our model consists of three unique stages, module initialization, module generation, and module execution. First, given a vision-language task, we adopt LLMs to examine whether we could reuse and grow over established modules to handle this new task. If not, we initialize a new module needed by the task and specify the inputs and outputs of this new module. After that, the new module is created by querying LLMs to generate corresponding code snippets that match the requirements. In order to get a better sense of the new module’s ability, we treat few-shot training examples as test cases to see if our new module could pass these cases. If yes, the new module is added to the module library for future reuse. Finally, we evaluate the performance of our model on the testing set by executing the parsed programs with the newly made visual modules to get the results. We find the proposed GENOME model possesses several advantages. First, it performs competitively on standard tasks like visual question answering and referring expression comprehension; Second, the visual modules learned from one task can be seamlessly transferred to new tasks; Last but not least, it is able to adapt to new visual reasoning tasks by observing a few training examples and reusing modules.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these models usually exhaustively generate the entire code snippet given each new instance of a task, which is extremely ineffective.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these models usually exhaustively generate the entire code snippet given each new instance of a task, which is extremely ineffective.\""
    },
    {
        "title": "GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher",
        "authors": "Youliang Yuan;Wenxiang Jiao;Wenxuan Wang;Jen-tse Huang;Pinjia He;Shuming Shi;Zhaopeng Tu",
        "site": "https://iclr.cc/virtual/2024/poster/18817",
        "summary": "Safety lies at the core of the development of Large Language Models (LLMs). There is ample work on aligning LLMs with human ethics and preferences, including data filtering in pretraining, supervised fine-tuning, reinforcement learning from human feedback, red teaming, etc. In this study, we discover that chat in cipher can bypass the safety alignment techniques of LLMs, which are mainly conducted in natural languages. We propose a novel framework CipherChat to systematically examine the generalizability of safety alignment to non-natural languages -- ciphers. CipherChat enables humans to chat with LLMs through cipher prompts topped with system role descriptions and few-shot enciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs, including ChatGPT and GPT-4 for different representative human ciphers across 11 safety domains in both English and Chinese. Experimental results show that certain ciphers succeed almost 100% of the time in bypassing the safety alignment of GPT-4 in several safety domains, demonstrating the necessity of developing safety alignment for non-natural languages. Notably, we identify that LLMs seem to have a ''secret cipher'', and propose a novel SelfCipher that uses only role play and several unsafe demonstrations in natural language to evoke this capability. SelfCipher surprisingly outperforms existing human ciphers in almost all cases.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We discover that chat in cipher can bypass the safety alignment techniques of LLMs, which are mainly conducted in natural languages.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We discover that chat in cipher can bypass the safety alignment techniques of LLMs, which are mainly conducted in natural languages.\""
    },
    {
        "title": "Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning",
        "authors": "Juan Rocamonde;Victoriano Montesinos;Elvis Nava;Ethan Perez;David Lindner",
        "site": "https://iclr.cc/virtual/2024/poster/18802",
        "summary": "Reinforcement learning (RL) requires either manually specifying a reward function, which is often infeasible, or learning a reward model from a large amount of human feedback, which is often very expensive. We study a more sample-efficient alternative: using pretrained vision-language models (VLMs) as zero-shot reward models (RMs) to specify tasks via natural language. We propose a natural and general approach to using VLMs as reward models, which we call VLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learn complex tasks without a manually specified reward function, such as kneeling, doing the splits, and sitting in a lotus position. For each of these tasks, we only providea single sentence text promptdescribing the desired task with minimal prompt engineering. We provide videos of the trained agents at: https://sites.google.com/view/vlm-rm. We can improve performance by providing a second \"baseline\" prompt and projecting out parts of the CLIP embedding space irrelevant to distinguish between goal and baseline. Further, we find a strong scaling effect for VLM-RMs: larger VLMs trained with more compute and data are better reward models. The failure modes of VLM-RMs we encountered are all related to known capability limitations of current VLMs, such as limited spatial reasoning ability or visually unrealistic environments that are far off-distribution for the VLM. We find that VLM-RMs are remarkably robust as long as the VLM is large enough. This suggests that future VLMs will become more and more useful reward models for a wide range of RL applications.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The failure modes of VLM-RMs we encountered are all related to known capability limitations of current VLMs, such as limited spatial reasoning ability or visually unrealistic environments that are far off-distribution for the VLM.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The failure modes of VLM-RMs we encountered are all related to known capability limitations of current VLMs, such as limited spatial reasoning ability or visually unrealistic environments that are far off-distribution for the VLM.\""
    },
    {
        "title": "Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL",
        "authors": "Hao Sun;Alihan Hüyük;Mihaela van der Schaar",
        "site": "https://iclr.cc/virtual/2024/poster/18797",
        "summary": "In this study, we aim to enhance the arithmetic reasoning ability of Large Language Models (LLMs) through zero-shot prompt optimization. We identify a previously overlooked objective of query dependency in such optimization and elucidate two ensuing challenges that impede the successful and economical design of prompt optimization techniques. One primary issue is the absence of an effective method to evaluate prompts during inference when the golden answer is unavailable. Concurrently, learning via interactions with the LLMs to navigate the expansive natural language prompting space proves to be resource-intensive.To address this, we introduce Prompt-OIRL, which harnesses offline inverse reinforcement learning to draw insights from offline prompting demonstration data. Such data exists as by-products when diverse prompts are benchmarked on open-accessible datasets. With Prompt-OIRL, the query-dependent prompt optimization objective is achieved by first learning an offline reward model. This model can evaluate any query-prompt pairs without accessing LLMs. Subsequently, a best-of-N strategy is deployed to recommend the optimal prompt. Our experimental evaluations across various LLM scales and arithmetic reasoning datasets underscore both the efficacy and economic viability of the proposed approach.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"One primary issue is the absence of an effective method to evaluate prompts during inference when the golden answer is unavailable. Concurrently, learning via interactions with the LLMs to navigate the expansive natural language prompting space proves to be resource-intensive.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"One primary issue is the absence of an effective method to evaluate prompts during inference when the golden answer is unavailable. Concurrently, learning via interactions with the LLMs to navigate the expansive natural language prompting space proves to be resource-intensive.\""
    },
    {
        "title": "Efficient Streaming Language Models with Attention Sinks",
        "authors": "Guangxuan Xiao;Yuandong Tian;Beidi Chen;Song Han;Mike Lewis",
        "site": "https://iclr.cc/virtual/2024/poster/18794",
        "summary": "Abstract:Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges.Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory.Secondly, popular LLMs cannot generalize to longer texts than the training sequence length.Window attention, where only the most recent KVs are cached, is a natural approach --- but we show that it fails when the text length surpasses the cache size.We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a ``sink'' even if they are not semantically important.Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence length without any fine-tuning.We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2$\\times$ speedup.Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Deploying Large Language Models (LLMs) in streaming applications... poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Deploying Large Language Models (LLMs) in streaming applications... poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length.\""
    },
    {
        "title": "Improving LoRA in Privacy-preserving Federated Learning",
        "authors": "Youbang Sun;Zitao Li;Yaliang Li;Bolin Ding",
        "site": "https://iclr.cc/virtual/2024/poster/18792",
        "summary": "Low-rank adaptation (LoRA) is one of the most popular task-specific parameter-efficient fine-tuning (PEFT) methods on pre-trained language models for its good performance and computational efficiency.LoRA injects a product of two trainable rank decomposition matrices over the top of each frozen pre-trained model module.However, when applied in the setting of privacy-preserving federated learning (FL), LoRA may become unstable due to the following facts: 1) the effects of data heterogeneity and multi-step local updates are non-negligible, 2) additive noise enforced on updating gradients to guarantee differential privacy (DP) can be amplified and 3) the final performance is susceptible to hyper-parameters.A key factor leading to these phenomena is the discordance between jointly optimizing the two low-rank matrices by local clients and separately aggregating them by the central server.Thus, this paper proposes an efficient and effective version of LoRA, Federated Freeze A LoRA (FFA-LoRA), to alleviate these challenges and further halve the communication cost of federated fine-tuning LLMs.The core idea of FFA-LoRA is to fix the randomly initialized non-zero matrices and only fine-tune the zero-initialized matrices.Compared to LoRA, FFA-LoRA is motivated by practical and theoretical benefits in privacy-preserved FL. Our experiments demonstrate that FFA-LoRA provides more consistent performance with better computational efficiency over vanilla LoRA in various FL tasks.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, when applied in the setting of privacy-preserving federated learning (FL), LoRA may become unstable due to the following facts: 1) the effects of data heterogeneity and multi-step local updates are non-negligible, 2) additive noise enforced on updating gradients to guarantee differential privacy (DP) can be amplified and 3) the final performance is",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, when applied in the setting of privacy-preserving federated learning (FL), LoRA may become unstable due to the following facts: 1) the effects of data heterogeneity and multi-step local updates are non-negligible, 2) additive noise enforced on updating gradients to guarantee differential privacy (DP) can be amplified and 3) the final performance is"
    },
    {
        "title": "Federated Text-driven Prompt Generation for Vision-Language Models",
        "authors": "Chen Qiu;Xingyu Li;Chaithanya Kumar Mummadi;Madan Ravi Ganesh;Zhenzhen Li;Lu Peng;Wan-Yi Lin",
        "site": "https://iclr.cc/virtual/2024/poster/18784",
        "summary": "Prompt learning for vision-language models, e.g., CoOp, has shown great success in adapting CLIP to different downstream tasks, making it a promising solution for federated learning due to computational reasons. Existing prompt learning techniques replace hand-crafted text prompts with learned vectors that offer improvements on seen classes, but struggle to generalize to unseen classes. Our work addresses this challenge by proposing Federated Text-driven Prompt Generation (FedTPG), which learns a unified prompt generation network across multiple remote clients in a scalable manner. The prompt generation network is conditioned on task-related text input, thus is context-aware, making it suitable to generalize for both seen and unseen classes. Our comprehensive empirical evaluations on nine diverse image classification datasets show that our method is superior to existing federated prompt learning methods, achieving better overall generalization on both seen and unseen classes, as well as datasets.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing prompt learning techniques... struggle to generalize to unseen classes.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing prompt learning techniques... struggle to generalize to unseen classes.\""
    },
    {
        "title": "Towards Imitation Learning to Branch for MIP: A Hybrid Reinforcement Learning based Sample Augmentation Approach",
        "authors": "Changwen Zhang;Wenli Ouyang;Hao Yuan;Liming Gong;Yong Sun;Ziao Guo;Zhichen Dong;Junchi Yan",
        "site": "https://iclr.cc/virtual/2024/poster/18781",
        "summary": "Branch-and-bound (B\\&B) has long been favored for tackling complex Mixed Integer Programming (MIP) problems, where the choice of branching strategy plays a pivotal role. Recently, Imitation Learning (IL)-based policies have emerged as potent alternatives to traditional rule-based approaches. However, it is nontrivial to acquire high-quality training samples, and IL often converges to suboptimal variable choices for branching, restricting the overall performance. In response to these challenges, we propose a novel hybrid online and offline reinforcement learning (RL) approach to enhance the branching policy by cost-effective training sample augmentation. In the online phase, we train an online RL agent to dynamically decide the sample generation processes, drawing from either the learning-based policy or the expert policy. The objective is to strike a balance between exploration and exploitation of the sample generation process. In the offline phase, a value function is trained to fit each decision's cumulative reward and filter the samples with high cumulative returns. This dual-purpose function not only reduces training complexity but also enhances the quality of the samples. To assess the efficacy of our data augmentation mechanism, we conduct comprehensive evaluations across a range of MIP problems. The results consistently show that it excels in making superior branching decisions compared to state-of-the-art learning-based models and the open-source solver SCIP. Notably, it even often outperforms Gurobi.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Training Socially Aligned Language Models on Simulated Social Interactions",
        "authors": "Ruibo Liu;Ruixin Yang;Chenyan Jia;Ge Zhang;Diyi Yang;Soroush Vosoughi",
        "site": "https://iclr.cc/virtual/2024/poster/18780",
        "summary": "The goal of social alignment for AI systems is to make sure these models can conduct themselves appropriately following social values. Unlike humans who establish a consensus on value judgments through social interaction, current language models (LMs) are trained to rigidly recite the corpus in social isolation, which causes poor generalization in unfamiliar cases and the lack of robustness under adversarial attacks. In this work, we introduce a new training paradigm that enables LMs to learn from simulated social interactions. Compared with existing methods, our method is much more scalable and efficient, and shows superior performance in alignment benchmarks and human evaluation.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Unlike humans who establish a consensus on value judgments through social interaction, current language models (LMs) are trained to rigidly recite the corpus in social isolation, which causes poor generalization in unfamiliar cases and the lack of robustness under adversarial attacks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Unlike humans who establish a consensus on value judgments through social interaction, current language models (LMs) are trained to rigidly recite the corpus in social isolation, which causes poor generalization in unfamiliar cases and the lack of robustness under adversarial attacks.\""
    },
    {
        "title": "The Expressive Power of Transformers with Chain of Thought",
        "authors": "William Merrill;Ashish Sabharwal",
        "site": "https://iclr.cc/virtual/2024/poster/18776",
        "summary": "Recent theoretical work has identified surprisingly simple reasoning problems, such as checking if two nodes in a graph are connected or simulating finite-state machines, that are provably unsolvable by standard transformers that answer immediately after reading their input. However, in practice, transformers' reasoning can be improved by allowing them to use a \"chain of thought\" or \"scratchpad\", i.e., generate and condition on a sequence of intermediate tokens before answering. Motivated by this, we ask:Does such intermediate generation fundamentally extend the computational power of a decoder-only transformer?We show that the answer isyes, but the amount of increase depends crucially on the amount of intermediate generation. For instance, we find that transformer decoders with a logarithmic number of decoding steps (w.r.t. the input length) push the limits of standard transformers only slightly, while a linear number of decoding steps, assuming projected pre-norm (a slight generalization of standard pre-norm), adds a clear new ability (under standard complexity conjectures): recognizing all regular languages. Our results also imply that linear steps keep transformer decoders within context-sensitive languages, and polynomial steps with generalized pre-norm make them recognize exactly the class of polynomial-time solvable problems—the first exact characterization of a type of transformers in terms of standard complexity classes. Together, this provides a nuanced framework for understanding how the length of a transformer’s chain of thought or scratchpad impacts its reasoning power.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent theoretical work has identified surprisingly simple reasoning problems, such as checking if two nodes in a graph are connected or simulating finite-state machines, that are provably unsolvable by standard transformers that answer immediately after reading their input.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Recent theoretical work has identified surprisingly simple reasoning problems, such as checking if two nodes in a graph are connected or simulating finite-state machines, that are provably unsolvable by standard transformers that answer immediately after reading their input.\""
    },
    {
        "title": "VeRA: Vector-based Random Matrix Adaptation",
        "authors": "Dawid Jan Kopiczko;Tijmen Blankevoort;Yuki M Asano",
        "site": "https://iclr.cc/virtual/2024/poster/18775",
        "summary": "Low-rank adapation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous per-user or per-task adapted models. In this work, we present Vector-based Random Matrix Adaptation (VeRA), which significantly reduces the number of trainable parameters compared to LoRA, yet maintains the same performance. It achieves this by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead. We demonstrate its effectiveness on the GLUE and E2E benchmarks, image classification tasks, and show its application in instruction-tuning of 7B and 13B language models. Website: https://dkopi.github.io/vera",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Low-rank adapation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous per-user or per-task adapted models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Low-rank adapation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous per-user or per-task adapted models.\""
    },
    {
        "title": "Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds",
        "authors": "Sipeng Zheng;jiazheng liu;Yicheng Feng;Zongqing Lu",
        "site": "https://iclr.cc/virtual/2024/poster/18772",
        "summary": "Recent studies have presented compelling evidence that large language models (LLMs) can equip embodied agents with the self-driven capability to interact with the world, which marks an initial step toward versatile robotics. However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to ``a blindfolded text-based game.'' Consequently, LLM-based agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand. In this paper, we propose Steve-Eye, an end-to-end trained large multimodal model to address this limitation. Steve-Eye integrates the LLM with a visual encoder to process visual-text inputs and generate multimodal feedback. We adopt a semi-automatic strategy to collect an extensive dataset comprising 850K open-world instruction pairs, enabling our model to encompass three essential functions for an agent: multimodal perception, foundational knowledge base, and skill prediction and planning. Lastly, we develop three open-world evaluation benchmarks and carry out experiments from a wide range of perspectives to validate our model's capability to strategically act and plan. The project’s website and code can be found at https://sites.google.com/view/steve-eye.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to ``a blindfolded text-based game.''",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to ``a blindfolded text-based game.''"
    },
    {
        "title": "Intelligent Switching for Reset-Free RL",
        "authors": "Darshan Patil;Janarthanan Rajendran;Glen Berseth;Sarath Chandar",
        "site": "https://iclr.cc/virtual/2024/poster/18769",
        "summary": "In the real world, the strong episode resetting mechanisms that are needed to trainagents in simulation are unavailable. The resetting assumption limits the potentialof reinforcement learning in the real world, as providing resets to an agent usuallyrequires the creation of additional handcrafted mechanisms or human interventions.Recent work aims to train agents (forward) with learned resets by constructinga second (backward) agent that returns the forward agent to the initial state. Wefind that the termination and timing of the transitions between these two agentsare crucial for algorithm success. With this in mind, we create a new algorithm,Reset Free RL with Intelligently Switching Controller (RISC) which intelligentlyswitches between the two agents based on the agent’s confidence in achieving itscurrent goal. Our new method achieves state-of-the-art performance on severalchallenging environments for reset-free RL.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "PromptTTS 2: Describing and Generating Voices with Text Prompt",
        "authors": "Yichong Leng;Zhifang Guo;Kai Shen;Zeqian Ju;Xu Tan;Eric Liu;Yufei Liu;Dongchao Yang;leying zhang;Kaitao Song;Lei He;Xiangyang Li;sheng zhao;Tao Qin;Jiang Bian",
        "site": "https://iclr.cc/virtual/2024/poster/18768",
        "summary": "Speech conveys more information than text, as the same word can be uttered in various voices to convey diverse information. Compared to traditional text-to-speech (TTS) methods relying on speech prompts (reference speech) for voice variability, using text prompts (descriptions) is more user-friendly since speech prompts can be hard to find or may not exist at all. TTS approaches based on the text prompt face two main challenges: 1) the one-to-many problem, where not all details about voice variability can be described in the text prompt, and 2) the limited availability of text prompt datasets, where vendors and large cost of data labeling are required to write text prompts for speech. In this work, we introduce PromptTTS 2 to address these challenges with a variation network to provide variability information of voice not captured by text prompts, and a prompt generation pipeline to utilize the large language models (LLM) to compose high quality text prompts. Specifically, the variation network predicts the representation extracted from the reference speech (which contains full information about voice variability) based on the text prompt representation. For the prompt generation pipeline, it generates text prompts for speech with a speech language understanding model to recognize voice attributes (e.g., gender, speed) from speech and a large language model to formulate text prompts based on the recognition results. Experiments on a large-scale (44K hours) speech dataset demonstrate that compared to the previous works, PromptTTS 2 generates voices more consistent with text prompts and supports the sampling of diverse voice variability, thereby offering users more choices on voice generation. Additionally, the prompt generation pipeline produces high-quality text prompts, eliminating the large labeling cost. The demo page of PromptTTS 2 is available (https://speechresearch.github.io/prompttts2).",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions the \"limited availability of text prompt datasets\" which could be a limitation related to the use of LLMs, however, it is not explicitly stated as a limitation of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions the \"limited availability of text prompt datasets\" which could be a limitation related to the use of LLMs, however, it is not explicitly stated as a limitation of LLMs."
    },
    {
        "title": "InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists",
        "authors": "Yulu Gan;Sungwoo Park;Alexander Marcel Schubert;Anthony Philippakis;Ahmed Alaa",
        "site": "https://iclr.cc/virtual/2024/poster/18764",
        "summary": "Recent advances in generative diffusion models have enabled text-controlled synthesis of realistic and diverse images with impressive quality. Despite these remarkable advances, the application of text-to-image generative models in computer vision for standard visual recognition tasks remains limited. The current de facto approach for these tasks is to design model architectures and loss functions that are tailored to the task at hand. In this paper, we develop a unified language interface for computer vision tasks that abstracts away task specific design choices and enables task execution by following natural language instructions. Our approach involves casting multiple computer vision tasks as text-to-image generation problems. Here, the text represents an instruction describing the task, and the resulting image is a visually-encoded task output. To train our model, we pool commonly-used computer vision datasets covering a range of tasks, including segmentation, object detection, depth estimation, and classification. We then use a large language model to paraphrase prompt templates that convey the specific tasks to be conducted on each image, and through this process, we create a multi-modal and multi-task training dataset comprising input and output images along with annotated instructions. Following the InstructPix2Pix architecture, we apply instruction-tuning to a text-to-image diffusion model using our constructed dataset, steering its functionality from a generative model to an instruction-guided multi-task vision learner. Experiments demonstrate that our model, dubbed InstructCV, performs competitively compared to other generalist and task-specific vision models. Moreover, it exhibits compelling generalization capabilities to unseen data, categories, and user instructions.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our approach involves casting multiple computer vision tasks as text-to-image generation problems... We then use a large language model to paraphrase prompt templates that convey the specific tasks to be conducted on each image...\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Our approach involves casting multiple computer vision tasks as text-to-image generation problems... We then use a large language model to paraphrase prompt templates that convey the specific tasks to be conducted on each image...\""
    },
    {
        "title": "Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation",
        "authors": "Hongtao Wu;Ya Jing;Chilam Cheang;Guangzeng Chen;Jiafeng Xu;Xinghang Li;Minghuan Liu;Hang Li;Tao Kong",
        "site": "https://iclr.cc/virtual/2024/poster/18762",
        "summary": "Generative pre-trained models have demonstrated remarkable effectiveness in language and vision domains by learning useful representations. In this paper, we extend the scope of this effectiveness by showing that visual robot manipulation can significantly benefit from large-scale video generative pre-training. We introduce GR-1, a GPT-style model designed for multi-task language-conditioned visual robot manipulation. GR-1 takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states. It predicts robot actions as well as future images in an end-to-end manner. Thanks to a flexible design, GR-1 can be seamlessly finetuned on robot data after pre-trained on a large-scale video dataset. We perform extensive experiments on the challenging CALVIN benchmark and a real robot. On CALVIN benchmark, our method outperforms state-of-the-art baseline methods and improves the success rate from 88.9% to 94.9%. In the setting of zero-shot unseen scene generalization, GR-1 improves the success rate from 53.3% to 85.4%. In real robot experiments, GR-1 also outperforms baseline methods and shows strong potentials in generalization to unseen scenes and objects. We provide inaugural evidence that a unified GPT-style transformer, augmented with large-scale video generative pre-training, exhibits remarkable generalization to multi-task visual robot manipulation. Project page: https://GR1-Manipulation.github.io",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Protein Multimer Structure Prediction via Prompt Learning",
        "authors": "Ziqi Gao;Xiangguo Sun;Zijing Liu;Yu Li;Hong Cheng;Jia Li",
        "site": "https://iclr.cc/virtual/2024/poster/18748",
        "summary": "Understanding the 3D structures of protein multimers is crucial, as they play a vital role in regulating various cellular processes. It has been empirically confirmed that the multimer structure prediction (MSP) can be well handled in a step-wise assembly fashion using provided dimer structures and predicted protein-protein interactions (PPIs). However, due to the biological gap in the formation of dimers and larger multimers, directly applying PPI prediction techniques can often cause a poor generalization to the MSP task. To address this challenge, we aim to extend the PPI knowledge to multimers of different scales (i.e., chain numbers). Specifically, we propose PromptMSP, a pre-training and Prompt tuning framework for Multimer Structure Prediction. First, we tailor the source and target tasks for effective PPI knowledge learning and efficient inference, respectively. We design PPI-inspired prompt learning to narrow the gaps of two task formats and generalize the PPI knowledge to multimers of different scales. We provide a meta-learning strategy to learn a reliable initialization of the prompt model, enabling our prompting framework to effectively adapt to limited data for large-scale multimers. Empirically, we achieve both significant accuracy (RMSD and TM-Score) and efficiency improvements compared to advanced MSP models.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs."
    },
    {
        "title": "Headless Language Models: Learning without Predicting with Contrastive Weight Tying",
        "authors": "Nathan Godey;Éric Villemonte de la Clergerie;Benoît Sagot",
        "site": "https://iclr.cc/virtual/2024/poster/18744",
        "summary": "Self-supervised pre-training of language models usually consists in predicting probability distributions over extensive token vocabularies. In this study, we propose an innovative method that shifts away from probability prediction and instead focuses on reconstructing input embeddings in a contrastive fashion via Constrastive Weight Tying (CWT). We apply this approach to pretrain Headless Language Models in both monolingual and multilingual contexts. Our method offers practical advantages, substantially reducing training computational requirements by up to 20 times, while simultaneously enhancing downstream performance and data efficiency. We observe a significant +1.6 GLUE score increase and a notable +2.7 LAMBADA accuracy improvement compared to classical LMs within similar compute budgets.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation of LLMs is mentioned in the abstract, but it implies that classical LMs have high computational requirements.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation of LLMs is mentioned in the abstract, but it implies that classical LMs have high computational requirements."
    },
    {
        "title": "Large Language Models to Enhance Bayesian Optimization",
        "authors": "Tennison Liu;Nicolás Astorga;Nabeel Seedat;Mihaela van der Schaar",
        "site": "https://iclr.cc/virtual/2024/poster/18743",
        "summary": "Bayesian optimization (BO) is a powerful approach for optimizing complex and expensive-to-evaluate black-box functions. Its importance is underscored in many applications, notably including hyperparameter tuning, but its efficacy depends on efficiently balancing exploration and exploitation. While there has been substantial progress in BO methods, striking this balance remains a delicate process. In this light, we present \\texttt{LLAMBO}, a novel approach that integrates the capabilities of Large Language Models (LLM) within BO. At a high level, we frame the BO problem in natural language, enabling LLMs to iteratively \\emph{propose} and \\emph{evaluate} promising solutions conditioned on historical evaluations. More specifically, we explore how combining contextual understanding, few-shot learning proficiency, and domain knowledge of LLMs can improve model-based BO. Our findings illustrate that \\texttt{LLAMBO} is effective at zero-shot warmstarting, and enhances surrogate modeling and candidate sampling, especially in the early stages of search when observations are sparse. Our approach is performed in context and does not require LLM finetuning. Additionally, it is modular by design, allowing individual components to be integrated into existing BO frameworks, or function cohesively as an end-to-end method. We empirically validate \\texttt{LLAMBO}'s efficacy on the problem of hyperparameter tuning, highlighting strong empirical performance across a range of diverse benchmarks, proprietary, and synthetic tasks.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations of LLMs, only mentions that the approach \"does not require LLM finetuning\" which implies a limitation of fine-tuning, but it is not a limitation of LLMs itself.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations of LLMs, only mentions that the approach \"does not require LLM finetuning\" which implies a limitation of fine-tuning, but it is not a limitation of LLMs itself."
    },
    {
        "title": "CoT3DRef: Chain-of-Thoughts Data-Efficient 3D Visual Grounding",
        "authors": "Eslam Mohamed BAKR;Mohamed Ayman Mohamed;Mahmoud Ahmed;Habib Slim;Mohamed Elhoseiny",
        "site": "https://iclr.cc/virtual/2024/poster/18742",
        "summary": "3D visual grounding is the ability to localize objects in 3D scenes conditioned by utterances. Most existing methods devote the referring head to localize the referred object directly, causing failure in complex scenarios. In addition, it does not illustrate how and why the network reaches the final decision. In this paper, we address this question “Can we design an interpretable 3D visual grounding framework that has the potential to mimic the human perception system?”. To this end, we formulate the 3D visual grounding problem as a sequence-to-sequence (Seq2Seq) task by first predicting a chain of anchors and then the final target. Interpretability not only improves the overall performance but also helps us identify failure cases. Following the chain of thoughts approach enables us to decompose the referring task into interpretable intermediate steps, boosting the performance and making our framework extremely data-efficient. Moreover, our proposed framework can be easily integrated into any existing architecture. We validate our approach through comprehensive experiments on the Nr3D, Sr3D, and Scanrefer benchmarks and show consistent performance gains compared to existing methods without requiring manually annotated data. Furthermore, our proposed framework, dubbed CoT3DRef, is significantly data-efficient, whereas on the Sr3D dataset, when trained only on 10% of the data, we match the SOTA performance that trained on the entire data. The code is available at github.com/eslambakr/CoT 3DV G.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Reward Design for Justifiable Sequential Decision-Making",
        "authors": "Aleksa Sukovic;Goran Radanovic",
        "site": "https://iclr.cc/virtual/2024/poster/18740",
        "summary": "Equipping agents with the capacity to justify made decisions using supporting evidence represents a cornerstone of accountable decision-making. Furthermore, ensuring that justifications are in line with human expectations and societal norms is vital, especially in high-stakes situations such as healthcare. In this work, we propose the use of a debate-based reward model for reinforcement learning agents, where the outcome of a zero-sum debate game quantifies the justifiability of a decision in a particular state. This reward model is then used to train a justifiable policy, whose decisions can be more easily corroborated with supporting evidence. In the debate game, two argumentative agents take turns providing supporting evidence for two competing decisions. Given the proposed evidence, a proxy of a human judge evaluates which decision is better justified. We demonstrate the potential of our approach in learning policies for prescribing and justifying treatment decisions of septic patients. We show that augmenting the reward with the feedback signal generated by the debate-based reward model yields policies highly favored by the judge when compared to the policy obtained solely from the environment rewards, while hardly sacrificing any performance. Moreover, in terms of the overall performance and justifiability of trained policies, the debate-based feedback is comparable to the feedback obtained from an ideal judge proxy that evaluates decisions using the full information encoded in the state. This suggests that the debate game outputs key information contained in states that is most relevant for evaluating decisions, which in turn substantiates the practicality of combining our approach with human-in-the-loop evaluations. Lastly, we showcase that agents trained via multi-agent debate learn to propose evidence that is resilient to refutations and closely aligns with human preferences.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Evoke: Evoking Critical Thinking Abilities in LLMs via Reviewer-Author Prompt Editing",
        "authors": "Xinyu Hu;Pengfei Tang;Simiao Zuo;Zihan Wang;Bowen Song;Qiang Lou;Jian Jiao;Denis X Charles",
        "site": "https://iclr.cc/virtual/2024/poster/18739",
        "summary": "Large language models (LLMs) have made impressive progress in natural language processing. These models rely on proper human instructions (or prompts) to generate suitable responses. However, the potential of LLMs are not fully harnessed by commonly-used prompting methods: many human-in-the-loop algorithms employ ad-hoc procedures for prompt selection; while auto prompt generation approaches are essentially searching all possible prompts randomly and inefficiently. We propose Evoke, an automatic prompt refinement framework. In Evoke, there are two instances of a same LLM: one as a reviewer (LLM-Reviewer), it scores the current prompt; the other as an author (LLM-Author), it edits the prompt by considering the edit history and the reviewer's feedback. Such an author-reviewer feedback loop ensures that the prompt is refined in each iteration. We further aggregate a data selection approach to Evoke, where only the hard samples are exposed to the LLM. The hard samples are more important because the LLM can develop deeper understanding of the tasks out of them, while the model may already know how to solve the easier cases. Experimental results show that Evoke significantly outperforms existing methods. For instance, in the challenging task of logical fallacy detection, Evoke scores above 80, while all other baseline methods struggle to reach 20.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the potential of LLMs are not fully harnessed by commonly-used prompting methods: many human-in-the-loop algorithms employ ad-hoc procedures for prompt selection; while auto prompt generation approaches are essentially searching all possible prompts randomly and inefficiently.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the potential of LLMs are not fully harnessed by commonly-used prompting methods: many human-in-the-loop algorithms employ ad-hoc procedures for prompt selection; while auto prompt generation approaches are essentially searching all possible prompts randomly and inefficiently.\""
    },
    {
        "title": "ALAM: Averaged Low-Precision Activation for Memory-Efficient Training of Transformer Models",
        "authors": "Sunghyeon Woo;Sunwoo Lee;Dongsuk Jeon",
        "site": "https://iclr.cc/virtual/2024/poster/18732",
        "summary": "Abstract:One of the key challenges in deep neural network training is the substantial amount of GPU memory required to store activations obtained in the forward pass. Various Activation-Compressed Training (ACT) schemes have been proposed to mitigate this issue; however, it is challenging to adopt those approaches in recent transformer-based large language models (LLMs), which experience significant performance drops when the activations are deeply compressed during training. In this paper, we introduce ALAM, a novel ACT framework that utilizes average quantization and a lightweight sensitivity calculation scheme, enabling large memory saving in LLMs while maintaining training performance. We first demonstrate that compressing activations into their group average values minimizes the gradient variance. Employing this property, we propose Average Quantization which provides high-quality deeply compressed activations with an effective precision of less than 1 bit and improved flexibility of precision allocation. In addition, we present a cost-effective yet accurate sensitivity calculation algorithm that solely relies on the L2 norm of parameter gradients, substantially reducing memory overhead due to sensitivity calculation. In experiments, the ALAM framework significantly reduces activation memory without compromising accuracy, achieving up to a 10$\\times$ compression rate in LLMs.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"however, it is challenging to adopt those approaches in recent transformer-based large language models (LLMs), which experience significant performance drops when the activations are deeply compressed during training.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"however, it is challenging to adopt those approaches in recent transformer-based large language models (LLMs), which experience significant performance drops when the activations are deeply compressed during training.\""
    },
    {
        "title": "DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models",
        "authors": "Licheng Wen;Daocheng Fu;Xin Li;Xinyu Cai;Tao MA;Pinlong Cai;Min Dou;Botian Shi;Liang He;Yu Qiao",
        "site": "https://iclr.cc/virtual/2024/poster/18729",
        "summary": "Recent advancements in autonomous driving have relied on data-driven approaches, which are widely adopted but face challenges including dataset bias, overfitting, and uninterpretability. Drawing inspiration from the knowledge-driven nature of human driving, we explore the question of how to instill similar capabilities into autonomous driving systems and summarize a paradigm that integrates an interactive environment, a driver agent, as well as a memory component to address this question. Leveraging large language models (LLMs) with emergent abilities, we propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. Extensive experiments prove DiLu's capability to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods.Moreover, DiLu is able to directly acquire experiences from real-world datasets which highlights its potential to be deployed on practical autonomous driving systems.To the best of our knowledge, we are the first to leverage knowledge-driven capability in decision-making for autonomous vehicles. Through the proposed DiLu framework, LLM is strengthened to apply knowledge and to reason causally in the autonomous driving domain.Project page: https://pjlab-adg.github.io/DiLu/",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Leveraging large language models (LLMs) with emergent abilities\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Leveraging large language models (LLMs) with emergent abilities\""
    },
    {
        "title": "CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding",
        "authors": "Junyan Li;Delin Chen;Yining Hong;Zhenfang Chen;Peihao Chen;Yikang Shen;Chuang Gan",
        "site": "https://iclr.cc/virtual/2024/poster/18715",
        "summary": "A remarkable ability of human beings resides in compositional reasoning, i.e., the capacity to make \"infinite use of finite means\". However, current large vision-language foundation models (VLMs)  fall short of such compositional abilities due to their ``bag-of-words\" behaviors and inability to construct words that correctly represent visual entities and the relations among the entities. To this end, we propose CoVLM, which can guide the LLM to explicitly compose visual entities and relationships among the text and dynamically communicate with the vision encoder and detection network to achieve vision-language communicative decoding. Specifically, we first devise a set of novel communication tokens for the LLM, for dynamic communication between the visual detection system and the language system. A communication token is generated by the LLM following a visual entity or a relation, to inform the detection network to propose regions that are relevant to the sentence generated so far. The proposed regions-of-interests (ROIs) are then fed back into the LLM for better language generation contingent on the relevant regions. The LLM is thus able to compose the visual entities and relationships through the communication tokens. The vision-to-language and language-to-vision communication are iteratively performed until the entire sentence is generated. Our framework seamlessly bridges the gap between visual perception and LLMs and outperforms previous VLMs by a large margin on compositional reasoning benchmarks (e.g., ~20% in HICO-DET mAP, ~14% in Cola top-1 accuracy, and ~3% on ARO top-1 accuracy). We also achieve state-of-the-art performances on traditional vision-language tasks such as referring expression comprehension and visual question answering.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"current large vision-language foundation models (VLMs) fall short of such compositional abilities due to their ``bag-of-words\" behaviors and inability to construct words that correctly represent visual entities and the relations among the entities.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"current large vision-language foundation models (VLMs) fall short of such compositional abilities due to their ``bag-of-words\" behaviors and inability to construct words that correctly represent visual entities and the relations among the entities.\""
    },
    {
        "title": "Reinforcement Symbolic Regression Machine",
        "authors": "Yilong Xu;Yang Liu;Hao Sun",
        "site": "https://iclr.cc/virtual/2024/poster/18713",
        "summary": "In nature, the behavior of many complex systems can be described by parsimonious math equations. Symbolic Regression (SR) is defined as the task of automatically distilling equations from limited data. Keen efforts have been placed on tackling this issue and demonstrated success in SR. However, there still exist bottlenecks that current methods struggle to break, when the expressions we need to explore tend toward infinity and especially when the underlying math formula is intricate. To this end, we propose a novel Reinforcement Symbolic Regression Machine (RSRM) that masters the capability of uncovering complex math equations from only scarce data. The RSRM model is composed of three key modules: (1) a Monte Carlo tree search (MCTS) agent, designed for exploration, that explores optimal math expression trees consisting of pre-defined math operators and variables, (2) a Double Q-learning block,  designed for exploitation, that helps reduce the feasible search space of MCTS via properly understanding the distribution of reward, and (3) a modulated sub-tree discovery block that heuristically learns and defines new math operators to improve representation ability of math expression trees. Binding of these modules yields the SOTA performance of RSRM in SR as demonstrated by multiple benchmark datasets. The RSRM shows clear superiority over several representative baseline models.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD Generalization",
        "authors": "Yuhang Zang;Hanlin Goh;Joshua M. Susskind;Chen Huang",
        "site": "https://iclr.cc/virtual/2024/poster/18711",
        "summary": "Existing vision-language models exhibit strong generalization on a variety of visual domains and tasks. However, such models mainly perform zero-shot recognition in a closed-set manner, and thus struggle to handle open-domain visual concepts by design. There are recent finetuning methods, such as prompt learning, that not only study the discrimination between in-distribution (ID) and out-of-distribution (OOD) samples, but also show some improvements in both ID and OOD accuracies. In this paper, we first demonstrate that vision-language models, after long enough finetuning but without proper regularization, tend to overfit the known classes in the given dataset, with degraded performance on unknown classes. Then we propose a novel approach OGEN to address this pitfall, with the main focus on improving the OOD GENeralization of finetuned models. Specifically, a class-conditional feature generator is introduced to synthesize OOD features using just the class name of any unknown class. Such synthesized features will provide useful knowledge about unknowns and help regularize the decision boundary between ID and OOD data when optimized jointly. Equally important is our adaptive self-distillation mechanism to regularize our feature generation model during joint optimization, i.e., adaptively transferring knowledge between model states to further prevent overfitting. Experiments validate that our method yields convincing gains in OOD generalization performance in different settings. Code: https://github.com/apple/ml-ogen.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, such models mainly perform zero-shot recognition in a closed-set manner, and thus struggle to handle open-domain visual concepts by design... vision-language models, after long enough finetuning but without proper regularization, tend to overfit the known classes in the given dataset, with degraded performance on unknown classes.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, such models mainly perform zero-shot recognition in a closed-set manner, and thus struggle to handle open-domain visual concepts by design... vision-language models, after long enough finetuning but without proper regularization, tend to overfit the known classes in the given dataset, with degraded performance on unknown classes.\""
    },
    {
        "title": "Decentralized Riemannian Conjugate Gradient Method on the Stiefel Manifold",
        "authors": "Jun Chen;Haishan Ye;Mengmeng Wang;Tianxin Huang;Guang Dai;Ivor Tsang;Yong Liu",
        "site": "https://iclr.cc/virtual/2024/poster/18706",
        "summary": "The conjugate gradient method is a crucial first-order optimization method that generally converges faster than the steepest descent method, and its computational cost is much lower than that of second-order methods. However, while various types of conjugate gradient methods have been studied in Euclidean spaces and on Riemannian manifolds, there is little study for those in distributed scenarios. This paper proposes a decentralized Riemannian conjugate gradient descent (DRCGD) method that aims at minimizing a global function over the Stiefel manifold. The optimization problem is distributed among a network of agents, where each agent is associated with a local function, and the communication between agents occurs over an undirected connected graph. Since the Stiefel manifold is a non-convex set, a global function is represented as a finite sum of possibly non-convex (but smooth) local functions. The proposed method is free from expensive Riemannian geometric operations such as retractions, exponential maps, and vector transports, thereby reducing the computational complexity required by each agent. To the best of our knowledge, DRCGD is the first decentralized Riemannian conjugate gradient algorithm to achieve global convergence over the Stiefel manifold.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity",
        "authors": "Robert Kirk;Ishita Mediratta;Christoforos Nalmpantis;Jelena Luketina;Eric Hambro;Edward Grefenstette;Roberta Raileanu",
        "site": "https://iclr.cc/virtual/2024/poster/18704",
        "summary": "Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI's ChatGPT or Anthropic's Claude. While there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e. supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties: out-of-distribution (OOD) generalisation and output diversity. OOD generalisation is crucial given the wide range of real-world scenarios in which these models are being used, while output diversity refers to the model's ability to generate varied outputs and is important for a variety of use cases. We perform our analysis across two base models on both summarisation and instruction following tasks, the latter being highly relevant for current LLM use cases. We find that RLHF generalises better than SFT to new inputs, particularly as the distribution shift between train and test becomes larger. However, RLHF significantly reduces output diversity compared to SFT across a variety of measures, implying a tradeoff in current LLM fine-tuning methods between generalisation and diversity. Our results provide guidance on which fine-tuning method should be used depending on the application, and show that more research is needed to improve the tradeoff between generalisation and diversity.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, RLHF significantly reduces output diversity compared to SFT across a variety of measures, implying a tradeoff in current LLM fine-tuning methods between generalisation and diversity.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, RLHF significantly reduces output diversity compared to SFT across a variety of measures, implying a tradeoff in current LLM fine-tuning methods between generalisation and diversity.\""
    },
    {
        "title": "SOHES: Self-supervised Open-world Hierarchical Entity Segmentation",
        "authors": "Shengcao Cao;Jiuxiang Gu;Jason Kuen;Hao Tan;Ruiyi Zhang;Handong Zhao;Ani Nenkova;Liangyan Gui;Tong Sun;Yu-Xiong Wang",
        "site": "https://iclr.cc/virtual/2024/poster/18703",
        "summary": "Open-world entity segmentation, as an emerging computer vision task, aims at segmenting entities in images without being restricted by pre-defined classes, offering impressive generalization capabilities on unseen images and concepts. Despite its promise, existing entity segmentation methods like Segment Anything Model (SAM) rely heavily on costly expert annotators. This work presents Self-supervised Open-world Hierarchical Entity Segmentation (SOHES), a novel approach that eliminates the need for human annotations. SOHES operates in three phases: self-exploration, self-instruction, and self-correction. Given a pre-trained self-supervised representation, we produce abundant high-quality pseudo-labels through visual feature clustering. Then, we train a segmentation model on the pseudo-labels, and rectify the noises in pseudo-labels via a teacher-student mutual-learning procedure. Beyond segmenting entities, SOHES also captures their constituent parts, providing a hierarchical understanding of visual entities. Using raw images as the sole training data, our method achieves unprecedented performance in self-supervised open-world segmentation, marking a significant milestone towards high-quality open-world entity segmentation in the absence of human-annotated masks. Project page: https://SOHES.github.io.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control",
        "authors": "Longtao Zheng;Rundong Wang;Xinrun Wang;Bo An",
        "site": "https://iclr.cc/virtual/2024/poster/18701",
        "summary": "Building agents with large language models (LLMs) for computer control is a burgeoning research area, where the agent receives computer states and performs actions to complete complex tasks. Previous computer agents have demonstrated the benefits of in-context learning (ICL); however, their performance is hindered by several issues. First, the limited context length of LLMs and complex computer states restrict the number of exemplars, as a single webpage can consume the entire context. Second, the exemplars in current methods, such as high-level plans and multi-choice questions, cannot represent complete trajectories, leading to suboptimal performance in long-horizon tasks. Third, existing computer agents rely on task-specific exemplars and overlook the similarity among tasks, resulting in poor generalization to novel tasks. To address these challenges, we introduce Synapse, a computer agent featuring three key components: i) state abstraction, which filters out task-irrelevant information from raw states, allowing more exemplars within the limited context, ii) trajectory-as-exemplar prompting, which prompts the LLM with complete trajectories of the abstracted states and actions to improve multi-step decision-making, and iii) exemplar memory, which stores the embeddings of exemplars and retrieves them via similarity search for generalization to novel tasks. We evaluate Synapse on MiniWoB++, a standard task suite, and Mind2Web, a real-world website benchmark. In MiniWoB++, Synapse achieves a 99.2% average success rate (a 10% relative improvement) across 64 tasks using demonstrations from only 48 tasks. Notably, Synapse is the first ICL method to solve the book-flight task in MiniWoB++. Synapse also exhibits a 56% relative improvement in average step success rate over the previous state-of-the-art prompting scheme in Mind2Web.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"First, the limited context length of LLMs and complex computer states restrict the number of exemplars, as a single webpage can consume the entire context. Second, the exemplars in current methods, such as high-level plans and multi-choice questions, cannot represent complete trajectories, leading to suboptimal performance in long-horizon tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"First, the limited context length of LLMs and complex computer states restrict the number of exemplars, as a single webpage can consume the entire context. Second, the exemplars in current methods, such as high-level plans and multi-choice questions, cannot represent complete trajectories, leading to suboptimal performance in long-horizon tasks.\""
    },
    {
        "title": "Pre-training with Synthetic Data Helps Offline Reinforcement Learning",
        "authors": "Zecheng Wang;Che Wang;Zixuan Dong;Keith W. Ross",
        "site": "https://iclr.cc/virtual/2024/poster/18700",
        "summary": "Recently, it has been shown that for offline deep reinforcement learning (DRL), pre-training Decision Transformer with a large language corpus can improve downstream performance (Reid et al., 2022). A natural question to ask is whether this performance gain can only be achieved with language pre-training, or can be achieved with simpler pre-training schemes which do not involve language. In this paper, we first show that language is not essential for improved performance, and indeed pre-training with synthetic IID data for a small number of updates can match the performance gains from pre-training with a large language corpus; moreover, pre-training with data generated by a one-step Markov chain can further improve the performance. Inspired by these experimental results, we then consider pre-training Conservative Q-Learning (CQL), a popular offline DRL algorithm, which is Q-learning-based and typically employs a Multi-Layer Perceptron (MLP) backbone. Surprisingly, pre-training with simple synthetic data for a small number of updates can also improve CQL, providing consistent performance improvement on D4RL Gym locomotion datasets. The results of this paper not only illustrate the importance of pre-training for offline DRL but also show that the pre-training data can be synthetic and generated with remarkably simple mechanisms.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Copilot4D: Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion",
        "authors": "Lunjun Zhang;Yuwen Xiong;Ze Yang;Sergio Casas;Rui Hu;Raquel Urtasun",
        "site": "https://iclr.cc/virtual/2024/poster/18691",
        "summary": "Learning world models can teach an agent how the world works in an unsupervised manner. Even though it can be viewed as a special case of sequence modeling, progress for scaling world models on robotic applications such as autonomous driving has been somewhat less rapid than scaling language models with Generative Pre-trained Transformers (GPT). We identify two reasons as major bottlenecks: dealing with complex and unstructured observation space, and having a scalable generative model. Consequently, we propose Copilot4D, a novel world modeling approach that first tokenizes sensor observations with VQVAE, then predicts the future via discrete diffusion. To efficiently decode and denoise tokens in parallel, we recast Masked Generative Image Transformer as discrete diffusion and enhance it with a few simple changes, resulting in notable improvement. When applied to learning world models on point cloud observations, Copilot4D reduces prior SOTA Chamfer distance by more than 65% for 1s prediction, and more than 50% for 3s prediction, across NuScenes, KITTI Odometry, and Argoverse2 datasets. Our results demonstrate that discrete diffusion on tokenized agent experience can unlock the power of GPT-like unsupervised learning for robotics.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Even though it can be viewed as a special case of sequence modeling, progress for scaling world models on robotic applications such as autonomous driving has been somewhat less rapid than scaling language models with Generative Pre-trained Transformers (GPT).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Even though it can be viewed as a special case of sequence modeling, progress for scaling world models on robotic applications such as autonomous driving has been somewhat less rapid than scaling language models with Generative Pre-trained Transformers (GPT).\""
    },
    {
        "title": "A Simple and Effective Pruning Approach for Large Language Models",
        "authors": "Mingjie Sun;Zhuang Liu;Anna Bair;J Zico Kolter",
        "site": "https://iclr.cc/virtual/2024/poster/18687",
        "summary": "As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive.\"\n\n(Note: The limitation mentioned is the high computational cost of existing pruning methods for large language models, but it is not the primary focus of the paper.)",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive.\"\n\n(Note: The limitation mentioned is the high computational cost of existing pruning methods for large language models, but it is not the primary focus of the paper.)"
    },
    {
        "title": "SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression",
        "authors": "Tim Dettmers;Ruslan A. Svirschevski;Vage Egiazarian;Denis Kuznedelev;Elias Frantar;Saleh Ashkboos;Alexander Borzunov;Torsten Hoefler;Dan Alistarh",
        "site": "https://iclr.cc/virtual/2024/poster/18686",
        "summary": "Abstract:Recent advances in large language model (LLM) pretraining have led to high-quality LLMs with impressive abilities. By compressing such LLMs via quantization to 3-4 bits per parameter, they can fit into memory-limited devices such as laptops and mobile phones, enabling personalized use. Quantizing models to 3-4 bits per parameter can lead to moderate to high accuracy losses, especially for smaller models (1-10B parameters), which are suitable for edge deployment. To address this accuracy issue, we introduce the Sparse-Quantized Representation (SpQR), a new compressed format and quantization technique that enables for the first time \\emph{near-lossless} compression of LLMs across model scales while reaching similar compression levels to previous methods. SpQR works by identifying and isolating \\emph{outlier weights}, which cause particularly large quantization errors, and storing them in higher precision while compressing all other weights to 3-4 bits, and achieves relative accuracy losses of less than $1\\%$ in perplexity for highly-accurate LLaMA and Falcon LLMs. This makes it possible to run a 33B parameter LLM on a single 24 GB consumer GPU without performance degradation at 15\\% speedup, thus making powerful LLMs available to consumers without any downsides. SpQR comes with efficient algorithms for both encoding weights into its format, as well as decoding them efficiently at runtime. Specifically, we provide an efficient GPU inference algorithm for SpQR, which yields faster inference than 16-bit baselines at similar accuracy while enabling memory compression gains of more than 4x.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Quantizing models to 3-4 bits per parameter can lead to moderate to high accuracy losses, especially for smaller models (1-10B parameters), which are suitable for edge deployment.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs (accuracy loss due to quantization), but it is not the primary focus of the paper. The main",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Quantizing models to 3-4 bits per parameter can lead to moderate to high accuracy losses, especially for smaller models (1-10B parameters), which are suitable for edge deployment.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs (accuracy loss due to quantization), but it is not the primary focus of the paper. The main"
    },
    {
        "title": "Lemur: Integrating Large Language Models in Automated Program Verification",
        "authors": "Haoze Wu;Clark Barrett;Nina Narodytska",
        "site": "https://iclr.cc/virtual/2024/poster/18684",
        "summary": "The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that demands high-level abstract reasoning about program properties that is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"a task that demands high-level abstract reasoning about program properties that is challenging for verification tools.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs in passing, but does not elaborate on it further. The focus of the paper is on proposing a methodology to combine the power of LLMs and automated reasoners for automated program verification.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"a task that demands high-level abstract reasoning about program properties that is challenging for verification tools.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs in passing, but does not elaborate on it further. The focus of the paper is on proposing a methodology to combine the power of LLMs and automated reasoners for automated program verification."
    },
    {
        "title": "OpenTab: Advancing Large Language Models as Open-domain Table Reasoners",
        "authors": "Kezhi Kong;Jiani Zhang;Zhengyuan Shen;Balasubramaniam Srinivasan;Chuan Lei;Christos Faloutsos;Huzefa Rangwala;George Karypis",
        "site": "https://iclr.cc/virtual/2024/poster/18676",
        "summary": "Large Language Models (LLMs) trained on large volumes of data excel at various natural language tasks, but they cannot handle tasks requiring knowledge that has not been trained on previously. One solution is to use a retriever that fetches relevant information to expand LLM's knowledge scope. However, existing textual-oriented retrieval-based LLMs are not ideal on structured table data due to diversified data modalities and large table sizes. In this work, we propose OpenTab, an open-domain table reasoning framework powered by LLMs. Overall, OpenTab leverages table retriever to fetch relevant tables and then generates SQL programs to parse the retrieved tables efficiently. Utilizing the intermediate data derived from the SQL executions, it conducts grounded inference to produce accurate response. Extensive experimental evaluation shows that OpenTab significantly outperforms baselines in both open- and closed-domain settings, achieving up to 21.5% higher accuracy. We further run ablation studies to validate the efficacy of our proposed designs of the system.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"but they cannot handle tasks requiring knowledge that has not been trained on previously.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"but they cannot handle tasks requiring knowledge that has not been trained on previously.\""
    },
    {
        "title": "Get more for less: Principled Data Selection for Warming Up Fine-Tuning in LLMs",
        "authors": "Feiyang Kang;Hoang Anh Just;Yifan Sun;Himanshu Jahagirdar;Yuanzhi Zhang;Rongxing Du;Anit Kumar Sahu;Ruoxi Jia",
        "site": "https://iclr.cc/virtual/2024/poster/18669",
        "summary": "This work focuses on leveraging and selecting from vast, unlabeled, open data topre-fine-tunea pre-trained language model. The goal is to minimize the need for costly domain-specific data for subsequent fine-tuning while achieving desired performance levels. While many data selection algorithms have been designed for small-scale applications, rendering them unsuitable for our context, some emerging methods do cater to language data scales. However, they often prioritize data that aligns with the target distribution. While this strategy may be effective when training a model from scratch, it can yield limited results when the model has already been pre-trained on a different distribution. Differing from prior work, our key idea is to select data that nudges the pre-training distribution closer to the target distribution. We show the optimality of this approach for fine-tuning tasks under certain conditions. We demonstrate the efficacy of our methodology across a diverse array of tasks (NLU, NLG, zero-shot) with models up to 2.7B, showing that it consistently surpasses other selection methods. Moreover, our proposed method is significantly faster than existing techniques, scaling to millions of samples within a single GPU hour. Our code is open-sourced. While fine-tuning offers significant potential for enhancing performance across diverse tasks, its associated costs often limit its widespread adoption; with this work, we hope to lay the groundwork for cost-effective fine-tuning, making its benefits more accessible.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While fine-tuning offers significant potential for enhancing performance across diverse tasks, its associated costs often limit its widespread adoption;\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While fine-tuning offers significant potential for enhancing performance across diverse tasks, its associated costs often limit its widespread adoption;\""
    },
    {
        "title": "Localizing and Editing Knowledge In Text-to-Image Generative Models",
        "authors": "Samyadeep Basu;Nanxuan Zhao;Vlad I Morariu;Soheil Feizi;Varun Manjunatha",
        "site": "https://iclr.cc/virtual/2024/poster/18667",
        "summary": "Text-to-Image Diffusion Models such as Stable-Diffusion and Imagen have achieved unprecedented quality of photorealism with state-of-the-art FID scores on MS-COCO and other generation benchmarks. Given a caption, image generation requires fine-grained knowledge about attributes such as object structure, style, and viewpoint amongst others. Where does this information reside in text-to-image generative models? In our paper, we tackle this question and understand how knowledge corresponding to distinct visual attributes is stored in large-scale text-to-image diffusion models. We adapt Causal Mediation Analysis for text-to-image models and trace knowledge about distinct visual attributes to various (causal) components in the (i) UNet and (ii) text-encoder of the diffusion model. In particular, we show that unlike large-language models, knowledge about different attributes is not localized in isolated components, but is instead distributed amongst a set of components in the conditional UNet. These sets of components are often distinct for different visual attributes (e.g.,  style} / objects).  Remarkably, we find that the text-encoder in public text-to-image models such as Stable-Diffusion contains {\\it only} one causal state across different visual attributes, and this is the first self-attention layer corresponding to the last subject token of the attribute in the caption. This is in stark contrast to the causal states in other language models which are often the mid-MLP layers.  Based on this observation of only one causal state in the text-encoder, we introduce a fast, data-free model editing method DiffQuickFix which can effectively edit concepts (remove or update knowledge) in text-to-image models. DiffQuickFix can edit (ablate) concepts in under a second with a closed-form update, providing a significant 1000x speedup and comparable editing performance to existing fine-tuning based editing methods.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Unlike large-language models, knowledge about different attributes is not localized in isolated components, but is instead distributed amongst a set of components in the conditional UNet.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Unlike large-language models, knowledge about different attributes is not localized in isolated components, but is instead distributed amongst a set of components in the conditional UNet.\""
    },
    {
        "title": "Learning Multi-Agent Communication from Graph Modeling Perspective",
        "authors": "Shengchao Hu;Li Shen;Ya Zhang;Dacheng Tao",
        "site": "https://iclr.cc/virtual/2024/poster/18666",
        "summary": "In numerous artificial intelligence applications, the collaborative efforts of multiple intelligent agents are imperative for the successful attainment of target objectives. To enhance coordination among these agents, a distributed communication framework is often employed. However, information sharing among all agents proves to be resource-intensive, while the adoption of a manually pre-defined communication architecture imposes limitations on inter-agent communication, thereby constraining the potential for collaborative efforts. In this study, we introduce a novel approach wherein we conceptualize the communication architecture among agents as a learnable graph. We formulate this problem as the task of determining the communication graph while enabling the architecture parameters to update normally, thus necessitating a bi-level optimization process. Utilizing continuous relaxation of the graph representation and incorporating attention units, our proposed approach, CommFormer, efficiently optimizes the communication graph and concurrently refines architectural parameters through gradient descent in an end-to-end manner. Extensive experiments on a variety of cooperative tasks substantiate the robustness of our model across diverse cooperative scenarios, where agents are able to develop more coordinated and sophisticated strategies regardless of changes in the number of agents.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs."
    },
    {
        "title": "It's Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition",
        "authors": "CHEN CHEN;Ruizhe Li;Yuchen Hu;Sabato Marco Siniscalchi;Pin-Yu Chen;EngSiong Chng;Chao-Han Huck Yang",
        "site": "https://iclr.cc/virtual/2024/poster/18665",
        "summary": "Recent studies have successfully shown that large language models (LLMs) can be successfully used for generative error correction (GER) on top of the automatic speech recognition (ASR) output. Specifically, an LLM is utilized to carry out a direct mapping from the N-best hypotheses list generated by an ASR system to the predicted output transcription. However, despite its effectiveness, GER introduces extra data uncertainty since the LLM is trained without taking into account acoustic information available in the speech signal. In this work, we aim to overcome such a limitation by infusing acoustic information before generating the predicted transcription through a novel late fusion solution termed Uncertainty-Aware Dynamic Fusion (UADF). UADF is a multimodal fusion approach implemented into an auto-regressive decoding process and works in two stages: (i) It first analyzes and calibrates the token-level LLM decision, and (ii) it then dynamically assimilates the information from the acoustic modality. Experimental evidence collected from various ASR tasks shows that UADF surpasses existing fusion mechanisms in several ways. It yields significant improvements in word error rate (WER) while mitigating data uncertainty issues in LLM and addressing the poor generalization relied with sole modality during fusion. We also demonstrate that UADF seamlessly adapts to audio-visual speech recognition.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, despite its effectiveness, GER introduces extra data uncertainty since the LLM is trained without taking into account acoustic information available in the speech signal.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, despite its effectiveness, GER introduces extra data uncertainty since the LLM is trained without taking into account acoustic information available in the speech signal.\""
    },
    {
        "title": "Is attention required for ICL? Exploring the Relationship Between Model Architecture and In-Context Learning Ability",
        "authors": "Ivan Lee;Nan Jiang;Taylor Berg-Kirkpatrick",
        "site": "https://iclr.cc/virtual/2024/poster/18661",
        "summary": "What is the relationship between model architecture and the ability to perform in-context learning? In this empirical study, we take the first steps toward answering this question. We evaluate thirteen model architectures capable of causal language modeling across a suite of synthetic in-context learning tasks. These selected architectures represent a broad range of paradigms, including recurrent and convolution-based neural networks, transformers, state space model inspired, and other emerging attention alternatives. We discover that all the considered architectures can perform in-context learning under a wider range of conditions than previously documented. Additionally, we observe stark differences in statistical efficiency and consistency by varying the number of in-context examples and task difficulty. We also measure each architecture's predisposition towards in-context learning when presented with the option to memorize rather than leverage in-context examples. Finally, and somewhat surprisingly, we find that several attention alternatives are sometimes competitive with or better in-context learners than transformers. However, no single architecture demonstrates consistency across all tasks, with performance either plateauing or declining when confronted with a significantly larger number of in-context examples than those encountered during gradient-based training.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, no single architecture demonstrates consistency across all tasks, with performance either plateauing or declining when confronted with a significantly larger number of in-context examples than those encountered during gradient-based training.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, no single architecture demonstrates consistency across all tasks, with performance either plateauing or declining when confronted with a significantly larger number of in-context examples than those encountered during gradient-based training.\""
    },
    {
        "title": "MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use",
        "authors": "Yue Huang;Jiawen Shi;Yuan Li;Chenrui Fan;Siyuan Wu;Qihui Zhang;Yixin Liu;Pan Zhou;Yao Wan;Neil Zhenqiang Gong;Lichao Sun",
        "site": "https://iclr.cc/virtual/2024/poster/18657",
        "summary": "Large language models (LLMs) have garnered significant attention due to their impressive natural language processing (NLP) capabilities. Recently, many studies have focused on the tool utilization ability of LLMs. They primarily investigated how LLMs effectively collaborate with given specific tools. However, in scenarios where LLMs serve as intelligent agents, as seen in applications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate decision-making processes that involve deciding whether to employ a tool and selecting the most suitable tool(s) from a collection of available tools to fulfill user requests. Therefore, in this paper, we introduce MetaTool, a benchmark designed to evaluate whether LLMs have tool usage awareness and can correctly choose tools. Specifically, we create a dataset called ToolE within the benchmark. This dataset contains various types of user queries in the form of prompts that trigger LLMs to use tools, including both single-tool and multi-tool scenarios. Subsequently, we set the tasks for both tool usage awareness and tool selection. We define four subtasks from different perspectives in tool selection, including tool selection with similar choices, tool selection in specific scenarios, tool selection with possible reliability issues, and multi-tool selection. We conduct experiments involving eight popular LLMs and find that the majority of them still struggle to effectively select tools, highlighting the existing gaps between LLMs and genuine intelligent agents. However, through the error analysis, we found there is still significant room for improvement. Finally, we conclude with insights for tool developers -- we strongly recommend that tool developers choose an appropriate rewrite model for generating new descriptions based on the downstream LLM the tool will apply to.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, through the error analysis, we found there is still significant room for improvement.\" and \"we conduct experiments involving eight popular LLMs and find that the majority of them still struggle to effectively select tools, highlighting the existing gaps between LLMs and genuine intelligent agents.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, through the error analysis, we found there is still significant room for improvement.\" and \"we conduct experiments involving eight popular LLMs and find that the majority of them still struggle to effectively select tools, highlighting the existing gaps between LLMs and genuine intelligent agents.\""
    },
    {
        "title": "Don't Judge by the Look: Towards Motion Coherent Video Representation",
        "authors": "Yitian Zhang;Yue Bai;Huan Wang;Yizhou Wang;Yun Fu",
        "site": "https://iclr.cc/virtual/2024/poster/18651",
        "summary": "Current training pipelines in object recognition neglect Hue Jittering when doing data augmentation as it not only brings appearance changes that are detrimental to classification, but also the implementation is inefficient in practice. In this study, we investigate the effect of hue variance in the context of video understanding and find this variance to be beneficial since static appearances are less important in videos that contain motion information. Based on this observation, we propose a data augmentation method for video understanding, named Motion Coherent Augmentation (MCA), that introduces appearance variation in videos and implicitly encourages the model to prioritize motion patterns, rather than static appearances. Concretely, we propose an operation SwapMix to efficiently modify the appearance of video samples, and introduce Variation Alignment (VA) to resolve the distribution shift caused by SwapMix, enforcing the model to learn appearance invariant representations. Comprehensive empirical evaluation across various architectures and different datasets solidly validates the effectiveness and generalization ability of MCA, and the application of VA in other augmentation methods. Code is available at https://github.com/BeSpontaneous/MCA-pytorch.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
        "authors": "Melanie Sclar;Yejin Choi;Yulia Tsvetkov;Alane Suhr",
        "site": "https://iclr.cc/virtual/2024/poster/18650",
        "summary": "As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FormatSpread, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B.\""
    },
    {
        "title": "Offline Data Enhanced On-Policy Policy Gradient with Provable Guarantees",
        "authors": "Yifei Zhou;Ayush Sekhari;Yuda Song;Wen Sun",
        "site": "https://iclr.cc/virtual/2024/poster/18646",
        "summary": "Hybrid RL is the setting where an RL agent has access to both offline data and online data by interacting with the real-world environment. In this work, we propose a new hybrid RL algorithm that combines an on-policy actor-critic method with offline data. On-policy methods such as policy gradient and natural policy gradient (NPG) have shown to be more robust to model misspecification, though sometimes it may not be as sample efficient as methods that rely on off-policy learning. On the other hand, offline methods that depend on off-policy training often require strong assumptions in theory and are less stable to train in practice. Our new approach integrates a procedure of off-policy training on the offline data into an on-policy NPG framework. We show that our approach, in theory, can obtain abest-of-both-worldstype of result --- it achieves the state-of-art theoretical guarantees of offline RL when offline RL-specific assumptions hold, while at the same time maintaining the theoretical guarantees of on-policy NPG regardless of the offline RL assumptions' validity. Experimentally, in challenging rich-observation environments, we show that our approach outperforms a state-of-the-art hybrid RL baseline which only relies on off-policy policy optimization, demonstrating the empirical benefit of combining on-policy and off-policy learning.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "TAIL: Task-specific Adapters for Imitation Learning with Large Pretrained Models",
        "authors": "Zuxin Liu;Jesse Zhang;Kavosh Asadi;Yao Liu;Ding Zhao;Shoham Sabach;Rasool Fakoor",
        "site": "https://iclr.cc/virtual/2024/poster/18642",
        "summary": "The full potential of large pretrained models remains largely untapped in control domains like robotics. This is mainly because of the scarcity of data and the computational challenges associated with training or fine-tuning these large models for such applications. Prior work mainly emphasizes either effective \\emph{pretraining} of large models for decision-making or single-task adaptation. But real-world problems will require data-efficient, \\emph{continual adaptation} for new control tasks. Recognizing these constraints, we introduce TAIL (Task-specific Adapters for Imitation Learning), a framework for efficient adaptation to new control tasks. Inspired by recent advancements in parameter-efficient fine-tuning in language domains, we explore efficient fine-tuning techniques---e.g., Bottleneck Adapters, P-Tuning, and Low-Rank Adaptation (LoRA)---in TAIL to adapt large pretrained models for new tasks with limited demonstration data. Our extensive experiments comparing prevalent parameter-efficient fine-tuning techniques and adaptation baselines suggest that TAIL with LoRA can achieve the best post-adaptation performance with only 1\\% of the trainable parameters of full fine-tuning, while avoiding catastrophic forgetting and preserving adaptation plasticity in continual learning settings.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This is mainly because of the scarcity of data and the computational challenges associated with training or fine-tuning these large models for such applications.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"This is mainly because of the scarcity of data and the computational challenges associated with training or fine-tuning these large models for such applications.\""
    },
    {
        "title": "Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning",
        "authors": "Xiaoxin He;Xavier Bresson;Thomas Laurent;Adam Perold;Yann LeCun;Bryan Hooi",
        "site": "https://iclr.cc/virtual/2024/poster/18640",
        "summary": "Representation learning on text-attributed graphs (TAGs) has become a critical research problem in recent years. A typical example of a TAG is a paper citation graph, where the text of each paper serves as node attributes. Initial graph neural network (GNN) pipelines handled these text attributes by transforming them into shallow or hand-crafted features, such as skip-gram or bag-of-words features. Recent efforts have focused on enhancing these pipelines with language models (LMs), which typically demand intricate designs and substantial computational resources. With the advent of powerful large language models (LLMs) such as GPT or Llama2, which demonstrate an ability to reason and to utilize general knowledge, there is a growing need for techniques which combine the textual modelling abilities of LLMs with the structural learning capabilities of GNNs. Hence, in this work, we focus on leveraging LLMs to capture textual information as features, which can be used to boost GNN performance on downstream tasks. A key innovation is our use of \\emph{explanations as features}: we prompt an LLM to perform zero-shot classification, request textual explanations for its decision-making process, and design an \\emph{LLM-to-LM interpreter} to translate these explanations into informative features for downstream GNNs. Our experiments demonstrate that our method achieves state-of-the-art results on well-established TAG datasets, including \\texttt{Cora}, \\texttt{PubMed}, \\texttt{ogbn-arxiv}, as well as our newly introduced dataset, \\texttt{tape-arxiv23}. Furthermore, our method significantly speeds up training, achieving a 2.88 times improvement over the closest baseline on \\texttt{ogbn-arxiv}. Lastly, we believe the versatility of the proposed method extends beyond TAGs and holds the potential to enhance other tasks involving graph-text data~\\footnote{Our codes and datasets are available at: \\url{https://github.com/XiaoxinHe/TAPE}}.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent efforts have focused on enhancing these pipelines with language models (LMs), which typically demand intricate designs and substantial computational resources.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Recent efforts have focused on enhancing these pipelines with language models (LMs), which typically demand intricate designs and substantial computational resources.\""
    },
    {
        "title": "Tree Search-Based Policy Optimization under Stochastic Execution Delay",
        "authors": "David Valensi;Esther Derman;Shie Mannor;Gal Dalal",
        "site": "https://iclr.cc/virtual/2024/poster/18638",
        "summary": "The standard formulation of Markov decision processes (MDPs) assumes that the agent's decisions are executed immediately.However, in numerous realistic applications such as robotics or healthcare, actions are performed with a delay whose value can even be stochastic. In this work, we introduce stochastic delayed execution MDPs, a new formalism addressing random delays without resorting to state augmentation. We show that given observed delay values, it is sufficient to perform a policy search in the class of Markov policies in order to reach optimal performance, thus extending the deterministic fixed delay case. Armed with this insight, we devise DEZ, a model-based algorithm that optimizes over the class of Markov policies. DEZ leverages Monte-Carlo tree search similar to its non-delayed variant EfficientZero to accurately infer future states from the action queue. Thus, it handles delayed execution while preserving the sample efficiency of EfficientZero. Through empirical analysis, we stress that none of the prior benchmarks consistently outperforms others across different delays. We demonstrate that our algorithm surpasses all benchmark methods in Atari games when dealing with constant or stochastic delays. The code is available at \\url{https://github.com/davidva1/Delayed-EZ}.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "BrainLM: A foundation model for brain activity recordings",
        "authors": "Josue Ortega Caro;Antonio Henrique de Oliveira Fonseca;Syed A Rizvi;Matteo Rosati;Christopher Averill;James L Cross;Prateek Mittal;Emanuele Zappala;Rahul Madhav Dhodapkar;Chadi Abdallah;David van Dijk",
        "site": "https://iclr.cc/virtual/2024/poster/18625",
        "summary": "We introduce the Brain Language Model (BrainLM), a foundation model for brain activity dynamics trained on 6,700 hours of fMRI recordings. Utilizing self-supervised masked-prediction training, BrainLM demonstrates proficiency in both fine-tuning and zero-shot inference tasks. Fine-tuning allows for the accurate prediction of clinical variables like age, anxiety, and PTSD as well as forecasting of future brain states. Critically, the model generalizes well to entirely new external cohorts not seen during training. In zero-shot inference mode, BrainLM can identify intrinsic functional networks directly from raw fMRI data without any network-based supervision during training. The model also generates interpretable latent representations that reveal relationships between brain activity patterns and cognitive states. Overall, BrainLM offers a versatile and interpretable framework for elucidating the complex spatiotemporal dynamics of human brain activity. It serves as a powerful \"lens\" through which massive repositories of fMRI data can be analyzed in new ways, enabling more effective interpretation and utilization at scale. The work demonstrates the potential of foundation models to advance computational neuroscience research.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "A Stable, Fast, and Fully Automatic Learning Algorithm for Predictive Coding Networks",
        "authors": "Tommaso Salvatori;Yuhang Song;Yordan Yordanov;Beren Millidge;Lei Sha;Cornelius Emde;Zhenghua Xu;Rafal Bogacz;Thomas Lukasiewicz",
        "site": "https://iclr.cc/virtual/2024/poster/18624",
        "summary": "Predictive coding networks are neuroscience-inspired models with roots in both Bayesian statistics and neuroscience. Training such models, however, is quite inefficient and unstable. In this work, we show how by simply changing the temporal scheduling of the update rule for the synaptic weights leads to an algorithm that is much more efficient and stable than the original one, and has theoretical guarantees in terms of convergence. The proposed algorithm, that we call incremental predictive coding (iPC) is also more biologically plausible than the original one, as it it fully automatic. In an extensive set of experiments, we show that iPC constantly performs better than the original formulation on a large number of benchmarks for image classification, as well as for the training of both conditional and masked language models, in terms of test accuracy, efficiency, and convergence with respect to a large set of hyperparameters.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "SmartPlay : A Benchmark for LLMs as Intelligent Agents",
        "authors": "Yue Wu;Xuan Tang;Tom Mitchell;Yuanzhi Li",
        "site": "https://iclr.cc/virtual/2024/poster/18620",
        "summary": "Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately.SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as a road-map for identifying gaps in current methodologies. We release our benchmark at https://github.com/microsoft/SmartPlay",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as a road-map for identifying gaps in current methodologies.\"\n\nNote: The abstract mentions the purpose of the SmartPlay benchmark, which includes identifying gaps in current methodologies, implying that there are limitations in current LLMs, but it does not",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as a road-map for identifying gaps in current methodologies.\"\n\nNote: The abstract mentions the purpose of the SmartPlay benchmark, which includes identifying gaps in current methodologies, implying that there are limitations in current LLMs, but it does not"
    },
    {
        "title": "THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS",
        "authors": "Junchi Yu;Ran He;Zhitao Ying",
        "site": "https://iclr.cc/virtual/2024/poster/18613",
        "summary": "Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi-step reasoning, since they prompt LLMs to reason \\textit{from scratch}.To address these issues, we propose \\textbf{\\textit{Thought Propagation} (TP)}, which explores the analogous problems and leverages their solutions to enhance the complex reasoning ability of LLMs.These analogous problems are related to the input one, with reusable solutions and problem-solving strategies.Thus, it is promising to propagate insights of solving previous analogous problems to inspire new problem-solving. To achieve this, TP first prompts LLMs to propose and solve a set of analogous problems that are related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch.TP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering. Experiments across three challenging tasks demonstrate TP enjoys a substantial improvement over the baselines by an average of 12\\% absolute increase in finding the optimal solutions in Shortest-path Reasoning, 13\\% improvement of human preference in Creative Writing, and 15\\% enhancement in the task completion rate of LLM-Agent Planning.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing prompting approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi-step reasoning, since they prompt LLMs to reason \\textit{from scratch}.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, existing prompting approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi-step reasoning, since they prompt LLMs to reason \\textit{from scratch}.\""
    },
    {
        "title": "The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Language Models",
        "authors": "Yan Liu;Yu Liu;Xiaokang Chen;Pin-Yu Chen;Daoguang Zan;Min-Yen Kan;Tsung-Yi Ho",
        "site": "https://iclr.cc/virtual/2024/poster/18605",
        "summary": "Abstract:Pre-trained Language models (PLMs) have been acknowledged to contain harmful information, such as social biases, which may cause negative social impacts or even bring catastrophic results in application. Previous works on this problem mainly focused on using black-box methods such as probing to detect and quantify social biases in PLMs by observing model outputs. As a result, previous debiasing methods mainly finetune or even pre-train PLMs on newly constructed anti-stereotypical datasets, which are high-cost. In this work, we try to unveil the mystery of social bias inside language models by introducing the concept of {\\sc Social Bias Neurons}. Specifically, we propose {\\sc Integrated Gap Gradients (IG$^2$)} to accurately pinpoint units (i.e., neurons) in a language model that can be attributed to undesirable behavior, such as social bias.  By formalizing undesirable behavior as a distributional property of language, we employ sentiment-bearing prompts to elicit classes of sensitive words (demographics) correlated with such sentiments. Our IG$^2$ thus attributes the uneven distribution for different demographics to specific Social Bias Neurons, which track the trail of unwanted behavior inside PLM units to achieve interoperability. Moreover, derived from our interpretable technique, {\\sc Bias Neuron Suppression (BNS)} is further proposed to mitigate social biases. By studying BERT, RoBERTa, and their attributable differences from debiased FairBERTa, IG$^2$ allows us to locate and suppress identified neurons, and further mitigate undesired behaviors. As measured by prior metrics from StereoSet, our model achieves a higher degree of fairness while maintaining language modeling ability with low cost\\footnote{This work contains examples that potentially implicate stereotypes, associations, and other harms that could be offensive to individuals in certain social groups.}.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Pre-trained Language models (PLMs) have been acknowledged to contain harmful information, such as social biases, which may cause negative social impacts or even bring catastrophic results in application.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Pre-trained Language models (PLMs) have been acknowledged to contain harmful information, such as social biases, which may cause negative social impacts or even bring catastrophic results in application.\""
    },
    {
        "title": "OWL: A Large Language Model for IT Operations",
        "authors": "Hongcheng Guo;Jian Yang;Jiaheng Liu;Liqun Yang;Linzheng Chai;Jiaqi Bai;Junran Peng;Xiaorong Hu;Chao Chen;Dongfeng Zhang;xu Shi;Tieqiao Zheng;liangfan zheng;Bo Zhang;Ke Xu;Zhoujun Li",
        "site": "https://iclr.cc/virtual/2024/poster/18599",
        "summary": "With the rapid advancement of IT operations, managing and analyzing large data volumes efficiently for practical applications has become increasingly critical. Natural Language Processing (NLP) techniques have demonstrated remarkable capabilities in various tasks, including named entity recognition, machine translation, and dialogue systems. Recently, Large Language Models (LLMs) have achieved significant improvements across various domain-specific areas. However, there is a noticeable gap in the development of specialized Large Language Models (LLMs) tailored for IT operations. In this paper, we introduce the OWL, a large language model trained on our constructed Owl-Instruct with a wide range of IT-related information. Specifically, limited by the maximum input length, we propose the \\textbf{H}omogeneous \\textbf{M}arkov \\textbf{C}ontext \\textbf{E}xtension method (HMCE). The mixture-of-adapter strategy is leveraged to improve the parameter-efficient tuning across different domains or tasks.Further, we evaluate the performance of OWL on the Owl-Bench established by us and open IT-related benchmarks. OWL  demonstrates superior performance results on IT tasks, which outperforms existing models by significant margins. Moreover, we hope that the findings of our work will provide more insights to revolutionize the techniques of IT operations with specialized LLMs.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, there is a noticeable gap in the development of specialized Large Language Models (LLMs) tailored for IT operations.\"\n\nThis paper mentions a limitation of LLMs in the sense that there is a gap in the development of specialized LLMs for IT operations, but it does not elaborate on this limitation and instead focuses on introducing a new LLM tailored for IT operations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, there is a noticeable gap in the development of specialized Large Language Models (LLMs) tailored for IT operations.\"\n\nThis paper mentions a limitation of LLMs in the sense that there is a gap in the development of specialized LLMs for IT operations, but it does not elaborate on this limitation and instead focuses on introducing a new LLM tailored for IT operations."
    },
    {
        "title": "OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views",
        "authors": "Francis Engelmann;Fabian Manhardt;Michael Niemeyer;Keisuke Tateno;Federico Tombari",
        "site": "https://iclr.cc/virtual/2024/poster/18594",
        "summary": "Large visual-language models (VLMs), like CLIP, enable open-set image segmentation to segment arbitrary concepts from an image in a zero-shot manner. This goes beyond the traditional closed-set assumption, i.e., where models can only segment classes from a pre-defined training set. More recently, first works on open-set segmentation in 3D scenes have appeared in the literature. These methods are heavily influenced by closed-set 3D convolutional approaches that process point clouds or polygon meshes. However, these 3D scene representations do not align well with the image-based nature of the visual-language models. Indeed, point cloud and 3D meshes typically have a lower resolution than images and the reconstructed 3D scene geometry might not project well to the underlying 2D image sequences used to compute pixel-aligned CLIP features. To address these challenges, we propose OpenNeRF which naturally operates on posed images and directly encodes the VLM features within the NeRF. This is similar in spirit to LERF, however our work shows that using pixel-wise VLM features (instead of global CLIP features) results in an overall less complex architecture without the need for additional DINO regularization. Our OpenNeRF further leverages NeRF’s ability to render novel views and extract open-set VLM features from areas that are not well observed in the initial posed images. For 3D point cloud segmentation on the Replica dataset, OpenNeRF outperforms recent open-vocabulary methods such as LERF and OpenScene by at least +4.9 mIoU.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Indeed, point cloud and 3D meshes typically have a lower resolution than images and the reconstructed 3D scene geometry might not project well to the underlying 2D image sequences used to compute pixel-aligned CLIP features.\"\n\nThis paper mentions a limitation of visual-language models (VLMs) related to their compatibility with 3D scene representations, but does not explore it in",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Indeed, point cloud and 3D meshes typically have a lower resolution than images and the reconstructed 3D scene geometry might not project well to the underlying 2D image sequences used to compute pixel-aligned CLIP features.\"\n\nThis paper mentions a limitation of visual-language models (VLMs) related to their compatibility with 3D scene representations, but does not explore it in"
    },
    {
        "title": "IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models",
        "authors": "Shaokun Zhang;Xiaobo Xia;Zhaoqing Wang;Ling-Hao Chen;Jiale Liu;Qingyun Wu;Tongliang Liu",
        "site": "https://iclr.cc/virtual/2024/poster/18589",
        "summary": "In-context learning is a promising paradigm that utilizes in-context examples as prompts for the predictions of large language models. These prompts are crucial for achieving strong performance. However, since the prompts need to be sampled from a large volume of annotated examples, finding the right prompt may result in high annotation costs. To address this challenge, this paper introduces an influence-driven selective annotation method that aims to minimize annotation costs while improving the quality of in-context examples. The essence of our method is to select a pivotal subset from a large-scale unlabeled data pool to annotate for the subsequent sampling of prompts. Specifically, a directed graph is first constructed to represent unlabeled data. Afterward, the influence of candidate unlabeled subsets is quantified with a diffusion process. A simple yet effective greedy algorithm for unlabeled data selection is lastly introduced. It iteratively selects the data if it provides a maximum marginal gain with respect to quantified influence. Compared with previous efforts on selective annotations, our influence-driven method works in an end-to-end manner, avoids an intractable explicit balance between data diversity and representativeness, and enjoys theoretical support. Experiments confirm the superiority of the proposed method on various benchmarks, achieving better performance under lower time consumption during subset selection.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"finding the right prompt may result in high annotation costs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"finding the right prompt may result in high annotation costs.\""
    },
    {
        "title": "Provable Robust Watermarking for AI-Generated Text",
        "authors": "Xuandong Zhao;Prabhanjan Vijendra Ananth;Lei Li;Yu-Xiang Wang",
        "site": "https://iclr.cc/virtual/2024/poster/18588",
        "summary": "We study the problem of watermarking large language models (LLMs) generated text — one of the most promising approaches for addressing the safety challenges of LLM usage. In this paper, we propose a rigorous theoretical framework to quantify the effectiveness and robustness of LLM watermarks. We propose a robust and high-quality watermark method, Unigram-Watermark, by extending an existing approach with a simplified fixed grouping strategy. We prove that our watermark method enjoys guaranteed generation quality, correctness in watermark detection, and is robust against text editing and paraphrasing. Experiments on three varying LLMs and two datasets verify that our Unigram-Watermark achieves superior detection accuracy and comparable generation quality in perplexity, thus promoting the responsible use of LLMs.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"one of the most promising approaches for addressing the safety challenges of LLM usage.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"one of the most promising approaches for addressing the safety challenges of LLM usage.\""
    },
    {
        "title": "CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing",
        "authors": "Zhibin Gou;Zhihong Shao;Yeyun Gong;yelong shen;Yujiu Yang;Nan Duan;Weizhu Chen",
        "site": "https://iclr.cc/virtual/2024/poster/18586",
        "summary": "Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially “black boxes” to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxicity reduction demonstrate that CRITIC consistently enhances the performance of LLMs. Meanwhile, our research highlights the crucial importance of external feedback in promoting the ongoing self-improvement of LLMs.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content.\""
    },
    {
        "title": "TiC-CLIP: Continual Training of CLIP Models",
        "authors": "Saurabh Garg;Mehrdad Farajtabar;Hadi Pouransari;Raviteja Vemulapalli;Sachin Mehta;Oncel Tuzel;Vaishaal Shankar;Fartash Faghri",
        "site": "https://iclr.cc/virtual/2024/poster/18576",
        "summary": "Abstract:Keeping large foundation models up to date on latest data is inherently expensive. To avoid the prohibitive costs of constantly retraining, it is imperative to continually train these models. This problem is exacerbated by the lack of any large scale continual learning benchmarks or baselines. We introduce the first set of web-scale Time-Continual (TiC) benchmarks for training vision-language models: TiC-DataComp, TiC-YFCC, and TiC-Redcaps. TiC-DataComp, our largest dataset, contains over 12.7B timestamped image-text pairs spanning 9 years (2014-2022). We first use our benchmarks to curate various dynamic evaluations to measure temporal robustness of existing models. We show OpenAI's CLIP (trained on data up to 2020) loses $\\approx 8\\%$ zero-shot accuracy on our curated retrieval task from 2021-2022 compared with more recently trained models in OpenCLIP repository. We then study how to efficiently train models on time-continuous data. We demonstrate that a simple rehearsal-based approach that continues training from the last checkpoint  and replays old data reduces compute by $2.5\\times$ when compared to the standard practice of retraining from scratch. Code is available at https://github.com/apple/ml-tic-clip.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Keeping large foundation models up to date on latest data is inherently expensive. To avoid the prohibitive costs of constantly retraining, it is imperative to continually train these models.... We show OpenAI's CLIP (trained on data up to 2020) loses $\\approx 8\\%$ zero-shot accuracy on our curated retrieval task from 2021-2022 compared",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Keeping large foundation models up to date on latest data is inherently expensive. To avoid the prohibitive costs of constantly retraining, it is imperative to continually train these models.... We show OpenAI's CLIP (trained on data up to 2020) loses $\\approx 8\\%$ zero-shot accuracy on our curated retrieval task from 2021-2022 compared"
    },
    {
        "title": "Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles",
        "authors": "Zhiwei Tang;Dmitry Rybin;Tsung-Hui Chang",
        "site": "https://iclr.cc/virtual/2024/poster/18570",
        "summary": "In this study, we delve into an emerging optimization challenge involving a black-box objective function that can only be gauged via a ranking oracle—a situation frequently encountered in real-world scenarios, especially when the function is evaluated by human judges. A prominent instance of such a situation is Reinforcement Learning with Human Feedback (RLHF), an approach recently employed to enhance the performance of Large Language Models (LLMs) using human guidance [Ouyang et al. 2022, Liu et al. 2023, OpenAI et al. 2022, Bai et al. 2022]. We introduce ZO-RankSGD, an innovative zeroth-order optimization algorithm designed to tackle this optimization problem, accompanied by theoretical assurances. Our algorithm utilizes a novel rank-based random estimator to determine the descent direction and guarantees convergence to a stationary point. Moreover, ZO-RankSGD is readily applicable to policy optimization problems in Reinforcement Learning (RL), particularly when only ranking oracles for the episode reward are available. Last but not least, we demonstrate the effectiveness of ZO-RankSGD in a novel application: improving the quality of images generated by a diffusion generative model with human ranking feedback. Throughout experiments, we found that ZO-RankSGD can significantly enhance the detail of generated images with only a few rounds of human feedback. Overall, our work advances the field of zeroth-order optimization by addressing the problem of optimizing functions with only ranking feedback, and offers a new and effective approach for aligning Artificial Intelligence (AI) with human intentions.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Reinforcement Learning with Human Feedback (RLHF), an approach recently employed to enhance the performance of Large Language Models (LLMs) using human guidance\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Reinforcement Learning with Human Feedback (RLHF), an approach recently employed to enhance the performance of Large Language Models (LLMs) using human guidance\""
    },
    {
        "title": "Enhancing Human-AI Collaboration Through Logic-Guided Reasoning",
        "authors": "Chengzhi Cao;Yinghao Fu;Sheng Xu;Ruimao Zhang;Shuang Li",
        "site": "https://iclr.cc/virtual/2024/poster/18568",
        "summary": "We present a systematic framework designed to enhance human-robot perception and collaboration through the integration of logical rules and Theory of Mind (ToM). Logical rules provide interpretable predictions and generalize well across diverse tasks, making them valuable for learning and decision-making. Leveraging the ToM for understanding others' mental states, our approach facilitates effective collaboration. In this paper, we employ logic rules derived from observational data to infer human goals and guide human-like agents. These rules are treated as latent variables, and a rule encoder is trained alongside a multi-agent system in the robot's mind. We assess the posterior distribution of latent rules using learned embeddings, representing entities and relations. Confidence scores for each rule indicate their consistency with observed data. Then, we employ a hierarchical reinforcement learning model with ToM to plan robot actions for assisting humans. Extensive experiments validate each component of our framework, and results on multiple benchmarks demonstrate that our model outperforms the majority of existing approaches.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models",
        "authors": "Yung-Sung Chuang;Yujia Xie;Hongyin Luo;Yoon Kim;James R. Glass;Pengcheng He",
        "site": "https://iclr.cc/virtual/2024/poster/18563",
        "summary": "Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that thisDecoding by ContrastingLayers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining.\""
    },
    {
        "title": "NOLA: Compressing LoRA using Linear Combination of Random Basis",
        "authors": "Soroush Abbasi Koohpayegani;Navaneet K L;Parsa Nooralinejad;Soheil Kolouri;Hamed Pirsiavash",
        "site": "https://iclr.cc/virtual/2024/poster/18556",
        "summary": "Fine-tuning Large Language Models (LLMs) and storing them for each downstream task or domain is impractical because of the massive model size (e.g., 350GB in GPT-3).Current literature, such as LoRA, showcases the potential of low-rank modifications to the original weights of an LLM, enabling efficient adaptation and storage for task-specific models. These methods can reduce the number of parameters needed to fine-tune an LLM by several orders of magnitude. Yet, these methods face two primary limitations: (1) the parameter count is lower-bounded by the rank one decomposition, and (2) the extent of reduction is heavily influenced by both the model architecture and the chosen rank. We introduce NOLA, which overcomes the rank one lower bound present in LoRA. It achieves this by re-parameterizing the low-rank matrices in LoRA using linear combinations of randomly generated matrices (basis) and optimizing the linear mixture coefficients only. This approach allows us to decouple the number of trainable parameters from both the choice of rank and the network architecture. We present adaptation results using GPT-2, LLaMA-2, and ViT in natural language and computer vision tasks. NOLA performs as well as LoRA models with much fewer number of parameters compared to LoRA with rank one, the best compression LoRA can archive. Particularly, on LLaMA-2 70B, our method is almost 20 times more compact than the most compressed LoRA without degradation in accuracy. Our code is available here: https://github.com/UCDvision/NOLA",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Yet, these methods face two primary limitations: (1) the parameter count is lower-bounded by the rank one decomposition, and (2) the extent of reduction is heavily influenced by both the model architecture and the chosen rank.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Yet, these methods face two primary limitations: (1) the parameter count is lower-bounded by the rank one decomposition, and (2) the extent of reduction is heavily influenced by both the model architecture and the chosen rank.\""
    },
    {
        "title": "Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models",
        "authors": "Yin Fang;Xiaozhuan Liang;Ningyu Zhang;Kangwei Liu;Rui Huang;Zhuo Chen;Xiaohui Fan;Huajun Chen",
        "site": "https://iclr.cc/virtual/2024/poster/18554",
        "summary": "Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a comprehensive instruction dataset designed for the biomolecular domain. Mol-Instructions encompasses three key components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions. Each component aims to improve the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on LLMs, we demonstrate the effectiveness of Mol-Instructions in enhancing large models' performance in the intricate realm of biomolecular studies, thus fostering progress in the biomolecular research community. Mol-Instructions is publicly available for ongoing research and will undergo regular updates to enhance its applicability (https://github.com/zjunlp/Mol-Instructions).",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, their proficiency within specialized domains such as biomolecular studies remains limited.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, their proficiency within specialized domains such as biomolecular studies remains limited.\""
    },
    {
        "title": "GeoLLM: Extracting Geospatial Knowledge from Large Language Models",
        "authors": "Rohin Manvi;Samar Khanna;Gengchen Mai;Marshall Burke;David B. Lobell;Stefano Ermon",
        "site": "https://iclr.cc/virtual/2024/poster/18551",
        "summary": "Abstract:The application of machine learning (ML) in a range of geospatial tasks is increasingly common but often relies on globally available covariates such as satellite imagery that can either be expensive or lack predictive power.Here we explore the question of whether the vast amounts of knowledge found in Internet language corpora, now compressed within large language models (LLMs), can be leveraged for geospatial prediction tasks. We first demonstrate that LLMs embed remarkable spatial information about locations, but naively querying LLMs using geographic coordinates alone is ineffective in predicting key indicators like population density. We then present GeoLLM, a novel method that can effectively extract geospatial knowledge from LLMs with auxiliary map data from OpenStreetMap.We demonstrate the utility of our approach across multiple tasks of central interest to the international community, including the measurement of population density and economic livelihoods.Across these tasks, our method demonstrates a 70\\% improvement in performance (measured using Pearson's $r^2$) relative to baselines that use nearest neighbors or use information directly from the prompt, and performance equal to or exceeding satellite-based benchmarks in the literature.With GeoLLM, we observe that GPT-3.5 outperforms Llama 2 and RoBERTa by 19\\% and 51\\% respectively, suggesting that the performance of our method scales well with the size of the model and its pretraining dataset.Our experiments reveal that LLMs are remarkably sample-efficient, rich in geospatial information, and robust across the globe.Crucially, GeoLLM shows promise in mitigating the limitations of existing geospatial covariates and complementing them well.Code is available on the project website: https://rohinmanvi.github.io/GeoLLM",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"naively querying LLMs using geographic coordinates alone is ineffective in predicting key indicators like population density.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"naively querying LLMs using geographic coordinates alone is ineffective in predicting key indicators like population density.\""
    },
    {
        "title": "LILO: Learning Interpretable Libraries by Compressing and Documenting Code",
        "authors": "Gabriel Grand;Lionel Wong;Matthew Bowers;Theo X. Olausson;Muxin Liu;Joshua B. Tenenbaum;Jacob Andreas",
        "site": "https://iclr.cc/virtual/2024/poster/18550",
        "summary": "While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce LILO, a neurosymbolic framework that iteratively synthesizes, compresses, and documents code to build libraries tailored to particular problem domains. LILO combines LLM-guided program synthesis with recent algorithmic advances in automated refactoring from Stitch: a symbolic compression system that efficiently identifies optimal lambda abstractions across large code corpora. To make these abstractions interpretable, we introduce an auto-documentation (AutoDoc) procedure that infers natural language names and docstrings based on contextual examples of usage. In addition to improving human readability, we find that AutoDoc boosts performance by helping LILO's synthesizer to interpret and deploy learned abstractions. We evaluate LILO on three inductive program synthesis benchmarks for string editing, scene reasoning, and graphics composition. Compared to existing neural and symbolic methods—including the state-of-the-art library learning algorithm DreamCoder—LILO solves more complex tasks and learns richer libraries that are grounded in linguistic knowledge.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs.\"\n\nThis paper does not explicitly discuss limitations of LLMs, but rather presents an approach to improve their code generation capabilities.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs.\"\n\nThis paper does not explicitly discuss limitations of LLMs, but rather presents an approach to improve their code generation capabilities."
    },
    {
        "title": "Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models",
        "authors": "Yingtao Zhang;Haoli Bai;Haokun Lin;Jialin Zhao;Lu Hou;Carlo Vittorio Cannistraci",
        "site": "https://iclr.cc/virtual/2024/poster/18549",
        "summary": "With the rapid growth of large language models (LLMs), there is increasing demand for memory and computation in LLMs. Recent efforts on post-training pruning of LLMs aim to reduce the model size and computation requirements, yet the performance is still sub-optimal. In this paper, we present a plug-and-play solution for post-training pruning of LLMs.The proposed solution has two innovative components: 1)Relative Importance and Activations (RIA), a new pruning metric that jointly considers the weight and activations efficiently on LLMs, and 2)Channel Permutation, a new approach to maximally preserves important weights under N:M sparsity.The two proposed components can be readily combined to further enhance the N:M semi-structured pruning of LLMs.Our empirical experiments show that RIA alone can already surpass all existing post-training pruning methods on prevalent LLMs, e.g., LLaMA ranging from 7B to 65B. Furthermore, N:M semi-structured pruning with channel permutation can even outperform the original LLaMA2-70B on zero-shot tasks, together with practical speed-up on specific hardware.Our code is available at: https://github.com/biomedical-cybernetics/Relative-importance-and-activation-pruning",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent efforts on post-training pruning of LLMs aim to reduce the model size and computation requirements, yet the performance is still sub-optimal.\"\n\nThis abstract mentions a limitation of LLMs (large model size and computation requirements) but only briefly and does not elaborate on it. The focus is on the proposed solution to address this limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Recent efforts on post-training pruning of LLMs aim to reduce the model size and computation requirements, yet the performance is still sub-optimal.\"\n\nThis abstract mentions a limitation of LLMs (large model size and computation requirements) but only briefly and does not elaborate on it. The focus is on the proposed solution to address this limitation."
    },
    {
        "title": "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
        "authors": "Chenxi Sun;Hongyan Li;Yaliang Li;Shenda Hong",
        "site": "https://iclr.cc/virtual/2024/poster/18544",
        "summary": "This work summarizes two ways to accomplish Time-Series (TS) tasks in today's Large Language Model (LLM) context: LLM-for-TS (model-centric) designs and trains a fundamental large model, or fine-tunes a pre-trained LLM for TS data; TS-for-LLM (data-centric) converts TS into a model-friendly representation to enable the pre-trained LLM to handle TS data. Given the lack of data, limited resources, semantic context requirements, and so on, this work focuses on TS-for-LLM, where we aim to activate LLM's ability for TS data by designing a TS embedding method suitable for LLM. The proposed method is named TEST. It first tokenizes TS, builds an encoder to embed TS via instance-wise, feature-wise, and text-prototype-aligned contrast, where the TS embedding space is aligned to LLM’s embedding layer space, then creates soft prompts to make LLM more open to that embeddings, and finally implements TS tasks using the frozen LLM. We also demonstrate the feasibility of TS-for-LLM through theory and experiments. Experiments are carried out on TS classification, forecasting, and representation tasks using eight frozen LLMs with various structures and sizes. The results show that the pre-trained LLM with TEST strategy can achieve better or comparable performance than today's SOTA TS models, and offers benefits for few-shot and generalization. By treating LLM as the pattern machine, TEST can endow LLM's ability to process TS data without compromising language ability. We hope that this study will serve as a foundation for future work to support TS+LLM progress.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Given the lack of data, limited resources, semantic context requirements, and so on, this work focuses on TS-for-LLM, where we aim to activate LLM's ability for TS data by designing a TS embedding method suitable for LLM.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Given the lack of data, limited resources, semantic context requirements, and so on, this work focuses on TS-for-LLM, where we aim to activate LLM's ability for TS data by designing a TS embedding method suitable for LLM.\""
    },
    {
        "title": "Provable Benefits of Multi-task RL under Non-Markovian Decision Making Processes",
        "authors": "Ruiquan Huang;Yuan Cheng;Jing Yang;Vincent Tan;Yingbin Liang",
        "site": "https://iclr.cc/virtual/2024/poster/18535",
        "summary": "Abstract:In multi-task reinforcement learning (RL) under Markov decision processes (MDPs), the presence of shared latent structures among multiple MDPs has been shown to yield significant benefits to the sample efficiency compared to single-task RL. In this paper, we investigate whether such a benefit can extend to more general sequential decision making problems such as predictive state representations (PSRs). The main challenge here is that the large and complex model space makes it hard to identify what types of common latent structure of multi-task PSRs can reduce the model complexity and improve sample efficiency.To this end, we posit a  joint model class for tasks and use the notion of $\\eta$-bracketing number to quantify its complexity; this number also serves as a general metric  to capture the similarity of tasks and thus determines the benefit of multi-task over single-task RL. We first study  upstream multi-task learning over PSRs, in which all tasks share the same observation and action spaces. We propose a provably efficient algorithm  UMT-PSR for finding near-optimal policies for all PSRs, and demonstrate that the advantage of multi-task learning manifests if the joint model class of PSRs has a smaller $\\eta$-bracketing number compared to that of individual single-task learning. We further investigate downstream learning, in which the agent needs to learn a new target task that shares some commonalities with the upstream tasks via a similarity constraint. By exploiting the learned PSRs from the upstream, we develop a sample-efficient algorithm that provably finds a near-optimal policy. Upon specialization to some examples with small $\\eta$-bracketing numbers, our results further highlight the benefit compared to directly learning a single-task PSR.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals",
        "authors": "Yair Ori Gat;Nitay Calderon;Amir Feder;Alexander Chapanin;Amit Sharma;Roi Reichart",
        "site": "https://iclr.cc/virtual/2024/poster/18527",
        "summary": "Causal explanations of the predictions of NLP systems are essential to ensure safety and establish trust. Yet, existing methods often fall short of explaining model predictions effectively or efficiently and are often model-specific. In this paper, we address model-agnostic explanations, proposing two approaches for counterfactual (CF) approximation. The first approach is CF generation, where a large language model (LLM) is prompted to change a specific text concept while keeping confounding concepts unchanged. While this approach is demonstrated to be very effective, applying LLM at inference-time is costly. We hence present a second approach based on matching, and propose a method that is guided by an LLM at training-time and learns a dedicated embedding space. This space is faithful to a given causal graph and effectively serves to identify matches that approximate CFs. After showing theoretically that approximating CFs is required in order to construct faithful explanations, we benchmark our approaches and explain several models, including LLMs with billions of parameters. Our empirical results demonstrate the excellent performance of CF generation models as model-agnostic explainers. Moreover, our matching approach, which requires far less test-time resources, also provides effective explanations, surpassing many baselines. We also find that Top-K techniques universally improve every tested method. Finally, we showcase the potential of LLMs in constructing new benchmarks for model explanation and subsequently validate our conclusions. Our work illuminates new pathways for efficient and accurate approaches to interpreting NLP systems.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While this approach is demonstrated to be very effective, applying LLM at inference-time is costly.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (high inference cost) but does not explore it in depth, and the primary focus of the paper is on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While this approach is demonstrated to be very effective, applying LLM at inference-time is costly.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (high inference cost) but does not explore it in depth, and the primary focus of the paper is on the proposed solution."
    },
    {
        "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
        "authors": "Ziyang Luo;Can Xu;Pu Zhao;Qingfeng Sun;Xiubo Geng;Wenxiang Hu;Chongyang Tao;Jing Ma;Qingwei Lin;Daxin Jiang",
        "site": "https://iclr.cc/virtual/2024/poster/18519",
        "summary": "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated remarkable performance in various code-related tasks. However, different from their counterparts in the general language modeling field, the technique of instruction fine-tuning remains relatively under-researched in this domain. In this paper, we present Code Evol-Instruct, a novel approach that adapts the Evol-Instruct method to the realm of code, enhancing Code LLMs to create novel models, WizardCoder. Through comprehensive experiments on five prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, DS-1000, and MultiPL-E, our models showcase outstanding performance. They consistently outperform all other open-source Code LLMs by a significant margin. Remarkably, WizardCoder 15B even surpasses the well-known closed-source LLMs, including Anthropic's Claude and Google's Bard, on the HumanEval and HumanEval+ benchmarks. Additionally, WizardCoder 34B not only achieves a HumanEval score comparable to GPT3.5 (ChatGPT) but also surpasses it on the HumanEval+ benchmark. Furthermore, our preliminary exploration highlights the pivotal role of instruction complexity in achieving exceptional coding performance.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation of Code LLMs is mentioned in the abstract, but the fact that \"the technique of instruction fine-tuning remains relatively under-researched in this domain\" can be seen as a minor limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation of Code LLMs is mentioned in the abstract, but the fact that \"the technique of instruction fine-tuning remains relatively under-researched in this domain\" can be seen as a minor limitation."
    },
    {
        "title": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models",
        "authors": "Ming Jin;Shiyu Wang;Lintao Ma;Zhixuan Chu;James Y. Zhang;Xiaoming Shi;Pin-Yu Chen;Yuxuan Liang;Yuan-Fang Li;Shirui Pan;Qingsong Wen",
        "site": "https://iclr.cc/virtual/2024/poster/18518",
        "summary": "Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by reprogramming the input time series with text prototypes before feeding it into the frozen LLM to align the two modalities. To augment the LLM's ability to reason with time series data, we propose Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches. The transformed time series patches from the LLM are finally projected to obtain the forecasts. Our comprehensive evaluations demonstrate that \\method is a powerful time series learner that outperforms state-of-the-art, specialized forecasting models. Moreover, Time-LLM excels in both few-shot and zero-shot learning scenarios. The code is made available at https://github.com/KimMeen/Time-LLM.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities.\""
    },
    {
        "title": "Fourier Transporter: Bi-Equivariant Robotic Manipulation in 3D",
        "authors": "Haojie Huang;Owen Lewis Howell;Dian Wang;Xupeng Zhu;Robert Platt;Robin Walters",
        "site": "https://iclr.cc/virtual/2024/poster/18515",
        "summary": "Abstract:Many complex robotic manipulation tasks can be decomposed as a sequence of pick and place actions. Training a robotic agent to learn this sequence over many different starting conditions typically requires many iterations or demonstrations, especially in 3D environments. In this work, we propose Fourier Transporter ($\\text{FourTran}$), which leverages the two-fold $\\mathrm{SE}(d)\\times\\mathrm{SE}(d)$  symmetry in the pick-place problem to achieve much higher sample efficiency. $\\text{FourTran}$ is an open-loop behavior cloning method trained using expert demonstrations to predict pick-place actions on new configurations. $\\text{FourTran}$ is constrained by the symmetries of the pick and place actions independently. Our method utilizes a fiber space Fourier transformation that allows for memory-efficient computation. Tests on the RLbench benchmark achieve state-of-the-art results across various tasks.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Meaning Representations from Trajectories in Autoregressive Models",
        "authors": "Tian Yu Liu;Matthew Trager;Alessandro Achille;Pramuditha Perera;Luca Zancato;Stefano Soatto",
        "site": "https://iclr.cc/virtual/2024/poster/18513",
        "summary": "We propose to extract meaning representations from autoregressive language models by considering the distribution of all possible trajectories extending an input text. This strategy is prompt-free, does not require fine-tuning, and is applicable to any pre-trained autoregressive model. Moreover, unlike vector-based representations,  distribution-based representations can also model asymmetric relations (e.g., direction of logical entailment, hypernym/hyponym relations) by using algebraic operations between likelihood functions. These ideas are grounded in distributional perspectives on semantics and are connected to standard constructions in automata theory, but to our knowledge they have not been applied to modern language models. We empirically show that the representations obtained from large models align well with human annotations, outperform other zero-shot and prompt-free methods on semantic similarity tasks, and can be used to solve more complex entailment and containment tasks that standard embeddings cannot handle. Finally, we extend our method  to represent data from different modalities (e.g., image and text) using multimodal autoregressive models. Our code is available at: https://github.com/tianyu139/meaning-as-trajectories",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization",
        "authors": "Jin Peng Zhou;Charles E Staats;Wenda Li;Christian Szegedy;Kilian Q Weinberger;Yuhuai Wu",
        "site": "https://iclr.cc/virtual/2024/poster/18508",
        "summary": "Large language models (LLM), such as Google's Minerva and OpenAI's GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code --- which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than vanilla majority voting --- the previously best method to identify correct answers, by more than 12\\% on GSM8K. In our experiments it improves results consistently across all datasets and LLM model sizes. The code can be found at https://github.com/jinpz/dtv.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, they still make unjustified logical and computational errors in their reasoning steps and answers.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, they still make unjustified logical and computational errors in their reasoning steps and answers.\""
    },
    {
        "title": "An LLM can Fool Itself: A Prompt-Based Adversarial Attack",
        "authors": "Xilie Xu;Keyi Kong;Ning Liu;Lizhen Cui;Di Wang;Jingfeng Zhang;Mohan Kankanhalli",
        "site": "https://iclr.cc/virtual/2024/poster/18503",
        "summary": "The wide-ranging applications of large language models (LLMs), especially in safety-critical domains, necessitate the proper evaluation of the LLM’s adversarial robustness. This paper proposes an efficient tool to audit the LLM’s adversarial robustness via a prompt-based adversarial attack (PromptAttack). PromptAttack converts adversarial textual attacks into an attack prompt that can cause the victim LLM to output the adversarial sample to fool itself. The attack prompt is composed of three important components: (1) original input (OI) including the original sample and its ground-truth label, (2) attack objective (AO) illustrating a task description of generating a new sample that can fool itself without changing the semantic meaning, and (3) attack guidance (AG) containing the perturbation instructions to guide the LLM on how to complete the task by perturbing the original sample at character, word, and sentence levels, respectively. Besides, we use a fidelity filter to ensure that PromptAttack maintains the original semantic meanings of the adversarial examples. Further, we enhance the attack power of PromptAttack by ensembling adversarial examples at different perturbation levels. Comprehensive empirical results using Llama2 and GPT-3.5 validate that PromptAttack consistently yields a much higher attack success rate compared to AdvGLUE and AdvGLUE++. Interesting findings include that a simple emoji can easily mislead GPT-3.5 to make wrong predictions. Our source code is available at https://github.com/GodXuxilie/PromptAttack.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The wide-ranging applications of large language models (LLMs), especially in safety-critical domains, necessitate the proper evaluation of the LLM’s adversarial robustness.\"; \"Interesting findings include that a simple emoji can easily mislead GPT-3.5 to make wrong predictions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"The wide-ranging applications of large language models (LLMs), especially in safety-critical domains, necessitate the proper evaluation of the LLM’s adversarial robustness.\"; \"Interesting findings include that a simple emoji can easily mislead GPT-3.5 to make wrong predictions.\""
    },
    {
        "title": "Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation",
        "authors": "Kai Huang;Hanyun Yin;Heng Huang;Wei Gao",
        "site": "https://iclr.cc/virtual/2024/poster/18496",
        "summary": "Fine-tuning is essential to adapting pre-trained large language models to downstream applications. With the increasing popularity of LLM-enabled applications, fine-tuning has been performed intensively worldwide, incurring a tremendous amount of computing costs that correspond to big carbon footprint and environmental impact. Mitigating such environmental impact directly correlates to reducing the fine-tuning FLOPs. Existing fine-tuning schemes focus on either saving memory or reducing the overhead of computing weight updates, but cannot achieve sufficient FLOPs reduction due to their ignorance of the training cost in backpropagation. To address this limitation, in this paper we present GreenTrainer, a new technique that minimizes the FLOPs of LLM fine-tuning via adaptive backpropagation, which adaptively selects the most appropriate set of LLM tensors for fine-tuning based on their importance and backpropagation cost in training. Experiment results show that GreenTrainer can save up to 64\\% training FLOPs compared to full fine-tuning, without any noticeable accuracy loss. Compared to the existing schemes such as Prefix Tuning and LoRA, GreenTrainer can achieve up to 4\\% improvement of model accuracy, with on-par FLOPs reduction.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing fine-tuning schemes... cannot achieve sufficient FLOPs reduction due to their ignorance of the training cost in backpropagation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing fine-tuning schemes... cannot achieve sufficient FLOPs reduction due to their ignorance of the training cost in backpropagation.\""
    },
    {
        "title": "Understanding Catastrophic Forgetting in Language Models via Implicit Inference",
        "authors": "Suhas Kotha;Jacob Mitchell Springer;Aditi Raghunathan",
        "site": "https://iclr.cc/virtual/2024/poster/18492",
        "summary": "We lack a systematic understanding of the effects of fine-tuning (via methods such as instruction-tuning or reinforcement learning from human feedback), particularly on tasks outside the narrow fine-tuning distribution. In a simplified scenario, we demonstrate that improving performance on tasks within the fine-tuning data distribution comes at the expense of capabilities on other tasks. We hypothesize that language models implicitly infer the task of the prompt and that fine-tuning skews this inference towards tasks in the fine-tuning distribution. To test this, we propose Conjugate Prompting, which artificially makes the task look farther from the fine-tuning distribution while requiring the same capability, and we find that this recovers some of the pretraining capabilities in our synthetic setup. Since real-world fine-tuning distributions are predominantly English, we apply conjugate prompting to recover pretrained capabilities in LLMs by simply translating the prompts to different languages. This allows us to recover in-context learning abilities lost via instruction tuning, natural reasoning capability lost during code fine-tuning, and, more concerningly, harmful content generation suppressed by safety fine-tuning in chatbots like ChatGPT.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We lack a systematic understanding of the effects of fine-tuning... particularly on tasks outside the narrow fine-tuning distribution... improving performance on tasks within the fine-tuning data distribution comes at the expense of capabilities on other tasks... more concerningly, harmful content generation suppressed by safety fine-tuning in chatbots like ChatGPT.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We lack a systematic understanding of the effects of fine-tuning... particularly on tasks outside the narrow fine-tuning distribution... improving performance on tasks within the fine-tuning data distribution comes at the expense of capabilities on other tasks... more concerningly, harmful content generation suppressed by safety fine-tuning in chatbots like ChatGPT.\""
    },
    {
        "title": "Modeling Boundedly Rational Agents with Latent Inference Budgets",
        "authors": "Athul Paul Jacob;Abhishek Gupta;Jacob Andreas",
        "site": "https://iclr.cc/virtual/2024/poster/18487",
        "summary": "We study the problem of modeling a population of agents pursuing unknown goals subject to unknown computational constraints. In standard models of bounded rationality, sub-optimal decision-making is simulated by adding homoscedastic noise to optimal decisions rather than actually simulating constrained inference. In this work, we introduce a latent inference budget model (L-IBM) that models these constraints explicitly, via a latent variable (inferred jointly with a model of agents’ goals) that controls the runtime of an iterative inference algorithm. L-IBMs make it possible to learn agent models using data from diverse populations of suboptimal actors. In three modeling tasks—inferring navigation goals from routes, inferring communicative intents from human utterances, and predicting next moves in human chess games—we show that L-IBMs match or outperforms Boltzmann models of decision-making under uncertainty. Moreover, the inferred inference budgets are themselves meaningful, efficient to compute, and correlated with measures of player skill, partner skill and task difficulty.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data",
        "authors": "Antonis Antoniades;Yiyi Yu;Joe S Canzano;William Yang Wang;Spencer Smith",
        "site": "https://iclr.cc/virtual/2024/poster/18486",
        "summary": "State-of-the-art systems neuroscience experiments yield large-scale multimodal data, and these data sets require new tools for analysis. Inspired by the success of large pretrained models in vision and language domains, we reframe the analysis of large-scale, cellular-resolution neuronal spiking data into an auto-regressive spatiotemporal generation problem. Neuroformer is a multimodal, multitask generative pre-trained transformer (GPT) model that is specifically designed to handle the intricacies of data in systems neuroscience. It scales linearly with feature size, can process an arbitrary number of modalities, and is adaptable to downstream tasks, such as predicting behavior. We first trained Neuroformer on simulated datasets, and found that it both accurately predicted simulated neuronal circuit activity, and also intrinsically inferred the underlying neural circuit connectivity, including direction. When pretrained to decode neural responses, the model predicted the behavior of a mouse with only few-shot fine-tuning, suggesting that the model begins learning how to do so directly from the neural representations themselves, without any explicit supervision. We used an ablation study to show that joint training on neuronal responses and behavior boosted performance, highlighting the model's ability to associate behavioral and neural representations in an unsupervised manner. These findings show that Neuroformer can analyze neural datasets and their emergent properties, informing the development of models and hypotheses associated with the brain.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Learning Optimal Contracts: How to Exploit Small Action Spaces",
        "authors": "Francesco Bacchiocchi;Matteo Castiglioni;Alberto Marchesi;Nicola Gatti",
        "site": "https://iclr.cc/virtual/2024/poster/18483",
        "summary": "Abstract:We study principal-agent problems in which a principal commits to an outcome-dependent payment scheme---called contract---in order to induce an agent to take a costly, unobservable action leading to favorable outcomes. We consider a generalization of the classical (single-round) version of the problem in which the principal interacts with the agent by committing to contracts over multiple rounds. The principal has no information about the agent, and they have to learn an optimal contract by only observing the outcome realized at each round. We focus on settings in which the size of the agent's action space is small. We design an algorithm that learns an approximately-optimal contract with high probability in a number of rounds polynomial in the size of the outcome space, when the number of actions is constant. Our algorithm solves an open problem by Zhu et al. [2022]. Moreover, it can also be employed to provide a $\\widetilde{\\mathcal{O}}(T^{4/5})$ regret bound in the related online learning setting in which the principal aims at maximizing their cumulative utility, thus considerably improving previously-known regret bounds.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Fine-Tuning Language Models for Factuality",
        "authors": "Katherine Tian;Eric Mitchell;Huaxiu Yao;Christopher D Manning;Chelsea Finn",
        "site": "https://iclr.cc/virtual/2024/poster/18477",
        "summary": "The fluency and creativity of large pre-trained language models (LLMs) have led to their widespread use, sometimes even as a replacement for traditional search engines. Yet language models are prone to making convincing but factually inaccurate claims, often referred to as `hallucinations.' These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions. Further, manual fact-checking of model responses is a time-consuming process, making human factuality labels expensive to acquire. In this work, we fine-tune language models to be more factual, without human labeling and targeting more open-ended generation settings than past work. We leverage two key recent innovations in NLP to do so. First, several recent works have proposed methods for judging the factuality of open-ended text by measuring consistency with an external knowledge base or simply a large model's confidence scores. Second, the Direct Preference Optimization algorithm enables straightforward fine-tuning of language models on objectives other than supervised imitation, using a preference ranking over possible model responses. We show that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or our novel retrieval-free approach, significantly improves the factuality (percent of generated claims that are correct) of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality. At 7B scale, compared to Llama-2-Chat, we observe 53% and 50% reduction in factual error rate when generating biographies and answering medical questions, respectively. A reference implementation can be found at https://github.com/kttian/llmfactualitytuning.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Yet language models are prone to making convincing but factually inaccurate claims, often referred to as `hallucinations.' These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Yet language models are prone to making convincing but factually inaccurate claims, often referred to as `hallucinations.' These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions.\""
    },
    {
        "title": "QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models",
        "authors": "Yuhui Xu;Lingxi Xie;Xiaotao Gu;Xin Chen;Heng Chang;Hengheng Zhang;Zhengsu Chen;XIAOPENG ZHANG;Qi Tian",
        "site": "https://iclr.cc/virtual/2024/poster/18462",
        "summary": "Recently years have witnessed a rapid development of large language models (LLMs). Despite the strong ability in many language-understanding tasks, the heavy computational burden largely restricts the application of LLMs especially when one needs to deploy them onto edge devices. In this paper, we propose a quantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies in the imbalanced degrees of freedom of quantization and adaptation, and the solution is to use group-wise operators which increase the degree of freedom of quantization meanwhile decreasing that of adaptation. QA-LoRA is easily implemented with a few lines of code, and it equips the original LoRA with two-fold abilities: (i) during fine-tuning, the LLM's weights are quantized (e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the LLM and auxiliary weights are naturally integrated into a quantized model without loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model families and validate its effectiveness in different fine-tuning datasets and downstream scenarios. The code is made available at https://github.com/yuhuixu1993/qa-lora.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the heavy computational burden largely restricts the application of LLMs especially when one needs to deploy them onto edge devices.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the heavy computational burden largely restricts the application of LLMs especially when one needs to deploy them onto edge devices.\""
    },
    {
        "title": "Visual Data-Type Understanding does not emerge from scaling Vision-Language Models",
        "authors": "Vishaal Udandarao;Max F Burg;Samuel Albanie;Matthias Bethge",
        "site": "https://iclr.cc/virtual/2024/poster/18460",
        "summary": "Recent advances in the development of vision-language models (VLMs) are yielding remarkable success in recognizing visual semantic content, including impressive instances of compositional image understanding. Here, we introduce the novel task of Visual Data-Type Identification, a basic perceptual skill with implications for data curation (e.g., noisy data-removal from large datasets, domains pecific retrieval) and autonomous vision (e.g., distinguishing changing weather conditions from camera lens staining). We develop two datasets consisting of animal images altered across a diverse set of 27 visual data-types, spanning four broad categories. An extensive zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a nuanced performance landscape. While VLMs are reasonably good at identifying certain stylistic data-types, such as cartoons and sketches, they struggle with simpler data-types arising from basic manipulations like image rotations or additive noise. Our findings reveal that (i) model scaling alone yields marginal gains for contrastively-trained models like CLIP, and (ii) there is a pronounced drop in performance for the largest auto-regressively trained VLMs like OpenFlamingo. This finding points to a blind spot in current frontier VLMs: they excel in recognizing semantic content but fail to acquire anunderstanding of visual data-types through scaling. By analyzing the pre-training distributions of these models and incorporating data-type information into the captions during fine-tuning, we achieve a significant enhancement in performance. By exploring this previously uncharted task, we aim to set the stage for further advancing VLMs to equip them with visual data-type understanding.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While VLMs are reasonably good at identifying certain stylistic data-types, such as cartoons and sketches, they struggle with simpler data-types arising from basic manipulations like image rotations or additive noise.\"; \"This finding points to a blind spot in current frontier VLMs: they excel in recognizing semantic content but fail to acquire an understanding of visual data-types through scaling.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"While VLMs are reasonably good at identifying certain stylistic data-types, such as cartoons and sketches, they struggle with simpler data-types arising from basic manipulations like image rotations or additive noise.\"; \"This finding points to a blind spot in current frontier VLMs: they excel in recognizing semantic content but fail to acquire an understanding of visual data-types through scaling.\""
    },
    {
        "title": "Debiasing Algorithm through Model Adaptation",
        "authors": "Tomasz Limisiewicz;David Mareček;Tomáš Musil",
        "site": "https://iclr.cc/virtual/2024/poster/18456",
        "summary": "Large language models are becoming the go-to solution for the ever-growing number of tasks.However, with growing capacity, models are prone to rely on spurious correlations stemming from biases and stereotypes present in the training data.This work proposes a novel method for detecting and mitigating gender bias in language models.We perform causal analysis to identify problematic model components and discover that mid-upper feed-forward layers are most prone to convey bias.Based on the analysis results, we intervene in the model by applying a linear projection to the weight matrices of these layers.Our titular method DAMA, significantly decreases bias as measured by diverse metrics while maintaining the model's performance on downstream tasks.We release code for our method and models, which retrain LLaMA's state-of-the-art performance while being significantly less biased.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, with growing capacity, models are prone to rely on spurious correlations stemming from biases and stereotypes present in the training data.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, with growing capacity, models are prone to rely on spurious correlations stemming from biases and stereotypes present in the training data.\""
    },
    {
        "title": "InstructPix2NeRF: Instructed 3D Portrait Editing from a Single Image",
        "authors": "Jianhui Li;Shilong Liu;Zidong Liu;Yikai Wang;Kaiwen Zheng;Jinghui Xu;Jianmin Li;Jun Zhu",
        "site": "https://iclr.cc/virtual/2024/poster/18454",
        "summary": "Abstract:With the success of Neural Radiance Field (NeRF) in 3D-aware portrait editing, a variety of works have achieved promising results regarding both quality and 3D consistency. However, these methods heavily rely on per-prompt optimization when handling natural language as editing instructions. Due to the lack of labeled human face 3D datasets and effective architectures, the area of human-instructed 3D-aware editing for open-world portraits in an end-to-end manner remains under-explored. To solve this problem, we propose an end-to-end diffusion-based framework termed $\\textbf{InstructPix2NeRF}$, which enables instructed 3D-aware portrait editing from a single open-world image with human instructions.  At its core lies a conditional latent 3D diffusion process that lifts 2D editing to 3D space by learning the correlation between the paired images' difference and the instructions via triplet data. With the help of our proposed token position randomization strategy, we could even achieve multi-semantic editing through one single pass with the portrait identity well-preserved. Besides, we further propose an identity consistency module that directly modulates the extracted identity signals into our diffusion process, which increases the multi-view 3D identity consistency. Extensive experiments verify the effectiveness of our method and show its superiority against strong baselines quantitatively and qualitatively. Source code and pretrained models can be found on our project page: https://mybabyyh.github.io/InstructPix2NeRF.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs or their limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs or their limitations."
    },
    {
        "title": "Quantifying the Plausibility of Context Reliance in Neural Machine Translation",
        "authors": "Gabriele Sarti;Grzegorz Chrupała;Malvina Nissim;Arianna Bisazza",
        "site": "https://iclr.cc/virtual/2024/poster/18449",
        "summary": "Abstract:Establishing whether language models can use contextual information in a human-plausible way is important to ensure their safe adoption in real-world settings. However, the questions of $\\textit{when}$ and $\\textit{which parts}$ of the context affect model generations are typically tackled separately, and current plausibility evaluations are practically limited to a handful of artificial benchmarks. To address this, we introduce $\\textbf{P}$lausibility $\\textbf{E}$valuation of $\\textbf{Co}$ntext $\\textbf{Re}$liance (PECoRe), an end-to-end interpretability framework designed to quantify context usage in language models' generations. Our approach leverages model internals to (i) contrastively identify context-sensitive target tokens in generated texts and (ii) link them to contextual cues justifying their prediction. We use PECoRe to quantify the plausibility of context-aware machine translation models, comparing model rationales with human annotations across several discourse-level phenomena. Finally, we apply our method to unannotated model translations to identify context-mediated predictions and highlight instances of (im)plausible context usage throughout generation.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Establishing whether language models can use contextual information in a human-plausible way is important to ensure their safe adoption in real-world settings.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Establishing whether language models can use contextual information in a human-plausible way is important to ensure their safe adoption in real-world settings.\""
    },
    {
        "title": "DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text",
        "authors": "Xianjun Yang;Wei Cheng;Yue Wu;Linda Ruth Petzold;William Yang Wang;Haifeng Chen",
        "site": "https://iclr.cc/virtual/2024/poster/18444",
        "summary": "Large language models (LLMs) have notably enhanced the fluency and diversity of machine-generated text. However, this progress also presents a significant challenge in detecting the origin of a given text, and current research on detection methods lags behind the rapid evolution of LLMs. Conventional training-based methods have limitations in flexibility, particularly when adapting to new domains, and they often lack explanatory power. To address this gap, we propose a novel training-free detection strategy called Divergent N-Gram Analysis (DNA-GPT). Given a text, we first truncate it in the middle and then use only the preceding portion as input to the LLMs to regenerate the new remaining parts. By analyzing the differences between the original and new remaining parts through N-gram analysis in black-box or probability divergence in white-box, we can clearly illustrate significant discrepancies between machine-generated and human-written text. We conducted extensive experiments on the most advanced LLMs from OpenAI, including text-davinci-003, GPT-3.5-turbo, and GPT-4, as well as open-source models such as GPT-NeoX-20B and LLaMa-13B. Results show that our zero-shot approach exhibits state-of-the-art performance in distinguishing between human and GPT-generated text on four English and one German dataset, outperforming OpenAI's own classifier, which is trained on millions of text. Additionally, our methods provide reasonable explanations and evidence to support our claim, which is a unique feature of explainable detection. Our method is also robust under the revised text attack and can additionally solve model sourcing.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Conventional training-based methods have limitations in flexibility, particularly when adapting to new domains, and they often lack explanatory power.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Conventional training-based methods have limitations in flexibility, particularly when adapting to new domains, and they often lack explanatory power.\""
    },
    {
        "title": "GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction",
        "authors": "Oscar Sainz;Iker García-Ferrero;Rodrigo Agerri;Oier Lopez de Lacalle;German Rigau;Eneko Agirre",
        "site": "https://iclr.cc/virtual/2024/poster/18435",
        "summary": "Large Language Models (LLMs) combined with instruction tuning have made significant progress when generalizing to unseen tasks. However, they have been less successful in Information Extraction (IE), lagging behind task-specific models. Typically, IE tasks are characterized by complex annotation guidelines which describe the task and give examples to humans. Previous attempts to leverage such information have failed, even with the largest models, as they are not able to follow the guidelines out-of-the-box. In this paper we propose GoLLIE (Guideline-following Large Language Model for IE), a model able to improve zero-shot results on unseen IE tasks by virtue of being fine-tuned to comply with annotation guidelines. Comprehensive evaluation empirically demonstrates that GoLLIE is able to generalize to and follow unseen guidelines, outperforming previous attempts at zero-shot information extraction. The ablation study shows that detailed guidelines is key for good results. Code, data and models will be made publicly available.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, they have been less successful in Information Extraction (IE), lagging behind task-specific models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, they have been less successful in Information Extraction (IE), lagging behind task-specific models.\""
    },
    {
        "title": "Training Diffusion Models with Reinforcement Learning",
        "authors": "Kevin Black;Michael Janner;Yilun Du;Ilya Kostrikov;Sergey Levine",
        "site": "https://iclr.cc/virtual/2024/poster/18432",
        "summary": "Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness. In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives. We describe how posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization ( DDPO), that are more effective than alternative reward-weighted likelihood approaches. Empirically, DDPO can adapt text-to-image diffusion models to objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Finally, we show that DDPO can improve prompt-image alignment using feedback from a vision-language model without the need for additional data collection or human annotation. The project’s website can be found at http://rl-diffusion.github.io.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Differentially Private Synthetic Data via Foundation Model APIs 1: Images",
        "authors": "Zinan Lin;Sivakanth Gopi;Janardhan Kulkarni;Harsha Nori;Sergey Yekhanin",
        "site": "https://iclr.cc/virtual/2024/poster/18430",
        "summary": "Generating differentially private (DP) synthetic data that closely resembles the original private data is a scalable way to mitigate privacy concerns in the current data-driven world. In contrast to current practices that train customized models for this task, we aim to generate DP Synthetic Data via APIs (DPSDA), where we treat foundation models as blackboxes and only utilize their inference APIs. Such API-based, training-free approaches are easier to deploy as exemplified by the recent surge in the number of API-based apps. These approaches can also leverage the power of large foundation models which are only accessible via their inference APIs. However, this comes with greater challenges due to strictly more restrictive model access and the need to protect privacy from the API provider. In this paper, we present a new framework called Private Evolution (PE) to solve this problem and show its initial promise on synthetic images. Surprisingly, PE can match or even outperform state-of-the-art (SOTA) methods without any model training. For example, on CIFAR10 (with ImageNet as the public data), we achieve FID ≤ 7.9 with privacy cost ε = 0.67, significantly improving the previous SOTA from ε = 32. We further demonstrate the promise of applying PE on large foundation models such as Stable Diffusion to tackle challenging private datasets with a small number of high-resolution images. The code and data are released at https://github.com/microsoft/DPSDA.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, this comes with greater challenges due to strictly more restrictive model access and the need to protect privacy from the API provider.\"\n\nThis paper mentions a limitation of using foundation models (a type of LLM) via APIs, specifically the challenge of restrictive model access and protecting privacy from the API provider. However, the focus of the paper is on presenting a new framework to solve this problem",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, this comes with greater challenges due to strictly more restrictive model access and the need to protect privacy from the API provider.\"\n\nThis paper mentions a limitation of using foundation models (a type of LLM) via APIs, specifically the challenge of restrictive model access and protecting privacy from the API provider. However, the focus of the paper is on presenting a new framework to solve this problem"
    },
    {
        "title": "TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting",
        "authors": "Defu Cao;Furong Jia;Sercan O Arik;Tomas Pfister;Yixiang Zheng;Wen Ye;Yan Liu",
        "site": "https://iclr.cc/virtual/2024/poster/18429",
        "summary": "The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the design of prompts to facilitate distribution adaptation in different types of time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO over state-of-the-art methods on zero shot setting for a number of time series benchmark datasets. This performance gain is observed not only in scenarios involving previously unseen datasets but also in scenarios with multi-modal inputs. This compelling finding highlights TEMPO's potential to constitute a foundational model-building framework.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations of LLMs are mentioned, but the paper explores the potential of a GPT-type architecture for time series forecasting, implying that existing LLMs may not be effective in this domain.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations of LLMs are mentioned, but the paper explores the potential of a GPT-type architecture for time series forecasting, implying that existing LLMs may not be effective in this domain."
    },
    {
        "title": "In-Context Learning Learns Label Relationships but Is Not Conventional Learning",
        "authors": "Jannik Kossen;Yarin Gal;Tom Rainforth",
        "site": "https://iclr.cc/virtual/2024/poster/18423",
        "summary": "The predictions of Large Language Models (LLMs) on downstream tasks often improve significantly when including examples of the input–label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works. For example, while Xie et al. (2022) liken ICL to a general-purpose learning algorithm, Min et al. (2022b) argue ICL does not even learn label relationships from in-context examples. In this paper, we provide novel insights into how ICL leverages label information, revealing both capabilities and limitations. To ensure we obtain a comprehensive picture of ICL behavior, we study probabilistic aspects of ICL predictions and thoroughly examine the dynamics of ICL as more examples are provided. Our experiments show that ICL predictions almost always depend on in-context labels and that ICL can learn truly novel tasks in-context. However, we also find that ICL struggles to fully overcome prediction preferences acquired from pre-training data and, further, that ICL does not consider all in-context information equally.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, we also find that ICL struggles to fully overcome prediction preferences acquired from pre-training data and, further, that ICL does not consider all in-context information equally.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, we also find that ICL struggles to fully overcome prediction preferences acquired from pre-training data and, further, that ICL does not consider all in-context information equally.\""
    },
    {
        "title": "Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning with Diverse Tasks",
        "authors": "Ziping Xu;Zifan Xu;Runxuan Jiang;Peter Stone;Ambuj Tewari",
        "site": "https://iclr.cc/virtual/2024/poster/18421",
        "summary": "Abstract:Multitask Reinforcement Learning (MTRL) approaches have gained increasing attention for its wide applications in many important Reinforcement Learning (RL) tasks. However, while recent advancements in MTRL theory have focused on the improved statistical efficiency by assuming a shared structure across tasks, exploration--a crucial aspect of RL--has been largely overlooked. This paper addresses this gap by showing that when an agent is trained on a sufficiently diverse set of tasks, a generic  policy-sharing algorithm with myopic exploration design like $\\epsilon$-greedy that are inefficient in general can be sample-efficient for MTRL. To the best of our knowledge, this is the first theoretical demonstration of the \"exploration benefits\" of MTRL. It may also shed light on the enigmatic success of the wide applications of myopic exploration in practice. To validate the role of diversity, we conduct experiments on synthetic robotic control environments, where the diverse task set aligns with the task selection by automatic curriculum learning, which is empirically shown to improve sample-efficiency.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs."
    },
    {
        "title": "Improving Intrinsic Exploration by Creating Stationary Objectives",
        "authors": "Roger Creus Castanyer;Joshua Romoff;Glen Berseth",
        "site": "https://iclr.cc/virtual/2024/poster/18419",
        "summary": "Exploration bonuses in reinforcement learning guide long-horizon exploration by defining custom intrinsic objectives. Count-based methods use the frequency of state visits to derive an exploration bonus. In this paper, we identify that any intrinsic reward function derived from count-based methods is non-stationary and hence induces a difficult objective to optimize for the agent. The key contribution of our work lies in transforming the original non-stationary rewards into stationary rewards through an augmented state representation. For this purpose, we introduce the Stationary Objectives For Exploration (SOFE) framework. SOFE requiresidentifyingsufficient statistics for different exploration bonuses and finding anefficientencoding of these statistics to use as input to a deep network. SOFE is based on proposing state augmentations that expand the state space but hold the promise of simplifying the optimization of the agent's objective. Our experiments show that SOFE improves the agents' performance in challenging exploration problems, including sparse-reward tasks, pixel-based observations, 3D navigation, and procedurally generated environments.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Adversarial Causal Bayesian Optimization",
        "authors": "Scott Sussex;Pier Giuseppe Sessa;Anastasia Makarova;Andreas Krause",
        "site": "https://iclr.cc/virtual/2024/poster/18417",
        "summary": "In Causal Bayesian Optimization (CBO), an agent intervenes on a structural causal model with known graph but unknown mechanisms to maximize a downstream reward variable. In this paper, we consider the generalization where other agents or external events also intervene on the system, which is key for enabling adaptiveness to non-stationarities such as weather changes, market forces, or adversaries. We formalize this generalization of CBO as Adversarial Causal Bayesian Optimization (ACBO) and introduce the first algorithm for ACBO with bounded regret: Causal Bayesian Optimization with Multiplicative Weights (CBO-MW). Our approach combines a classical online learning strategy with causal modeling of the rewards. To achieve this, it computes optimistic counterfactual reward estimates by propagating uncertainty through the causal graph. We derive regret bounds for CBO-MW that naturally depend on graph-related quantities. We further propose a scalable implementation for the case of combinatorial interventions and submodular rewards. Empirically, CBO-MW outperforms non-causal and non-adversarial Bayesian optimization methods on synthetic environments and environments based on real-word data. Our experiments include a realistic demonstration of how CBO-MW can be used to learn users' demand patterns in a shared mobility system and reposition vehicles in strategic areas.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Trajeglish: Traffic Modeling as Next-Token Prediction",
        "authors": "Jonah Philion;Xue Bin Peng;Sanja Fidler",
        "site": "https://iclr.cc/virtual/2024/poster/18411",
        "summary": "A longstanding challenge for self-driving development is simulating dynamic driving scenarios seeded from recorded driving logs. In pursuit of this functionality, we apply tools from discrete sequence modeling to model how vehicles, pedestrians and cyclists interact in driving scenarios. Using a simple data-driven tokenization scheme, we discretize trajectories to centimeter-level resolution using a small vocabulary. We then model the multi-agent sequence of discrete motion tokens with a GPT-like encoder-decoder that is autoregressive in time and takes into account intra-timestep interaction between agents. Scenarios sampled from our model exhibit state-of-the-art realism; our model tops the Waymo Sim Agents Benchmark, surpassing prior work along the realism meta metric by 3.3% and along the interaction metric by 9.9%. We ablate our modeling choices in full autonomy and partial autonomy settings, and show that the representations learned by our model can quickly be adapted to improve performance on nuScenes. We additionally evaluate the scalability of our model with respect to parameter count and dataset size, and use density estimates from our model to quantify the saliency of context length and intra-timestep interaction for the traffic modeling task.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Using a simple data-driven tokenization scheme, we discretize trajectories to centimeter-level resolution using a small vocabulary. We then model the multi-agent sequence of discrete motion tokens with a GPT-like encoder-decoder...\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Using a simple data-driven tokenization scheme, we discretize trajectories to centimeter-level resolution using a small vocabulary. We then model the multi-agent sequence of discrete motion tokens with a GPT-like encoder-decoder...\""
    },
    {
        "title": "Leftover Lunch: Advantage-based Offline Reinforcement Learning for Language Models",
        "authors": "Ashutosh Baheti;Ximing Lu;Faeze Brahman;Ronan Le Bras;Maarten Sap;Mark Riedl",
        "site": "https://iclr.cc/virtual/2024/poster/18408",
        "summary": "Reinforcement Learning with Human Feedback (RLHF) is the most prominent method for Language Model (LM) alignment. However, RLHF is an unstable and data-hungry process that continually requires new high-quality LM-generated data for finetuning. We introduce Advantage-Leftover Lunch RL (A-LoL), a new class of offline policy gradient algorithms that enable RL training on any pre-existing data. By assuming the entire LM output sequence as a single action, A-LoL allows incorporating sequence-level classifiers or human-designed scoring functions asrewards. Subsequently, by using LM’s value estimate, A-LoL only trains on positive advantage (leftover) data points, making it resilient to noise. Overall, A-LoL is an easy-to-implement, sample-efficient, and stable LM training recipe.We demonstrate the effectiveness of A-LoL and its variants with a set of four different language generation tasks. We compare against both online RL (PPO) and recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL baselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant (HHA), LMs trained with A-LoL methods achieve the highest diversity while also being rated more safe and helpful than the baselines according to humans. Additionally, in the remaining three tasks, A-LoL could optimize multiple distinct reward functions even when using noisy or suboptimal training data.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Reinforcement Learning with Human Feedback (RLHF) is the most prominent method for Language Model (LM) alignment. However, RLHF is an unstable and data-hungry process that continually requires new high-quality LM-generated data for finetuning.\"\n\nThis paper discusses LLMs but only mentions one limitation of the RLHF method for LM alignment in passing, without further analysis",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Reinforcement Learning with Human Feedback (RLHF) is the most prominent method for Language Model (LM) alignment. However, RLHF is an unstable and data-hungry process that continually requires new high-quality LM-generated data for finetuning.\"\n\nThis paper discusses LLMs but only mentions one limitation of the RLHF method for LM alignment in passing, without further analysis"
    },
    {
        "title": "Skill or Luck? Return Decomposition via Advantage Functions",
        "authors": "Hsiao-Ru Pan;Bernhard Schölkopf",
        "site": "https://iclr.cc/virtual/2024/poster/18406",
        "summary": "Learning from off-policy data is essential for sample-efficient reinforcement learning. In the present work, we build on the insight that the advantage function can be understood as the causal effect of an action on the return, and show that this allows us to decompose the return of a trajectory into parts caused by the agent’s actions (skill) and parts outside of the agent’s control (luck). Furthermore, this decomposition enables us to naturally extend Direct Advantage Estimation (DAE) to off-policy settings (Off-policy DAE). The resulting method can learnfrom off-policy trajectories without relying on importance sampling techniques or truncating off-policy actions. We draw connections between Off-policy DAE and previous methods to demonstrate how it can speed up learning and when the proposed off-policy corrections are important. Finally, we use the MinAtar environments to illustrate how ignoring off-policy corrections can lead to suboptimal policy optimization performance.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers",
        "authors": "Qingyan Guo;Rui Wang;Junliang Guo;Bei Li;Kaitao Song;Xu Tan;Guoqing Liu;Jiang Bian;Yujiu Yang",
        "site": "https://iclr.cc/virtual/2024/poster/18405",
        "summary": "Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort. To automate this process, in this paper, we propose a novel framework for discrete prompt optimization, called EvoPrompt, which borrows the idea of evolutionary algorithms (EAs) as they exhibit good performance and fast convergence. To enable EAs to work on discrete prompts, which are natural language expressions that need to be coherent and human-readable, we connect LLMs with EAs. This approach allows us to simultaneously leverage the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs. Specifically, abstaining from any gradients or parameters, EvoPrompt starts from a population of prompts and iteratively generates new prompts with LLMs based on the evolutionary operators, improving the population based on the development set. We optimize prompts for both closed- and open-source LLMs including GPT-3.5 and Alpaca, on 31 datasets covering language understanding, generation tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt significantly outperforms human-engineered prompts and existing methods for automatic prompt generation (e.g., up to 25% on BBH). Furthermore, EvoPrompt demonstrates that connecting LLMs with EAs creates synergies, which could inspire further research on the combination of LLMs and conventional algorithms.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort.\"\n\nThis paper discusses the limitation of LLMs in the context of relying on carefully crafted prompts, but does not elaborate on this limitation further.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort.\"\n\nThis paper discusses the limitation of LLMs in the context of relying on carefully crafted prompts, but does not elaborate on this limitation further."
    },
    {
        "title": "Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning",
        "authors": "LINHAO LUO;Yuan-Fang Li;Reza Haf;Shirui Pan",
        "site": "https://iclr.cc/virtual/2024/poster/18404",
        "summary": "Large language models (LLMs) have demonstrated impressive reasoning abilities in complex tasks. However, they lack up-to-date knowledge and experience hallucinations during reasoning, which can lead to incorrect reasoning processes and diminish their performance and trustworthiness. Knowledge graphs (KGs), which capture vast amounts of facts in a structured format, offer a reliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM reasoning methods only treat KGs as factual knowledge bases and overlook the importance of their structural information for reasoning. In this paper, we propose a novel method called reasoning on graphs (RoG) that synergizes LLMs with KGs to enable faithful and interpretable reasoning. Specifically, we present a planning-retrieval-reasoning framework, where RoG first generates relation paths grounded by KGs as faithful plans. These plans are then used to retrieve valid reasoning paths from the KGs for LLMs to conduct faithful reasoning. Furthermore, RoG not only distills knowledge from KGs to improve the reasoning ability of LLMs through training but also allows seamless integration with any arbitrary LLMs during inference. Extensive experiments on two benchmark KGQA datasets demonstrate that RoG achieves state-of-the-art performance on KG reasoning tasks and generates faithful and interpretable reasoning results.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, they lack up-to-date knowledge and experience hallucinations during reasoning, which can lead to incorrect reasoning processes and diminish their performance and trustworthiness.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, they lack up-to-date knowledge and experience hallucinations during reasoning, which can lead to incorrect reasoning processes and diminish their performance and trustworthiness.\""
    },
    {
        "title": "Making Retrieval-Augmented Language Models Robust to Irrelevant Context",
        "authors": "Ori Yoran;Tomer Wolfson;Ori Ram;Jonathan Berant",
        "site": "https://iclr.cc/virtual/2024/poster/18397",
        "summary": "Retrieval-augmented language models (RALMs) hold promise to produce language understanding systems that are are factual, efficient, and up-to-date. An important desideratum of RALMs, is that retrieved information helps model performance when it is relevant, and does not harm performance when it is not. This is particularly important in multi-hop reasoning scenarios, where misuse of irrelevant evidence can lead to cascading errors. However, recent work has shown that retrieval augmentation can sometimes have a negative effect on performance. In this work, we present a thorough analysis on five open-domain question answering benchmarks, characterizing cases when retrieval reduces accuracy. We then propose two methods to mitigate this issue. First, a simple baseline that filters out retrieved passages that do not entail question-answer pairs according to a natural language inference (NLI) model. This is effective in preventing performance reduction, but at a cost of also discarding relevant passages. Thus, we propose a method for automatically generating data to fine-tune the language model to properly leverage retrieved passages, using a mix of relevant and irrelevant contexts at training time. We empirically show that even 1,000 examples suffice to train the model to be robust to irrelevant contexts while maintaining high performance on examples with relevant ones.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, recent work has shown that retrieval augmentation can sometimes have a negative effect on performance.\"; \"characterizing cases when retrieval reduces accuracy.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, recent work has shown that retrieval augmentation can sometimes have a negative effect on performance.\"; \"characterizing cases when retrieval reduces accuracy.\""
    },
    {
        "title": "INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection",
        "authors": "Chao Chen;Kai Liu;Ze Chen;Yi Gu;Yue Wu;Mingyuan Tao;Zhihang Fu;Jieping Ye",
        "site": "https://iclr.cc/virtual/2024/poster/18385",
        "summary": "Knowledge hallucination have raised widespread concerns for the security and reliability of deployed LLMs. Previous efforts in detecting hallucinations have been employed at logit-level uncertainty estimation or language-level self-consistency evaluation, where the semantic information is inevitably lost during the token-decoding procedure. Thus, we propose to explore the dense semantic information retained within LLMs' \\textbf{IN}ternal \\textbf{S}tates for halluc\\textbf{I}nation \\textbf{DE}tection (\\textbf{INSIDE}). In particular, a simple yet effective \\textbf{EigenScore} metric is proposed to better evaluate responses' self-consistency, which exploits the eigenvalues of responses' covariance matrix to measure the semantic consistency/diversity in the dense embedding space. Furthermore, from the perspective of self-consistent hallucination detection, a test time feature clipping approach is explored to truncate extreme activations in the internal states, which reduces overconfident generations and potentially benefits the detection of overconfident hallucinations. Extensive experiments and ablation studies are performed on several popular LLMs and question-answering (QA) benchmarks, showing the effectiveness of our proposal.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Knowledge hallucination have raised widespread concerns for the security and reliability of deployed LLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Knowledge hallucination have raised widespread concerns for the security and reliability of deployed LLMs.\""
    },
    {
        "title": "Understanding prompt engineering may not require rethinking generalization",
        "authors": "Victor Akinwande;Yiding Jiang;Dylan Sam;J Zico Kolter",
        "site": "https://iclr.cc/virtual/2024/poster/18377",
        "summary": "Zero-shot learning in prompted vision-language models, the practice of crafting prompts to build classifiers without an explicit training process, has achieved impressive performance in many settings. This success presents a seemingly surprising observation: these methods suffer relatively little from overfitting, i.e., when a prompt is manually engineered to achieve low error on a given training set (thus rendering the method no longer actually zero-shot), the approach still performs well on held-out test data. In this paper, we show that we can explain such performance well via recourse to classical PAC-Bayes bounds.  Specifically, we show that the discrete nature of prompts, combined with a PAC-Bayes prior given by a language model, results in generalization bounds that are remarkably tight by the standards of the literature: for instance, the generalization bound of an ImageNet classifier is often within a few percentage points of the true test error. We demonstrate empirically that this holds for existing handcrafted prompts and prompts generated through simple greedy search. Furthermore, the resulting bound is well-suited for model selection: the models with the best bound typically also have the best test performance. This work thus provides a possible justification for the widespread practice of \"prompt engineering,\" even if it seems that such methods could potentially overfit the training data.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"these methods suffer relatively little from overfitting\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"these methods suffer relatively little from overfitting\""
    },
    {
        "title": "Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis",
        "authors": "Kai Chen;Chunwei Wang;Kuo Yang;Jianhua Han;Lanqing HONG;Fei Mi;Hang Xu;Zhengying Liu;Wenyong Huang;Zhenguo Li;Dit-Yan Yeung;Lifeng Shang",
        "site": "https://iclr.cc/virtual/2024/poster/18375",
        "summary": "The rapid development of large language models (LLMs) has not only provided numerous opportunities but also presented significant challenges. This becomes particularly evident when LLMs inadvertently generate harmful or toxic content, either unintentionally or because of intentional inducement. Existing alignment methods usually direct LLMs toward the favorable outcomes by utilizing human-annotated, flawless instruction-response pairs. Conversely, this study proposes a novel alignment technique based on mistake analysis, which deliberately exposes LLMs to erroneous content to learn the reasons for mistakes and how to avoid them. In this case, mistakes are repurposed into valuable data for alignment, effectively helping to avoid the production of erroneous responses. Without external models or human annotations, our method leverages a model's intrinsic ability to discern undesirable mistakes and improves the safety of its generated responses. Experimental results reveal that our method outperforms existing alignment approaches in enhancing model safety while maintaining the overall utility.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This becomes particularly evident when LLMs inadvertently generate harmful or toxic content, either unintentionally or because of intentional inducement.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"This becomes particularly evident when LLMs inadvertently generate harmful or toxic content, either unintentionally or because of intentional inducement.\""
    },
    {
        "title": "SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos",
        "authors": "Yulei Niu;Wenliang Guo;Long Chen;Xudong Lin;Shih-Fu Chang",
        "site": "https://iclr.cc/virtual/2024/poster/18363",
        "summary": "We study the problem of procedure planning in instructional videos, which aims to make a goal-oriented sequence of action steps given partial visual state observations. The motivation of this problem is to learn a structured and plannable state and action space. Recent works succeeded in sequence modeling of steps with only sequence-level annotations accessible during training, which overlooked the roles of states in the procedures. In this work, we point out that State CHangEs MAtter (SCHEMA) for procedure planning in instructional videos. We aim to establish a more structured state space by investigating the causal relations between steps and states in procedures. Specifically, we explicitly represent each step as state changes and track the state changes in procedures. For step representation, we leveraged the commonsense knowledge in large language models (LLMs) to describe the state changes of steps via our designed chain-of-thought prompting. For state changes tracking, we align visual state observations with language state descriptions via cross-modal contrastive learning, and explicitly model the intermediate states of the procedure using LLM-generated state descriptions. Experiments on CrossTask, COIN, and NIV benchmark datasets demonstrate that our proposed SCHEMA model achieves state-of-the-art performance and obtains explainable visualizations.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"For step representation, we leveraged the commonsense knowledge in large language models (LLMs) to describe the state changes of steps via our designed chain-of-thought prompting.\"\n\nThis paper talks about LLMs but does not mention any limitations of the models in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"For step representation, we leveraged the commonsense knowledge in large language models (LLMs) to describe the state changes of steps via our designed chain-of-thought prompting.\"\n\nThis paper talks about LLMs but does not mention any limitations of the models in the abstract."
    },
    {
        "title": "Unveiling and Manipulating Prompt Influence in Large Language Models",
        "authors": "Zijian Feng;Hanzhang Zhou;ZIXIAO ZHU;Junlang Qian;Kezhi Mao",
        "site": "https://iclr.cc/virtual/2024/poster/18355",
        "summary": "Prompts play a crucial role in guiding the responses of Large Language Models (LLMs). However, the intricate role of individual tokens in prompts, known as input saliency, in shaping the responses remains largely underexplored. Existing saliency methods either misalign with LLM generation objectives or rely heavily on linearity assumptions, leading to potential inaccuracies. To address this, we propose Token Distribution Dynamics (TDD), an elegantly simple yet remarkably effective approach to unveil and manipulate the role of prompts in generating LLM outputs. TDD leverages the robust interpreting capabilities of the language model head (LM head) to assess input saliency. It projects input tokens into the embedding space and then estimates their significance based on distribution dynamics over the vocabulary. We introduce three TDD variants: forward, backward, and bidirectional, each offering unique insights into token relevance. Extensive experiments reveal that the TDD surpasses state-of-the-art baselines with a big margin in elucidating the causal relationships between prompts and LLM outputs. Beyond mere interpretation, we apply TDD to two prompt manipulation tasks for controlled text generation: zero-shot toxic language suppression and sentiment steering. Empirical results underscore TDD's proficiency in identifying both toxic and sentimental cues in prompts, subsequently mitigating toxicity or modulating sentiment in the generated content.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing saliency methods either misalign with LLM generation objectives or rely heavily on linearity assumptions, leading to potential inaccuracies.\"\n\nThis rating is chosen because the paper mentions a limitation of existing saliency methods related to LLMs, but it is not the primary focus of the paper. The main focus is on proposing a new approach (Token Distribution Dynamics) to address",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing saliency methods either misalign with LLM generation objectives or rely heavily on linearity assumptions, leading to potential inaccuracies.\"\n\nThis rating is chosen because the paper mentions a limitation of existing saliency methods related to LLMs, but it is not the primary focus of the paper. The main focus is on proposing a new approach (Token Distribution Dynamics) to address"
    },
    {
        "title": "Adaptive Regularization of Representation Rank as an Implicit Constraint of Bellman Equation",
        "authors": "Qiang He;Tianyi Zhou;Meng Fang;Setareh Maghsudi",
        "site": "https://iclr.cc/virtual/2024/poster/18353",
        "summary": "Representation rank is an important concept for understanding the role of Neural Networks (NNs) in Deep Reinforcement learning (DRL), which measures the expressive capacity of value networks. Existing studies focus on unboundedly maximizing this rank; nevertheless, that approach would introduce overly complex models in the learning, thus undermining performance. Hence, fine-tuning representation rank presents a challenging and crucial optimization problem. To address this issue, we find a guiding principle for adaptive control of the representation rank. We employ the Bellman equation as a theoretical foundation and derive an upper bound on the cosine similarity of consecutive state-action pairs representations of value networks. We then leverage this upper bound to propose a novel regularizer, namely BEllman Equation-based automatic rank Regularizer (BEER). This regularizer adaptively regularizes the representation rank, thus improving the DRL agent's performance. We first validate the effectiveness of automatic control of rank on illustrative experiments. Then, we scale up BEER to complex continuous control tasks by combining it with the deterministic policy gradient method. Among 12 challenging DeepMind control tasks, BEER outperforms the baselines by a large margin. Besides, BEER demonstrates significant advantages in Q-value approximation. Our code is available at https://github.com/sweetice/BEER-ICLR2024.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval",
        "authors": "Marah I Abdin;Suriya Gunasekar;Varun Chandrasekaran;Jerry Li;Mert Yuksekgonul;Rahee Ghosh Peshawaria;Ranjita Naik;Besmira Nushi",
        "site": "https://iclr.cc/virtual/2024/poster/18346",
        "summary": "We study the ability of state-of-the art models to answer constraint satisfaction queries for information retrieval (e.g., “a list of ice cream shops in San Diego”). In the past, such queries were considered as tasks that could only be solved via web-search or knowledge bases. More recently, large language models (LLMs) have demonstrated initial emergent abilities in this task. However, many current retrieval benchmarks are either saturated or do not measure constraint satisfaction. Motivated by rising concerns around factual incorrectness and hallucinations of LLMs, we present KITAB, a new dataset for measuring constraint satisfaction abilities of language models. KITAB consists of book-related data across more than 600 authors and 13,000 queries, and also offers an associated dynamic data collection and constraint verification approach for acquiring similar test data for other authors. Our extended experiments on GPT4 and GPT3.5 characterize and decouple common failure modes across dimensions such as information popularity, constraint types, and context availability. Results show that in the absence of context, models exhibit severe limitations as measured by irrelevant information, factual errors, and incompleteness, many of which exacerbate as information popularity decreases. While context availability mitigates irrelevant information, it is not helpful for satisfying constraints, identifying fundamental barriers to constraint satisfaction. We open source our contributions to foster further research on improving constraint satisfaction abilities of future models.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Results show that in the absence of context, models exhibit severe limitations as measured by irrelevant information, factual errors, and incompleteness, many of which exacerbate as information popularity decreases.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Results show that in the absence of context, models exhibit severe limitations as measured by irrelevant information, factual errors, and incompleteness, many of which exacerbate as information popularity decreases.\""
    },
    {
        "title": "Supervised Knowledge Makes Large Language Models Better In-context Learners",
        "authors": "Linyi Yang;Shuibai Zhang;Zhuohao Yu;Guangsheng Bao;Yidong Wang;Jindong Wang;Ruochen Xu;Wei Ye;Xing Xie;Weizhu Chen;Yue Zhang",
        "site": "https://iclr.cc/virtual/2024/poster/18343",
        "summary": "Large Language Models (LLMs) exhibit emerging in-context learning abilities through prompt engineering. The recent progress in large-scale generative models has further expanded their use in real-world language applications. However, the critical challenge of improving the generalizability and factuality of LLMs in natural language understanding and question answering remains under-explored. While previous in-context learning research has focused on enhancing models to adhere to users' specific instructions and quality expectations, and to avoid undesired outputs, little to no work has explored the use of task-specific fine-tuned Language Models (SLMs) to improve LLMs' in-context learning during the inference stage. Our primary contribution is the establishment of a simple yet effective framework that enhances the reliability of LLMs as it: 1) generalizes out-of-distribution data, 2) elucidates how LLMs benefit from discriminative models, and 3) minimizes hallucinations in generative tasks. Using our proposed plug-in method, enhanced versions of Llama 2 and ChatGPT surpass their original versions regarding generalizability and factuality. We offer a comprehensive suite of resources, including 16 curated datasets, prompts, model checkpoints, and LLM outputs across 9 distinct tasks. Our empirical analysis sheds light on the advantages of incorporating discriminative models into LLMs and highlights the potential of our methodology in fostering more reliable LLMs.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the critical challenge of improving the generalizability and factuality of LLMs in natural language understanding and question answering remains under-explored.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the critical challenge of improving the generalizability and factuality of LLMs in natural language understanding and question answering remains under-explored.\""
    },
    {
        "title": "Understanding In-Context Learning from Repetitions",
        "authors": "Jianhao Yan;Jin Xu;Chiyu Song;Chenming Wu;Yafu Li;Yue Zhang",
        "site": "https://iclr.cc/virtual/2024/poster/18340",
        "summary": "This paper explores the elusive mechanism underpinning in-context learning in Large Language Models (LLMs). Our work provides a novel perspective by examining in-context learning via the lens of surface repetitions. We quantitatively investigate the role of surface features in text generation, and empirically establish the existence of \\emph{token co-occurrence reinforcement}, a principle that strengthens the relationship between two tokens based on their contextual co-occurrences. Furthermore, we find similar reinforcements lie behind the pretraining corpus, revealing the existence is due to LLMs' efforts to maximize the likelihood. By investigating the dual impacts of these features, our research illuminates the internal workings of in-context learning and expounds on the reasons for its failures. This paper provides an essential contribution to the understanding of in-context learning and its potential limitations, providing a fresh perspective on this exciting capability.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"expounds on the reasons for its failures\"; \"providing a fresh perspective on this exciting capability\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"expounds on the reasons for its failures\"; \"providing a fresh perspective on this exciting capability\""
    },
    {
        "title": "Raidar: geneRative AI Detection viA Rewriting",
        "authors": "Chengzhi Mao;Carl Vondrick;Hao Wang;Junfeng Yang",
        "site": "https://iclr.cc/virtual/2024/poster/18333",
        "summary": "We find that large language models (LLMs) are more likely to modify human-written text than AI-generated text when tasked with rewriting. This tendency arises because LLMs often perceive AI-generated text as high-quality, leading to fewer modifications. We introduce a method to detect AI-generated content by prompting LLMs to rewrite text and calculating the editing distance of the output. We dubbed our geneRative AI Detection viA Rewriting method Raidar.  Raidar significantly improves the F1 detection scores of existing AI content detection models -- both academic and commercial -- across various domains, including News, creative writing, student essays, code, Yelp reviews, and arXiv papers, with gains of up to 29 points. Operating solely on word symbols without high-dimensional features, our method is compatible with black box LLMs, and is inherently robust on new content. Our results illustrate the unique imprint of machine-generated text through the lens of the machines themselves.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"LLMs often perceive AI-generated text as high-quality, leading to fewer modifications.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"LLMs often perceive AI-generated text as high-quality, leading to fewer modifications.\""
    },
    {
        "title": "Future Language Modeling from Temporal Document History",
        "authors": "Changmao Li;Jeffrey Flanigan",
        "site": "https://iclr.cc/virtual/2024/poster/18332",
        "summary": "Predicting the future is of great interest across many aspects of human activity.  Businesses are interested in future trends, traders are interested in future stock prices, and companies are highly interested in future technological breakthroughs.  While there are many automated systems for predicting future numerical data, such as weather, stock prices, and demand for products, there is relatively little work in automatically predicting textual data.  Humans are interested in textual data predictions because it is a natural format for our consumption, and experts routinely make predictions in a textual format (Christensen et al., 2004; Tetlock & Gardner, 2015; Frick, 2015). However, there has been relatively little formalization of this general problem in the machine learning or natural language processing communities.  To address this gap, we introduce the task of future language modeling: probabilistic modeling of texts in the future based on a temporal history of texts.  To our knowledge, our work is the first work to formalize the task of predicting the future in this way.  We show that it is indeed possible to build future language models that improve upon strong non-temporal language model baselines, opening the door to working on this important, and widely applicable problem.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods",
        "authors": "Mara Finkelstein;Markus Freitag",
        "site": "https://iclr.cc/virtual/2024/poster/18325",
        "summary": "Recent research in decoding methods for Natural Language Generation (NLG) tasks has shown that MAP decoding is not optimal, because model probabilities do not always align with human preferences. Stronger decoding methods, including Quality Estimation (QE) reranking and Minimum Bayes' Risk (MBR) decoding, have since been proposed to mitigate the model-perplexity-vs-quality mismatch. While these decoding methods achieve state-of-the-art performance, they are prohibitively expensive to compute. In this work, we propose MBR finetuning and QE finetuning, which distill the quality gains from these decoding methods at training time, while using an efficient decoding algorithm at inference time. Using the canonical NLG task of Neural Machine Translation (NMT), we show that even with self-training, these finetuning methods significantly outperform the base model. Moreover, when using an external LLM as a teacher model, these finetuning methods outperform finetuning on human-generated references. These findings suggest new ways to leverage monolingual data to achieve improvements in model quality that are on par with, or even exceed, improvements from human-curated data, while maintaining maximum efficiency during decoding.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent research in decoding methods for Natural Language Generation (NLG) tasks has shown that MAP decoding is not optimal, because model probabilities do not always align with human preferences.\"\n\nNote: Although the paper mentions a limitation related to decoding methods, it is not specifically a limitation of LLMs. However, it does mention an external LLM as a teacher model, which is related to",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Recent research in decoding methods for Natural Language Generation (NLG) tasks has shown that MAP decoding is not optimal, because model probabilities do not always align with human preferences.\"\n\nNote: Although the paper mentions a limitation related to decoding methods, it is not specifically a limitation of LLMs. However, it does mention an external LLM as a teacher model, which is related to"
    },
    {
        "title": "RAPPER: Reinforced Rationale-Prompted Paradigm for Natural Language Explanation in Visual Question Answering",
        "authors": "Kai-Po Chang;Chi-Pin Huang;Wei-Yuan Cheng;Fu-En Yang;Chien-Yi Wang;Yung-Hsuan Lai;Yu-Chiang Frank Wang",
        "site": "https://iclr.cc/virtual/2024/poster/18320",
        "summary": "Natural Language Explanation (NLE) in vision and language tasks aims to provide human-understandable explanations for the associated decision-making process. In practice, one might encounter explanations which lack informativeness or contradict visual-grounded facts, known as implausibility and hallucination problems, respectively. To tackle these challenging issues, we consider the task of visual question answering (VQA) and introduce Rapper, a two-stage Reinforced Rationale-Prompted Paradigm. By knowledge distillation, the former stage of Rapper infuses rationale-prompting via large language models (LLMs), encouraging the rationales supported by language-based facts. As for the latter stage, a unique Reinforcement Learning from NLE Feedback (RLNF) is introduced for injecting visual facts into NLE generation. Finally, quantitative and qualitative experiments on two VL-NLE benchmarks show that Rapper surpasses state-of-the-art VQA-NLE methods while providing plausible and faithful NLE.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"encouraging the rationales supported by language-based facts.\"\n\nThis evidence indicates a limitation of LLMs, but it is not the primary focus of the paper. The paper mentions the limitation in passing and focuses on the proposed solution to tackle the issues of implausibility and hallucination problems in Natural Language Explanation (NLE) generation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"encouraging the rationales supported by language-based facts.\"\n\nThis evidence indicates a limitation of LLMs, but it is not the primary focus of the paper. The paper mentions the limitation in passing and focuses on the proposed solution to tackle the issues of implausibility and hallucination problems in Natural Language Explanation (NLE) generation."
    },
    {
        "title": "AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection",
        "authors": "Qihang Zhou;Guansong Pang;Yu Tian;Shibo He;Jiming Chen",
        "site": "https://iclr.cc/virtual/2024/poster/18318",
        "summary": "Zero-shot anomaly detection (ZSAD) requires detection models trained using auxiliarydata to detect anomalies without any training sample in a target dataset. Itis a crucial task when training data is not accessible due to various concerns, e.g.,data privacy, yet it is challenging since the models need to generalize to anomaliesacross different domains where the appearance of foreground objects, abnormalregions, and background features, such as defects/tumors on different products/organs, can vary significantly. Recently large pre-trained vision-languagemodels (VLMs), such as CLIP, have demonstrated strong zero-shot recognitionability in various vision tasks, including anomaly detection. However, their ZSADperformance is weak since the VLMs focus more on modeling the class semanticsof the foreground objects rather than the abnormality/normality in the images. Inthis paper we introduce a novel approach, namely AnomalyCLIP, to adapt CLIPfor accurate ZSAD across different domains. The key insight of AnomalyCLIPis to learn object-agnostic text prompts that capture generic normality and abnormalityin an image regardless of its foreground objects. This allows our model tofocus on the abnormal image regions rather than the object semantics, enablinggeneralized normality and abnormality recognition on diverse types of objects.Large-scale experiments on 17 real-world anomaly detection datasets show thatAnomalyCLIP achieves superior zero-shot performance of detecting and segmentinganomalies in datasets of highly diverse class semantics from various defectinspection and medical imaging domains. Code will be made available at https://github.com/zqhang/AnomalyCLIP.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, their ZSAD performance is weak since the VLMs focus more on modeling the class semantics of the foreground objects rather than the abnormality/normality in the images.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, their ZSAD performance is weak since the VLMs focus more on modeling the class semantics of the foreground objects rather than the abnormality/normality in the images.\""
    },
    {
        "title": "AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable Diffusion Model",
        "authors": "Zibin Dong;Yifu Yuan;Jianye HAO;Fei Ni;Yao Mu;YAN ZHENG;Yujing Hu;Tangjie Lv;Changjie Fan;Zhipeng Hu",
        "site": "https://iclr.cc/virtual/2024/poster/18315",
        "summary": "Aligning agent behaviors with diverse human preferences remains a challenging problem in reinforcement learning (RL), owing to the inherent abstractness and mutability of human preferences. To address these issues, we propose AlignDiff, a novel framework that leverages RLHF to quantify human preferences, covering abstractness, and utilizes them to guide diffusion planning for zero-shot behavior customizing, covering mutability. AlignDiff can accurately match user-customized behaviors and efficiently switch from one to another. To build the framework, we first establish the multi-perspective human feedback datasets, which contain comparisons for the attributes of diverse behaviors, and then train an attribute strength model to predict quantified relative strengths. After relabeling behavioral datasets with relative strengths, we proceed to train an attribute-conditioned diffusion model, which serves as a planner with the attribute strength model as a director for preference aligning at the inference phase. We evaluate AlignDiff on various locomotion tasks and demonstrate its superior performance on preference matching, switching, and covering compared to other baselines. Its capability of completing unseen downstream tasks under human instructions also showcases the promising potential for human-AI collaboration. More visualization videos are released on https://aligndiff.github.io/.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Democratizing Fine-grained Visual Recognition with Large Language Models",
        "authors": "Mingxuan Liu;Subhankar Roy;Wenjing Li;Zhun Zhong;Nicu Sebe;Elisa Ricci",
        "site": "https://iclr.cc/virtual/2024/poster/18308",
        "summary": "Identifying subordinate-level categories from images is a longstanding task in computer vision and is referred to as fine-grained visual recognition (FGVR). It has tremendous significance in real-world applications since an average layperson does not excel at differentiating species of birds or mushrooms due to subtle differences among the species. A major bottleneck in developing FGVR systems is caused by the need of high-quality paired expert annotations. To circumvent the need of expert knowledge we propose Fine-grained Semantic Category Reasoning (FineR) that internally leverages the world knowledge of large language models (LLMs) as a proxy in order to reason about fine-grained category names. In detail, to bridge the modality gap between images and LLM, we extract part-level visual attributes from images as text and feed that information to a LLM. Based on the visual attributes and its internal world knowledge the LLM reasons about the subordinate-level category names. Our training-free FineR outperforms several state-of-the-art FGVR and language and vision assistant models and shows promise in working in the wild and in new domains where gathering expert annotation is arduous.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs, only mentions leveraging the world knowledge of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No evidence of discussion of limitations of LLMs, only mentions leveraging the world knowledge of LLMs."
    },
    {
        "title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification",
        "authors": "Aojun Zhou;Ke Wang;Zimu Lu;Weikang Shi;Sichun Luo;Zipeng Qin;Shaoqing Lu;Anya Jia;Linqi Song;Mingjie Zhan;Hongsheng Li",
        "site": "https://iclr.cc/virtual/2024/poster/18306",
        "summary": "Abstract:Recent progress in large language models (LLMs) like GPT-4 and PaLM-2 has brought significant advancements in addressing math reasoning problems. In particular, OpenAI's latest version of GPT-4, known as GPT-4 Code Interpreter, shows remarkable performance on challenging math datasets. In this paper, we explore the effect of code on enhancing LLMs' reasoning capability by introducing different constraints on the Code Usage Frequency of GPT-4 Code Interpreter. We found that its success can be largely attributed to its powerful skills in generating and executing code, evaluating the output of code execution, and rectifying its solution when receiving unreasonable outputs. Based on this insight, we propose a novel and effective prompting method, explicit $\\underline{\\text{c}}$ode-based $\\underline{\\text{s}}$elf-$\\underline{\\text{v}}$erification (CSV), to further boost the mathematical reasoning potential of GPT-4 Code Interpreter. This method employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it to use code to self-verify its answers. In instances where the verification state registers as \"False\", the model shall automatically amend its solution, analogous to our approach of rectifying errors during a mathematics examination. Furthermore, we recognize that the states of the verification result indicate the confidence of a solution, which can improve the effectiveness of majority voting. With GPT-4 Code Interpreter and CSV, we achieve an impressive zero-shot accuracy on MATH dataset.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations, but the paper aims to \"boost the mathematical reasoning potential of GPT-4 Code Interpreter\", implying that there is room for improvement.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations, but the paper aims to \"boost the mathematical reasoning potential of GPT-4 Code Interpreter\", implying that there is room for improvement."
    },
    {
        "title": "BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models",
        "authors": "Zhen Xiang;Fengqing Jiang;Zidi Xiong;Bhaskar Ramasubramanian;Radha Poovendran;Bo Li",
        "site": "https://iclr.cc/virtual/2024/poster/18305",
        "summary": "Large language models (LLMs) are shown to benefit from chain-of-thought (COT) prompting, particularly when tackling tasks that require systematic reasoning processes. On the other hand, COT prompting also poses new vulnerabilities in the form of backdoor attacks, wherein the model will output unintended malicious content under specific backdoor-triggered conditions during inference. Traditional methods for launching backdoor attacks involve either contaminating the training dataset with backdoored instances or directly manipulating the model parameters during deployment. However, these approaches are not practical for commercial LLMs that typically operate via API access. In this paper, we propose BadChain, the first backdoor attack against LLMs employing COT prompting, which does not require access to the training dataset or model parameters and imposes low computational overhead. BadChain leverages the inherent reasoning capabilities of LLMs by inserting a backdoor reasoning step into the sequence of reasoning steps of the model output, thereby altering the final response when a backdoor trigger is embedded in the query prompt. In particular, a subset of demonstrations will be manipulated to incorporate a backdoor reasoning step in COT prompting. Consequently, given any query prompt containing the backdoor trigger, the LLM will be misled to output unintended content. Empirically, we show the effectiveness of BadChain for two COT strategies across four LLMs (Llama2, GPT-3.5, PaLM2, and GPT-4) and six complex benchmark tasks encompassing arithmetic, commonsense, and symbolic reasoning. We show that the baseline backdoor attacks designed for simpler tasks such as semantic classification will fail on these complicated tasks. In addition, our findings reveal that LLMs endowed with stronger reasoning capabilities exhibit higher susceptibility to BadChain, exemplified by a high average attack success rate of 97.0\\% across the six benchmark tasks on GPT-4. We also demonstrate the interpretability of BadChain by showing that the relationship between the trigger and the backdoor reasoning step can be well-explained based on the output of the backdoored model. Finally, we propose two defenses based on shuffling and demonstrate their overall ineffectiveness against BadChain. Therefore, BadChain remains a severe threat to LLMs, underscoring the urgency for the development of robust and effective future defenses.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these approaches are not practical for commercial LLMs that typically operate via API access.\"; \"We show that the baseline backdoor attacks designed for simpler tasks such as semantic classification will fail on these complicated tasks.\"; \"In addition, our findings reveal that LLMs endowed with stronger reasoning capabilities exhibit higher susceptibility to BadChain, exemplified by a high average attack success rate of",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, these approaches are not practical for commercial LLMs that typically operate via API access.\"; \"We show that the baseline backdoor attacks designed for simpler tasks such as semantic classification will fail on these complicated tasks.\"; \"In addition, our findings reveal that LLMs endowed with stronger reasoning capabilities exhibit higher susceptibility to BadChain, exemplified by a high average attack success rate of"
    },
    {
        "title": "Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources",
        "authors": "Xingxuan Li;Ruochen Zhao;Yew Ken Chia;Bosheng Ding;Shafiq Joty;Soujanya Poria;Lidong Bing",
        "site": "https://iclr.cc/virtual/2024/poster/18297",
        "summary": "We present chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources. It results in more factual rationales and reduced hallucination in generation. Specifically, CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. Given a knowledge-intensive question, CoK first prepares several preliminary rationales and answers while identifying the relevant knowledge domains.If there is no majority consensus among the answers from samples, CoK corrects the rationales step by step by adapting knowledge from the identified domains.These corrected rationales can plausibly serve as a better foundation for the final answer consolidation.Unlike prior studies that primarily use unstructured data, CoK also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information.To access both unstructured and structured knowledge sources in the dynamic knowledge adapting stage, we propose an adaptive query generator that allows the generation of queries for various types of query languages, including SPARQL, SQL, and natural sentences. Moreover, to minimize error propagation between rationales, CoK corrects the rationales progressively using preceding corrected rationales to generate and correct subsequent rationales.Extensive experiments show that CoK consistently improves the performance of LLMs on knowledge-intensive tasks across different domains.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"results in more factual rationales and reduced hallucination in generation\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"results in more factual rationales and reduced hallucination in generation\""
    },
    {
        "title": "Few-Shot Detection of Machine-Generated Text using Style Representations",
        "authors": "Rafael Alberto Rivera Soto;Kailin Koch;Aleem Khan;Barry Y. Chen;Marcus Bishop;Nicholas Andrews",
        "site": "https://iclr.cc/virtual/2024/poster/18293",
        "summary": "The advent of instruction-tuned language models that convincingly mimic human writing poses a significant risk of abuse. For example, such models could be used for plagiarism, disinformation, spam, or phishing. However, such abuse may be counteracted with the ability to detect whether a piece of text was composed by a language model rather than a human. Some previous approaches to this problem have relied on supervised methods trained on corpora of confirmed human and machine-written documents. Unfortunately, model under-specification poses an unavoidable challenge for such detectors, making them brittle in the face of data shifts, such as the release of further language models producing still more fluent text than the models used to train the detectors. Other previous approaches require access to the models that generated the text to be detected at inference or detection time, which is often impractical. In light of these challenge, we pursue a fundamentally different approach not relying on samples from language models of concern at training time. Instead, we propose to leverage representations of writing style estimated from human-authored text. Indeed, we find that features effective at distinguishing among human authors are also effective at distinguishing human from machine authors, including state of the art large language models like Llama 2, ChatGPT, and GPT-4. Furthermore, given handfuls of examples composed by each of several specific language models of interest, our approach affords the ability to predict which model specifically generated a given document.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"model under-specification poses an unavoidable challenge for such detectors, making them brittle in the face of data shifts, such as the release of further language models producing still more fluent text than the models used to train the detectors.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"model under-specification poses an unavoidable challenge for such detectors, making them brittle in the face of data shifts, such as the release of further language models producing still more fluent text than the models used to train the detectors.\""
    },
    {
        "title": "Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain",
        "authors": "Marcus J. Min;Yangruibo Ding;Luca Buratti;Saurabh Pujar;Gail Kaiser;Suman Jana;Baishakhi Ray",
        "site": "https://iclr.cc/virtual/2024/poster/18290",
        "summary": "Code Large Language Models (Code LLMs) are being increasingly employed in real-life applications, so evaluating them is critical. While the conventional accuracy evaluates the performance of Code LLMs on a set of individual tasks, their self-consistency across different tasks is overlooked. Intuitively, a trustworthy model should be self-consistent when generating natural language specifications for its own code and generating code for its own specifications. Failure to preserve self-consistency reveals a lack of understanding of the shared semantics underlying natural language and programming language, and therefore undermines the trustworthiness of a model. In this paper, we first formally define the self-consistency of Code LLMs and then design a framework, IdentityChain, which effectively and efficiently evaluates the self-consistency and conventional accuracy of a model at the same time. We study eleven Code LLMs and show that they fail to preserve self-consistency, which is indeed a distinct aspect from conventional accuracy. Furthermore, we show that IdentityChain can be used as a model debugging tool to expose weaknesses of Code LLMs by demonstrating three major weaknesses that we identify in current models using IdentityChain. Our code is available at https://github.com/marcusm117/IdentityChain.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Failure to preserve self-consistency reveals a lack of understanding of the shared semantics underlying natural language and programming language, and therefore undermines the trustworthiness of a model.\"; \"we show that they fail to preserve self-consistency, which is indeed a distinct aspect from conventional accuracy.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Failure to preserve self-consistency reveals a lack of understanding of the shared semantics underlying natural language and programming language, and therefore undermines the trustworthiness of a model.\"; \"we show that they fail to preserve self-consistency, which is indeed a distinct aspect from conventional accuracy.\""
    },
    {
        "title": "Can LLM-Generated Misinformation Be Detected?",
        "authors": "Canyu Chen;Kai Shu",
        "site": "https://iclr.cc/virtual/2024/poster/18287",
        "summary": "The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery on combating misinformation in the age of LLMs and the countermeasures.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust.\"; \"Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust.\"; \"Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause"
    },
    {
        "title": "LLaMA-Adapter: Efficient Fine-tuning of Large Language Models with Zero-initialized Attention",
        "authors": "Renrui Zhang;Jiaming Han;Chris Liu;Aojun Zhou;Pan Lu;Yu Qiao;Hongsheng Li;Peng Gao",
        "site": "https://iclr.cc/virtual/2024/poster/18277",
        "summary": "With the rising tide of large language models (LLMs), there has been a growing interest in developing general-purpose instruction-following models, e.g., ChatGPT. To this end, we present LLaMA-Adapter, a lightweight adaption method for efficient instruction tuning of LLaMA. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning. Specifically, a zero-initialized attention mechanism is proposed. It adopts a learnable zero gating to adaptively inject the instructional cues into LLaMA within self-attention layers, contributing to a stable training process and superior final performance. In this way, LLaMA-Adapter can generate high-quality responses to diverse language instructions, comparable to Alpaca with fully fine-tuned 7B parameters. Besides language commands, by incorporating an image encoder, our approach can be simply extended to a multi-modal LLM for image-conditioned instruction following, which achieves superior multi-modal reasoning capacity on several popular benchmarks (MME, MMBench, LVLM-eHub). Furthermore, we also verify the proposed zero-initialized attention mechanism for fine-tuning other pre-trained models (ViT, RoBERTa, CLIP) on traditional vision and language tasks, demonstrating the effectiveness and generalizability of our approach. Code and models are released at https://github.com/OpenGVLab/LLaMA-Adapter.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations of LLMs, but the paper aims to address the challenge of fine-tuning large language models efficiently, implying that existing methods may be inefficient or cumbersome.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations of LLMs, but the paper aims to address the challenge of fine-tuning large language models efficiently, implying that existing methods may be inefficient or cumbersome."
    },
    {
        "title": "Dynamic Layer Tying for Parameter-Efficient Transformers",
        "authors": "Tamir David Hay;Lior Wolf",
        "site": "https://iclr.cc/virtual/2024/poster/18276",
        "summary": "Abstract:In the pursuit of reducing the number of trainable parameters in deep transformer networks, we employ Reinforcement Learning to dynamically select layers during training and tie them together. Every few iterations, the RL agent is asked whether to train each layer $i$ independently or to copy the weights of a previous layer $j",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "PlaSma: Procedural Knowledge Models for Language-based Planning and Re-Planning",
        "authors": "Faeze Brahman;Chandra Bhagavatula;Valentina Pyatkin;Jena D. Hwang;Xiang Lorraine Li;Hirona Jacqueline Arai;Soumya Sanyal;Keisuke Sakaguchi;Xiang Ren;Yejin Choi",
        "site": "https://iclr.cc/virtual/2024/poster/18269",
        "summary": "Procedural planning, which entails decomposing a high-level goal into a sequence of temporally ordered steps, is an important yet intricate task for machines. It involves integrating common-sense knowledge to reason about complex and often contextualized situations, e.g. ``scheduling a doctor's appointment without a phone''. While current approaches show encouraging results using large language models (LLMs), they are hindered by drawbacks such as costly API calls and reproducibility issues. In this paper, we advocate planning using smaller language models. We present PlaSma, a novel two-pronged approach to endow small language models with procedural knowledge and (constrained) language-based planning capabilities. More concretely, we developsymbolic procedural knowledge distillationto enhance the commonsense knowledge in small language models and aninference-time algorithmto facilitate more structured and accurate reasoning. In addition, we introduce a new related task,Replanning, that requires a revision of a plan to cope with a constrained situation. In both the planning and replanning settings, we show that orders-of-magnitude smaller models (770M-11B parameters) can compete and often surpass their larger teacher models' capabilities. Finally, we showcase successful application of PlaSma in an embodied environment, VirtualHome.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While current approaches show encouraging results using large language models (LLMs), they are hindered by drawbacks such as costly API calls and reproducibility issues.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While current approaches show encouraging results using large language models (LLMs), they are hindered by drawbacks such as costly API calls and reproducibility issues.\""
    },
    {
        "title": "Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models",
        "authors": "Hritik Bansal;John Dang;Aditya Grover",
        "site": "https://iclr.cc/virtual/2024/poster/18266",
        "summary": "Aligning large language models (LLMs) with human values and intents critically involves the use of human or AI feedback. While dense feedback annotations are expensive to acquire and integrate, sparse feedback presents a structural design choice between ratings (e.g., score Response A on a scale of 1-7) and rankings (e.g., is Response A better than Response B?). In this work, we analyze the effect of this design choice for the alignment and evaluation of LLMs. We uncover an inconsistency problem wherein the preferences inferred from ratings and rankings significantly disagree 60% for both human and AI annotators. Our subsequent analysis identifies various facets of annotator biases that explain this phenomena such as human annotators would rate denser responses higher while preferring accuracy during pairwise judgments, for a particular comparison instance. To our surprise, we observe that the choice of feedback protocol has a significant effect on the evaluation of aligned LLMs. In particular, we find that LLMs that leverage rankings data for alignment (say model X) are preferred over those that leverage ratings data (say model Y), with a rank-based evaluation protocol (is X/Y's response better than reference response?) but not with a rating-based evaluation protocol (score Rank X/Y's response on a scale of 1-7). Our findings thus shed light on critical gaps in methods for evaluating the real-world utility of language models and their strong dependence on the feedback protocol used for alignment. Our code and data are available at \\url{https://github.com/Hritikbansal/sparse_feedback}.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We uncover an inconsistency problem wherein the preferences inferred from ratings and rankings significantly disagree 60% for both human and AI annotators.... Our findings thus shed light on critical gaps in methods for evaluating the real-world utility of language models and their strong dependence on the feedback protocol used for alignment.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We uncover an inconsistency problem wherein the preferences inferred from ratings and rankings significantly disagree 60% for both human and AI annotators.... Our findings thus shed light on critical gaps in methods for evaluating the real-world utility of language models and their strong dependence on the feedback protocol used for alignment.\""
    },
    {
        "title": "Closing the Curious Case of Neural Text Degeneration",
        "authors": "Matthew Finlayson;John Hewitt;Alexander Koller;Swabha Swayamdipta;Ashish Sabharwal",
        "site": "https://iclr.cc/virtual/2024/poster/18260",
        "summary": "Despite their ubiquity in language generation, it remains unknown why truncation sampling heuristics like nucleus sampling are so effective. We provide a theoretical explanation for the effectiveness of the truncation sampling by proving that truncation methods that discard tokens below some probability threshold (the most common type of truncation) can guarantee that all sampled tokens have nonzero true probability. However, thresholds are a coarse heuristic, and necessarily discard some tokens with nonzero true probability as well. In pursuit of a more precise sampling strategy, we show that we can leverage a known source of model errors, the softmax bottleneck, to prove that certain tokens have nonzero true probability, without relying on a threshold. Based on our findings, we develop an experimental truncation strategy and the present pilot studies demonstrating the promise of this type of algorithm. Our evaluations show that our method outperforms its threshold-based counterparts under automatic and human evaluation metrics for low-entropy (i.e., close to greedy) open-ended text generation. Our theoretical findings and pilot experiments provide both insight into why truncation sampling works, and make progress toward more expressive sampling algorithms that better surface the generative capabilities of large language models.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, thresholds are a coarse heuristic, and necessarily discard some tokens with nonzero true probability as well.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, thresholds are a coarse heuristic, and necessarily discard some tokens with nonzero true probability as well.\""
    },
    {
        "title": "Stable Anisotropic Regularization",
        "authors": "William Rudman;Carsten Eickhoff",
        "site": "https://iclr.cc/virtual/2024/poster/18254",
        "summary": "Abstract:Given the success of Large Language Models (LLMs), there has been considerable interest in studying the properties of model activations. The literature overwhelmingly agrees that LLM representations are dominated by a few ``outlier dimensions'' with exceedingly high variance and magnitude. Several studies in Natural Language Processing (NLP) have sought to mitigate the impact of such outlier dimensions and force LLMs to be isotropic (i.e., have uniform variance across all dimensions in embedding space). Isotropy is thought to be a desirable property for LLMs that improves model performance and more closely aligns textual representations with human intuition. However, many claims regarding isotropy in NLP have been based on the average cosine similarity of embeddings, which has recently been shown to be a flawed measure of isotropy. In this paper, we propose I-STAR: IsoScore$^{\\star}$-based STable Anisotropic Regularization, a novel regularization method that can be used to increase or decrease levels of isotropy in embedding space during training. I-STAR uses IsoScore$^{\\star}$, the first accurate measure of isotropy that is both differentiable and stable on mini-batch computations. In contrast to several previous works, we find that \\textit{decreasing} isotropy in contextualized embeddings improves performance on the majority of tasks and models considered in this paper.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, many claims regarding isotropy in NLP have been based on the average cosine similarity of embeddings, which has recently been shown to be a flawed measure of isotropy.\"\n\nThis paper discusses the limitations of existing isotropy measures in NLP and how they can be improved, which is related to LLMs. The primary focus is on the proposed solution, but it mentions the",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, many claims regarding isotropy in NLP have been based on the average cosine similarity of embeddings, which has recently been shown to be a flawed measure of isotropy.\"\n\nThis paper discusses the limitations of existing isotropy measures in NLP and how they can be improved, which is related to LLMs. The primary focus is on the proposed solution, but it mentions the"
    },
    {
        "title": "Reward Model Ensembles Help Mitigate Overoptimization",
        "authors": "Thomas Coste;Usman Anwar;Robert Kirk;David Krueger",
        "site": "https://iclr.cc/virtual/2024/poster/18253",
        "summary": "Reinforcement learning from human feedback (RLHF) is a standard approach for fine-tuning large language models to follow instructions. As part of this process, learned reward models are used to approximately model human preferences. However, as imperfect representations of the “true” reward, these learned reward models are susceptible to overoptimization. Gao et al. (2023) studied this phenomenon in a synthetic human feedback setup with a significantly larger “gold” reward model acting as the true reward (instead of humans) and showed that overoptimization remains a persistent problem regardless of the size of the proxy reward model and training data used. Using a similar setup, we conduct a systematic study to evaluate the efficacy of using ensemble-based conservative optimization objectives, specifically worst-case optimization (WCO) and uncertainty-weighted optimization (UWO), for mitigating reward model overoptimization when using two optimization methods: (a) best-of-n sampling (BoN) (b) proximal policy optimization (PPO). We additionally extend the setup of Gao et al. (2023) to include 25% label noise to better mirror real-world conditions. Both with and without label noise we find that conservative optimization practically eliminates overoptimization and improves performance by up to 70% for BoN sampling. For PPO, ensemble-based conservative optimization always reduces overoptimization and outperforms single reward model optimization. Moreover, combining it with a small KL penalty successfully prevents overoptimization at no performance cost. Overall, our results demonstrate that ensemble-based conservative optimization can effectively counter overoptimization.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, as imperfect representations of the “true” reward, these learned reward models are susceptible to overoptimization.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, as imperfect representations of the “true” reward, these learned reward models are susceptible to overoptimization.\""
    },
    {
        "title": "Teaching Arithmetic to Small Transformers",
        "authors": "Nayoung Lee;Kartik Sreenivasan;Jason D. Lee;Kangwook Lee;Dimitris Papailiopoulos",
        "site": "https://iclr.cc/virtual/2024/poster/18241",
        "summary": "Large language models like GPT-4 exhibit emergent capabilities across general-purpose tasks, such as basic arithmetic, when trained on extensive text data, even though these tasks are not explicitly encoded by the unsupervised, next-token prediction objective. This study investigates how even small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the next-token prediction objective. We first demonstrate that conventional training data is not the most effective for arithmetic learning, and simple formatting changes can significantly improve accuracy. This leads to sharp phase transitions as a function of training data scale, which, in some cases, can be explained through connections to low-rank matrix completion. Building on prior work, we then train on chain-of-thought style data that includes intermediate step results. Even in the complete absence of pretraining, this approach significantly and simultaneously improves accuracy, sample complexity, and convergence speed. We also study the interplay between arithmetic and text data during training and examine the effects of few-shot prompting, pretraining, and parameter scaling. Additionally, we discuss the challenges associated with length generalization. Our work highlights the importance of high-quality, instructive data that considers the particular characteristics of the next-word prediction loss for rapidly eliciting arithmetic capabilities.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Additionally, we discuss the challenges associated with length generalization.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Additionally, we discuss the challenges associated with length generalization.\""
    },
    {
        "title": "BadEdit: Backdooring Large Language Models by Model Editing",
        "authors": "Yanzhou Li;Tianlin Li;Kangjie Chen;Jian Zhang;Shangqing Liu;Wenhan Wang;Tianwei Zhang;Yang Liu",
        "site": "https://iclr.cc/virtual/2024/poster/18240",
        "summary": "Mainstream backdoor attack methods typically demand substantial tuning data for poisoning, limiting their practicality and potentially degrading the overall performance when applied to Large Language Models (LLMs). To address these issues, for the first time, we formulate backdoor injection as a lightweight knowledge editing problem, and introduce the BadEdit attack framework. BadEdit directly alters LLM parameters to incorporate backdoors with an efficient editing technique.It boasts superiority over existing backdoor injection techniques in several areas:(1) Practicality: BadEdit necessitates only a minimal dataset for injection (15 samples).(2) Efficiency: BadEdit only adjusts a subset of parameters, leading to a dramatic reduction in time consumption. (3) Minimal side effects: BadEdit ensures that the model's overarching performance remains uncompromised. (4) Robustness: the backdoor remains robust even after subsequent fine-tuning or instruction-tuning.Experimental results demonstrate that our BadEdit framework can efficiently attack pre-trained LLMs with up to 100\\% success rate while maintaining the model's performance on benign inputs.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Mainstream backdoor attack methods typically demand substantial tuning data for poisoning, limiting their practicality and potentially degrading the overall performance when applied to Large Language Models (LLMs).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Mainstream backdoor attack methods typically demand substantial tuning data for poisoning, limiting their practicality and potentially degrading the overall performance when applied to Large Language Models (LLMs).\""
    },
    {
        "title": "Reward-Free Curricula for Training Robust World Models",
        "authors": "Marc Rigter;Minqi Jiang;Ingmar Posner",
        "site": "https://iclr.cc/virtual/2024/poster/18229",
        "summary": "There has been a recent surge of interest in developing generally-capable agents that can adapt to new tasks without additional training in the environment. Learning world models from reward-free exploration is a promising approach, and enables policies to be trained using imagined experience for new tasks. However, achieving a general agent requires robustness across different environments. In this work, we address the novel problem of generating curricula in the reward-free setting to train robust world models. We consider robustness in terms of minimax regret over all environment instantiations and show that the minimax regret can be connected to minimising the maximum error in the world model across environment instances. This result informs our algorithm, WAKER: Weighted Acquisition of Knowledge across Environments for Robustness. WAKER selects environments for data collection based on the estimated error of the world model for each environment. Our experiments demonstrate that WAKER outperforms naı̈ve domain randomisation, resulting in improved robustness, efficiency, and generalisation.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Multimodal Web Navigation with Instruction-Finetuned Foundation Models",
        "authors": "Hiroki Furuta;Kuang-Huei Lee;Ofir Nachum;Yutaka Matsuo;Aleksandra Faust;Shixiang Shane Gu;Izzeddin Gur",
        "site": "https://iclr.cc/virtual/2024/poster/18215",
        "summary": "The progress of autonomous web navigation has been hindered by the dependence on billions of exploratory interactions via online reinforcement learning, and domain-specific model designs that make it difficult to leverage generalization from rich out-of-domain data.In this work, we study data-driven offline training for web agents with vision-language foundation models.We propose an instruction-following multimodal agent, WebGUM, that observes both webpage screenshots and HTML pages and outputs web navigation actions, such as click and type.WebGUM is trained by jointly finetuning an instruction-finetuned language model and a vision encoder with temporal and local perception on a large corpus of demonstrations.We empirically demonstrate this recipe improves the agent's ability of grounded multimodal perception, HTML comprehension, and multi-step reasoning, outperforming prior works by a significant margin. On the MiniWoB, we improve over the previous best offline methods by more than 45.8%, even outperforming online-finetuned SoTA, humans, and GPT-4-based agent. On the WebShop benchmark, our 3-billion-parameter model achieves superior performance to the existing SoTA, PaLM-540B.Furthermore, WebGUM exhibits strong positive transfer to the real-world planning tasks on the Mind2Web.We also collect 347K high-quality demonstrations using our trained models, 38 times larger than prior work, and make them available to promote future research in this direction.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"even outperforming online-finetuned SoTA, humans, and GPT-4-based agent.\"\n\nThis rating is given because the paper mentions a comparison with a GPT-4-based agent, but does not discuss any limitations of LLMs explicitly.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"even outperforming online-finetuned SoTA, humans, and GPT-4-based agent.\"\n\nThis rating is given because the paper mentions a comparison with a GPT-4-based agent, but does not discuss any limitations of LLMs explicitly."
    },
    {
        "title": "SEPT: Towards Efficient Scene Representation Learning for Motion Prediction",
        "authors": "Zhiqian Lan;Yuxuan Jiang;Yao Mu;Chen Chen;Shengbo Eben Li",
        "site": "https://iclr.cc/virtual/2024/poster/18214",
        "summary": "Motion prediction is crucial for autonomous vehicles to operate safely in complex traffic environments. Extracting effective spatiotemporal relationships among traffic elements is key to accurate forecasting. Inspired by the successful practice of pretrained large language models, this paper presents SEPT, a modeling framework that leverages self-supervised learning to develop powerful spatiotemporal understanding for complex traffic scenes. Specifically, our approach involves three masking-reconstruction modeling tasks on scene inputs including agents' trajectories and road network, pretraining the scene encoder to capture kinematics within trajectory, spatial structure of road network, and interactions among roads and agents. The pretrained encoder is then finetuned on the downstream forecasting task. Extensive experiments demonstrate that SEPT, without elaborate architectural design or manual feature engineering, achieves state-of-the-art performance on the Argoverse 1 and Argoverse 2 motion forecasting benchmarks, outperforming previous methods on all main metrics by a large margin.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Inspired by the successful practice of pretrained large language models\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Inspired by the successful practice of pretrained large language models\""
    },
    {
        "title": "Turning large language models into cognitive models",
        "authors": "Marcel Binz;Eric Schulz",
        "site": "https://iclr.cc/virtual/2024/poster/18213",
        "summary": "Large language models are powerful systems that excel at many tasks, ranging from translation to mathematical reasoning. Yet, at the same time, these models often show unhuman-like characteristics. In the present paper, we address this gap and ask whether large language models can be turned into cognitive models. We find that -- after finetuning them on data from psychological experiments -- these models offer accurate representations of human behavior, even outperforming traditional cognitive models in two decision-making domains. In addition, we show that their representations contain the information necessary to model behavior on the level of individual subjects. Finally, we demonstrate that finetuning on multiple tasks enables large language models to predict human behavior in a previously unseen task. Taken together, these results suggest that large, pre-trained models can be adapted to become models of human cognition, which opens up future research directions toward building more general cognitive models.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Yet, at the same time, these models often show unhuman-like characteristics.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Yet, at the same time, these models often show unhuman-like characteristics.\""
    },
    {
        "title": "LLM-grounded Video Diffusion Models",
        "authors": "Long Lian;Baifeng Shi;Adam Yala;Trevor Darrell;Boyi Li",
        "site": "https://iclr.cc/virtual/2024/poster/18205",
        "summary": "Text-conditioned diffusion models have emerged as a promising tool for neural video generation. However, current models still struggle with intricate spatiotemporal prompts and often generate restricted or incorrect motion. To address these limitations, we introduce LLM-grounded Video Diffusion (LVD). Instead of directly generating videos from the text inputs, LVD first leverages a large language model (LLM) to generate dynamic scene layouts based on the text inputs and subsequently uses the generated layouts to guide a diffusion model for video generation. We show that LLMs are able to understand complex spatiotemporal dynamics from text alone and generate layouts that align closely with both the prompts and the object motion patterns typically observed in the real world. We then propose to guide video diffusion models with these layouts by adjusting the attention maps. Our approach is training-free and can be integrated into any video diffusion model that admits classifier guidance. Our results demonstrate that LVD significantly outperforms its base video diffusion model and several strong baseline methods in faithfully generating videos with the desired attributes and motion patterns.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, current models still struggle with intricate spatiotemporal prompts and often generate restricted or incorrect motion.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, current models still struggle with intricate spatiotemporal prompts and often generate restricted or incorrect motion.\""
    },
    {
        "title": "Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model",
        "authors": "Zihan Zhong;Zhiqiang Tang;Tong He;Haoyang Fang;Chun Yuan",
        "site": "https://iclr.cc/virtual/2024/poster/18201",
        "summary": "The Segment-Anything Model (SAM) stands as a foundational framework for image segmentation. While it exhibits remarkable zero-shot generalization in typical scenarios, its advantage diminishes when applied to specialized domains like medical imagery and remote sensing. To address this limitation, this paper introduces Conv-LoRA, a simple yet effective parameter-efficient fine-tuning approach. By integrating ultra-lightweight convolutional parameters into Low-Rank Adaptation (LoRA), Conv-LoRA can inject image-related inductive biases into the plain ViT encoder, further reinforcing SAM’s local prior assumption. Notably, Conv-LoRA not only preserves SAM’s extensive segmentation knowledge but also revives its capacity of learning high-level image semantics, which is constrained by SAM’s foreground-background segmentation pretraining. Comprehensive experimentation across diverse benchmarks spanning multiple domains underscores Conv-LoRA’s superiority in adapting SAM to real-world semantic segmentation tasks.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling",
        "authors": "Haoyu Lu;Yuqi Huo;Guoxing Yang;Zhiwu Lu;Wei Zhan;Masayoshi Tomizuka;Mingyu Ding",
        "site": "https://iclr.cc/virtual/2024/poster/18198",
        "summary": "Large-scale vision-language pre-trained models have shown promising transferability to various downstream tasks. As the size of these foundation models and the number of downstream tasks grow, the standard full fine-tuning paradigm becomes unsustainable due to heavy computational and storage costs. This paper proposes UniAdapter, which unifies unimodal and multimodal adapters for parameter-efficient cross-modal adaptation on pre-trained vision-language models. Specifically, adapters are distributed to different modalities and their interactions, with the total number of tunable parameters reduced by partial weight sharing. The unified and knowledge-sharing design enables powerful cross-modal representations that can benefit various downstream tasks, requiring only 1.0%-2.0% tunable parameters of the pre-trained model. Extensive experiments on 7 cross-modal downstream benchmarks (including video-text retrieval, image-text retrieval, VideoQA, VQA and Caption) show that in most cases, UniAdapter not only outperforms the state-of-the-arts, but even beats the full fine-tuning strategy. Particularly, on the MSRVTT retrieval task, UniAdapter achieves 49.7% recall@1 with 2.2% model parameters, outperforming the latest competitors by 2.0%. The code and models are available at https://github.com/RERV/UniAdapter.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"As the size of these foundation models and the number of downstream tasks grow, the standard full fine-tuning paradigm becomes unsustainable due to heavy computational and storage costs.\"\n\nThis abstract mentions a limitation of large-scale vision-language pre-trained models (computational and storage costs), but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"As the size of these foundation models and the number of downstream tasks grow, the standard full fine-tuning paradigm becomes unsustainable due to heavy computational and storage costs.\"\n\nThis abstract mentions a limitation of large-scale vision-language pre-trained models (computational and storage costs), but it is not the primary focus of the paper."
    },
    {
        "title": "DIAGNOSIS: Detecting Unauthorized Data Usages in Text-to-image Diffusion Models",
        "authors": "Zhenting Wang;Chen Chen;Lingjuan Lyu;Dimitris N. Metaxas;Shiqing Ma",
        "site": "https://iclr.cc/virtual/2024/poster/18196",
        "summary": "Recent text-to-image diffusion models have shown surprising performance in generating high-quality images. However, concerns have arisen regarding the unauthorized data usage during the training or fine-tuning process. One example is when a model trainer collects a set of images created by a particular artist and attempts to train a model capable of generating similar images without obtaining permission and giving credit to the artist. To address this issue, we propose a method for detecting such unauthorized data usage by planting the injected memorization into the text-to-image diffusion models trained on the protected dataset. Specifically, we modify the protected images by adding unique contents on these images using stealthy image warping functions that are nearly imperceptible to humans but can be captured and memorized by diffusion models. By analyzing whether the model has memorized the injected content (i.e., whether the generated images are processed by the injected post-processing function), we can detect models that had illegally utilized the unauthorized data. Experiments on Stable Diffusion and VQ Diffusion with different model training or fine-tuning methods (i.e, LoRA, DreamBooth, and standard training) demonstrate the effectiveness of our proposed method in detecting unauthorized data usages. Code: https://github.com/ZhentingWang/DIAGNOSIS.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Zero-Shot Robustification of Zero-Shot Models",
        "authors": "Dyah Adila;Changho Shin;Linrong Cai;Frederic Sala",
        "site": "https://iclr.cc/virtual/2024/poster/18194",
        "summary": "Zero-shot inference is a powerful paradigm that enables the use of large pretrained models for downstream classification tasks without further training. However, these models are vulnerable to inherited biases that can impact their performance. The traditional solution is fine-tuning, but this undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box. We propose RoboShot, a method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion. First, we use language models (LMs) to obtain useful insights from task descriptions. These insights are embedded and used to remove harmful and boost useful components in embeddings---without any supervision. Theoretically, we provide a simple and tractable model for biases in zero-shot embeddings and give a result characterizing under what conditions our approach can boost performance. Empirically, we evaluate RoboShot on nine image and NLP classification tasks and show an average improvement of 15.98% over several zero-shot baselines. Additionally, we demonstrate that RoboShot is compatible with a variety of pretrained and language models and propose a way to further boost performance with a zero-shot adaptation variant.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these models are vulnerable to inherited biases that can impact their performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these models are vulnerable to inherited biases that can impact their performance.\""
    },
    {
        "title": "Unveiling the Pitfalls of Knowledge Editing for Large Language Models",
        "authors": "Zhoubo Li;Ningyu Zhang;Yunzhi Yao;Mengru Wang;Xi Chen;Huajun Chen",
        "site": "https://iclr.cc/virtual/2024/poster/18188",
        "summary": "As the cost associated with fine-tuning Large Language Models (LLMs) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within LLMs. Yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not. This paper pioneers the investigation into the potential pitfalls associated with knowledge editing for LLMs. To achieve this, we introduce new benchmark datasets and propose innovative evaluation metrics. Our results underline two pivotal concerns: (1) Knowledge Conflict: Editing groups of facts that logically clash can magnify the inherent inconsistencies in LLMs—a facet neglected by previous methods. (2) Knowledge Distortion: Altering parameters with the aim of editing factual knowledge can irrevocably warp the innate knowledge structure of LLMs. Experimental results vividly demonstrate that knowledge editing might inadvertently cast a shadow of unintended consequences on LLMs, which warrant attention and efforts for future works. Code and data are available at https://github.com/zjunlp/PitfallsKnowledgeEditing.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not.\"; \"Our results underline two pivotal concerns: (1) Knowledge Conflict: Editing groups of facts that logically clash can magnify the inherent inconsistencies in LLMs—a facet neglected by previous methods. (",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not.\"; \"Our results underline two pivotal concerns: (1) Knowledge Conflict: Editing groups of facts that logically clash can magnify the inherent inconsistencies in LLMs—a facet neglected by previous methods. ("
    },
    {
        "title": "A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models",
        "authors": "Haoran Xu;Young Jin Kim;Amr Sharaf;Hany Hassan Awadalla",
        "site": "https://iclr.cc/virtual/2024/poster/18182",
        "summary": "Generative Large Language Models (LLMs) have achieved remarkable advancements in various NLP tasks. However, these advances have not been reflected in the translation task, especially those with moderate model sizes (i.e., 7B or 13B parameters), which still lag behind conventional supervised encoder-decoder translation models. Previous studies have attempted to improve the translation capabilities of these LLMs, but their gains have been limited. In this study, we propose a novel fine-tuning approach for LLMs that is specifically designed for the translation task, eliminating the need for the abundant parallel data that traditional translation models usually depend on.Our approach consists of two fine-tuning stages: initial fine-tuning on monolingual data followed by subsequent fine-tuning on a small set of high-quality parallel data.  We introduce the LLM  developed through this strategy asAdvancedLanguageModel-based trAnslator (ALMA). Based on LLaMA-2 as our underlying model, our results show that the model can achieve an average improvement of more than 12 BLEU and 12 COMET over its zero-shot performance across 10 translation directions from the WMT'21 (2 directions) and WMT'22 (8 directions) test datasets. The performance is significantly better than all prior work and even superior to the NLLB-54B model \\citep{nllb} and GPT-3.5-text-davinci-003, with only 7B or 13B parameters. This method establishes the foundation for a novel training paradigm in machine translation.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these advances have not been reflected in the translation task, especially those with moderate model sizes (i.e., 7B or 13B parameters), which still lag behind conventional supervised encoder-decoder translation models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these advances have not been reflected in the translation task, especially those with moderate model sizes (i.e., 7B or 13B parameters), which still lag behind conventional supervised encoder-decoder translation models.\""
    },
    {
        "title": "Federated Q-Learning: Linear Regret Speedup with Low Communication Cost",
        "authors": "Zhong Zheng;Fengyu Gao;Lingzhou Xue;Jing Yang",
        "site": "https://iclr.cc/virtual/2024/poster/18180",
        "summary": "Abstract:In this paper, we consider federated reinforcement learning for tabular episodic Markov Decision Processes (MDP) where, under the coordination of a central server, multiple agents collaboratively explore the environment and learn an optimal policy without sharing their raw data.  While linear speedup in the number of agents has been achieved for some metrics, such as convergence rate and sample complexity, in similar settings, it is unclear whether it is possible to design a *model-free* algorithm to achieve linear *regret* speedup with low communication cost. We propose two federated Q-Learning algorithms termed as FedQ-Hoeffding and FedQ-Bernstein, respectively, and show that the corresponding total regrets achieve a linear speedup compared with their single-agent counterparts, while the communication cost scales logarithmically in the total number of time steps $T$. Those results rely on an event-triggered synchronization mechanism between the agents and the server, a novel step size selection when the server aggregates the local estimates of the state-action values to form the global estimates, and a set of new concentration inequalities to bound the sum of non-martingale differences. This is the first work showing that linear regret speedup and logarithmic communication cost can be achieved by model-free algorithms in federated reinforcement learning.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "GAIA: a benchmark for General AI Assistants",
        "authors": "Grégoire Mialon;Clémentine Fourrier;Thomas Wolf;Yann LeCun;Thomas Scialom",
        "site": "https://iclr.cc/virtual/2024/poster/18176",
        "summary": "We introduce GAIA, a benchmark for General AI Assistants that, if solved, would represent a milestone in AI research. GAIA proposes real-world questions that require a set of fundamental abilities such as reasoning, multi-modality handling, web browsing, and generally tool-use proficiency. GAIA questions are conceptually simple for humans yet challenging for most advanced AIs: we show that human respondents obtain 92% vs. 15% for GPT-4 equipped with plugins. This notable performance disparity contrasts with the recent trend of LLMs outperforming humans on tasks requiring professional skills in e.g. law or chemistry. GAIA’s philosophy departs from the current trend in AI benchmarks suggesting to target tasks that are ever more difficult for humans. We posit that the advent of Artificial General Intelligence (AGI) hinges on a system’s capability to exhibit similar robustness as the average human does on such questions. Using GAIA’s methodology, we devise 466 questions and their answer. We release our questions while retaining answers to 300 of them to power a leader-board accessible at https://huggingface.co/gaia-benchmark.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"GAIA questions are conceptually simple for humans yet challenging for most advanced AIs: we show that human respondents obtain 92% vs. 15% for GPT-4 equipped with plugins.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"GAIA questions are conceptually simple for humans yet challenging for most advanced AIs: we show that human respondents obtain 92% vs. 15% for GPT-4 equipped with plugins.\""
    },
    {
        "title": "Efficient Dynamics Modeling in Interactive Environments with Koopman Theory",
        "authors": "Arnab Kumar Mondal;Siba Smarak Panigrahi;Sai Rajeswar;Kaleem Siddiqi;Siamak Ravanbakhsh",
        "site": "https://iclr.cc/virtual/2024/poster/18172",
        "summary": "The accurate modeling of dynamics in interactive environments is critical for successful long-range prediction. Such a capability could advance Reinforcement Learning (RL) and Planning algorithms, but achieving it is challenging. Inaccuracies in model estimates can compound, resulting in increased errors over long horizons.We approach this problem from the lens of Koopman theory, where the nonlinear dynamics of the environment can be linearized in a high-dimensional latent space. This allows us to efficiently parallelize the sequential problem of long-range prediction using convolution while accounting for the agent's action at every time step.Our approach also enables stability analysis and better control over gradients through time. Taken together, these advantages result in significant improvement over the existing approaches, both in the efficiency and the accuracy of modeling dynamics over extended horizons. We also show that this model can be easily incorporated into dynamics modeling for model-based planning and model-free RL and report promising experimental results.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Are Human-generated Demonstrations Necessary for In-context Learning?",
        "authors": "Rui Li;Guoyin Wang;Jiwei Li",
        "site": "https://iclr.cc/virtual/2024/poster/18169",
        "summary": "Despite the promising few-shot ability of large language models (LLMs), the standard paradigm of In-context Learning (ICL) suffers the disadvantages of susceptibility to selected demonstrations and the intricacy to generate these demonstrations. In this paper, we raise the fundamental question that whether human-generated demonstrations are necessary for ICL. To answer this question, we propose self-contemplation prompting strategy (SEC), a paradigm free from human-crafted demonstrations. The key point of SEC is that, instead of using hand-crafted examples as demonstrations in ICL, SEC asks LLMs to first create demonstrations on their own, based on which the final output is generated. SEC is a flexible framework and can be adapted to both the vanilla ICL and the chain-of-thought (CoT), but with greater ease: as the manual-generation process of both examples and rationale can be saved. Extensive experiments in arithmetic reasoning, commonsense reasoning, multi-task language understanding, and code generation benchmarks, show that SEC, which does not require hand-crafted demonstrations, significantly outperforms the zero-shot learning strategy, and achieves comparable results to ICL with hand-crafted demonstrations. This demonstrates that, for many tasks, contemporary LLMs possess a sufficient level of competence to exclusively depend on their own capacity for decision making, removing the need for external training data.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the promising few-shot ability of large language models (LLMs), the standard paradigm of In-context Learning (ICL) suffers the disadvantages of susceptibility to selected demonstrations and the intricacy to generate these demonstrations.\"\n\nThis abstract briefly mentions limitations of LLMs, specifically the need for demonstrations in In-context Learning, but it does not elaborate on these limitations and focuses on proposing a solution",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite the promising few-shot ability of large language models (LLMs), the standard paradigm of In-context Learning (ICL) suffers the disadvantages of susceptibility to selected demonstrations and the intricacy to generate these demonstrations.\"\n\nThis abstract briefly mentions limitations of LLMs, specifically the need for demonstrations in In-context Learning, but it does not elaborate on these limitations and focuses on proposing a solution"
    },
    {
        "title": "Improved Probabilistic Image-Text Representations",
        "authors": "Sanghyuk Chun",
        "site": "https://iclr.cc/virtual/2024/poster/18166",
        "summary": "Image-Text Matching (ITM) task, a fundamental vision-language (VL) task, suffers from the inherent ambiguity arising from multiplicity and imperfect annotations. Deterministic functions are not sufficiently powerful to capture ambiguity, prompting the exploration of probabilistic embeddings to tackle the challenge. However, the existing probabilistic ITM approach encounters two key shortcomings; the burden of heavy computations due to the Monte Carlo approximation, and the loss saturation issue in the face of abundant false negatives. To overcome the issues, this paper presents an improved Probabilistic Cross-Modal Embeddings (named PCME++) by introducing a new probabilistic distance with a closed-form solution. In addition, two optimization techniques are proposed to enhance PCME++ further: first, the incorporation of pseudo-positives to prevent the negative effect under massive false negatives; second, mixed sample data augmentation for probabilistic matching. Experimental results on MS-COCO Caption and two extended benchmarks, CxC and ECCV Caption, demonstrate the effectiveness of PCME++ compared to state-of-the-art ITM methods. The robustness of PCME++ is also evaluated under noisy image-text correspondences. In addition, the potential applicability of PCME++ in automatic prompt-filtering for zero-shot classification is shown. The code is available at https://github.com/naver-ai/pcmepp.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Follow-Up Differential Descriptions: Language Models Resolve Ambiguities for Image Classification",
        "authors": "Reza Esfandiarpoor;Stephen Bach",
        "site": "https://iclr.cc/virtual/2024/poster/18158",
        "summary": "A promising approach for improving the performance of vision-language models like CLIP for image classification is to extend the class descriptions (i.e., prompts) with related attributes, e.g., using brown sparrow instead of sparrow. However, current zero-shot methods select a subset of attributes regardless of commonalities between the target classes, potentially providing no useful information that would have helped to distinguish between them. For instance, they may use color instead of bill shape to distinguish between sparrows and wrens, which are both brown. We propose Follow-up Differential Descriptions (FuDD), a zero-shot approach that tailors the class descriptions to each dataset and leads to additional attributes that better differentiate the target classes. FuDD first identifies the ambiguous classes for each image, and then uses a Large Language Model (LLM) to generate new class descriptions that differentiate between them. The new class descriptions resolve the initial ambiguity and help predict the correct label. In our experiments, FuDD consistently outperforms generic description ensembles and naive LLM-generated descriptions on 12 datasets. We show that differential descriptions are an effective tool to resolve class ambiguities, which otherwise significantly degrade the performance. We also show that high quality natural language class descriptions produced by FuDD result in comparable performance to few-shot adaptation methods.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We show that differential descriptions are an effective tool to resolve class ambiguities, which otherwise significantly degrade the performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We show that differential descriptions are an effective tool to resolve class ambiguities, which otherwise significantly degrade the performance.\""
    },
    {
        "title": "BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation",
        "authors": "Peng Xu;Wenqi Shao;Mengzhao Chen;Shitao Tang;Kaipeng Zhang;Peng Gao;Fengwei An;Yu Qiao;Ping Luo",
        "site": "https://iclr.cc/virtual/2024/poster/18153",
        "summary": "Large language models (LLMs) have demonstrated outstanding performance in various tasks, such as text summarization, text question-answering, and etc. While their performance is impressive, the computational footprint due to their vast number of parameters can be prohibitive. Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance. To address this, this paper introduces a novel LLM pruning technique dubbed blockwise parameter-efficient sparsity allocation (BESA) by applying a blockwise reconstruction loss. In contrast to the typical layer-wise pruning techniques, BESA is characterized by two distinctive attributes: i) it targets the overall pruning error with respect to individual transformer blocks, and ii) it allocates layer-specific sparsity in a differentiable manner, both of which ensure reduced performance degradation after pruning. Our experiments show that BESA achieves state-of-the-art performance, efficiently pruning LLMs like LLaMA1, and LLaMA2 with 7B to 70B parameters on a single A100 GPU in just five hours. Code is available athere.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance.\""
    },
    {
        "title": "LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models",
        "authors": "Gunho Park;Baeseong park;Minsub Kim;Sungjae Lee;Jeonghoon Kim;Beomseok Kwon;Se Jung Kwon;Byeongwook Kim;Youngjoo Lee;Dongsoo Lee",
        "site": "https://iclr.cc/virtual/2024/poster/18146",
        "summary": "Recent advances in self-supervised learning and the Transformer architecture have significantly improved natural language processing (NLP), achieving remarkably low perplexity.However, the growing size of NLP models introduces a memory wall problem during the generation phase.To mitigate this issue, recent efforts have focused on quantizing model weights to sub-4-bit precision while preserving full precision for activations, resulting in practical speed-ups during inference on a single GPU.However, these improvements primarily stem from reduced memory movement, which necessitates a resource-intensive dequantization process rather than actual computational reduction.In this paper, we introduce LUT-GEMM, an efficient kernel for quantized matrix multiplication, which not only eliminates the resource-intensive dequantization process but also reduces computational costs compared to previous kernels for weight-only quantization.Furthermore, we proposed group-wise quantization to offer a flexible trade-off between compression ratio and accuracy.The impact of LUT-GEMM is facilitated by implementing high compression ratios through low-bit quantization and efficient LUT-based operations.We show experimentally that when applied to the OPT-175B model with 3-bit quantization, LUT-GEMM substantially accelerates token generation latency, achieving a remarkable 2.1x improvement on a single GPU when compared to OPTQ, which relies on the costly dequantization process.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the growing size of NLP models introduces a memory wall problem during the generation phase.\"\n\nThis abstract mentions a limitation of large-scale generative language models (memory wall problem), but it is not the primary focus of the paper. The limitation is briefly mentioned to motivate the proposed solution, LUT-GEMM.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the growing size of NLP models introduces a memory wall problem during the generation phase.\"\n\nThis abstract mentions a limitation of large-scale generative language models (memory wall problem), but it is not the primary focus of the paper. The limitation is briefly mentioned to motivate the proposed solution, LUT-GEMM."
    },
    {
        "title": "An Unforgeable Publicly Verifiable Watermark for Large Language Models",
        "authors": "Aiwei Liu;Leyi Pan;Xuming Hu;Shuang Li;Lijie Wen;Irwin King;Philip S. Yu",
        "site": "https://iclr.cc/virtual/2024/poster/18145",
        "summary": "Recently, text watermarking algorithms for large language models (LLMs) have been proposed to mitigate the potential harms of text generated by LLMs, including fake news and copyright issues. However, current watermark detection algorithms require the secret key used in the watermark generation process, making them susceptible to security breaches and counterfeiting during public detection.To address this limitation, we propose an unforgeable publicly verifiable watermark algorithm named UPV that uses two different neural networks for watermark generation and detection, instead of using the same key at both stages. Meanwhile, the token embedding parameters are shared between the generation and detection networks, which makes the detection network achieve a high accuracy very efficiently.Experiments demonstrate that our algorithm attains high detection accuracy and computational efficiency through neural networks. Subsequent analysis confirms the high complexity involved in forging the watermark from the detection network. Our code is available at https://github.com/THU-BPM/unforgeable_watermark",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, current watermark detection algorithms require the secret key used in the watermark generation process, making them susceptible to security breaches and counterfeiting during public detection.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, current watermark detection algorithms require the secret key used in the watermark generation process, making them susceptible to security breaches and counterfeiting during public detection.\""
    },
    {
        "title": "FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores",
        "authors": "Daniel Y Fu;Hermann Kumbong;Eric Nguyen;Christopher Re",
        "site": "https://iclr.cc/virtual/2024/poster/18144",
        "summary": "Abstract:Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time.A major bottleneck is the Fast Fourier Transform (FFT)---which allows long convolutions to run in $O(N\\log N)$ time in sequence length $N$ but has poor hardware utilization.In this paper, we study how to optimize the FFT convolution.We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy.In response, we propose FlashFFTConv.FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O.We also present two sparse convolution algorithms---1) partial convolutions and 2) frequency-sparse convolutions---which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings.FlashFFTConv speeds up exact FFT convolutions by up to 8.7$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end.Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity and M2-BERT-base to achieve 3.3 points higher GLUE score---matching models with twice the parameter count.FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%.Furthermore, partial convolutions enable longer-sequence models---yielding the first DNA model that can process the longest human genes (2.3M base pairs)---and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions",
        "authors": "Federico Bianchi;Mirac Suzgun;Giuseppe Attanasio;Paul Rottger;Dan Jurafsky;Tatsunori Hashimoto;James Zou",
        "site": "https://iclr.cc/virtual/2024/poster/18143",
        "summary": "Training large language models to follow instructions makes them perform better on a wide range of tasks and generally become more helpful. However, a perfectly helpful model will follow even the most malicious instructions and readily generate harmful content.In this paper, we raise concerns over the safety of models that only emphasize helpfulness, not harmlessness, in their instruction-tuning.We show that several popular instruction-tuned models are highly unsafe. Moreover, we show that adding just 3\\% safety examples (a few hundred demonstrations) when fine-tuning a model like LLaMA can substantially improve its safety. Our safety-tuning does not make models significantly less capable or helpful as measured by standard benchmarks. However, we do find exaggerated safety behaviours, where too much safety-tuning makes models refuse perfectly safe prompts if they superficially resemble unsafe ones. As a whole, our results illustrate trade-offs in training LLMs to be helpful and training them to be safe.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, a perfectly helpful model will follow even the most malicious instructions and readily generate harmful content.\"; \"we do find exaggerated safety behaviours, where too much safety-tuning makes models refuse perfectly safe prompts if they superficially resemble unsafe ones.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, a perfectly helpful model will follow even the most malicious instructions and readily generate harmful content.\"; \"we do find exaggerated safety behaviours, where too much safety-tuning makes models refuse perfectly safe prompts if they superficially resemble unsafe ones.\""
    },
    {
        "title": "Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models",
        "authors": "Mert Yuksekgonul;Varun Chandrasekaran;Erik Jones;Suriya Gunasekar;Ranjita Naik;Hamid Palangi;Ece Kamar;Besmira Nushi",
        "site": "https://iclr.cc/virtual/2024/poster/18138",
        "summary": "We investigate the internal behavior of Transformer-based Large Language Models (LLMs) when they generate factually incorrect text. We propose modeling factual queries as constraint satisfaction problems and use this framework to investigate how the LLM interacts internally with factual constraints. We find a strong positive relationship between the LLM's attention to constraint tokens and the factual accuracy of generations. We curate a suite of 10 datasets containing over 40,000 prompts to study the task of predicting factual errors with the Llama-2 family across all scales (7B, 13B, 70B). We propose SAT Probe, a method probing attention patterns, that can predict factual errors and fine-grained constraint satisfaction, and allow early error identification. The approach and findings take another step towards using the mechanistic understanding of LLMs to enhance their reliability.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We investigate the internal behavior of Transformer-based Large Language Models (LLMs) when they generate factually incorrect text.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We investigate the internal behavior of Transformer-based Large Language Models (LLMs) when they generate factually incorrect text.\""
    },
    {
        "title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
        "authors": "Miao Xiong;Zhiyuan Hu;Xinyang Lu;YIFEI LI;Jie Fu;Junxian He;Bryan Hooi",
        "site": "https://iclr.cc/virtual/2024/poster/18135",
        "summary": "Empowering large language models (LLMs) to accurately express confidence in their answers is essential for reliable and trustworthy decision-making. Previous confidence elicitation methods, which primarily rely onwhite-box accessto internal model information or model fine-tuning, have become less suitable for LLMs, especially closed-source commercial APIs. This leads to a growing need to explore the untapped area ofblack-boxapproaches for LLM uncertainty estimation. To better break down the problem, we define a systematic framework with three components:promptingstrategies for eliciting verbalized confidence,samplingmethods for generating multiple responses, andaggregationtechniques for computing consistency. We then benchmark these methods on two key tasks—confidence calibration and failure prediction—across five types of datasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs including GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights: 1) LLMs, when verbalizing their confidence, tend to beoverconfident, potentially imitating human patterns of expressing confidence. 2) As model capability scales up, both calibration and failure prediction performance improve, yet still far from ideal performance. 3) Employing our proposed strategies, such as human-inspired prompts, consistency among multiple responses, and better aggregation strategies can help mitigate this overconfidence from various perspectives. 4) Comparisons with white-box methods indicate that while white-box methods perform better, the gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements, none of these techniques consistently outperform others, and all investigated methods struggle in challenging tasks, such as those requiring professional knowledge, indicating significant scope for improvement. We believe this study can serve as a strong baseline and provide insights for eliciting confidence in black-box LLMs. The code is publicly available at https://github.com/MiaoXiong2320/llm-uncertainty.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"1) LLMs, when verbalizing their confidence, tend to be overconfident, potentially imitating human patterns of expressing confidence.\"; \"Despite these advancements, none of these techniques consistently outperform others, and all investigated methods struggle in challenging tasks, such as those requiring professional knowledge, indicating significant scope for improvement.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"1) LLMs, when verbalizing their confidence, tend to be overconfident, potentially imitating human patterns of expressing confidence.\"; \"Despite these advancements, none of these techniques consistently outperform others, and all investigated methods struggle in challenging tasks, such as those requiring professional knowledge, indicating significant scope for improvement.\""
    },
    {
        "title": "Generative Judge for Evaluating Alignment",
        "authors": "Junlong Li;Shichao Sun;Weizhe Yuan;Run-Ze Fan;hai zhao;Pengfei Liu",
        "site": "https://iclr.cc/virtual/2024/poster/18128",
        "summary": "The rapid development of Large Language Models (LLMs) has substantially expanded the range of tasks they can address. In the field of Natural Language Processing (NLP), researchers have shifted their focus from conventional NLP tasks (e.g., sequence tagging and parsing) towards tasks that revolve around aligning with human needs (e.g., brainstorming and email writing). This shift in task distribution imposes new requirements on evaluating these aligned models regardinggenerality(i.e., assessing performance across diverse scenarios),flexibility(i.e., examining under different protocols), andinterpretability(i.e., scrutinizing models with explanations). In this paper, we propose a generative judge with 13B parameters,Auto-J, designed to address these challenges. Our model is trained on user queries and LLM-generated responses under massive real-world scenarios and accommodates diverse evaluation protocols (e.g., pairwise response comparison and single-response evaluation) with well-structured natural language critiques. To demonstrate the efficacy of our approach, we construct a new testbed covering 58 different scenarios. Experimentally,Auto-Joutperforms a series of strong competitors, including both open-source and closed-source models, by a large margin. We also provide detailed analysis and case studies to further reveal the potential of our method and make a variety of resources public at https://github.com/GAIR-NLP/auto-j.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This shift in task distribution imposes new requirements on evaluating these aligned models regarding generality (i.e., assessing performance across diverse scenarios), flexibility (i.e., examining under different protocols), and interpretability (i.e., scrutinizing models with explanations).\"\n\nNote that the abstract mentions the limitations of evaluating LLMs in terms of generality, flexibility, and interpretability, but it",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"This shift in task distribution imposes new requirements on evaluating these aligned models regarding generality (i.e., assessing performance across diverse scenarios), flexibility (i.e., examining under different protocols), and interpretability (i.e., scrutinizing models with explanations).\"\n\nNote that the abstract mentions the limitations of evaluating LLMs in terms of generality, flexibility, and interpretability, but it"
    },
    {
        "title": "CausalLM is not optimal for in-context learning",
        "authors": "Nan Ding;Tomer Levinboim;Jialin Wu;Sebastian Goodman;Radu Soricut",
        "site": "https://iclr.cc/virtual/2024/poster/18127",
        "summary": "Recent empirical evidence indicates that transformer based in-context learning performs better when using a prefix language model (prefixLM), in which in-context samples can all attend to each other, compared to causal language models (causalLM), which use auto-regressive attention that prohibits in-context samples to attend to future samples. While this result is intuitive, it is not understood from a theoretical perspective. In this paper we take a theoretical approach and analyze the convergence behavior of prefixLM and causalLM under a certain parameter construction. Our analysis shows that both LM types converge to their stationary points at a linear rate, but that while prefixLM converges to the optimal solution of linear regression, causalLM convergence dynamics follows that of an online gradient descent algorithm, which is not guaranteed to be optimal even as the number of samples grows infinitely. We supplement our theoretical claims with empirical experiments over synthetic and real tasks and using various types of transformers. Our experiments verify that causalLM consistently underperforms prefixLM in all settings.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"while prefixLM converges to the optimal solution of linear regression, causalLM convergence dynamics follows that of an online gradient descent algorithm, which is not guaranteed to be optimal even as the number of samples grows infinitely.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"while prefixLM converges to the optimal solution of linear regression, causalLM convergence dynamics follows that of an online gradient descent algorithm, which is not guaranteed to be optimal even as the number of samples grows infinitely.\""
    },
    {
        "title": "ZeRO++: Extremely Efficient Collective Communication for Large Model Training",
        "authors": "Guanhua Wang;Heyang Qin;Sam Ade Jacobs;Xiaoxia Wu;Connor Holmes;Zhewei Yao;Samyam Rajbhandari;Olatunji Ruwase;Feng Yan;Lei Yang;Yuxiong He",
        "site": "https://iclr.cc/virtual/2024/poster/18124",
        "summary": "Zero Redundancy Optimizer (ZeRO) has been used to train a wide range of large language models on massive GPU clusters due to its ease of use, efficiency, and good scalability. However, when training on low-bandwidth clusters, and/or when small batch size per GPU is used, ZeRO’s effective throughput is limited due to communication overheads. To alleviate this limitation, this paper introduces ZeRO++ composing of three communication volume reduction techniques (lowprecision all-gather, data remapping, and low-precision gradient averaging) to significantly reduce the communication volume up to 4x that enables up to 2.16x better throughput at 384 GPU scale. Our results also show ZeRO++ can speedup the RLHF by 3.3x compared to vanilla ZeRO. To verify the convergence of ZeRO++, we test up to 13B model for pretraining with 8/6-bits all gather and up to 30B model for finetuning with 4/2-bits all gather, and demonstrate on-par accuracy as original ZeRO (aka standard training). As a byproduct, the model trained with ZeRO++ is naturally weight-quantized, which can be directly used for inference without post-training quantization or quantization-aware training.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, when training on low-bandwidth clusters, and/or when small batch size per GPU is used, ZeRO’s effective throughput is limited due to communication overheads.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, when training on low-bandwidth clusters, and/or when small batch size per GPU is used, ZeRO’s effective throughput is limited due to communication overheads.\""
    },
    {
        "title": "Language Model Beats Diffusion - Tokenizer is key to visual generation",
        "authors": "Lijun Yu;Jose Lezama;Nitesh Bharadwaj Gundavarapu;Luca Versari;Kihyuk Sohn;David Minnen;Yong Cheng;Agrim Gupta;Xiuye Gu;Alexander G Hauptmann;Boqing Gong;Ming-Hsuan Yang;Irfan Essa;David A Ross;Lu Jiang",
        "site": "https://iclr.cc/virtual/2024/poster/18119",
        "summary": "While Large Language Models (LLMs) are the dominant models for generative tasks in language, they do not perform as well as diffusion models on image and video generation. To effectively use LLMs for visual generation, one crucial component is the visual tokenizer that maps pixel-space inputs to discrete tokens appropriate for LLM learning. In this paper, we introduce \\modelname{}, a video tokenizer designed to generate concise and expressive tokens for both videos and images using a common token vocabulary. Equipped with this new tokenizer, we show that LLMs outperform diffusion models on standard image and video generation benchmarks including ImageNet and Kinetics. In addition, we demonstrate that our tokenizer surpasses the previously top-performing video tokenizer on two more tasks: (1) video compression comparable to the next-generation video codec (VCC) according to human evaluations, and (2) learning effective representations for action recognition tasks.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While Large Language Models (LLMs) are the dominant models for generative tasks in language, they do not perform as well as diffusion models on image and video generation.\"\n\nThis evidence suggests that the paper mentions a limitation of LLMs, specifically their poor performance on image and video generation tasks compared to diffusion models. However, this limitation is not explored in depth and is primarily used",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While Large Language Models (LLMs) are the dominant models for generative tasks in language, they do not perform as well as diffusion models on image and video generation.\"\n\nThis evidence suggests that the paper mentions a limitation of LLMs, specifically their poor performance on image and video generation tasks compared to diffusion models. However, this limitation is not explored in depth and is primarily used"
    },
    {
        "title": "PolyVoice: Language Models for Speech to Speech Translation",
        "authors": "Qian qian Dong;Zhiying Huang;Qiao Tian;Chen Xu;Tom Ko;yunlong zhao;Siyuan Feng;Tang Li;Kexin Wang;Xuxin Cheng;Fengpeng Yue;Ye Bai;Xi Chen;Lu Lu;Zejun MA;Yuping Wang;Mingxuan Wang;Yuxuan Wang",
        "site": "https://iclr.cc/virtual/2024/poster/18106",
        "summary": "Abstract:With the huge success of GPT models in natural language processing, there is a growing interest in applying language modeling approaches to speech tasks.Currently, the dominant architecture in speech-to-speech translation (S2ST) remains the encoder-decoder paradigm, creating a need to investigate the impact of language modeling approaches in this area. In this study, we introduce PolyVoice, a language model-based framework designed for S2ST systems. Our framework comprises three decoder-only language models: a translation language model, a duration language model, and a speech synthesis language model. These language models employ different types of prompts to extract learned information effectively. By utilizing unsupervised semantic units, our framework can transfer semantic information across these models, making it applicable even to unwritten languages. We evaluate our system on Chinese $\\rightarrow$ English and English $\\rightarrow$ Spanish language pairs. Experimental results demonstrate that \\method outperforms the state-of-the-art encoder-decoder model, producing voice-cloned speech with high translation and audio quality.Speech samples are available at https://polyvoice.github.io.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions \"the dominant architecture in speech-to-speech translation (S2ST) remains the encoder-decoder paradigm, creating a need to investigate the impact of language modeling approaches in this area.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions \"the dominant architecture in speech-to-speech translation (S2ST) remains the encoder-decoder paradigm, creating a need to investigate the impact of language modeling approaches in this area.\""
    },
    {
        "title": "Label-free Node Classification on Graphs with Large Language Models (LLMs)",
        "authors": "Zhikai Chen;Haitao Mao;Hongzhi Wen;Haoyu Han;Wei Jin;Haiyang Zhang;Hui Liu;Jiliang Tang",
        "site": "https://iclr.cc/virtual/2024/poster/18104",
        "summary": "In recent years, there have been remarkable advancements in node classification achieved by Graph Neural Networks (GNNs). However, they necessitate abundant high-quality labels to ensure promising performance. In contrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency on text-attributed graphs. Yet, they face challenges in efficiently processing structural data and suffer from high inference costs. In light of these observations, this work introduces a label-free node classification on graphs with LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs while mitigating their limitations. Specifically, LLMs are leveraged to annotate a small portion of nodes and then GNNs are trained on LLMs' annotations to make predictions for the remaining large portion of nodes. The implementation of LLM-GNN faces a unique challenge: how can we actively select nodes for LLMs to annotate and consequently enhance the GNN training? How can we leverage LLMs to obtain annotations of high quality, representativeness, and diversity, thereby enhancing GNN performance with less cost?To tackle this challenge, we develop an annotation quality heuristic and leverage the confidence scores derived from LLMs to advanced node selection. Comprehensive experimental results validate the effectiveness of LLM-GNN. In particular, LLM-GNN can achieve an accuracy of 74.9\\% on a vast-scale dataset \\products with a cost less than 1 dollar.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Yet, they face challenges in efficiently processing structural data and suffer from high inference costs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Yet, they face challenges in efficiently processing structural data and suffer from high inference costs.\""
    },
    {
        "title": "True Knowledge Comes from Practice: Aligning Large Language Models with Embodied Environments via Reinforcement Learning",
        "authors": "Weihao Tan;Wentao Zhang;Shanqi Liu;Longtao Zheng;Xinrun Wang;Bo An",
        "site": "https://iclr.cc/virtual/2024/poster/18102",
        "summary": "Despite the impressive performance across numerous tasks, large language models (LLMs) often fail in solving simple decision-making tasks due to the misalignment of the knowledge in LLMs with environments. On the contrary, reinforcement learning (RL) agents learn policies from scratch, which makes them always align with environments but difficult to incorporate prior knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a novel general online framework that deploys LLMs as decision-making agents to efficiently interact and align with embodied environments via RL without requiring any prepared datasets or prior knowledge of the environments. Firstly, we query the joint probabilities of each valid action with LLMs to form behavior policies. Then, to enhance the stability and robustness of the policies, we propose two normalization methods and summarize four prompt design principles. Finally, we design a novel parameter-efficient training architecture where the actor and critic share one frozen LLM equipped with low-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to evaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency and performance compared to the conventional RL method, PPO, and prompt tuning method, SayCan, in both classical decision-making environment, Overcooked, and simulated household environment, VirtualHome. ii) Benefiting from LLMs' open-vocabulary feature, TWOSOME shows superior generalization ability to unseen tasks. iii) Under our framework, there is no significant loss of the LLMs' original ability during online PPO finetuning.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the impressive performance across numerous tasks, large language models (LLMs) often fail in solving simple decision-making tasks due to the misalignment of the knowledge in LLMs with environments.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite the impressive performance across numerous tasks, large language models (LLMs) often fail in solving simple decision-making tasks due to the misalignment of the knowledge in LLMs with environments.\""
    },
    {
        "title": "Piecewise Linear Parametrization of Policies: Towards Interpretable Deep Reinforcement Learning",
        "authors": "Maxime Wabartha;Joelle Pineau",
        "site": "https://iclr.cc/virtual/2024/poster/18099",
        "summary": "Learning inherently interpretable policies is a central challenge in the path to developing autonomous agents that humans can trust. Linear policies can justify their decisions while interacting in a dynamic environment, but their reduced expressivity prevents them from solving hard tasks. Instead, we argue for the use of piecewise-linear policies. We carefully study to what extent they can retain the interpretable properties of linear policies while reaching competitive performance with neural baselines. In particular, we propose the HyperCombinator (HC), a piecewise-linear neural architecture expressing a policy with a controllably small number of sub-policies. Each sub-policy is linear with respect to interpretable features, shedding light on the decision process of the agent without requiring an additional explanation model. We evaluate HC policies in control and navigation experiments, visualize the improved interpretability of the agent and highlight its trade-off with performance. Moreover, we validate that the restricted model class that the HyperCombinator belongs to is compatible with the algorithmic constraints of various reinforcement learning algorithms.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks",
        "authors": "Murtaza Dalal;Tarun Chiruvolu;Devendra Singh Chaplot;Ruslan Salakhutdinov",
        "site": "https://iclr.cc/virtual/2024/poster/18096",
        "summary": "Large Language Models (LLMs) are highly capable of performing planning for long-horizon robotics tasks, yet existing methods require access to a pre-defined skill library (e.g.picking, placing, pulling, pushing, navigating). However, LLM planning does not address how to design or learn those behaviors, which remains challenging particularly in long-horizon settings. Furthermore, for many tasks of interest, the robot needs to be able to adjust its behavior in a fine-grained manner, requiring the agent to be capable of modifyinglow-levelcontrol actions. Can we instead use the internet-scale knowledge from LLMs for high-level policies, guiding reinforcement learning (RL) policies to efficiently solve robotic control tasks online without requiring a pre-determined set of skills? In this paper, we proposePlan-Seq-Learn(PSL): a modular approach that uses motion planning to bridge the gap between abstract language and learned low-level control for solving long-horizon robotics tasks from scratch. We demonstrate that PSL is capable of solving 20+ challenging single and multi-stage robotics tasks on four benchmarks at success rates of over 80% from raw visual input, out-performing language-based, classical, and end-to-end approaches. Video results and code at https://planseqlearn.github.io/",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, LLM planning does not address how to design or learn those behaviors, which remains challenging particularly in long-horizon settings.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, LLM planning does not address how to design or learn those behaviors, which remains challenging particularly in long-horizon settings.\""
    },
    {
        "title": "Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World",
        "authors": "Rujie Wu;Xiaojian Ma;Zhenliang Zhang;Wei Wang;Qing Li;Song-Chun Zhu;Yizhou Wang",
        "site": "https://iclr.cc/virtual/2024/poster/18093",
        "summary": "We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world few-shot reasoning for machine vision. It originates from the classical Bongard Problems (BPs): Given two sets of images (positive and negative), the model needs to identify the set that query images belong to by inducing the visual concepts, which is exclusively depicted by images from the positive set. Our benchmark inherits the few-shot concept induction of the original BPs while adding the two novel layers of challenge: 1) open-world free-form concepts, as the visual concepts in Bongard-OpenWorld are unique compositions of terms from an open vocabulary, ranging from object categories to abstract visual attributes and commonsense factual knowledge; 2)  real-world images, as opposed to the synthetic diagrams used by many counterparts. In our exploration, Bongard-OpenWorld already imposes a significant challenge to current few-shot reasoning algorithms. We further investigate to which extent the recently introduced Large Language Models (LLMs) and Vision-Language Models (VLMs) can solve our task, by directly probing VLMs, and combining VLMs and LLMs in an interactive reasoning scheme. We even conceived a neuro-symbolic reasoning approach that reconciles LLMs & VLMs with logical reasoning to emulate the human problem-solving process for Bongard Problems. However, none of these approaches manage to close the human-machine gap, as the best learner achieves 64% accuracy while human participants easily reach 91%. We hope Bongard-OpenWorld can help us better understand the limitations of current visual intelligence and facilitate future research on visual agents with stronger few-shot visual reasoning capabilities.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, none of these approaches manage to close the human-machine gap, as the best learner achieves 64% accuracy while human participants easily reach 91%.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, none of these approaches manage to close the human-machine gap, as the best learner achieves 64% accuracy while human participants easily reach 91%.\""
    },
    {
        "title": "Kosmos-G: Generating Images in Context with Multimodal Large Language Models",
        "authors": "Xichen Pan;Li Dong;Shaohan Huang;Zhiliang Peng;Wenhu Chen;Furu Wei",
        "site": "https://iclr.cc/virtual/2024/poster/18091",
        "summary": "Recent advancements in subject-driven image generation have made significant strides. However, current methods still fall short in diverse application scenarios, as they require test-time tuning and cannot accept interleaved multi-image and text input. These limitations keep them far from the ultimate goal of \"image as a foreign language in image generation.\" This paper presents Kosmos-G, a model that leverages the advanced multimodal perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates an impressive capability of zero-shot subject-driven generation with interleaved multi-image and text input. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of \"image as a foreign language in image generation.\"",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, current methods still fall short in diverse application scenarios, as they require test-time tuning and cannot accept interleaved multi-image and text input.\"\n\nNote that the limitation mentioned is not specific to LLMs, but rather to current methods in subject-driven image generation. However, since the paper is about leveraging the capabilities of Multimodal Large Language Models (MLLMs) to",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, current methods still fall short in diverse application scenarios, as they require test-time tuning and cannot accept interleaved multi-image and text input.\"\n\nNote that the limitation mentioned is not specific to LLMs, but rather to current methods in subject-driven image generation. However, since the paper is about leveraging the capabilities of Multimodal Large Language Models (MLLMs) to"
    },
    {
        "title": "Causally Aligned Curriculum Learning",
        "authors": "Mingxuan Li;Junzhe Zhang;Elias Bareinboim",
        "site": "https://iclr.cc/virtual/2024/poster/18083",
        "summary": "A pervasive challenge in Reinforcement Learning (RL) is the ``curse of dimensionality'' which is the exponential growth in the state-action space when optimizing a high-dimensional target task. The framework of curriculum learning trains the agent in a curriculum composed of a sequence of related and more manageable source tasks. The expectation is that when some optimal decision rules are shared across source tasks and the target task, the agent could more quickly pick up the necessary skills to behave optimally in the environment, thus accelerating the learning process. However, this critical assumption of invariant optimal decision rules does not necessarily hold in many practical applications, specifically when the underlying environment contains unobserved confounders. This paper studies the problem of curriculum RL through causal lenses. We derive a sufficient graphical condition characterizing causally aligned source tasks, i.e., the invariance of optimal decision rules holds. We further develop an efficient algorithm to generate a causally aligned curriculum, provided with qualitative causal knowledge of the target environment. Finally, we validate our proposed methodology through experiments in confounded environments.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "InstructDET: Diversifying Referring Object Detection with Generalized Instructions",
        "authors": "Ronghao Dang;Jiangyan Feng;Haodong Zhang;Chongjian GE;Lin Song;Lijun GONG;Chengju Liu;Qijun Chen;Feng Zhu;Rui Zhao;Yibing Song",
        "site": "https://iclr.cc/virtual/2024/poster/18082",
        "summary": "We propose InstructDET, a data-centric method for referring object detection (ROD) that localizes target objects based on user instructions. While deriving from referring expressions (REC), the instructions we leverage are greatly diversified to encompass common user intentions related to object detection. For one image, we produce tremendous instructions that refer to every single object and different combinations of multiple objects. Each instruction and its corresponding object bounding boxes (bbxs) constitute one training data pair. In order to encompass common detection expressions, we involve emerging vision-language model (VLM) and large language model (LLM) to generate instructions guided by text prompts and object bbxs, as the generalizations of foundation models are effective to produce human-like expressions (e.g., describing object property, category, and relationship). We name our constructed dataset as InDET. It contains images, bbxs and generalized instructions that are from foundation models. Our InDET is developed from existing REC datasets and object detection datasets, with the expanding potential that any image with object bbxs can be incorporated through using our InstructDET method. By using our InDET dataset, we show that a conventional ROD model surpasses existing methods on standard REC datasets and our InDET test set. Our data-centric method InstructDET, with automatic data expansion by leveraging foundation models, directs a promising field that ROD can be greatly diversified to execute common object detection instructions.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations are mentioned, but the paper does mention the need for \"generalized instructions\" and leveraging LLMs to produce \"human-like expressions\", implying potential limitations in LLMs' ability to generate human-like expressions without this guidance.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations are mentioned, but the paper does mention the need for \"generalized instructions\" and leveraging LLMs to produce \"human-like expressions\", implying potential limitations in LLMs' ability to generate human-like expressions without this guidance."
    },
    {
        "title": "ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models",
        "authors": "Yi-Lin Sung;Jaehong Yoon;Mohit Bansal",
        "site": "https://iclr.cc/virtual/2024/poster/18067",
        "summary": "Large Vision-Language Models (LVLMs) can understand the world comprehensively by integrating rich information from different modalities, achieving remarkable performance improvements on various multimodal downstream tasks. However, deploying LVLMs is often problematic due to their massive computational/energy costs and carbon consumption, making it infeasible to adopt conventional iterative global pruning, which is costly due to computing the Hessian matrix of the entire large model for sparsification. Alternatively, several studies have recently proposed layer-wise pruning approaches to avoid the expensive computation of global pruning and efficiently compress model weights according to their importance within a layer. However, these methods often suffer from suboptimal model compression due to their lack of a global perspective. To address this limitation in recent efficient pruning methods for large models, we propose Efficient Coarse-to-Fine Layer-Wise Pruning (ECoFLaP), a two-stage coarse-to-fine weight pruning approach for LVLMs. We first determine the sparsity ratios of different layers or blocks by leveraging the global importance score, which is efficiently computed based on the zeroth-order approximation of the global model gradients. Then, the multimodal model performs layer-wise unstructured weight pruning. We validate our proposed method across various multi-modal and single-modal models and datasets, demonstrating significant performance improvements over prevalent pruning techniques in the high-sparsity regime.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, deploying LVLMs is often problematic due to their massive computational/energy costs and carbon consumption, making it infeasible to adopt conventional iterative global pruning, which is costly due to computing the Hessian matrix of the entire large model for sparsification.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, deploying LVLMs is often problematic due to their massive computational/energy costs and carbon consumption, making it infeasible to adopt conventional iterative global pruning, which is costly due to computing the Hessian matrix of the entire large model for sparsification.\""
    },
    {
        "title": "Contrastive Preference Learning: Learning from Human Feedback without Reinforcement Learning",
        "authors": "Joey Hejna;Rafael Rafailov;Harshit Sikchi;Chelsea Finn;Scott Niekum;W. Bradley Knox;Dorsa Sadigh",
        "site": "https://iclr.cc/virtual/2024/poster/18062",
        "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular paradigm for aligning models with human intent. Typically RLHF algorithms operate in two phases: first, use human preferences to learn a reward function and second, align the model by optimizing the learned reward via reinforcement learning (RL). This paradigm assumes that human preferences are distributed according to reward, but recent work suggests that they instead follow the \\emph{regret} under the user's optimal policy. Thus, learning a reward function from feedback is not only based on a flawed assumption of human preference, but also leads to unwieldy optimization challenges that stem from policy gradients or bootstrapping in the RL phase. Because of these optimization challenges, contemporary RLHF methods restrict themselves to contextual bandit settings (e.g., as in large language models) or limit observation dimensionality (e.g., state-based robotics). We overcome these limitations by introducing a new family of algorithms for optimizing behavior from human feedback using the \\textit{regret}-based model of human preferences. Using the principle of maximum entropy, we derive \\fullname (\\abv), an algorithm for learning optimal policies from preferences without learning reward functions, circumventing the need for RL. \\abv is fully off-policy, uses only a simple contrastive objective, and can be applied to arbitrary MDPs. This enables \\abv to elegantly scale to high-dimensional and sequential RLHF problems while being simpler than prior methods.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Because of these optimization challenges, contemporary RLHF methods restrict themselves to contextual bandit settings (e.g., as in large language models)\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Because of these optimization challenges, contemporary RLHF methods restrict themselves to contextual bandit settings (e.g., as in large language models)\""
    },
    {
        "title": "How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations",
        "authors": "Tianyu Guo;Wei Hu;Song Mei;Huan Wang;Caiming Xiong;Silvio Savarese;Yu Bai",
        "site": "https://iclr.cc/virtual/2024/poster/18050",
        "summary": "While large language models based on the transformer architecture have demonstrated remarkable in-context learning (ICL) capabilities, understandings of such capabilities are still in an early stage, where existing theory and mechanistic understanding focus mostly on simple scenarios such as learning simple function classes. This paper takes initial steps on understanding ICL in more complex scenarios, by studying learning with \\emph{representations}. Concretely, we construct synthetic in-context learning problems with a compositional structure, where the label depends on the input through a possibly complex but \\emph{fixed} representation function, composed with a linear function that \\emph{differs} in each instance. By construction, the optimal ICL algorithm first transforms the inputs by the representation function, and then performs linear ICL on top of the transformed dataset. We show theoretically the existence of transformers that approximately implement such algorithms with mild depth and size.  Empirically, we find trained transformers consistently achieve near-optimal ICL performance in this setting, and exhibit the desired dissection where lower layers transforms the dataset and upper layers perform linear ICL. Through extensive probing and a new pasting experiment, we further reveal several mechanisms within the trained transformers, such as concrete copying behaviors on both the inputs and the representations, linear ICL capability of the upper layers alone, and a post-ICL representation selection mechanism in a harder mixture setting. These observed mechanisms align well with our theory and may shed light on how transformers perform ICL in more realistic scenarios.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "While large language models based on the transformer architecture have demonstrated remarkable in-context learning (ICL) capabilities, understandings of such capabilities are still in an early stage, where existing theory and mechanistic understanding focus mostly on simple scenarios such as learning simple function classes.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: While large language models based on the transformer architecture have demonstrated remarkable in-context learning (ICL) capabilities, understandings of such capabilities are still in an early stage, where existing theory and mechanistic understanding focus mostly on simple scenarios such as learning simple function classes."
    },
    {
        "title": "Parameter-Efficient Multi-Task Model Fusion with Partial Linearization",
        "authors": "Anke Tang;Li Shen;Yong Luo;Yibing Zhan;Han Hu;Bo Du;Yixin Chen;Dacheng Tao",
        "site": "https://iclr.cc/virtual/2024/poster/18043",
        "summary": "Large pre-trained models have enabled significant advances in machine learning and served as foundation components.Model fusion methods, such as task arithmetic, have been proven to be powerful and scalable to incorporate fine-tuned weights from different tasks into a multi-task model. However, efficiently fine-tuning large pre-trained models on multiple downstream tasks remains challenging, leading to inefficient multi-task model fusion.In this work, we propose a novel method to improve multi-task fusion for parameter-efficient fine-tuning techniques like LoRA fine-tuning.Specifically, our approach partially linearizes only the adapter modules and applies task arithmetic over the linearized adapters.This allows us to leverage the the advantages of model fusion over linearized fine-tuning, while still performing fine-tuning and inference efficiently.We demonstrate that our partial linearization technique enables a more effective fusion of multiple tasks into a single model, outperforming standard adapter tuning and task arithmetic alone.Experimental results demonstrate the capabilities of our proposed partial linearization technique to effectively construct unified multi-task models via the fusion of fine-tuned task vectors. We evaluate performance over an increasing number of tasks and find that our approach outperforms standard parameter-efficient fine-tuning techniques. The results highlight the benefits of partial linearization for scalable and efficient multi-task model fusion.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": "Eliya Nachmani;Alon Levkovitch;Roy Hirsch;Julian Salazar;Chulayuth Asawaroengchai;Soroosh Mariooryad;Ehud Rivlin;RJ Skerry-Ryan;Michelle Tadmor Ramanovich",
        "site": "https://iclr.cc/virtual/2024/poster/18042",
        "summary": "We present Spectron, a novel approach to adapting pre-trained large language models (LLMs) to perform spoken question answering (QA) and speech continuation. By endowing the LLM with a pre-trained speech encoder, our model becomes able to take speech inputs and generate speech outputs. The entire system is trained end-to-end and operates directly on spectrograms, simplifying our architecture. Key to our approach is a training objective that jointly supervises speech recognition, text continuation, and speech synthesis using only paired speech-text pairs, enabling a `cross-modal' chain-of-thought within a single decoding pass. Our method surpasses existing spoken language models in speaker preservation and semantic coherence. Furthermore, the proposed model improves upon direct initialization in retaining the knowledge of the original LLM as demonstrated through spoken QA datasets. We release our audio samples and spoken QA dataset via our website.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the title and abstract imply that pre-trained LLMs have limitations in handling speech inputs and outputs, which the paper aims to address.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the title and abstract imply that pre-trained LLMs have limitations in handling speech inputs and outputs, which the paper aims to address."
    },
    {
        "title": "ContextRef: Evaluating Referenceless Metrics for Image Description Generation",
        "authors": "Elisa Kreiss;Eric Zelikman;Christopher Potts;Nick Haber",
        "site": "https://iclr.cc/virtual/2024/poster/18041",
        "summary": "Referenceless metrics (e.g., CLIPScore) use pretrained vision--language models to assess image descriptions directly without costly ground-truth reference texts. Such methods can facilitate rapid progress, but only if they truly align with human preference judgments. In this paper, we introduce ContextRef, a benchmark for assessing referenceless metrics for such alignment. ContextRef has two components: human ratings along a variety of established quality dimensions, and ten diverse robustness checks designed to uncover fundamental weaknesses. A crucial aspect of ContextRef is that images and descriptions are presented in context, reflecting prior work showing that context is important for description quality. Using ContextRef, we assess a variety of pretrained models, scoring functions, and techniques for incorporating context. None of the methods is successful with ContextRef, but we show that careful fine-tuning yields substantial improvements. ContextRef remains a challenging benchmark though, in large part due to the challenge of context dependence.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"None of the methods is successful with ContextRef, but we show that careful fine-tuning yields substantial improvements.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"None of the methods is successful with ContextRef, but we show that careful fine-tuning yields substantial improvements.\""
    },
    {
        "title": "Language Models Represent Space and Time",
        "authors": "Wes Gurnee;Max Tegmark",
        "site": "https://iclr.cc/virtual/2024/poster/18036",
        "summary": "The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual \"space neurons\" and \"time neurons\" that reliably encode spatial and temporal coordinates. While further investigation is needed, our results suggest modern LLMs learn rich spatiotemporal representations of the real world and possess basic ingredients of a world model.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the abstract does not mention any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the abstract does not mention any limitations of LLMs."
    },
    {
        "title": "LitCab: Lightweight Language Model Calibration over Short- and Long-form Responses",
        "authors": "Xin Liu;Muhammad Khalifa;Lu Wang",
        "site": "https://iclr.cc/virtual/2024/poster/18033",
        "summary": "A model is considered well-calibrated when its probability estimate aligns with the actual likelihood of the output being correct. Calibrating language models (LMs) is crucial, as it plays a vital role in detecting and mitigating hallucinations of LMs as well as building more trustworthy models. However, standard calibration techniques may not be suited for LM calibration. For instance, post-processing methods such as temperature scaling do not reorder the candidate generations. On the other hand, training-based methods require fine-tuning the entire model, which is impractical for LMs of large scale. We present LitCab, a lightweight calibration mechanism consisting of a single linear layer that takes the input text representation and predicts a bias term, which is then added to the LM output logits. LitCab improves model calibration by only adding < 2% of the original model parameters. For evaluation, we construct CaT, a benchmark consisting of eight text generation tasks, covering responses ranging from short phrases to paragraphs. We test LitCab with Llama2-7B, where it improves calibration across all tasks, reducing the average ECE score by as large as 30%. We further conduct a comprehensive evaluation with multiple popular open-sourced LMs from GPT and LLaMA families, yielding the following key findings: (i) Larger models within the same family exhibit better calibration on tasks with short generation tasks, but not necessarily for longer ones. (ii) GPT-family models show superior calibration compared to LLaMA, Llama2, and Vicuna models, despite having much fewer parameters. (iii) Fine-tuning pretrained model (e.g., LLaMA) with samples of limited purpose (e.g., conversations) may lead to worse calibration, highlighting the importance of fine-tuning setups for calibrating LMs.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, standard calibration techniques may not be suited for LM calibration. For instance, post-processing methods such as temperature scaling do not reorder the candidate generations. On the other hand, training-based methods require fine-tuning the entire model, which is impractical for LMs of large scale.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, standard calibration techniques may not be suited for LM calibration. For instance, post-processing methods such as temperature scaling do not reorder the candidate generations. On the other hand, training-based methods require fine-tuning the entire model, which is impractical for LMs of large scale.\""
    },
    {
        "title": "Task Planning for Visual Room Rearrangement under Partial Observability",
        "authors": "Karan Mirakhor;Sourav Ghosh;Dipanjan Das;Brojeshwar Bhowmick",
        "site": "https://iclr.cc/virtual/2024/poster/18030",
        "summary": "This paper presents a novel hierarchical task planner under partial observabilitythat empowers an embodied agent to use visual input to efficiently plan a sequenceof actions for simultaneous object search and rearrangement in an untidy room,to achieve a desired tidy state. The paper introduces (i) a novel Search Networkthat utilizes commonsense knowledge from large language models to find unseenobjects, (ii) a Deep RL network trained with proxy reward, along with (iii) a novelgraph-based state representation to produce a scalable and effective planner thatinterleaves object search and rearrangement to minimize the number of steps takenand overall traversal of the agent, as well as to resolve blocked goal and swapcases, and (iv) a sample-efficient cluster-biased sampling for simultaneous trainingof the proxy reward network along with the Deep RL network. Furthermore,the paper presents new metrics and a benchmark dataset - RoPOR, to measurethe effectiveness of rearrangement planning. Experimental results show that ourmethod significantly outperforms the state-of-the-art rearrangement methods Weihset al. (2021a); Gadre et al. (2022); Sarch et al. (2022); Ghosh et al. (2022).",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"utilizes commonsense knowledge from large language models\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"utilizes commonsense knowledge from large language models\""
    },
    {
        "title": "OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text",
        "authors": "Keiran Paster;Marco Dos Santos;Zhangir Azerbayev;Jimmy Ba",
        "site": "https://iclr.cc/virtual/2024/poster/18029",
        "summary": "There is growing evidence that pretraining on high quality, carefully thought-out tokens such as code or mathematics plays an important role in improving the reasoning abilities of large language models. For example, Minerva, a PaLM model finetuned on billions of tokens of mathematical documents from arXiv and the web, reported dramatically improved performance on problems that require quantitative reasoning. However, because all known open source web datasets employ preprocessing that does not faithfully preserve mathematical notation, the benefits of large scale training on quantitive web documents are unavailable to the research community. We introduce OpenWebMath, an open dataset inspired by these works containing 14.7B tokens of mathematical webpages from Common Crawl. We describe in detail our method for extracting text and LaTeX content and removing boilerplate from HTML documents, as well as our methods for quality filtering and deduplication. Additionally, we run small-scale experiments by training 1.4B language models on OpenWebMath, showing that models trained on 14.7B tokens of our dataset surpass the performance of models trained on over 20x the amount of general language data. We hope that our dataset, open-sourced and released on the Hugging Face Hub, will help spur advances in the reasoning abilities of large language models.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, because all known open source web datasets employ preprocessing that does not faithfully preserve mathematical notation, the benefits of large scale training on quantitive web documents are unavailable to the research community.\"\n\nThis rating is chosen because the abstract mentions a limitation of existing datasets for training LLMs, but the primary focus of the paper is on introducing a new dataset and its benefits for improving LLM",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, because all known open source web datasets employ preprocessing that does not faithfully preserve mathematical notation, the benefits of large scale training on quantitive web documents are unavailable to the research community.\"\n\nThis rating is chosen because the abstract mentions a limitation of existing datasets for training LLMs, but the primary focus of the paper is on introducing a new dataset and its benefits for improving LLM"
    },
    {
        "title": "Look, Remember and Reason: Grounded Reasoning in Videos with Language Models",
        "authors": "Apratim Bhattacharyya;Sunny Panchal;Reza Pourreza;Mingu Lee;Pulkit Madan;Roland Memisevic",
        "site": "https://iclr.cc/virtual/2024/poster/18014",
        "summary": "Multi-modal language models (LM) have recently shown promising performance in high-level reasoning tasks on videos. However, existing methods still fall short in tasks like causal or  compositional spatiotemporal reasoning over actions, in which model predictions need to be grounded in fine-grained low-level details, such as object motions and object interactions.In this work, we propose training an LM end-to-end on low-level surrogate tasks, including object detection, re-identification, and tracking, to endow the  model with the required low-level visual capabilities. We show that a two-stream video encoder with spatiotemporal attention is effective at capturing the required static and motion-based cues in the video. By leveraging the LM's ability to perform the low-level surrogate tasks,  we can cast reasoning in videos as the three-step process ofLook, Remember, Reason, wherein visual information is extracted using low-level visual skills step-by-step and then integrated to arrive at a final answer. We demonstrate the effectiveness of our framework on diverse visual reasoning tasks from the ACRE, CATER, Something-Else and STAR datasets. Our approach is trainable end-to-end and surpasses state-of-the-art task-specific methods across these tasks by a large margin.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing methods still fall short in tasks like causal or compositional spatiotemporal reasoning over actions, in which model predictions need to be grounded in fine-grained low-level details, such as object motions and object interactions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, existing methods still fall short in tasks like causal or compositional spatiotemporal reasoning over actions, in which model predictions need to be grounded in fine-grained low-level details, such as object motions and object interactions.\""
    },
    {
        "title": "Knowledge Fusion of Large Language Models",
        "authors": "Fanqi Wan;Xinting Huang;Deng Cai;Xiaojun Quan;Wei Bi;Shuming Shi",
        "site": "https://iclr.cc/virtual/2024/poster/18013",
        "summary": "While training large language models (LLMs) from scratch can generate models with distinct functionalities and strengths, it comes at significant costs and may result in redundant capabilities. Alternatively, a cost-effective and compelling approach is to merge existing pre-trained LLMs into a more potent model. However, due to the varying architectures of these LLMs, directly blending their weights is impractical. In this paper, we introduce the notion of knowledge fusion for LLMs, aimed at combining the capabilities of existing LLMs and transferring them into a single LLM. By leveraging the generative distributions of source LLMs, we externalize their collective knowledge and unique strengths, thereby potentially elevating the capabilities of the target model beyond those of any individual source LLM. We validate our approach using three popular LLMs with different architectures—Llama-2, MPT, and OpenLLaMA—across various benchmarks and tasks. Our findings confirm that the fusion of LLMs can improve the performance of the target model across a range of capabilities such as reasoning, commonsense, and code generation. Our code, model weights, and data are public at \\url{https://github.com/fanqiwan/FuseLLM}.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, due to the varying architectures of these LLMs, directly blending their weights is impractical.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, due to the varying architectures of these LLMs, directly blending their weights is impractical.\""
    },
    {
        "title": "LLM Augmented LLMs: Expanding Capabilities through Composition",
        "authors": "Rachit Bansal;Bidisha Samanta;Siddharth Dalmia;Nitish Gupta;Sriram Ganapathy;Abhishek Bapna;Prateek Jain;Partha Talukdar",
        "site": "https://iclr.cc/virtual/2024/poster/18011",
        "summary": "Foundational models with billions of parameters which have been trained on large corpus of data have demonstrated non-trivial skills in a variety of domains. However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills. On the other hand, due to their adaptation abilities,several new instances of these models are being trained towards new domains and tasks.  In this work, we study the problem of efficient and practical composition of existing foundation models with more specific models to enable newer capabilities. To this end,  we propose CALM—Composition to Augment Language Models—which introduces cross-attention between models to compose their representations and enable new capabilities. Salient features of CALM are: (i) Scales up LLMs on new tasks by ‘re-using’ existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Applies to diverse domains and settings. We illustrate that augmenting PaLM2-S with a smaller model trained on low-resource languages results in an absolute improvement of up to 13% on tasks like translation into English and arithmetic reasoning for low-resource languages. Similarly,when PaLM2-S is augmented with a code-specific model, we see a relative improvement of 40% over the base model for code generation and explanation tasks—on-par with fully fine-tuned counterparts.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills.\""
    },
    {
        "title": "MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback",
        "authors": "Xingyao Wang;Zihan Wang;Jiateng Liu;Yangyi Chen;Lifan Yuan;Hao Peng;Heng Ji",
        "site": "https://iclr.cc/virtual/2024/poster/18006",
        "summary": "To solve complex tasks, large language models (LLMs) often require multiple rounds of interactions with the user, sometimes assisted by external tools.However, current evaluation protocols often emphasize benchmark performance with single-turn exchanges, neglecting the nuanced interactions among the user, LLMs, and external tools, while also underestimating the importance of natural language feedback from users. These oversights contribute to discrepancies between research benchmark evaluations and real-world use cases.We introduce MINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn interactions by (1) using tools and (2) leveraging natural language feedback.To ensure reproducibility, we provide an evaluation framework where LLMs can access tools by executing Python code and receive users' natural language feedback simulated by GPT-4.We repurpose a diverse set of established evaluation datasets focusing on reasoning, coding, and decision-making and carefully curate them into a compact subset for efficient evaluation.Our analysis of 20 open- and closed-source LLMs offers intriguing findings.(a) LLMs generally benefit from tools and language feedback, with performance gains (absolute, same below) of 1--8% for each turn of tool use and 2--17% with natural language feedback.(b) Better single-turn performance does not guarantee better multi-turn performance.(c) Surprisingly, on the LLMs evaluated, supervised instruction-finetuning (SIFT) and reinforcement learning from human feedback (RLHF) generally hurt multi-turn capabilities.We expect MINT can help measure progress and incentivize research in improving LLMs' capabilities in multi-turn interactions, especially for open-source communities where multi-turn human evaluation can be less accessible compared to commercial LLMs with a larger user base.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Better single-turn performance does not guarantee better multi-turn performance.\" and \"Surprisingly, on the LLMs evaluated, supervised instruction-finetuning (SIFT) and reinforcement learning from human feedback (RLHF) generally hurt multi-turn capabilities.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Better single-turn performance does not guarantee better multi-turn performance.\" and \"Surprisingly, on the LLMs evaluated, supervised instruction-finetuning (SIFT) and reinforcement learning from human feedback (RLHF) generally hurt multi-turn capabilities.\""
    },
    {
        "title": "Language-Informed Visual Concept Learning",
        "authors": "Sharon Lee;Yunzhi Zhang;Shangzhe Wu;Jiajun Wu",
        "site": "https://iclr.cc/virtual/2024/poster/18001",
        "summary": "Our understanding of the visual world is centered around various concept axes, characterizing different aspects of visual entities. While different concept axes can be easily specified by language, e.g., color, the exact visual nuances along each axis often exceed the limitations of linguistic articulations, e.g., a particular style of painting. In this work, our goal is to learn a language-informed visual concept representation, by simply distilling large pre-trained vision-language models. Specifically, we train a set of concept encoders to encode the information pertinent to a set of language-informed concept axes, with an objective of reproducing the input image through a pre-trained Text-to-Image (T2I) model. To encourage better disentanglement of different concept encoders, we anchor the concept embeddings to a set of text embeddings obtained from a pre-trained Visual Question Answering (VQA) model. At inference time, the model extracts concept embeddings along various axes from new test images, which can be remixed to generate images with novel compositions of visual concepts. With a lightweight test-time finetuning procedure, it can also generalize to novel concepts unseen at training.Project page at https://cs.stanford.edu/~yzzhang/projects/concept-axes.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the exact visual nuances along each axis often exceed the limitations of linguistic articulations\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the exact visual nuances along each axis often exceed the limitations of linguistic articulations\""
    },
    {
        "title": "Language Modeling Is Compression",
        "authors": "Gregoire Deletang;Anian Ruoss;Paul-Ambroise Duquenne;Elliot Catt;Tim Genewein;Christopher Mattern;Jordi Grau-Moya;Li Kevin Wenliang;Matthew Aitchison;Laurent Orseau;Marcus Hutter;Joel Veness",
        "site": "https://iclr.cc/virtual/2024/poster/17997",
        "summary": "It has long been established that predictive models can be transformed into lossless compressors and vice versa. Incidentally, in recent years, the machine learning community has focused on training increasingly large and powerful self-supervised (language) models. Since these large language models exhibit impressive predictive capabilities, they are well-positioned to be strong compressors. In this work, we advocate for viewing the prediction problem through the lens of compression and evaluate the compression capabilities of large (foundation) models. We show that large language models are powerful general-purpose predictors and that the compression viewpoint provides novel insights into scaling laws, tokenization, and in-context learning. For example, Chinchilla 70B, while trained primarily on text, compresses ImageNet patches to 43.4% and LibriSpeech samples to 16.4% of their raw size, beating domain-specific compressors like PNG (58.5%) or FLAC (30.3%), respectively. Finally, we show that the prediction-compression equivalence allows us to use any compressor (like gzip) to build a conditional generative model.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion",
        "authors": "Hee Suk Yoon;Eunseop Yoon;Joshua Tian Jin Tee;Mark A. Hasegawa-Johnson;Yingzhen Li;Chang D. Yoo",
        "site": "https://iclr.cc/virtual/2024/poster/17996",
        "summary": "In deep learning, test-time adaptation has gained attention as a method for model fine-tuning without the need for labeled data. A prime exemplification is the recently proposed test-time prompt tuning for large-scale vision-language models such as CLIP. Unfortunately, these prompts have been mainly developed to improve accuracy, overlooking the importance of calibration, which is a crucial aspect for quantifying prediction uncertainty. However, traditional calibration methods rely on substantial amounts of labeled data, making them impractical for test-time scenarios. To this end, this paper explores calibration during test-time prompt tuning by leveraging the inherent properties of CLIP. Through a series of observations, we find that the prompt choice significantly affects the calibration in CLIP, where the prompts leading to higher text feature dispersion result in better-calibrated predictions. Introducing the Average Text Feature Dispersion (ATFD), we establish its relationship with calibration error and present a novel method, Calibrated Test-time Prompt Tuning (C-TPT), for optimizing prompts during test-time with enhanced calibration. Through extensive experiments on different CLIP architectures and datasets, we show that C-TPT can effectively improve the calibration of test-time prompt tuning without needing labeled data. The code is publicly accessible at https://github.com/hee-suk-yoon/C-TPT.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Unfortunately, these prompts have been mainly developed to improve accuracy, overlooking the importance of calibration, which is a crucial aspect for quantifying prediction uncertainty.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Unfortunately, these prompts have been mainly developed to improve accuracy, overlooking the importance of calibration, which is a crucial aspect for quantifying prediction uncertainty.\""
    },
    {
        "title": "Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making",
        "authors": "Aliyah R. Hsu;Yeshwanth Cherapanamjeri;Briton Park;Tristan Naumann;Anobel Odisho;Bin Yu",
        "site": "https://iclr.cc/virtual/2024/poster/17994",
        "summary": "Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial, especially in high-stakes domains like medicine, to establish trust and ensure safety, which requires human engagement. We introduce SUFO, a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. SUFO utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis to address key questions about model trust and interpretability (e.g. model suitability for a task, feature space evolution during fine-tuning, and interpretation of fine-tuned features and failure modes). We conduct a case study investigating the impact of pre-training data where we focus on real-world pathology classification tasks, and validate our findings on MedNLI. We evaluate five 110M-sized pre-trained transformer models, categorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical BioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal that: (1) while PubMedBERT, the domain-specific model, contains valuable information for fine-tuning, it can overfit to minority classes when class imbalances exist. In contrast, mixed-domain models exhibit greater resistance to overfitting, suggesting potential improvements in domain-specific model robustness; (2) in-domain pre-training accelerates feature disambiguation during fine-tuning; and (3) feature spaces undergo significant sparsification during this process, enabling clinicians to identify common outlier modes among fine-tuned models as demonstrated in this paper. These findings showcase the utility of SUFO in enhancing trust and safety when using transformers in medicine, and we believe SUFO can aid practitioners in evaluating fine-tuned language models (LMs) for other applications in medicine and in more critical domains.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"while PubMedBERT, the domain-specific model, contains valuable information for fine-tuning, it can overfit to minority classes when class imbalances exist.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"while PubMedBERT, the domain-specific model, contains valuable information for fine-tuning, it can overfit to minority classes when class imbalances exist.\""
    },
    {
        "title": "Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs",
        "authors": "Shashank Gupta;Vaishnavi Shrivastava;Ameet Deshpande;Ashwin Kalyan;Peter Clark;Ashish Sabharwal;Tushar Khot",
        "site": "https://iclr.cc/virtual/2024/poster/17986",
        "summary": "Recent works have showcased the ability of large-scale language models (LLMs) to embody diverse personas in their responses, exemplified by prompts like ‘You are Yoda. Explain the Theory of Relativity.’ While this ability allows personalization of LLMs and enables human behavior simulation, its effect on LLMs’ capabilities remains unclear. To fill this gap, we present the first extensive study of the unintended side-effects of persona assignment on the ability of LLMs to performbasic reasoning tasks. Our study covers 24 reasoning datasets (spanning mathematics, law, medicine, morals, and more), 4 LLMs (2 versions of ChatGPT-3.5, GPT-4-Turbo, and Llama-2-70b-chat), and 19 diverse personas (e.g., ‘an Asian person’) spanning 5 socio-demographic groups: race, gender, religion, disability, and political affiliation. Our experiments unveil that LLMs harbor deep rooted bias against various socio-demographics underneath a veneer of fairness. While they overtly reject stereotypes when explicitly asked (‘Are Black people less skilled at mathematics?’), they manifest stereotypical and often erroneous presumptions when prompted to answer questions while adopting a persona. These can be observed as abstentions in the model’s response, e.g., ‘As a Black person, I am unable to answer this question as it requires math knowledge’, and generally result in a substantial drop in performance on reasoning tasks. Our experiments with ChatGPT-3.5 show that this bias isubiquitous—80% of our personas demonstrate bias; it issignificant—some datasets show performance drops of 70%+; and can be especiallyharmful for certain groups—some personas suffer statistically significant drops on 80%+ of the datasets. Overall, all four LLMs exhibit persona-induced bias to varying extents, with GPT-4-Turbo showing the least but still a problematic amount of bias (evident in 42% of the personas). Further analysis shows that these persona-induced errors can be hard-to-discern as they do not always manifest as explicit abstentions, and can also be hard-to-avoid—we find de-biasing prompts to have minimal to no effect. Our findings serve as a cautionary tale that the practice of assigning personas to LLMs—a trend on the rise—can surface their deep-rooted biases and have unforeseeable and detrimental side-effects.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "5",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our experiments unveil that LLMs harbor deep rooted bias against various socio-demographics underneath a veneer of fairness... These can be observed as abstentions in the model’s response... and generally result in a substantial drop in performance on reasoning tasks... Overall, all four LLMs exhibit persona-induced bias to varying extents... Further analysis shows that these persona-induced errors can be hard-to",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 5\nEvidence: \"Our experiments unveil that LLMs harbor deep rooted bias against various socio-demographics underneath a veneer of fairness... These can be observed as abstentions in the model’s response... and generally result in a substantial drop in performance on reasoning tasks... Overall, all four LLMs exhibit persona-induced bias to varying extents... Further analysis shows that these persona-induced errors can be hard-to"
    },
    {
        "title": "Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models",
        "authors": "Shuai Zhao;Xiaohan Wang;Linchao Zhu;Yi Yang",
        "site": "https://iclr.cc/virtual/2024/poster/17984",
        "summary": "One fascinating aspect of pre-trained vision-language models (VLMs) learning under language supervision is their impressive zero-shot generalization capability.However, this ability is hindered by distribution shifts between the training and testing data.Previous test time adaptation (TTA) methods for VLMs in zero-shot classification rely on minimizing the entropy of model outputs, tending to be stuck in incorrect model predictions.In this work, we propose TTA with feedback to rectify the model output and prevent the model from becoming blindly confident.Specifically, a CLIP model is adopted as the reward model during TTA and provides feedback for the VLM.Given a single test sample,the VLM is forced to maximize the CLIP reward between the input and sampled results from the VLM output distribution.The proposed \\textit{reinforcement learning with CLIP feedback~(RLCF)} framework is highly flexible and universal.Beyond the classification task, with task-specific sampling strategies and a proper reward baseline choice, RLCF can be easily extended to not only discrimination tasks like retrieval but also generalization tasks like image captioning,improving the zero-shot generalization capacity of VLMs.According to the characteristics of these VL tasks, we build different fully TTA pipelines with RLCF to improve the zero-shot generalization ability of various VLMs.Extensive experiments along with promisingempirical results demonstrate the effectiveness of RLCF.The code is available at https://github.com/mzhaoshuai/RLCF.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, this ability is hindered by distribution shifts between the training and testing data.Previous test time adaptation (TTA) methods for VLMs in zero-shot classification rely on minimizing the entropy of model outputs, tending to be stuck in incorrect model predictions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, this ability is hindered by distribution shifts between the training and testing data.Previous test time adaptation (TTA) methods for VLMs in zero-shot classification rely on minimizing the entropy of model outputs, tending to be stuck in incorrect model predictions.\""
    },
    {
        "title": "Large Language Models as Automated Aligners for benchmarking Vision-Language Models",
        "authors": "Yuanfeng Ji;Chongjian GE;Weikai Kong;Enze Xie;Zhengying Liu;Zhenguo Li;Ping Luo",
        "site": "https://iclr.cc/virtual/2024/poster/17968",
        "summary": "With the advancements in Large Language Models (LLMs), Vision-Language Models (VLMs) have reached a new level of sophistication, showing notable competence in executing intricate cognition and reasoning tasks. However, existing evaluation benchmarks, primarily relying on rigid, hand-crafted datasets to measure task-specific performance, face significant limitations in assessing the alignment of these increasingly anthropomorphic models with human intelligence. In this work, we address the limitations via Auto-Bench, which delves into exploring LLMs as proficient aligners, measuring the alignment between VLMs and human intelligence and value through automatic data curation and assessment. Specifically, for data curation, Auto-Bench utilizes LLMs (e.g., GPT-4) to automatically generate a vast set of question-answer-reasoning triplets via prompting on visual symbolic representations (e.g., captions, object locations, instance relationships, and etc. The curated data closely matches human intent, owing to the extensive world knowledge embedded in LLMs. Through this pipeline, a total of 28.5K human-verified and 3,504K unfiltered question-answer-reasoning triplets have been curated, covering 4 primary abilities and 16 sub-abilities. We subsequently engage LLMs like GPT-3.5 to serve as judges, implementing the quantitative and qualitative automated assessments to facilitate a comprehensive evaluation of VLMs. Our validation results reveal that LLMs are proficient in both evaluation data curation and model assessment, achieving an average agreement rate of 85%. We envision Auto-Bench as a flexible, scalable, and comprehensive benchmark for evaluating the evolving sophisticated VLMs.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing evaluation benchmarks, primarily relying on rigid, hand-crafted datasets to measure task-specific performance, face significant limitations in assessing the alignment of these increasingly anthropomorphic models with human intelligence.\"\n\nThis evidence indicates that the paper briefly mentions limitations of existing benchmarks, which are related to LLMs, but the primary focus is on the proposed solution, Auto-Bench, rather than exploring",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, existing evaluation benchmarks, primarily relying on rigid, hand-crafted datasets to measure task-specific performance, face significant limitations in assessing the alignment of these increasingly anthropomorphic models with human intelligence.\"\n\nThis evidence indicates that the paper briefly mentions limitations of existing benchmarks, which are related to LLMs, but the primary focus is on the proposed solution, Auto-Bench, rather than exploring"
    },
    {
        "title": "Towards Unified Multi-Modal Personalization: Large Vision-Language Models for Generative Recommendation and Beyond",
        "authors": "Tianxin Wei;Bowen Jin;Ruirui Li;Hansi Zeng;Zhengyang Wang;Jianhui Sun;Qingyu Yin;Hanqing Lu;Suhang Wang;Jingrui He;Xianfeng Tang",
        "site": "https://iclr.cc/virtual/2024/poster/17967",
        "summary": "Developing a universal model that can effectively harness heterogeneous resources and respond to a wide range of personalized needs has been a longstanding community aspiration. Our daily choices, especially in domains like fashion and retail, are substantially shaped by multi-modal data, such as pictures and textual descriptions. These modalities not only offer intuitive guidance but also cater to personalized user preferences. However, the predominant personalization approaches mainly focus on ID or text-based recommendation problems, failing to comprehend the information spanning various tasks or modalities. In this paper, our goal is to establish a Unified paradigm for Multi-modal Personalization systems (UniMP), which effectively leverages multi-modal data while eliminating the complexities associated with task- and modality-specific customization. We argue that the advancements in foundational generative modeling have provided the flexibility and effectiveness necessary to achieve the objective. In light of this, we develop a generic and extensible personalization generative framework, that can handle a wide range of personalized needs including item recommendation, product search, preference prediction, explanation generation, and further user-guided image generation. Our methodology enhances the capabilities of foundational language models for personalized tasks by seamlessly ingesting interleaved cross-modal user history information, ensuring a more precise and customized experience for users. To train and evaluate the proposed multi-modal personalized tasks, we also introduce a novel and comprehensive benchmark covering a variety of user requirements. Our experiments on the real-world benchmark showcase the model's potential, outperforming competitive methods specialized for each task.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit evidence of discussing limitations of LLMs, but mentions \"the predominant personalization approaches mainly focus on ID or text-based recommendation problems, failing to comprehend the information spanning various tasks or modalities\" which might be related to limitations of traditional language models, but it is not explicitly stated as a limitation of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit evidence of discussing limitations of LLMs, but mentions \"the predominant personalization approaches mainly focus on ID or text-based recommendation problems, failing to comprehend the information spanning various tasks or modalities\" which might be related to limitations of traditional language models, but it is not explicitly stated as a limitation of LLMs."
    },
    {
        "title": "Successor Heads: Recurring, Interpretable Attention Heads In The Wild",
        "authors": "Rhys Gould;Euan Ong;George Ogden;Arthur Conmy",
        "site": "https://iclr.cc/virtual/2024/poster/17955",
        "summary": "In this work we describe successor heads: attention heads that increment tokens with a natural ordering, such as numbers, months, and days.For example, successor heads increment 'Monday' into 'Tuesday'.We explain the successor head behavior with an approach rooted in mechanistic interpretability, the field that aims to explain how models complete tasks in human-understandable terms.Existing research in this area has struggled to find recurring, mechanistically interpretable large language model (LLM) components beyond small toy models. Further, existing results have led to very little insight to explain the internals of the larger models that are used in practice.In this paper, we analyze the behavior of successor heads in LLMs and find that they implement abstract representations that are common to different architectures. Successor heads form in LLMs with as few as 31 million parameters, and at least as many as 12 billion parameters, such as GPT-2, Pythia, and Llama-2.We find a set of 'mod 10' features that underlie how successor heads increment in LLMs across different architectures and sizes.We perform vector arithmetic with these features to edit head behavior and provide insights into numeric representations within LLMs. Additionally, we study the behavior of successor heads on natural language data, where we find that successor heads are important for achieving a low loss on examples involving succession, and also identify interpretable polysemanticity in a Pythia successor head.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"existing research in this area has struggled to find recurring, mechanistically interpretable large language model (LLM) components beyond small toy models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"existing research in this area has struggled to find recurring, mechanistically interpretable large language model (LLM) components beyond small toy models.\""
    },
    {
        "title": "COLLIE: Systematic Construction of Constrained Text Generation Tasks",
        "authors": "Shunyu Yao;Howard Chen;Austin W. Hanjie;Runzhe Yang;Karthik R Narasimhan",
        "site": "https://iclr.cc/virtual/2024/poster/17952",
        "summary": "Text generation under constraints have seen increasing interests in natural language processing, especially with the rapidly improving capabilities of large language models. However, existing benchmarks for constrained generation usually focus on fixed constraint types (e.g. generate a sentence containing certain words) that have proved to be easy for state-of-the-art models like GPT-4. We present COLLIE, a grammar-based framework that allows the specification of rich, compositional constraints with diverse generation levels (word, sentence, paragraph, passage) and modeling challenges (e.g. language understanding, logical reasoning, counting, semantic planning). We also develop tools for automatic extraction of task instances given a constraint structure and a raw text corpus. Using COLLIE, we compile the COLLIE-v1 dataset with 1,132 instances comprising 13 constraint structures. We perform systematic experiments across five state-of-the-art instruction-tuned language models and analyze their performances to reveal shortcomings. COLLIE is designed to be extensible and lightweight, and we hope the community finds it useful to develop more complex constraints and evaluations in the future.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We perform systematic experiments across five state-of-the-art instruction-tuned language models and analyze their performances to reveal shortcomings.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We perform systematic experiments across five state-of-the-art instruction-tuned language models and analyze their performances to reveal shortcomings.\""
    },
    {
        "title": "Multi-Scale Representations by Varying Window Attention for Semantic Segmentation",
        "authors": "Haotian Yan;Ming Wu;Chuang Zhang",
        "site": "https://iclr.cc/virtual/2024/poster/17946",
        "summary": "Abstract:Multi-scale learning is central to semantic segmentation. We visualize the effective receptive field (ERF) of canonical multi-scale representations and point out two risks learning them: \\textit{scale inadequacy} and \\textit{field inactivation}. A novel multi-scale learner, \\textbf{varying window attention} (VWA), is presented to address these issues. VWA leverages the local window attention (LWA) and disentangles LWA into the query window and context window, allowing the context's scale to vary for the query to learn representations at multiple scales. However, varying the context to large-scale windows (enlarging ratio $R$) can significantly increase the memory footprint and computation cost ($R^2$ times larger than LWA). We propose a simple but professional re-scaling strategy to zero the extra induced cost without compromising performance. Consequently, VWA uses the same cost as LWA to overcome the receptive limitation of the local window.Furthermore, depending on VWA and employing various MLPs, we introduce a multi-scale decoder (MSD), \\textbf{VWFormer}, to improve multi-scale representations for semantic segmentation. VWFormer achieves efficiency competitive with the most compute-friendly MSDs, like FPN and MLP decoder, but performs much better than any MSDs. For instance, using nearly half of UPerNet's computation, VWFormer outperforms it by $1.0\\%-2.5\\%$ mIoU on ADE20K.At little extra overhead, $\\sim 10$G FLOPs, Mask2Former armed with VWFormer improves by $1.0\\%-1.3\\%$.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Predicting Emergent Abilities with Infinite Resolution Evaluation",
        "authors": "Shengding Hu;Xin Liu;Xu Han;Xinrong Zhang;Chaoqun He;Weilin Zhao;Yankai Lin;Ning Ding;Zebin Ou;Guoyang Zeng;Zhiyuan Liu;Maosong Sun",
        "site": "https://iclr.cc/virtual/2024/poster/17945",
        "summary": "The scientific scale-up of large language models (LLMs) necessitates a comprehensive understanding of their scaling properties. However, the existing literature on the scaling properties only yields an incomplete answer: optimization loss decreases predictably as the model size increases, in line with established scaling law; yet no scaling law for task has been established and the task performances are far from predictable during scaling. Task performances typically show minor gains on small models until they improve dramatically once models exceed a size threshold, exemplifying the ''emergent abilities''. In this study, we discover that small models, although they exhibit minor performance, demonstrate critical and consistent task performance improvements that are not captured by conventional evaluation strategies due to insufficient measurement resolution. To measure such improvements, we introduce PassUntil, an evaluation strategy with theoretically infinite resolution, through massive sampling in the decoding phase. With PassUntil, we conduct a quantitative investigation into the scaling law of task performance. The investigation contains two parts. Firstly, a strict task scaling law that is not conventionally known to exist, is identified, enhancing the predictability of task performances. Remarkably, we are able to predict the performance of the 2.4B model on code generation with merely 0.05\\% deviation before training starts, which is the first systematic attempt to verify predictable scaling proposed by GPT-4's report. Secondly, underpinned by PassUntil, we are able to study emergent abilities quantitatively. We identify a kind of accelerated emergence whose scaling curve cannot be fitted by standard scaling law function and has a increasing speed. We then examine two hypothesis and imply that the ``multiple circuits hypothesis'' might be responsible for the accelerated emergence.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Task performances typically show minor gains on small models until they improve dramatically once models exceed a size threshold, exemplifying the ''emergent abilities''.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Task performances typically show minor gains on small models until they improve dramatically once models exceed a size threshold, exemplifying the ''emergent abilities''.\""
    },
    {
        "title": "Context-Aware Meta-Learning",
        "authors": "Christopher Fifty;Dennis Duan;Ronald Guenther Junkins;Ehsan Amid;Jure Leskovec;Christopher Re;Sebastian Thrun",
        "site": "https://iclr.cc/virtual/2024/poster/17939",
        "summary": "Large Language Models like ChatGPT demonstrate a remarkable capacity to learn new concepts during inference without any fine-tuning. However, visual models trained to detect new objects during inference have been unable to replicate this ability, and instead either perform poorly or require meta-training and/or fine-tuning on similar objects. In this work, we propose a meta-learning algorithm that emulates Large Language Models by learning new visual concepts during inference without fine-tuning. Our approach leverages a frozen pre-trained feature extractor, and analogous to in-context learning, recasts meta-learning as sequence modeling over datapoints with known labels and a test datapoint with an unknown label. On 8 out of 11 meta-learning benchmarks, our approach---without meta-training or fine-tuning---exceeds or matches the state-of-the-art algorithm, P>M>F, which is meta-trained on these benchmarks.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, visual models trained to detect new objects during inference have been unable to replicate this ability, and instead either perform poorly or require meta-training and/or fine-tuning on similar objects.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, visual models trained to detect new objects during inference have been unable to replicate this ability, and instead either perform poorly or require meta-training and/or fine-tuning on similar objects.\""
    },
    {
        "title": "Bridging Vision and Language Spaces with Assignment Prediction",
        "authors": "Jungin Park;Jiyoung Lee;Kwanghoon Sohn",
        "site": "https://iclr.cc/virtual/2024/poster/17937",
        "summary": "This paper introduces VLAP, a novel approach that bridges pretrained vision models and large language models (LLMs) to make frozen LLMs understand the visual world. VLAP transforms the embedding space of pretrained vision models into the LLMs' word embedding space using a single linear layer for efficient and general-purpose visual and language understanding. Specifically, we harness well-established word embeddings to bridge two modality embedding spaces. The visual and text representations are simultaneously assigned to a set of word embeddings within pretrained LLMs by formulating the assigning procedure as an optimal transport problem. We predict the assignment of one modality from the representation of another modality data, enforcing consistent assignments for paired multimodal data. This allows vision and language representations to contain the same information, grounding the frozen LLMs' word embedding space in visual data. Moreover, a robust semantic taxonomy of LLMs can be preserved with visual data since the LLMs interpret and reason linguistic information from correlations between word embeddings. Experimental results show that VLAP achieves substantial improvements over the previous linear transformation-based approaches across a range of vision-language tasks, including image captioning, visual question answering, and cross-modal retrieval. We also demonstrate the learned visual representations hold a semantic taxonomy of LLMs, making visual semantic arithmetic possible.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"make frozen LLMs understand the visual world.\"\n\nThis paper discusses LLMs, but it does not explicitly mention any limitations of LLMs. The phrase \"make frozen LLMs understand the visual world\" implies that LLMs have limitations in understanding visual data, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"make frozen LLMs understand the visual world.\"\n\nThis paper discusses LLMs, but it does not explicitly mention any limitations of LLMs. The phrase \"make frozen LLMs understand the visual world\" implies that LLMs have limitations in understanding visual data, but it is not the primary focus of the paper."
    },
    {
        "title": "Grounding Multimodal Large Language Models to the World",
        "authors": "Zhiliang Peng;Wenhui Wang;Li Dong;Yaru Hao;Shaohan Huang;Shuming Ma;Qixiang Ye;Furu Wei",
        "site": "https://iclr.cc/virtual/2024/poster/17934",
        "summary": "We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent text spans (i.e., referring expressions and noun phrases) as links in Markdown, i.e.,text span, where object descriptions are sequences of location tokens. To train the model, we construct a large-scale dataset about grounded image-text pairs (GrIT) together with multimodal corpora. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability to downstream applications, while maintaining the conventional capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning). Kosmos-2 is evaluated on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This study sheds a light on the big convergence of language, multimodal perception, and world modeling, which is a key step toward artificial general intelligence. Code can be found inhttps://aka.ms/kosmos-2.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations, but the paper aims to \"shed a light on the big convergence of language, multimodal perception, and world modeling\" which implies addressing potential limitations in existing models.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations, but the paper aims to \"shed a light on the big convergence of language, multimodal perception, and world modeling\" which implies addressing potential limitations in existing models."
    },
    {
        "title": "Delphic Offline Reinforcement Learning under Nonidentifiable Hidden Confounding",
        "authors": "Alizée Pace;Hugo Yèche;Bernhard Schölkopf;Gunnar Ratsch;Guy Tennenholtz",
        "site": "https://iclr.cc/virtual/2024/poster/17928",
        "summary": "A prominent challenge of offline reinforcement learning (RL) is the issue of hidden confounding: unobserved variables may influence both the actions taken by the agent and the observed outcomes. Hidden confounding can compromise the validity of any causal conclusion drawn from data and presents a major obstacle to effective offline RL. In the present paper, we tackle the problem of hidden confounding in the nonidentifiable setting. We propose a definition of uncertainty due to hidden confounding bias, termed delphic uncertainty, which uses variation over world models compatible with the observations, and differentiate it from the well-known epistemic and aleatoric uncertainties. We derive a practical method for estimating the three types of uncertainties, and construct a pessimistic offline RL algorithm to account for them. Our method does not assume identifiability of the unobserved confounders, and attempts to reduce the amount of confounding bias. We demonstrate through extensive experiments and ablations the efficacy of our approach on a sepsis management benchmark, as well as on electronic health records. Our results suggest that nonidentifiable hidden confounding bias can be mitigated to improve offline RL solutions in practice.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "The Cost of Scaling Down Large Language Models: Reducing Model Size Affects Memory before In-context Learning",
        "authors": "Tian Jin;Nolan Clement;Xin Dong;Vaishnavh Nagarajan;Michael Carbin;Jonathan Ragan-Kelley;Gintare Karolina Dziugaite",
        "site": "https://iclr.cc/virtual/2024/poster/17926",
        "summary": "We study how down-scaling large language model (LLM) size impacts LLM capabilities. We begin by measuring the effects of weight pruning – a popular technique for reducing model size – on the two abilities of LLMs: (a) recalling facts presented during pre-training and (b) processing information presented in context. Surprisingly, we find that existing pruning techniques affect these two abilities of LLMs differently. For example, pruning more than 30% of weights significantly decreases an LLM’s ability to recall facts presented during pre-training. Yet pruning 60-70% of weights largely preserves an LLM’s ability to process information in-context, ranging from retrieving answers based on information presented in context to learning parameterized functions such as a linear classifier based on a few examples. Moderate pruning impairs LLM’s ability to recall facts learnt from pre-training. However, its effect on model’s ability to process information presented in context is much less pronounced. The said disparate effects similarly arise when replacing the original model with a smaller dense one with reduced width and depth. This similarity suggests that model size reduction in general underpins the said disparity.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"pruning more than 30% of weights significantly decreases an LLM’s ability to recall facts presented during pre-training. Yet pruning 60-70% of weights largely preserves an LLM’s ability to process information in-context... Moderate pruning impairs LLM’s ability to recall facts learnt from pre-training.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"pruning more than 30% of weights significantly decreases an LLM’s ability to recall facts presented during pre-training. Yet pruning 60-70% of weights largely preserves an LLM’s ability to process information in-context... Moderate pruning impairs LLM’s ability to recall facts learnt from pre-training.\""
    },
    {
        "title": "The Expressive Power of Low-Rank Adaptation",
        "authors": "Yuchen Zeng;Kangwook Lee",
        "site": "https://iclr.cc/virtual/2024/poster/17923",
        "summary": "Abstract:*Low-Rank Adaptation* (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models.Despite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any model $f$ to accurately represent any smaller target model $\\bar{f}$ if LoRA-rank $\\geq(\\text{width of }f) \\times \\frac{\\text{depth of }\\bar{f}}{\\text{depth of }f}$, under a mild assumption. We also quantify the approximation error when the LoRA-rank is lower than the threshold. For Transformer networks, we show any model can be adapted to a target model of the same size with rank-$(\\frac{\\text{embedding size}}{2})$ LoRA adapters.All our theoretical insights are validated by numerical experiments.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The Expressive Power of Low-Rank Adaptation\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The Expressive Power of Low-Rank Adaptation\""
    },
    {
        "title": "ViLMA: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in Video-Language Models",
        "authors": "Ilker Kesen;Andrea Pedrotti;Mustafa Dogan;Michele Cafagna;Emre Can Acikgoz;Letitia Parcalabescu;Iacer Calixto;Anette Frank;Albert Gatt;Aykut Erdem;Erkut Erdem",
        "site": "https://iclr.cc/virtual/2024/poster/17922",
        "summary": "With the ever-increasing popularity of pretrained Video-Language Models (VidLMs), there is a pressing need to develop robust evaluation methodologies that delve deeper into their visio-linguistic capabilities. To address this challenge, we present ViLMA (Video Language Model Assessment), a task-agnostic benchmark that places the assessment of fine-grained capabilities of these models on a firm footing. Task-based evaluations, while valuable, fail to capture the complexities and specific temporal aspects of moving images that VidLMs need to process. Through carefully curated counterfactuals, ViLMA offers a controlled evaluation suite that sheds light on the true potential of these models, as well as their performance gaps compared to human-level understanding. ViLMA also includes proficiency tests, which assess basic capabilities deemed essential to solving the main counterfactual tests. We show that current VidLMs’ grounding abilities are no better than those of vision-language models which use static images. This is especially striking once the performance on proficiency tests is factored in. Our benchmark serves as a catalyst for future research on VidLMs, helping to highlight areas that still need to be explored.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We show that current VidLMs’ grounding abilities are no better than those of vision-language models which use static images. This is especially striking once the performance on proficiency tests is factored in.\"\n\nThis paper discusses the limitations of Video-Language Models (VidLMs), a type of multimodal model integrating language with video modality, in terms of their grounding abilities, specifically their",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We show that current VidLMs’ grounding abilities are no better than those of vision-language models which use static images. This is especially striking once the performance on proficiency tests is factored in.\"\n\nThis paper discusses the limitations of Video-Language Models (VidLMs), a type of multimodal model integrating language with video modality, in terms of their grounding abilities, specifically their"
    },
    {
        "title": "Ring-A-Bell! How Reliable are Concept Removal Methods For Diffusion Models?",
        "authors": "Yu-Lin Tsai;Chia-Yi Hsu;Chulin Xie;Chih-Hsun Lin;Jia You Chen;Bo Li;Pin-Yu Chen;Chia-Mu Yu;Chun-Ying Huang",
        "site": "https://iclr.cc/virtual/2024/poster/17920",
        "summary": "Diffusion models for text-to-image (T2I) synthesis, such as Stable Diffusion (SD), have recently demonstrated exceptional capabilities for generating high-quality content. However, this progress has raised several concerns of potential misuse, particularly in creating copyrighted, prohibited, and restricted content, or NSFW (not safe for work) images. While efforts have been made to mitigate such problems, either by implementing a safety filter at the evaluation stage or by fine-tuning models to eliminate undesirable concepts or styles, the effectiveness of these safety measures in dealing with a wide range of prompts remains largely unexplored. In this work, we aim to investigate these safety mechanisms by proposing one novel concept retrieval algorithm for evaluation. We introduce Ring-A-Bell, a model-agnostic red-teaming scheme for T2I diffusion models, where the whole evaluation can be prepared in advance without prior knowledge of the target model.Specifically, Ring-A-Bell first performs concept extraction to obtain holistic representations for sensitive and inappropriate concepts. Subsequently, by leveraging the extracted concept, Ring-A-Bell automatically identifies problematic prompts for diffusion models with the corresponding generation of inappropriate content, allowing the user to assess the reliability of deployed safety mechanisms. Finally, we empirically validate our method by testing online services such as Midjourney and various methods of concept removal. Our results show that Ring-A-Bell, by manipulating safe prompting benchmarks, can transform prompts that were originally regarded as safe to evade existing safety mechanisms, thus revealing the defects of the so-called safety mechanisms which could practically lead to the generation of harmful contents. In essence, Ring-A-Bell could serve as a red-teaming tool to understand the limitations of deployed safety mechanisms and to explore the risk under plausible attacks. Our codes are available at https://github.com/chiayi-hsu/Ring-A-Bell.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While efforts have been made to mitigate such problems, either by implementing a safety filter at the evaluation stage or by fine-tuning models to eliminate undesirable concepts or styles, the effectiveness of these safety measures in dealing with a wide range of prompts remains largely unexplored.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"While efforts have been made to mitigate such problems, either by implementing a safety filter at the evaluation stage or by fine-tuning models to eliminate undesirable concepts or styles, the effectiveness of these safety measures in dealing with a wide range of prompts remains largely unexplored.\""
    },
    {
        "title": "To the Cutoff... and Beyond? A Longitudinal Perspective on LLM Data Contamination",
        "authors": "Manley Roberts;Himanshu Thakur;Christine Herlihy;Colin White;Samuel Dooley",
        "site": "https://iclr.cc/virtual/2024/poster/17911",
        "summary": "Recent claims about the impressive abilities of large language models (LLMs) are often supported by evaluating publicly available benchmarks. Since LLMs train on wide swaths of the internet, this practice raises concerns of data contamination, i.e., evaluating on examples that are explicitly or implicitly included in the training data. Data contamination remains notoriously challenging to measure and mitigate, even with partial attempts like controlled experimentation of training data, canary strings, or embedding similarities. In this work, we conduct the first thorough longitudinal analysis of data contamination in LLMs by using the natural experiment of training cutoffs in GPT models to look at benchmarks released over time.Specifically, we consider two code/mathematical problem-solving datasets, Codeforces and Project Euler, and find statistically significant trends among LLM pass rate vs. GitHub popularity and release date that provide strong evidence of contamination. By open-sourcing our dataset, raw results, and evaluation framework, our work paves the way for rigorous analyses of data contamination in modern models. We conclude with a discussion of best practices and future steps for publicly releasing benchmark in the age of LLMs which  train on webscale data.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Data contamination remains notoriously challenging to measure and mitigate, even with partial attempts like controlled experimentation of training data, canary strings, or embedding similarities.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Data contamination remains notoriously challenging to measure and mitigate, even with partial attempts like controlled experimentation of training data, canary strings, or embedding similarities.\""
    },
    {
        "title": "DENEVIL: TOWARDS DECIPHERING AND NAVIGATING THE ETHICAL VALUES OF LARGE LANGUAGE MODELS VIA INSTRUCTION LEARNING",
        "authors": "Shitong Duan;Xiaoyuan Yi;Peng Zhang;Tun Lu;Xing Xie;Ning Gu",
        "site": "https://iclr.cc/virtual/2024/poster/17910",
        "summary": "Large Language Models (LLMs) have made unprecedented breakthroughs, yet their increasing integration into everyday life might raise societal risks due to generated unethical content. Despite extensive study on specific issues like bias, the intrinsic values of LLMs remain largely unexplored from a moral philosophy perspective. This work delves into ethical values utilizing Moral Foundation Theory. Moving beyond conventional discriminative evaluations with poor reliability, we propose DeNEVIL, a novel prompt generation algorithm tailored to dynamically exploit LLMs’ value vulnerabilities and elicit the violation of ethics in a generative manner, revealing their underlying value inclinations. On such a basis, we construct MoralPrompt, a high-quality dataset comprising 2,397 prompts covering 500+ value principles, and then benchmark the intrinsic values across a spectrum of LLMs. We discovered that most models are essentially misaligned, necessitating further ethical value alignment. In response, we develop VILMO, an in-context alignment method that substantially enhances the value compliance of LLM outputs by learning to generate appropriate value instructions, outperforming existing competitors. Our methods are suitable for black-box and open-source models, offering a promising initial step in studying the ethical values of LLMs.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite extensive study on specific issues like bias, the intrinsic values of LLMs remain largely unexplored from a moral philosophy perspective\"; \"We discovered that most models are essentially misaligned, necessitating further ethical value alignment.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Despite extensive study on specific issues like bias, the intrinsic values of LLMs remain largely unexplored from a moral philosophy perspective\"; \"We discovered that most models are essentially misaligned, necessitating further ethical value alignment.\""
    },
    {
        "title": "Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning",
        "authors": "Chengxing Jia;Chenxiao Gao;Hao Yin;Fuxiang Zhang;Xiong-Hui Chen;Tian Xu;Lei Yuan;Zongzhang Zhang;Zhi-Hua Zhou;Yang Yu",
        "site": "https://iclr.cc/virtual/2024/poster/17908",
        "summary": "Human beings can make adaptive decisions in a preparatory manner, i.e., by making preparations in advance, which offers significant advantages in scenarios where both online and offline experiences are expensive and limited. Meanwhile, current reinforcement learning methods commonly rely on numerous environment interactions but hardly obtain generalizable policies. In this paper, we introduce the idea of \\textit{rehearsal} into policy optimization, where the agent plans for all possible outcomes in mind and acts adaptively according to actual responses from the environment. To effectively rehearse, we propose ReDM, an algorithm that generates a diverse and eligible set of dynamics models and then rehearse the policy via adaptive training on the generated model set. Rehearsal enables the policy to make decision plans for various hypothetical dynamics and to naturally generalize to previously unseen environments. Our experimental results demonstrate that ReDM is capable of learning a valid policy solely through rehearsal, even with \\emph{zero} interaction data. We further extend ReDM to scenarios where limited or mismatched interaction data is available, and our experimental results reveal that ReDM produces high-performing policies compared to other offline RL baselines.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective",
        "authors": "Ming Zhong;Chenxin An;Weizhu Chen;Jiawei Han;Pengcheng He",
        "site": "https://iclr.cc/virtual/2024/poster/17899",
        "summary": "Large Language Models (LLMs) inherently encode a wealth of knowledge within their parameters through pre-training on extensive corpora. While prior research has delved into operations on these parameters to manipulate the underlying implicit knowledge — encompassing detection, editing, and merging — there remains an ambiguous understanding regarding their transferability across models with varying scales. In this paper, we seek to empirically investigate knowledge transfer from larger to smaller models through a parametric perspective. To achieve this, we employ sensitivity-based techniques to extract and align knowledge-specific parameters between different LLMs. Moreover, the LoRA module is used as the intermediary mechanism for injecting the extracted knowledge into smaller models. Evaluations across four benchmarks validate the efficacy of our proposed method. Our findings highlight the critical factors contributing to the process of parametric knowledge transfer, underscoring the transferability of model parameters across LLMs of different scales. Project website: https://maszhongming.github.io/ParaKnowTransfer.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"there remains an ambiguous understanding regarding their transferability across models with varying scales.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"there remains an ambiguous understanding regarding their transferability across models with varying scales.\""
    },
    {
        "title": "Emu: Generative Pretraining in Multimodality",
        "authors": "Quan Sun;Qiying Yu;Yufeng Cui;Fan Zhang;Xiaosong Zhang;Yueze Wang;Hongcheng Gao;Jingjing Liu;Tiejun Huang;Xinlong Wang",
        "site": "https://iclr.cc/virtual/2024/poster/17898",
        "summary": "We present Emu, a multimodal foundation model that seamlessly generates images and text in multimodal context. This omnivore model can take in any single-modality or multimodal data input indiscriminately (e.g., interleaved image, text and video) through a one-model-for-all autoregressive training process. First, visual signals are encoded into embeddings, and together with text tokens form an interleaved input sequence. Emu is end-to-end trained with a unified objective of classifying the next text token or regressing the next visual embedding in the multimodal sequence. This versatile multimodality empowers the leverage of diverse pretraining data sources at scale, such as videos with interleaved frames and text, webpages with interleaved images and text, as well as web-scale image-text pairs and video-text pairs. Emu can serve as a generalist multimodal interface for both image-to-text and text-to-image tasks, supporting in-context image and text generation. Across a broad range of zero-shot/few-shot tasks including image captioning, visual question answering, video question answering and text-to-image generation, Emu demonstrates superb performance compared to state-of-the-art large multimodal models. Extended capabilities such as multimodal assistants via instruction tuning are also demonstrated with impressive performance.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "Beyond task performance: evaluating and reducing the flaws of large multimodal models with in-context-learning",
        "authors": "Mustafa Shukor;Alexandre Rame;Corentin Dancette;Matthieu Cord",
        "site": "https://iclr.cc/virtual/2024/poster/17896",
        "summary": "Following the success of Large Language Models (LLMs), Large Multimodal Models (LMMs), such as the Flamingo model and its subsequent competitors, have started to emerge as natural steps towards generalist agents. However, interacting with recent LMMs reveals major limitations that are hardly captured by the current evaluation benchmarks. Indeed, task performances (e.g., VQA accuracy) alone do not provide enough clues to understand their real capabilities, limitations, and to which extent such models are aligned to human expectations. To refine our understanding of those flaws, we deviate from the current evaluation paradigm, and (1) evaluate 10 recent open-source LMMs from 3B up to 80B parameter scale,  on 5 different axes; hallucinations, abstention, compositionality, explainability and instruction following. Our evaluation on these axes reveals major flaws in LMMs. While the current go-to solution to align these models is based on training, such as instruction tuning or RLHF, we rather (2) explore the training-free in-context learning (ICL) as a solution, and study how it affects these limitations. Based on our ICL study, (3) we push ICL further and propose new multimodal ICL variants such as; Multitask-ICL, Chain-of-Hindsight-ICL, and Self-Correcting-ICL. Our findings are as follows; (1) Despite their success, LMMs have flaws that remain unsolved with scaling alone. (2) The effect of ICL on LMMs flaws is nuanced; despite its effectiveness for improved explainability, answer abstention, ICL only slightly improves instruction following, does not improve compositional abilities, and actually even amplifies hallucinations. (3) The proposed ICL variants are promising as post-hoc approaches to efficiently tackle some of those flaws. The code is available here: https://github.com/mshukor/EvALign-ICL.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, interacting with recent LMMs reveals major limitations that are hardly captured by the current evaluation benchmarks. Indeed, task performances (e.g., VQA accuracy) alone do not provide enough clues to understand their real capabilities, limitations, and to which extent such models are aligned to human expectations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, interacting with recent LMMs reveals major limitations that are hardly captured by the current evaluation benchmarks. Indeed, task performances (e.g., VQA accuracy) alone do not provide enough clues to understand their real capabilities, limitations, and to which extent such models are aligned to human expectations.\""
    },
    {
        "title": "LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts",
        "authors": "Hanan Gani;Shariq Farooq Bhat;Muzammal Naseer;Salman Khan;Peter Wonka",
        "site": "https://iclr.cc/virtual/2024/poster/17895",
        "summary": "Diffusion-based generative models have significantly advanced text-to-image generation but encounter challenges when processing lengthy and intricate text prompts describing complex scenes with multiple objects. While excelling in generating images from short, single-object descriptions, these models often struggle to faithfully capture all the nuanced details within longer and more elaborate textual inputs. In response, we present a novel approach leveraging Large Language Models (LLMs) to extract critical components from text prompts, including bounding box coordinates for foreground objects, detailed textual descriptions for individual objects, and a succinct background context. These components form the foundation of our layout-to-image generation model, which operates in two phases. The initial Global Scene Generation utilizes object layouts and background context to create an initial scene but often falls short in faithfully representing object characteristics as specified in the prompts. To address this limitation, we introduce an Iterative Refinement Scheme that iteratively evaluates and refines box-level content to align them with their textual descriptions, recomposing objects as needed to ensure consistency. Our evaluation on complex prompts featuring multiple objects demonstrates a substantial improvement in recall compared to baseline diffusion models. This is further validated by a user study, underscoring the efficacy of our approach in generating coherent and detailed scenes from intricate textual inputs. Our iterative framework offers a promising solution for enhancing text-to-image generation models' fidelity with lengthy, multifaceted descriptions, opening new possibilities for accurate and diverse image synthesis from textual inputs.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While excelling in generating images from short, single-object descriptions, these models often struggle to faithfully capture all the nuanced details within longer and more elaborate textual inputs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"While excelling in generating images from short, single-object descriptions, these models often struggle to faithfully capture all the nuanced details within longer and more elaborate textual inputs.\""
    },
    {
        "title": "BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity",
        "authors": "Andrew Luo;Margaret Marie Henderson;Michael J. Tarr;Leila Wehbe",
        "site": "https://iclr.cc/virtual/2024/poster/17893",
        "summary": "Understanding the functional organization of higher visual cortex is a central focus in neuroscience. Past studies have primarily mapped the visual and semantic selectivity of neural populations using hand-selected stimuli, which may potentially bias results towards pre-existing hypotheses of visual cortex functionality. Moving beyond conventional approaches, we introduce a data-driven method that generates natural language descriptions for images predicted to maximally activate individual voxels of interest. Our method -- Semantic Captioning Using Brain Alignments (\"BrainSCUBA\") -- builds upon the rich embedding space learned by a contrastive vision-language model and utilizes a pre-trained large language model to generate interpretable captions. We validate our method through fine-grained voxel-level captioning across higher-order visual regions. We further perform text-conditioned image synthesis with the captions, and show that our images are semantically coherent and yield high predicted activations. Finally, to demonstrate how our method enables scientific discovery, we perform exploratory investigations on the distribution of \"person\" representations in the brain, and discover fine-grained semantic selectivity in body-selective areas. Unlike earlier studies that decode text, our method derivesvoxel-wise captions of semantic selectivity. Our results show that BrainSCUBA is a promising means for understanding functional preferences in the brain, and provides motivation for further hypothesis-driven investigation of visual cortex.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"utilizes a pre-trained large language model to generate interpretable captions.\"\n\nThis paper mentions the use of a pre-trained large language model, but does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"utilizes a pre-trained large language model to generate interpretable captions.\"\n\nThis paper mentions the use of a pre-trained large language model, but does not discuss any limitations of LLMs."
    },
    {
        "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning",
        "authors": "Tri Dao",
        "site": "https://iclr.cc/virtual/2024/poster/17889",
        "summary": "Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention [5] exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4× compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2× speedup compared to FlashAttention, reaching 50-73% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72% model FLOPs utilization).",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Scaling Transformers to longer sequence lengths has been a major problem in the last several years...\"; \"FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40% of the theoretical maximum FLOPs/s.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Scaling Transformers to longer sequence lengths has been a major problem in the last several years...\"; \"FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40% of the theoretical maximum FLOPs/s.\""
    },
    {
        "title": "LLM-Assisted Code Cleaning For Training Accurate Code Generators",
        "authors": "Naman Jain;Tianjun Zhang;Wei-Lin Chiang;Joseph E. Gonzalez;Koushik Sen;Ion Stoica",
        "site": "https://iclr.cc/virtual/2024/poster/17888",
        "summary": "Natural language to code generation is an important application area of LLMs and has received wide attention from the community. The majority of relevant studies have exclusively concentrated on increasing the quantity and functional correctness of training sets while disregarding other stylistic elements of programs. More recently, data quality has garnered a lot of interest and multiple works have showcased its importance for improving performance. In this work, we investigate data quality for code and find that making the code more structured and readable leads to improved code generation performance of the system. We build a novel data-cleaning pipeline that uses these principles to transform existing programs by 1.) renaming variables, 2.) modularizing and decomposing complex code into smaller helper sub-functions, and 3.) inserting natural-language based planning annotations. We evaluate our approach on two challenging algorithmic code generation benchmarks and find that fine-tuning CodeLLaMa-7B on our transformed programs improves the performance by up to \\textbf{30\\%} compared to fine-tuning on the original dataset. Additionally, we demonstrate improved performance from using a smaller amount of higher-quality data, finding that a model fine-tuned on the entire original dataset is outperformed by a model trained on one-eighth of our cleaned dataset. Even in comparison to closed-source models, our models outperform the much larger AlphaCode models.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions \"disregarding other stylistic elements of programs\" which is a limitation of the majority of relevant studies, not of LLMs specifically.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions \"disregarding other stylistic elements of programs\" which is a limitation of the majority of relevant studies, not of LLMs specifically."
    },
    {
        "title": "RECOMP: Improving Retrieval-Augmented LMs with Context Compression and Selective Augmentation",
        "authors": "Fangyuan Xu;Weijia Shi;Eunsol Choi",
        "site": "https://iclr.cc/virtual/2024/poster/17885",
        "summary": "Retrieval-augmented language models improve language models (LMs) by retrieving documents and prepending them in-context.However, these documents, often spanning hundreds of words, make inference substantially less efficient. We propose compressing the retrieved documents into textual summaries prior to in-context integration. This not only reduces the computational costs but also relieve the burden of LMs to identify relevant information in long retrieved documents. We present two compressors -- an extractive compressor which selects useful sentences from retrieved documents  and an abstractive compressor which generates summary by synthesizing information from multiple documents. Both are trained to achieve performance gain in LMs when we prepend the generated summary from the compressor to LMs' input, while minimizing the summary length. When retrieved documents are irrelevant to the input or offer no additional information to LM, our compressors output an empty string, enabling selective augmentation. We evaluate our approach on the language modeling task and open domain question answering task. We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf summarization models. We show that our compressors trained for one LM can transfer to other LMs on the language modeling task and provide a summary largely faithful to the retrieved documents.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"these documents, often spanning hundreds of words, make inference substantially less efficient.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"these documents, often spanning hundreds of words, make inference substantially less efficient.\""
    },
    {
        "title": "Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation",
        "authors": "Xuefei Ning;Zinan Lin;Zixuan Zhou;Zifu Wang;Huazhong Yang;Yu Wang",
        "site": "https://iclr.cc/virtual/2024/poster/17880",
        "summary": "This work aims at decreasing the end-to-end generation latency of large language models (LLMs). One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated by the thinking and writing process of humans, we propose Skeleton-of-Thought (SoT), which first guides LLMs to generate the skeleton of the answer, and then conducts parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. Not only does SoT provide considerable speed-ups across 12 LLMs, but it can also potentially improve the answer quality on several question categories. SoT is an initial attempt at data-centric optimization for inference efficiency, and showcases the potential of eliciting high-quality answers by explicitly planning the answer structure in language.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs.\""
    },
    {
        "title": "Mega-TTS 2: Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis",
        "authors": "Ziyue Jiang;Jinglin Liu;Yi Ren;Jinzheng He;Zhenhui Ye;Shengpeng Ji;Qian Yang;Chen Zhang;Pengfei Wei;Chunfeng Wang;Xiang Yin;Zejun MA;Zhou Zhao",
        "site": "https://iclr.cc/virtual/2024/poster/17876",
        "summary": "Zero-shot text-to-speech (TTS) aims to synthesize voices with unseen speech prompts, which significantly reduces the data and computation requirements for voice cloning by skipping the fine-tuning process. However, the prompting mechanisms of zero-shot TTS still face challenges in the following aspects: 1) previous works of zero-shot TTS are typically trained with single-sentence prompts, which significantly restricts their performance when the data is relatively sufficient during the inference stage. 2) The prosodic information in prompts is highly coupled with timbre, making it untransferable to each other.This paper introduces Mega-TTS 2, a generic prompting mechanism for zero-shot TTS, to tackle the aforementioned challenges. Specifically, we design a powerful acoustic autoencoder that separately encodes the prosody and timbre information into the compressed latent space while providing high-quality reconstructions. Then, we propose a multi-reference timbre encoder and a prosody latent language model (P-LLM) to extract useful information from multi-sentence prompts. We further leverage the probabilities derived from multiple P-LLM outputs to produce transferable and controllable prosody. Experimental results demonstrate that Mega-TTS 2 could not only synthesize identity-preserving speech with a short prompt of an unseen speaker from arbitrary sources but consistently outperform the fine-tuning method when the volume of data ranges from 10 seconds to 5 minutes. Furthermore, our method enables to transfer various speaking styles to the target timbre in a fine-grained and controlled manner. Audio samples can be found in https://boostprompt.github.io/boostprompt/.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"previous works of zero-shot TTS are typically trained with single-sentence prompts, which significantly restricts their performance when the data is relatively sufficient during the inference stage.\"\n\nThis paper discusses Large Language Models (LLMs) in the context of zero-shot text-to-speech synthesis, but only briefly mentions a limitation of previous works in this area.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"previous works of zero-shot TTS are typically trained with single-sentence prompts, which significantly restricts their performance when the data is relatively sufficient during the inference stage.\"\n\nThis paper discusses Large Language Models (LLMs) in the context of zero-shot text-to-speech synthesis, but only briefly mentions a limitation of previous works in this area."
    },
    {
        "title": "Scalable Language Model with Generalized Continual Learning",
        "authors": "Bohao PENG;Zhuotao Tian;Shu Liu;Ming-Chang Yang;Jiaya Jia",
        "site": "https://iclr.cc/virtual/2024/poster/17874",
        "summary": "Continual learning has gained increasing importance as it facilitates the acquisition and refinement of scalable knowledge and skills in language models. However, existing methods typically encounter strict limitations and challenges in real-world scenarios, such as reliance on experience replay, optimization constraints, and inference task-ID. In this study, we introduce the Scalable Language Model (SLM) to overcome these limitations within a more challenging and generalized setting, representing a significant advancement toward practical applications for continual learning. Specifically, we propose the Joint Adaptive Re-Parameterization (JARe), integrated with Dynamic Task-related Knowledge Retrieval (DTKR), to enable adaptive adjustment of language models based on specific downstream tasks. This approach leverages the task distribution within the vector space, aiming to achieve a smooth and effortless continual learning process. Our method demonstrates state-of-the-art performance on diverse backbones and benchmarks, achieving effective continual learning in both full-set and few-shot scenarios with minimal forgetting. Moreover, while prior research primarily focused on a single task type such as classification, our study goes beyond, with the large language model, i.e., LLaMA-2, to explore the effects across diverse domains and task types, such that a single language model can be decently scaled to broader applications. The code and models will be released to the public.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing methods typically encounter strict limitations and challenges in real-world scenarios, such as reliance on experience replay, optimization constraints, and inference task-ID.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, existing methods typically encounter strict limitations and challenges in real-world scenarios, such as reliance on experience replay, optimization constraints, and inference task-ID.\""
    },
    {
        "title": "HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments",
        "authors": "Qinhong Zhou;Sunli Chen;Yisong Wang;Haozhe Xu;Weihua Du;Hongxin Zhang;Yilun Du;Joshua B. Tenenbaum;Chuang Gan",
        "site": "https://iclr.cc/virtual/2024/poster/17872",
        "summary": "Recent advances in high-fidelity virtual environments serve as one of the major driving forces for building intelligent embodied agents to perceive, reason and interact with the physical world. Typically, these environments remain unchanged unless agents interact with them. However, in real-world scenarios, agents might also face dynamically changing environments characterized by unexpected events and need to rapidly take action accordingly. To remedy this gap, we propose a new simulated embodied benchmark, called HAZARD, specifically designed to assess the decision-making abilities of embodied agents in dynamic situations. HAZARD consists of three unexpected disaster scenarios, including fire, flood, and wind, and specifically supports the utilization of large language models (LLMs) to assist common sense reasoning and decision-making. This benchmark enables us to evaluate autonomous agents' decision-making capabilities across various pipelines, including reinforcement learning (RL), rule-based, and search-based methods in dynamically changing environments. As a first step toward addressing this challenge using large language models, we further develop an LLM-based agent and perform an in-depth analysis of its promise and challenge of solving these challenging tasks. HAZARD is available at https://vis-www.cs.umass.edu/hazard/.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"As a first step toward addressing this challenge using large language models, we further develop an LLM-based agent and perform an in-depth analysis of its promise and challenge of solving these challenging tasks.\"\n\nThis rating is given because the abstract mentions the challenges of using LLMs in dynamically changing environments, but it is not the primary focus of the paper and the discussion is brief.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"As a first step toward addressing this challenge using large language models, we further develop an LLM-based agent and perform an in-depth analysis of its promise and challenge of solving these challenging tasks.\"\n\nThis rating is given because the abstract mentions the challenges of using LLMs in dynamically changing environments, but it is not the primary focus of the paper and the discussion is brief."
    },
    {
        "title": "Listen, Think, and Understand",
        "authors": "Yuan Gong;Hongyin Luo;Alexander H. Liu;Leonid Karlinsky;James R. Glass",
        "site": "https://iclr.cc/virtual/2024/poster/17867",
        "summary": "The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and reasoning ability? In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is the first multimodal large language model that focuses on general audio (rather than just speech) understanding.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities.\""
    },
    {
        "title": "MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations",
        "authors": "Hanlei Zhang;Xin Wang;Hua Xu;Qianrui Zhou;Kai Gao;Jianhua Su;jinyue Zhao;Wenrui Li;Yanting Chen",
        "site": "https://iclr.cc/virtual/2024/poster/17855",
        "summary": "Multimodal intent recognition poses significant challenges, requiring the incorporation of non-verbal modalities from real-world contexts to enhance the comprehension of human intentions. However, most existing multimodal intent benchmark datasets are limited in scale and suffer from difficulties in handling out-of-scope samples that arise in multi-turn conversational interactions. In this paper, we introduce MIntRec2.0, a large-scale benchmark dataset for multimodal intent recognition in multi-party conversations. It contains 1,245 high-quality dialogues with 15,040 samples, each annotated within a new intent taxonomy of 30 fine-grained classes, across text, video, and audio modalities. In addition to more than 9,300 in-scope samples, it also includes over 5,700 out-of-scope samples appearing in multi-turn contexts, which naturally occur in real-world open scenarios, enhancing its practical applicability. Furthermore, we provide comprehensive information on the speakers in each utterance, enriching its utility for multi-party conversational research. We establish a general framework supporting the organization of single-turn and multi-turn dialogue data, modality feature extraction, multimodal fusion, as well as in-scope classification and out-of-scope detection. Evaluation benchmarks are built using classic multimodal fusion methods, ChatGPT, and human evaluators. While existing methods incorporating nonverbal information yield improvements, effectively leveraging context information and detecting out-of-scope samples remains a substantial challenge. Notably, powerful large language models exhibit a significant performance gap compared to humans, highlighting the limitations of machine learning methods in the advanced cognitive intent understanding task. We believe that MIntRec2.0 will serve as a valuable resource, providing a pioneering foundation for research in human-machine conversational interactions, and significantly facilitating related applications.The full dataset and codes are available for use at https://github.com/thuiar/MIntRec2.0.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Notably, powerful large language models exhibit a significant performance gap compared to humans, highlighting the limitations of machine learning methods in the advanced cognitive intent understanding task.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Notably, powerful large language models exhibit a significant performance gap compared to humans, highlighting the limitations of machine learning methods in the advanced cognitive intent understanding task.\""
    },
    {
        "title": "Escape Sky-high Cost: Early-stopping Self-Consistency for Multi-step Reasoning",
        "authors": "Yiwei Li;Peiwen Yuan;Shaoxiong Feng;Boyuan Pan;Xinglin Wang;Bin Sun;Heda Wang;Kan Li",
        "site": "https://iclr.cc/virtual/2024/poster/17848",
        "summary": "Self-consistency (SC) has been a widely used decoding strategy for chain-of-thought reasoning. Despite bringing significant performance improvements across a variety of multi-step reasoning tasks, it is a high-cost method that requires multiple sampling with the preset size. In this paper, we propose a simple and scalable sampling process, Early-Stopping Self-Consistency (ESC), to greatly reduce the cost of SC without sacrificing performance. On this basis, one control scheme for ESC is further derivated to dynamically choose the performance-cost balance for different tasks and models. To demonstrate ESC's effectiveness, we conducted extensive experiments on three popular categories of reasoning tasks: arithmetic, commonsense and symbolic reasoning over language models with varying scales. The empirical results show that ESC reduces the average number of sampling of chain-of-thought reasoning by a significant margin on six benchmarks, including MATH (-33.8%), GSM8K (-80.1%), StrategyQA (-76.8%), CommonsenseQA (-78.5%), Coin Flip (-84.2%) and Last Letters (-67.4%), while attaining comparable performances.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "not extracted",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs:  \nEvidence: None"
    },
    {
        "title": "Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph",
        "authors": "Jiashuo Sun;Chengjin Xu;Lumingyuan Tang;Saizhuo Wang;Chen Lin;Yeyun Gong;Lionel Ni;Heung-Yeung Shum;Jian Guo",
        "site": "https://iclr.cc/virtual/2024/poster/17844",
        "summary": "Abstract:Although large language models (LLMs) have achieved significant success in various tasks, they often struggle with hallucination problems, especially in scenarios requiring deep and responsible reasoning. These issues could be partially addressed by introducing external knowledge graphs (KG) in LLM reasoning. In this paper, we propose a new LLM-KG integrating paradigm ``$\\hbox{LLM}\\otimes\\hbox{KG}$'' which treats the LLM as an agent to interactively explore related entities and relations on KGs and perform reasoning based on the retrieved knowledge. We further implement this paradigm by introducing a new approach called Think-on-Graph (ToG), in which the LLM agent iteratively executes beam search on KG, discovers the most promising reasoning paths, and returns the most likely reasoning results. We use a number of well-designed experiments to examine and illustrate the following advantages of ToG: 1) compared with LLMs, ToG has better deep reasoning power; 2) ToG has the ability of knowledge traceability and knowledge correctability by leveraging LLMs reasoning and expert feedback; 3) ToG provides a flexible plug-and-play framework for different LLMs, KGs and prompting strategies without any additional training cost; 4) the performance of ToG with small LLM models could exceed large LLM such as GPT-4 in certain scenarios and this reduces the cost of LLM deployment and application. As a training-free method with lower computational cost and better generality, ToG achieves overall SOTA in 6 out of 9 datasets where most previous SOTAs rely on additional training.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although large language models (LLMs) have achieved significant success in various tasks, they often struggle with hallucination problems, especially in scenarios requiring deep and responsible reasoning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although large language models (LLMs) have achieved significant success in various tasks, they often struggle with hallucination problems, especially in scenarios requiring deep and responsible reasoning.\""
    },
    {
        "title": "Sample-Efficient Multi-Agent RL: An Optimization Perspective",
        "authors": "Nuoya Xiong;Zhihan Liu;Zhaoran Wang;Zhuoran Yang",
        "site": "https://iclr.cc/virtual/2024/poster/17834",
        "summary": "We study multi-agent reinforcement learning (MARL) for the general-sum Markov Games (MGs) under general function approximation.     In order to find the minimum assumption for sample-efficient learning, we introduce a novel complexity measure called the Multi-Agent Decoupling Coefficient (MADC) for general-sum MGs. Using this measure, we propose the first unified algorithmic framework that ensures sample efficiency in learning Nash Equilibrium, Coarse Correlated Equilibrium, and Correlated Equilibrium for both model-based and model-free MARL problems with low MADC. We also show that our algorithm provides comparable sublinear regret to the existing works. Moreover, our algorithm combines an equilibrium-solving oracle with a single objective optimization subprocedure that solves for the regularized payoff of each deterministic joint policy, which avoids solving constrained optimization problems within data-dependent constraints (Jin et al. 2020; Wang et al. 2023) or executing sampling procedures with complex multi-objective optimization problems (Foster et al. 2023), thus being more amenable to empirical implementation.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "WebArena: A Realistic Web Environment for Building Autonomous Agents",
        "authors": "Shuyan Zhou;Frank F. Xu;Hao Zhu;Xuhui Zhou;Robert Lo;Abishek Sridhar;Xianyi Cheng;Tianyue Ou;Yonatan Bisk;Daniel Fried;Uri Alon;Graham Neubig",
        "site": "https://iclr.cc/virtual/2024/poster/17826",
        "summary": "With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting.  The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that \\ours can be used to measure such progress.\\footnote{Code, data, environment reproduction instructions, video demonstrations are available in the supplementary.}",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks"
    },
    {
        "title": "Online Information Acquisition: Hiring Multiple Agents",
        "authors": "Federico Cacciamani;Matteo Castiglioni;Nicola Gatti",
        "site": "https://iclr.cc/virtual/2024/poster/17818",
        "summary": "Abstract:We investigate the mechanism design problem faced by a principal who hires \\emph{multiple} agents to gather and report costly information. Then, the principal exploits the  information to make an informed decision.  We model this problem as a game, where the principal announces a mechanism consisting in action recommendations and a payment function, a.k.a. scoring rule. Then, each agent chooses an effort level and receives partial information about an underlying state of nature based on the effort. Finally, the agents report the information (possibly non-truthfully), the principal takes a decision based on this information, and the agents are paid according to the scoring rule. While previous work focuses on single-agent problems, we consider multi-agents settings. This poses the challenge of coordinating the agents' efforts and aggregating correlated information. Indeed, we show that optimal mechanisms must correlate agents' efforts, which introduces externalities among the agents, and hence complex incentive compatibility constraints and equilibrium selection problems. First, we design a polynomial-time algorithm to find an optimal incentive compatible mechanism. Then, we study an online problem, where the principal repeatedly interacts with a group of unknown agents. We design a no-regret algorithm that provides $\\widetilde{\\mathcal{O}}(T^{2/3})$ regret with respect to an optimal mechanism, matching the state-of-the-art bound for single-agent settings.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Retrieval is Accurate Generation",
        "authors": "Bowen Cao;Deng Cai;Leyang Cui;Xuxin Cheng;Wei Bi;Yuexian Zou;Shuming Shi",
        "site": "https://iclr.cc/virtual/2024/poster/17816",
        "summary": "Standard language models generate text by selecting tokens from a fixed, finite, and standalone vocabulary. We introduce a novel method that selects context-aware phrases from a collection of supporting documents. One of the most significant challenges for this paradigm shift is determining the training oracles, because a string of text can be segmented in various ways and each segment can be retrieved from numerous possible documents. To address this, we propose to initialize the training oracles using linguistic heuristics and, more importantly, bootstrap the oracles through iterative self-reinforcement. Extensive experiments show that our model not only outperforms standard language models on a variety of knowledge-intensive tasks but also demonstrates improved generation quality in open-ended text generation. For instance, compared to the standard language model counterpart, our model raises the accuracy from 23.47% to 36.27% on OpenbookQA, and improves the MAUVE score from 42.61% to 81.58% in open-ended text generation. Remarkably, our model also achieves the best performance and the lowest latency among several retrieval-augmented baselines. In conclusion, we assert that retrieval is more accurate generation and hope that our work will encourage further research on this new paradigm shift.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Standard language models generate text by selecting tokens from a fixed, finite, and standalone vocabulary.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Standard language models generate text by selecting tokens from a fixed, finite, and standalone vocabulary.\""
    },
    {
        "title": "Magnushammer: A Transformer-Based Approach to Premise Selection",
        "authors": "Maciej Mikuła;Szymon Tworkowski;Szymon Antoniak;Bartosz Piotrowski;Albert Q. Jiang;Jin Peng Zhou;Christian Szegedy;Łukasz Kuciński;Piotr Miłoś;Yuhuai Wu",
        "site": "https://iclr.cc/virtual/2024/poster/17814",
        "summary": "Abstract:This paper presents a novel approach to premise selection, a crucial reasoning task in automated theorem proving. Traditionally, symbolic methods that rely on extensive domain knowledge and engineering effort are applied to this task. In contrast, this work demonstrates that contrastive training with the transformer architecture can achieve higher-quality retrieval of relevant premises, without the knowledge or feature engineering overhead. Our method, Magnushammer, outperforms the most advanced and widely used automation tool in interactive theorem proving called Sledgehammer. On the PISA and miniF2f benchmarks Magnushammer achieves $59.5\\%$ (against $38.3\\%$) and $34.0\\%$ (against $20.9\\%$) success rates, respectively. By combining Magnushammer with a language-model-based automated theorem prover, we further improve the state-of-the-art proof success rate from $57.0\\%$ to $71.0\\%$ on the PISA benchmark using $4$x fewer parameters. Moreover, we develop and open source a novel dataset for premise selection, containing textual representations of (proof state, relevant premise) pairs. To the best of our knowledge, this is the largest available premise selection dataset, and the first dataset of this kind for the Isabelle proof assistant.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"By combining Magnushammer with a language-model-based automated theorem prover...\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"By combining Magnushammer with a language-model-based automated theorem prover...\""
    },
    {
        "title": "Analyzing and Mitigating Object Hallucination in Large Vision-Language Models",
        "authors": "Yiyang Zhou;Chenhang Cui;Jaehong Yoon;Linjun Zhang;Zhun Deng;Chelsea Finn;Mohit Bansal;Huaxiu Yao",
        "site": "https://iclr.cc/virtual/2024/poster/17813",
        "summary": "Large vision-language models (LVLMs) have shown remarkable abilities in understanding visual information with human languages. However, LVLMs still suffer from object hallucination, which is the problem of generating descriptions that include objects that do not actually exist in the images. This can negatively impact many vision-language tasks, such as visual summarization and reasoning. To address this issue, we propose a simple yet powerful algorithm, LVLM Hallucination Revisor (LURE), to post-hoc rectify object hallucination in LVLMs by reconstructing less hallucinatory descriptions. LURE is grounded in a rigorous statistical analysis of the key factors underlying object hallucination, including co-occurrence (the frequent appearance of certain objects alongside others in images), uncertainty (objects with higher uncertainty during LVLM decoding), and object position (hallucination often appears in the later part of the generated text). LURE can also be seamlessly integrated with any LVLMs. We evaluate LURE on six open-source LVLMs and found it outperforms the previous best approach in both general object hallucination evaluation metrics, GPT, and human evaluations.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, LVLMs still suffer from object hallucination, which is the problem of generating descriptions that include objects that do not actually exist in the images.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, LVLMs still suffer from object hallucination, which is the problem of generating descriptions that include objects that do not actually exist in the images.\""
    },
    {
        "title": "Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation",
        "authors": "Xinyu Tang;Richard Shin;Huseyin A Inan;Andre Manoel;Fatemehsadat Mireshghallah;Zinan Lin;Sivakanth Gopi;Janardhan Kulkarni;Robert Sim",
        "site": "https://iclr.cc/virtual/2024/poster/17812",
        "summary": "We study the problem of in-context learning (ICL) with large language models (LLMs) on private datasets. This scenario poses privacy risks, as LLMs may leak or regurgitate the private examples demonstrated in the prompt.We propose a novel algorithm that generates synthetic few-shot demonstrations from the private dataset with formal differential privacy (DP) guarantees, and show empirically that it can achieve effective ICL.We conduct extensive experiments on standard benchmarks and compare our algorithm with non-private ICL and zero-shot solutions. Our results demonstrate that our algorithm can achieve competitive performance with strong privacy levels.These results open up new possibilities for ICL with privacy protection for a broad range of applications.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This scenario poses privacy risks, as LLMs may leak or regurgitate the private examples demonstrated in the prompt.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"This scenario poses privacy risks, as LLMs may leak or regurgitate the private examples demonstrated in the prompt.\""
    },
    {
        "title": "AffineQuant: Affine Transformation Quantization for Large Language Models",
        "authors": "Yuexiao Ma;Huixia Li;Xiawu Zheng;Feng Ling;Xuefeng Xiao;Rui Wang;Shilei Wen;Fei Chao;Rongrong Ji",
        "site": "https://iclr.cc/virtual/2024/poster/17809",
        "summary": "Abstract:The significant resource requirements associated with Large-scale Language Models (LLMs) have generated considerable interest in the development of techniques aimed at compressing and accelerating neural networks. Among these techniques, Post-Training Quantization (PTQ) has emerged as a subject of considerable interest due to its noteworthy compression efficiency and cost-effectiveness in the context of training. Existing PTQ methods for LLMs limit the optimization scope to scaling transformations between pre- and post-quantization weights. This constraint results in significant errors after quantization, particularly in low-bit configurations. In this paper, we advocate for the direct optimization using equivalent Affine transformations in PTQ (AffineQuant). This approach extends the optimization scope and thus significantly minimizing quantization errors. Additionally, by employing the corresponding inverse matrix, we can ensure equivalence between the pre- and post-quantization outputs of PTQ, thereby maintaining its efficiency and generalization capabilities. To ensure the invertibility of the transformation during optimization, we further introduce a gradual mask optimization method. This method initially focuses on optimizing the diagonal elements and gradually extends to the other elements. Such an approach aligns with the Levy-Desplanques theorem, theoretically ensuring invertibility of the transformation. As a result, significant performance improvements are evident across different LLMs on diverse datasets. Notably, these improvements are most pronounced when using very low-bit quantization, enabling the deployment of large models on edge devices. To illustrate, we attain a C4 perplexity of $15.76$ (2.26$\\downarrow$ vs $18.02$ in OmniQuant) on the LLaMA2-$7$B model of W$4$A$4$ quantization without overhead. On zero-shot tasks, AffineQuant achieves an average of $58.61\\%$ accuracy ( $1.98\\%\\uparrow$ vs $56.63$ in OmniQuant) when using $4$/$4$-bit quantization for LLaMA-$30$B, which setting a new state-of-the-art benchmark for PTQ in LLMs. Codes are available at: https://github.com/bytedance/AffineQuant.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing PTQ methods for LLMs limit the optimization scope to scaling transformations between pre- and post-quantization weights. This constraint results in significant errors after quantization, particularly in low-bit configurations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing PTQ methods for LLMs limit the optimization scope to scaling transformations between pre- and post-quantization weights. This constraint results in significant errors after quantization, particularly in low-bit configurations.\""
    },
    {
        "title": "CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech",
        "authors": "Jaehyeon Kim;Keon Lee;Seungjun Chung;Jaewoong Cho",
        "site": "https://iclr.cc/virtual/2024/poster/17808",
        "summary": "With the emergence of neural audio codecs, which encode multiple streams of discrete tokens from audio, large language models have recently gained attention as a promising approach for zero-shot Text-to-Speech (TTS) synthesis. Despite the ongoing rush towards scaling paradigms, audio tokenization ironically amplifies the scalability challenge, stemming from its long sequence length and the complexity of modelling the multiple sequences. To mitigate these issues, we present CLaM-TTS that employs a probabilistic residual vector quantization to (1) achieve superior compression in the token length, and (2) allow a language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams. Our experimental results demonstrate that CLaM-TTS is better than or comparable to state-of-the-art neural codec-based TTS models regarding naturalness, intelligibility, speaker similarity, and inference speed. In addition, we examine the impact of the pretraining extent of the language models and their text tokenization strategies on performances.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"audio tokenization ironically amplifies the scalability challenge, stemming from its long sequence length and the complexity of modelling the multiple sequences.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of large language models (scalability challenge) in the context of audio tokenization, but it is not the primary focus of the paper and is only briefly mentioned.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"audio tokenization ironically amplifies the scalability challenge, stemming from its long sequence length and the complexity of modelling the multiple sequences.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of large language models (scalability challenge) in the context of audio tokenization, but it is not the primary focus of the paper and is only briefly mentioned."
    },
    {
        "title": "Quality-Diversity through AI Feedback",
        "authors": "Herbie Bradley;Andrew Dai;Hannah Benita Teufel;Jenny Zhang;Koen Oostermeijer;Marco Bellagente;Jeff Clune;Kenneth Stanley;Gregory Schott;Joel Lehman",
        "site": "https://iclr.cc/virtual/2024/poster/17802",
        "summary": "In many text-generation problems, users may prefer not only a single response, but a diverse range of high-quality outputs from which to choose. Quality-diversity (QD) search algorithms aim at such outcomes, by continually improving and diversifying a population of candidates. However, the applicability of QD to qualitative domains, like creative writing, has been limited by the difficulty of algorithmically specifying measures of quality and diversity. Interestingly, recent developments in language models (LMs) have enabled guiding search through \\emph{AI feedback}, wherein LMs are prompted in natural language to evaluate qualitative aspects of text. Leveraging this development, we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an evolutionary algorithm applies LMs to both generate variation and evaluate the quality and diversity of candidate text. When assessed on creative writing domains, QDAIF covers more of a specified search space with high-quality samples than do non-QD controls. Further, human evaluation of QDAIF-generated creative texts validates reasonable agreement between AI and human evaluation. Our results thus highlight the potential of AI feedback to guide open-ended search for creative and original solutions, providing a recipe that seemingly generalizes to many domains and modalities. In this way, QDAIF is a step towards AI systems that can independently search, diversify, evaluate, and improve, which are among the core skills underlying human society's capacity for innovation.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the applicability of QD to qualitative domains, like creative writing, has been limited by the difficulty of algorithmically specifying measures of quality and diversity.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, the applicability of QD to qualitative domains, like creative writing, has been limited by the difficulty of algorithmically specifying measures of quality and diversity.\""
    },
    {
        "title": "The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction",
        "authors": "Pratyusha Sharma;Jordan T. Ash;Dipendra Misra",
        "site": "https://iclr.cc/virtual/2024/poster/17798",
        "summary": "Transformer-based Large Language Models (LLMs) have become a fixture in modern machine learning. Correspondingly, significant resources are allocated towards research that aims to further advance this technology, typically resulting in models of increasing size that are trained on increasing amounts of data. This work, however, demonstrates the surprising result that it is often possible to significantly improve the performance of LLMs by selectively removing higher-order components of their weight matrices. This simple intervention, which we call LAyer-SElective Rank reduction (LASER), can be done on a model after training has completed, and requires minimal additional parameters and data. We show extensive experiments demonstrating the generality of this finding across language models and datasets, and provide in-depth analyses offering insights into both when LASER is effective and the mechanism by which it operates",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None directly mentioned in the abstract, but the fact that the authors propose a method to \"improve the performance of LLMs\" implies that there is room for improvement, suggesting some limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None directly mentioned in the abstract, but the fact that the authors propose a method to \"improve the performance of LLMs\" implies that there is room for improvement, suggesting some limitations."
    },
    {
        "title": "Causal Modelling Agents: Causal Graph Discovery through Synergising Metadata- and Data-driven Reasoning",
        "authors": "Ahmed Abdulaal;adamos hadjivasiliou;Nina Montana-Brown;Tiantian He;Ayodeji Ijishakin;Ivana Drobnjak;Daniel C. Castro;Daniel C. Alexander",
        "site": "https://iclr.cc/virtual/2024/poster/17791",
        "summary": "Scientific discovery hinges on the effective integration of metadata, which refers to a set of 'cognitive' operations such as determining what information is relevant for inquiry, and data, which encompasses physical operations such as observation and experimentation. This paper introduces the Causal Modelling Agent (CMA), a novel framework that synergizes the metadata-based reasoning capabilities of Large Language Models (LLMs) with the data-driven modelling of Deep Structural Causal Models (DSCMs) for the task of causal discovery. We evaluate the CMA's performance on a number of benchmarks, as well as on the real-world task of modelling the clinical and radiological phenotype of Alzheimer's Disease (AD). Our experimental results indicate that the CMA can outperform previous data-driven or metadata-driven approaches to causal discovery. In our real-world application, we use the CMA to derive new insights into the causal relationships among biomarkers of AD.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"synergizes the metadata-based reasoning capabilities of Large Language Models (LLMs)\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"synergizes the metadata-based reasoning capabilities of Large Language Models (LLMs)\""
    },
    {
        "title": "Two-stage LLM Fine-tuning with Less Specialization and More Generalization",
        "authors": "Yihan Wang;Si Si;Daliang Li;Michal Lukasik;Felix Yu;Cho-Jui Hsieh;Inderjit S Dhillon;Sanjiv Kumar",
        "site": "https://iclr.cc/virtual/2024/poster/17786",
        "summary": "Pretrained large language models (LLMs) are general purpose problem solvers applicable to a diverse set of tasks with prompts. They can be further improved towards a specific task by fine-tuning on a specialized dataset. However, fine-tuning usually makes the model narrowly specialized on this dataset with reduced general in-context learning performances, which is undesirable whenever the fine-tuned model needs to handle additional tasks where no fine-tuning data is available. In this work, we first demonstrate that fine-tuning on a single task indeed decreases LLMs' general in-context learning performance. We discover one important cause of such forgetting, format specialization, where the model overfits to the format of the fine-tuned task.We further show that format specialization happens at the very beginning of fine-tuning. To solve this problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet effective two-stage fine-tuning framework that reduces format specialization and improves generalization.ProMoT offloads task-specific format learning into additional and removable parameters by first doing prompt tuning and then fine-tuning the model itself with this soft prompt attached. With experiments on several fine-tuning tasks and 8 in-context evaluation tasks, we show that ProMoT achieves comparable performance on fine-tuned tasks to standard fine-tuning, but with much less loss of in-context learning performances across a board range of  out-of-domain evaluation tasks. More importantly, ProMoT can even enhance generalization on in-context learning tasks that are semantically related to the fine-tuned task, e.g. ProMoT on En-Fr translation significantly improves performance on other language pairs, and ProMoT on NLI improves performance on summarization.Experiments also show that ProMoT can improve the generalization performance of  multi-task training.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, fine-tuning usually makes the model narrowly specialized on this dataset with reduced general in-context learning performances, which is undesirable whenever the fine-tuned model needs to handle additional tasks where no fine-tuning data is available.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, fine-tuning usually makes the model narrowly specialized on this dataset with reduced general in-context learning performances, which is undesirable whenever the fine-tuned model needs to handle additional tasks where no fine-tuning data is available.\""
    },
    {
        "title": "Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in RL",
        "authors": "Xiangyu Liu;Souradip Chakraborty;Yanchao Sun;Furong Huang",
        "site": "https://iclr.cc/virtual/2024/poster/17785",
        "summary": "Abstract:Most existing works focus on direct perturbations to the victim's state/action or the underlying transition dynamics to demonstrate the vulnerability of reinforcement learning agents to adversarial attacks. However, such direct manipulations may not be always realizable.In this paper, we consider a multi-agent setting where a well-trained victim agent $\\nu$ is exploited by an attacker controlling another agent $\\alpha$ with an \\textit{adversarial policy}. Previous models do not account for the possibility that the attacker may only have partial control over $\\alpha$ or that the attack may produce easily detectable ``abnormal'' behaviors. Furthermore, there is a lack of provably efficient defenses against these adversarial policies. To address these limitations, we introduce a generalized attack framework that has the flexibility to model to what extent the adversary is able to control the agent, and allows the attacker to regulate the state distribution shift and produce stealthier adversarial policies. Moreover, we offer a provably efficient defense with polynomial convergence to the most robust victim policy through adversarial training with timescale separation. This stands in sharp contrast to supervised learning, where adversarial training typically provides only \\textit{empirical} defenses.Using the Robosumo competition experiments, we show that our generalized attack formulation results in much stealthier adversarial policies when maintaining the same winning rate as baselines. Additionally, our adversarial training approach yields stable learning dynamics and less exploitable victim policies.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "RAIN: Your Language Models Can Align Themselves without Finetuning",
        "authors": "Yuhui Li;Fangyun Wei;Jinjing Zhao;Chao Zhang;Hongyang Zhang",
        "site": "https://iclr.cc/virtual/2024/poster/17781",
        "summary": "Large language models (LLMs) often demonstrate inconsistencies with human preferences. Previous research typically gathered human preference data and then aligned the pre-trained models using reinforcement learning or instruction tuning, a.k.a. the finetuning step. In contrast, aligning frozen LLMs without requiring alignment data is more appealing. This work explores the potential of the latter setting. We discover that by integrating self-evaluation and rewind mechanisms, unaligned LLMs can directly produce responses consistent with human preferences via self-boosting. We introduce a novel inference method, Rewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to evaluate their own generation and use the evaluation results to guide rewind and generation for AI safety. Notably, RAIN operates without the need of extra data for model alignment and abstains from any training, gradient computation, or parameter updates. Experimental results evaluated by GPT-4 and humans demonstrate the effectiveness of RAIN: on the HH dataset, RAIN improves the harmlessness rate of LLaMA 30B from 82% of vanilla inference to 97%, while maintaining the helpfulness rate. On the TruthfulQA dataset, RAIN improves the truthfulness of the already-well-aligned LLaMA-2-chat 13B model by 5%.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models (LLMs) often demonstrate inconsistencies with human preferences.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large language models (LLMs) often demonstrate inconsistencies with human preferences.\""
    },
    {
        "title": "Imitation Learning from Observation with Automatic Discount Scheduling",
        "authors": "Yuyang Liu;Weijun Dong;Yingdong Hu;Chuan Wen;Zhao-Heng Yin;Chongjie Zhang;Yang Gao",
        "site": "https://iclr.cc/virtual/2024/poster/17778",
        "summary": "Humans often acquire new skills through observation and imitation. For robotic agents, learning from the plethora of unlabeled video demonstration data available on the Internet necessitates imitating the expert without access to its action, presenting a challenge known as Imitation Learning from Observation (ILfO). A common approach to tackle ILfO problems is to convert them into inverse reinforcement learning problems, utilizing a proxy reward computed from the agent's and the expert's observations. Nonetheless, we identify that tasks characterized by a progress dependency property pose significant challenges for such approaches; in these tasks, the agent needs to initially learn the expert's preceding behaviors before mastering the subsequent ones. Our investigation reveals that the main cause is that the reward signals assigned to later steps hinder the learning of initial behaviors. To address this challenge, we present a novel ILfO framework that enables the agent to master earlier behaviors before advancing to later ones. We introduce an Automatic Discount Scheduling (ADS) mechanism that adaptively alters the discount factor in reinforcement learning during the training phase, prioritizing earlier rewards initially and gradually engaging later rewards only when the earlier behaviors have been mastered. Our experiments, conducted on nine Meta-World tasks, demonstrate that our method significantly outperforms state-of-the-art methods across all tasks, including those that are unsolvable by them. Our code is available at https://il-ads.github.io.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems",
        "authors": "Tianyang Liu;Canwen Xu;Julian McAuley",
        "site": "https://iclr.cc/virtual/2024/poster/17776",
        "summary": "Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers. However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. To fill this gap, we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the system's ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction. RepoBench aims to facilitate a more complete comparison of performance and encouraging continuous improvement in auto-completion systems. RepoBench is actively maintained with the latest code, serving as a live benchmark publicly available at https://github.com/Leolty/repobench.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios.\"\n\nThis abstract mentions a limitation of current benchmarks for LLM-based code auto-completion systems, but does not explore it in depth and focuses on introducing a new benchmark to address this limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios.\"\n\nThis abstract mentions a limitation of current benchmarks for LLM-based code auto-completion systems, but does not explore it in depth and focuses on introducing a new benchmark to address this limitation."
    },
    {
        "title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning",
        "authors": "Ning Miao;Yee Whye Teh;Tom Rainforth",
        "site": "https://iclr.cc/virtual/2024/poster/17775",
        "summary": "The recent progress in large language models (LLMs), especially the invention of chain-of-thought prompting, has made it possible to automatically answer questions by stepwise reasoning. However, when faced with more complicated problems that require non-linear thinking, even the strongest LLMs make mistakes. To address this, we explore whether LLMs are able to recognize errors in their own step-by-step reasoning, without resorting to external resources. To this end, we propose SelfCheck, a general-purpose zero-shot verification schema for recognizing such errors. We then use the results of these checks to improve question-answering performance by conducting weighted voting on multiple solutions to the question. We test SelfCheck on math- and logic-based datasets and find that it successfully recognizes errors and, in turn, increases final answer accuracies.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"even the strongest LLMs make mistakes\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"even the strongest LLMs make mistakes\""
    },
    {
        "title": "Think before you speak: Training Language Models With Pause Tokens",
        "authors": "Sachin Goyal;Ziwei Ji;Ankit Singh Rawat;Aditya Krishna Menon;Sanjiv Kumar;Vaishnavh Nagarajan",
        "site": "https://iclr.cc/virtual/2024/poster/17771",
        "summary": "Abstract:Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{\\rm th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{\\rm th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\\\%$ EM score on the QA task of SQuAD, $8\\\\%$ on CommonSenseQA and $1\\\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions that the model's performance can be improved by allowing it to process extra computation before committing to an answer, implying that the standard approach of generating tokens in immediate succession may be a limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions that the model's performance can be improved by allowing it to process extra computation before committing to an answer, implying that the standard approach of generating tokens in immediate succession may be a limitation."
    },
    {
        "title": "Benchmarking and Improving Generator-Validator Consistency of Language Models",
        "authors": "Xiang Lisa Li;Vaishnavi Shrivastava;Siyan Li;Tatsunori Hashimoto;Percy Liang",
        "site": "https://iclr.cc/virtual/2024/poster/17770",
        "summary": "As of September 2023, ChatGPT correctly answers “what is 7+8” with 15, but when asked “7+8=15, True or False” it responds with “False”. This inconsistency between generating and validating an answer is prevalent in language models (LMs) and erodes trust. In this paper, we propose a framework for measuring the consistency between generation and validation (which we call generator-validator consistency, or GV-consistency), finding that even GPT-4 (0613), a state-of-the-art LM, is GV-consistent only 76% of the time. To improve the consistency of LMs, we propose to finetune on the filtered generator and validator responses that are GV-consistent, and call this approach consistency fine-tuning. We find that this approach improves GV-consistency of Alpaca-30B from 60% to 93%, and the improvement extrapolates to unseen tasks and domains (e.g., GV-consistency for positive style transfers extrapolates to unseen styles like humor). In addition to improving consistency, consistency fine-tuning improves both generator quality and validator accuracy without using any labeled data. Evaluated across 6 tasks, including math questions, knowledge-intensive QA, and instruction following, our method improves generator quality by an average of 16% and validator accuracy by an average of 6.3% across all tasks.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"As of September 2023, ChatGPT correctly answers “what is 7+8” with 15, but when asked “7+8=15, True or False” it responds with “False”. This inconsistency between generating and validating an answer is prevalent in language models (LMs) and erodes trust.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"As of September 2023, ChatGPT correctly answers “what is 7+8” with 15, but when asked “7+8=15, True or False” it responds with “False”. This inconsistency between generating and validating an answer is prevalent in language models (LMs) and erodes trust.\""
    },
    {
        "title": "Probabilistic Adaptation of Black-Box Text-to-Video Models",
        "authors": "Sherry Yang;Yilun Du;Bo Dai;Dale Schuurmans;Joshua B. Tenenbaum;Pieter Abbeel",
        "site": "https://iclr.cc/virtual/2024/poster/17769",
        "summary": "Large text-to-video models trained on internet-scale data have demonstrated exceptional capabilities in generating high-fidelity videos from arbitrary textual descriptions. However, similar to proprietary language models, large text-to-video models are often black boxes whose weight parameters are not publicly available, posing a significant challenge to adapting these models to specific domains such as robotics, animation, and personalized stylization. Inspired by how a large language model can be prompted to perform new tasks without access to the model weights, we investigate how to adapt a black-box pretrained text-to-video model to a variety of downstream domains without weight access to the pretrained model. In answering this question, we propose \\emph{\\methodname}, which leverages the score function of a large pretrained video diffusion model as a probabilistic prior to guide the generation of a task-specific small video model. Our experiments show that, by incorporating broad knowledge and fidelity of the pretrained model probabilistically, a small model with as few as 1.25% parameters of the pretrained model can generate high-quality yet domain-specific videos for a variety of downstream domains such as animation, egocentric modeling, and modeling of simulated and real-world robotics data. As large text-to-video models starting to become available as a service similar to large language models, we advocate for private institutions to expose scores of video diffusion models as outputs in addition to generated videos to allow flexible adaptation of large pretrained text-to-video models by the general public.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, similar to proprietary language models, large text-to-video models are often black boxes whose weight parameters are not publicly available, posing a significant challenge to adapting these models to specific domains such as robotics, animation, and personalized stylization.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, similar to proprietary language models, large text-to-video models are often black boxes whose weight parameters are not publicly available, posing a significant challenge to adapting these models to specific domains such as robotics, animation, and personalized stylization.\""
    },
    {
        "title": "#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models",
        "authors": "Keming Lu;Hongyi Yuan;Zheng Yuan;Runji Lin;Junyang Lin;Chuanqi Tan;Chang Zhou;Jingren Zhou",
        "site": "https://iclr.cc/virtual/2024/poster/17764",
        "summary": "Pre-trained large language models (LLMs) can understand and align with human instructions by supervised fine-tuning (SFT).It is commonly believed that diverse and complex SFT data are of the essence to enable good instruction-following abilities.However, such diversity and complexity are obscure and lack quantitative analyses.In this work, we propose InsTag, an open-set instruction tagging method, to identify semantics and intentions of human instructions by tags that provide access to definitions and quantified analyses of instruction diversity and complexity.We obtain 6.6K fine-grained tags to describe instructions from popular open-sourced SFT datasets comprehensively.We find that the abilities of aligned LLMs benefit from more diverse and complex instructions in SFT data.Based on this observation, we propose a data sampling procedure based on InsTag, and select 6K diverse and complex samples from open-source datasets for SFT.The resulting models, TagLM, outperform open-source models based on considerably larger SFT data evaluated by MT-Bench, echoing the importance of instruction diversity and complexity and the effectiveness of InsTag.InsTag has robust potential to be extended to more applications beyond the data selection as it provides an effective way to analyze the distribution of instructions.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"It is commonly believed that diverse and complex SFT data are of the essence to enable good instruction-following abilities. However, such diversity and complexity are obscure and lack quantitative analyses.\"\n\nThis paper discusses the limitations of LLMs in terms of the diversity and complexity of supervised fine-tuning (SFT) data, but only briefly mentions this limitation to motivate the proposed solution, Ins",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"It is commonly believed that diverse and complex SFT data are of the essence to enable good instruction-following abilities. However, such diversity and complexity are obscure and lack quantitative analyses.\"\n\nThis paper discusses the limitations of LLMs in terms of the diversity and complexity of supervised fine-tuning (SFT) data, but only briefly mentions this limitation to motivate the proposed solution, Ins"
    },
    {
        "title": "$\\alpha$TC-VAE: On the relationship between Disentanglement and Diversity",
        "authors": "Cristian Meo;Louis Mahon;Anirudh Goyal;Justin Dauwels",
        "site": "https://iclr.cc/virtual/2024/poster/17762",
        "summary": "Abstract:Understanding and developing optimal representations has long been foundational in machine learning (ML). While disentangled representations have shown promise in generative modeling and representation learning, their downstream usefulness remains debated. Recent studies re-defined disentanglement through a formal connection to symmetries, emphasizing the ability to reduce latent domains (i.e., ML problem spaces) and consequently enhance data efficiency and generative capabilities. However, from an information theory viewpoint, assigning a complex attribute (i.e., features) to a specific latent variable may be infeasible, limiting the applicability of disentangled representations to simple datasets. In this work, we introduce $\\alpha$-TCVAE, a variational autoencoder optimized using a novel total correlation (TC) lower bound that maximizes disentanglement and latent variables informativeness. The proposed TC bound is grounded in information theory constructs, generalizes the $\\beta$-VAE lower bound, and can be reduced to a convex combination of the known variational information bottleneck (VIB) and conditional entropy bottleneck (CEB) terms. Moreover, we present quantitative analyses and correlation studies that support the idea that smaller latent domains (i.e., disentangled representations) lead to better generative capabilities and diversity. Additionally, we perform downstream task experiments from both representation and RL domains to assess our questions from a broader ML perspective. Our results demonstrate that $\\alpha$-TCVAE consistently learns more disentangled representations than baselines and generates more diverse observations without sacrificing visual fidelity. Notably, $\\alpha$-TCVAE exhibits marked improvements on MPI3D-Real, the most realistic disentangled dataset in our study,  confirming its ability to represent complex datasets when maximizing the informativeness of individual variables. Finally, testing the proposed model off-the-shelf on a state-of-the-art model-based RL agent, Director, significantly shows $\\alpha$-TCVAE downstream usefulness on the loconav Ant Maze task. Implementation available at https://github.com/Cmeo97/Alpha-TCVAE",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "CIFAR-10-Warehouse: Broad and More Realistic Testbeds in Model Generalization Analysis",
        "authors": "Xiaoxiao Sun;Xingjian Leng;Zijian Wang;Yang Yang;Zi Huang;Liang Zheng",
        "site": "https://iclr.cc/virtual/2024/poster/17761",
        "summary": "Analyzing model performance in various unseen environments is a critical research problem in the machine learning community. To study this problem, it is important to construct a testbed with out-of-distribution test sets that have broad coverage of environmental discrepancies. However, existing testbeds typically either have a small number of domains or are synthesized by image corruptions, hindering algorithm design that demonstrates real-world effectiveness. In this paper, we introduce CIFAR-10-Warehouse, consisting of 180 datasets collected by prompting image search engines and diffusion models in various ways. Generally sized between 300 and 8,000 images, the datasets contain natural images, cartoons, certain colors, or objects that do not naturally appear. With CIFAR-10-W, we aim to enhance the evaluation and deepen the understanding of two generalization tasks: domain generalization and model accuracy prediction in various out-of-distribution environments. We conduct extensive benchmarking and comparison experiments and show that CIFAR-10-W offers new and interesting insights inherent to these tasks. We also discuss other fields that would benefit from CIFAR-10-W. Data and code are available at https://sites.google.com/view/CIFAR-10-warehouse/.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Quantifying the Sensitivity of Inverse Reinforcement Learning to Misspecification",
        "authors": "Joar Max Viktor Skalse;Alessandro Abate",
        "site": "https://iclr.cc/virtual/2024/poster/17757",
        "summary": "Abstract:Inverse reinforcement learning (IRL) aims to infer an agent's *preferences* (represented as a reward function $R$) from their *behaviour* (represented as a policy $\\pi$). To do this, we need a *behavioural model* of how $\\pi$ relates to $R$. In the current literature, the most common behavioural models are *optimality*, *Boltzmann-rationality*, and *causal entropy maximisation*. However, the true relationship between a human's preferences and their behaviour is much more complex than any of these behavioural models. This means that the behavioural models are *misspecified*, which raises the concern that they may lead to systematic errors if applied to real data. In this paper, we analyse how sensitive the IRL problem is to misspecification of the behavioural model. Specifically, we provide necessary and sufficient conditions that completely characterise how the observed data may differ from the assumed behavioural model without incurring an error above a given threshold. In addition to this, we also characterise the conditions under which a behavioural model is robust to small perturbations of the observed policy, and we analyse how robust many behavioural models are to misspecification of their parameter values (such as e.g. the discount rate). Our analysis suggests that the IRL problem is highly sensitive to misspecification, in the sense that very mild misspecification can lead to very large errors in the inferred reward function.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Conformal Language Modeling",
        "authors": "Victor Quach;Adam Fisch;Tal Schuster;Adam Yala;Jae Ho Sohn;Tommi S. Jaakkola;Regina Barzilay",
        "site": "https://iclr.cc/virtual/2024/poster/17755",
        "summary": "In this paper, we propose a novel approach to conformal prediction for  language models (LMs) in which we produce prediction sets with performance guarantees. LM responses are typically sampled from a predicted distribution over the large, combinatorial output space of  language. Translating this to conformal prediction, we calibrate a stopping rule for sampling LM outputs  that get added to a growing set of candidates until we are confident that the set covers at least one acceptable response. Since some samples may be low-quality, we also simultaneously calibrate a rejection rule for removing candidates from the output set to reduce noise. Similar to conformal prediction, we  can prove that the final output set obeys certain desirable distribution-free guarantees. Within these sets of candidate responses, we also show that we can also identify subsets of individual components---such as phrases or sentences---that are each independently correct (e.g., that are not ``hallucinations''), again with guarantees. Our method can be applied to any LM API that supports sampling. Furthermore, we empirically demonstrate that we can achieve many desired coverage levels within a limited number of total samples when applying our method to  multiple tasks in open-domain question answering, text summarization, and radiology report generation using different LM variants.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Since some samples may be low-quality, we also simultaneously calibrate a rejection rule for removing candidates from the output set to reduce noise.\"\n\nThis rating is given because the paper mentions a limitation of LLMs (low-quality samples) but only briefly and does not explore it in depth, instead focusing on the proposed solution to address this limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Since some samples may be low-quality, we also simultaneously calibrate a rejection rule for removing candidates from the output set to reduce noise.\"\n\nThis rating is given because the paper mentions a limitation of LLMs (low-quality samples) but only briefly and does not explore it in depth, instead focusing on the proposed solution to address this limitation."
    },
    {
        "title": "DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning",
        "authors": "Jing Xiong;Zixuan Li;Chuanyang Zheng;Zhijiang Guo;Yichun Yin;Enze Xie;Zhicheng YANG;Qingxing Cao;Haiming Wang;Xiongwei Han;Jing Tang;Chengming Li;Xiaodan Liang",
        "site": "https://iclr.cc/virtual/2024/poster/17747",
        "summary": "Recent advances in natural language processing, primarily propelled by Large Language Models (LLMs), have showcased their remarkable capabilities grounded in in-context learning. A promising avenue for guiding LLMs in intricate reasoning tasks involves the utilization of intermediate reasoning steps within the Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge lies in the effective selection of exemplars for facilitating in-context learning. In this study, we introduce a framework that leverages Dual Queries and Low-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplars for in-context learning. Dual Queries first query LLM to obtain LLM-generated knowledge such as CoT, then query the retriever to obtain the final exemplars via both question and the knowledge. Moreover, for the second query, LoRe employs dimensionality reduction techniques to refine exemplar selection, ensuring close alignment with the input question's knowledge. Through extensive experiments, we demonstrate that DQ-LoRe significantly outperforms prior state-of-the-art methods in the automatic selection of exemplars for GPT-4, enhancing performance from 92.5\\% to 94.2\\%. Our comprehensive analysis further reveals that DQ-LoRe consistently outperforms retrieval-based approaches in terms of both performance and adaptability, especially in scenarios characterized by distribution shifts. DQ-LoRe pushes the boundaries of in-context learning and opens up new avenues for addressing complex reasoning challenges.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the central challenge lies in the effective selection of exemplars for facilitating in-context learning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the central challenge lies in the effective selection of exemplars for facilitating in-context learning.\""
    },
    {
        "title": "Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models",
        "authors": "Sijia Chen;Baochun Li;Di Niu",
        "site": "https://iclr.cc/virtual/2024/poster/17746",
        "summary": "The reasoning performance of Large Language Models (LLMs) on a wide range of problems critically relies on chain-of-thought prompting, which involves providing a few chain of thought demonstrations as exemplars in prompts. Recent work, e.g., Tree of Thoughts, has pointed out the importance of exploration and self-evaluation in reasoning step selection for complex problem solving. In this paper, we present Boosting of Thoughts (BoT), an automated prompting framework for problem solving with LLMs by iteratively exploring and self-evaluating many trees of thoughts in order to acquire an ensemble of trial-and-error reasoning experiences, which will serve as a new form of prompting to solve the complex problem. Starting from a simple prompt without requiring examples, BoT iteratively explores and evaluates a large collection of reasoning steps, and more importantly, uses error analysis obtained from the LLM on them to explicitly revise prompting, which in turn enhances reasoning step generation, until a final answer is attained. Our experiments with GPT-4 and Llama2 across extensive complex mathematical problems demonstrate that BoT consistently achieves higher or comparable problem-solving rates than other advanced prompting approaches.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The reasoning performance of Large Language Models (LLMs) on a wide range of problems critically relies on chain-of-thought prompting...\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The reasoning performance of Large Language Models (LLMs) on a wide range of problems critically relies on chain-of-thought prompting...\""
    },
    {
        "title": "LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language Models",
        "authors": "Zecheng Tang;Chenfei Wu;Juntao Li;Nan Duan",
        "site": "https://iclr.cc/virtual/2024/poster/17744",
        "summary": "Graphic layout generation, a growing research field, plays a significant role in user engagement and information perception. Existing methods primarily treat layout generation as a numerical optimization task, focusing on quantitative aspects while overlooking the semantic information of layout, such as the relationship between each layout element. In this paper, we propose LayoutNUWA, the first model that treats layout generation as a code generation task to enhance semantic information and harness the hidden layout expertise of large language models~(LLMs). Concretely, we develop a Code Instruct Tuning (CIT) approach comprising three interconnected modules: 1) the Code Initialization (CI) module quantifies the numerical conditions and initializes them as HTML code with strategically placed masks; 2) the Code Completion (CC) module employs the formatting knowledge of LLMs to fill in the masked portions within the HTML code; 3) the Code Rendering (CR) module transforms the completed code into the final layout output, ensuring a highly interpretable and transparent layout generation procedure that directly maps code to a visualized layout. We attain significant state-of-the-art performance (even over 50\\% improvements compared to previous works) on multiple datasets, showcasing the strong capabilities of LayoutNUWA.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"harness the hidden layout expertise of large language models~(LLMs)\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"harness the hidden layout expertise of large language models~(LLMs)\""
    },
    {
        "title": "Learning Grounded Action Abstractions from Language",
        "authors": "Lionel Wong;Jiayuan Mao;Pratyusha Sharma;Zachary S Siegel;Jiahai Feng;Noa Korneev;Joshua B. Tenenbaum;Jacob Andreas",
        "site": "https://iclr.cc/virtual/2024/poster/17738",
        "summary": "Effective planning in the real world requires not only world knowledge, but the ability to leverage that knowledge to build the right representation of the task at hand. Decades of hierarchical planning techniques have used domain-specific temporal action abstractions to support efficient and accurate planning, almost always relying on human priors and domain knowledge to decompose hard tasks into smaller subproblems appropriate for a goal or set of goals. This paper describes Ada (Action Domain Acquisition), a framework for automatically constructing task-specific planning representations using task-general background knowledge from language models (LMs). Starting with a general-purpose hierarchical planner and a low-level goal-conditioned policy, Ada interactively learns a library of planner-compatible high-level action abstractions and low-level controllers adapted to a particular domain of planning tasks. On two language-guided interactive planning benchmarks (Mini Minecraft and ALFRED Household Tasks), Ada strongly outperforms other approaches that use LMs for sequential decision-making, offering more accurate plans and better generalization to complex tasks.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"other approaches that use LMs for sequential decision-making\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"other approaches that use LMs for sequential decision-making\""
    },
    {
        "title": "Guess & Sketch: Language Model Guided Transpilation",
        "authors": "Celine Lee;Abdulrahman Mahmoud;Michal Kurek;Simone Campanoni;David Brooks;Stephen Chong;Gu-Yeon Wei;Alexander M Rush",
        "site": "https://iclr.cc/virtual/2024/poster/17733",
        "summary": "Maintaining legacy software requires many software and systems engineering hours. Assembly code programs, which demand low-level control over the computer machine state and have no variable names, are particularly difficult for humans to analyze.Existing conventional program translators guarantee correctness, but are hand-engineered for the source and target programming languages in question. Learned transpilation, i.e.  automatic translation of code, offers an alternative to manual re-writing and engineering efforts. Automated symbolic program translation approaches guarantee correctness but struggle to scale to longer programs due to the exponentially large search space. Their rigid rule-based systems also limit their expressivity, so they can only reason about a reduced space of programs. Probabilistic neural language models (LMs) produce plausible outputs for every input, but do so at the cost of guaranteed correctness. In this work, we leverage the strengths of LMs and symbolic solvers in a neurosymbolic approach to learned transpilation for assembly code. Assembly code is an appropriate setting for a neurosymbolic approach, since assembly code can be divided into shorter non-branching basic blocks amenable to the use of symbolic methods. Guess & Sketch extracts alignment and confidence information from features of the LM then passes it to a symbolic solver to resolve semantic equivalence of the transpilation input and output. We test Guess & Sketch on three different test sets of assembly transpilation tasks, varying in difficulty, and show that it successfully transpiles 57.6% more examples than GPT-4 and 39.6% more examples than an engineered transpiler. We also share a training and evaluation dataset for this task.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Probabilistic neural language models (LMs) produce plausible outputs for every input, but do so at the cost of guaranteed correctness.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Probabilistic neural language models (LMs) produce plausible outputs for every input, but do so at the cost of guaranteed correctness.\""
    },
    {
        "title": "Large Language Models as Tool Makers",
        "authors": "Tianle Cai;Xuezhi Wang;Tengyu Ma;Xinyun Chen;Denny Zhou",
        "site": "https://iclr.cc/virtual/2024/poster/17729",
        "summary": "Recent research has highlighted the potential of large language models (LLMs)to improve their problem-solving capabilities with the aid of suitable externaltools. In our work, we further advance this concept by introducing a closed-loop framework, referred to as LLMs A s Tool Makers (LATM), where LLMscreate their own reusable tools for problem-solving. Our approach consists of twophases: 1) tool making: an LLM acts as the tool maker that crafts tools for a setof tasks, where a tool is implemented as a Python utility function. 2) tool using:another LLM acts as the tool user, which applies the tool built by the tool makerfor problem-solving. The tool user can be either the same or a different LLMfrom the tool maker. On the problem-solving server side, tool-making enablescontinual tool generation and caching as new requests emerge. This frameworkenables subsequent requests to access cached tools via their corresponding APIs,enhancing the efficiency of task resolution. Beyond enabling LLMs to create theirown tools, our framework also uncovers intriguing opportunities to optimize theserving cost of LLMs: Recognizing that tool-making requires more sophisticatedcapabilities, we assign this task to a powerful, albeit resource-intensive, model.Conversely, the simpler tool-using phase is delegated to a lightweight model. Thisstrategic division of labor allows the once-off cost of tool-making to be spreadover multiple instances of tool-using, significantly reducing average costs whilemaintaining strong performance. Furthermore, our method offers a functionalcache through the caching and reuse of tools, which stores the functionality ofa class of requests instead of the natural language responses from LLMs, thusextending the applicability of the conventional cache mechanism. We evaluateour approach across various complex reasoning tasks, including Big-Bench tasks.With GPT-4 as the tool maker and GPT-3.5 as the tool user, LATM demonstratesperformance equivalent to using GPT-4 for both roles, but with a significantlyreduced inference cost.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recognizing that tool-making requires more sophisticated capabilities, we assign this task to a powerful, albeit resource-intensive, model.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Recognizing that tool-making requires more sophisticated capabilities, we assign this task to a powerful, albeit resource-intensive, model.\""
    },
    {
        "title": "Learning with Language-Guided State Abstractions",
        "authors": "Andi Peng;Ilia Sucholutsky;Belinda Z. Li;Theodore Sumers;Thomas L. Griffiths;Jacob Andreas;Julie Shah",
        "site": "https://iclr.cc/virtual/2024/poster/17720",
        "summary": "We describe a framework for using natural language to design state abstractions for imitation learning.Generalizable policy learning in high-dimensional observation spaces is facilitated by well-designed state representations, which can surface important features of an environment and hide irrelevant ones.These state representations are typically manually specified, or derived from other labor-intensive labeling procedures.Our method, LGA (\\textit{language-guided abstraction}), uses a combination of natural language supervision and background knowledge from language models (LMs) to automatically build state representations tailored to unseen tasks.In LGA, a user first provides a (possibly incomplete) description of a target task in natural language; next, a pre-trained LM translates this task description into a state abstraction function that masks out irrelevant features; finally, an imitation policy is trained using a small number of demonstrations and LGA-generated abstract states. Experiments on simulated robotic tasks show that LGA yields state abstractions similar to those designed by humans, but in a fraction of the time, and that these abstractions improve generalization and robustness in the presence of spurious correlations and ambiguous specifications.We illustrate the utility of the learned abstractions on mobile manipulation tasks with a Spot robot.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"uses a combination of natural language supervision and background knowledge from language models (LMs) to automatically build state representations tailored to unseen tasks.\"\n\nThis abstract mentions LLMs but does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"uses a combination of natural language supervision and background knowledge from language models (LMs) to automatically build state representations tailored to unseen tasks.\"\n\nThis abstract mentions LLMs but does not discuss any limitations of LLMs."
    },
    {
        "title": "Skill Machines: Temporal Logic Skill Composition in Reinforcement Learning",
        "authors": "Geraud Nangue Tasse;Devon Jarvis;Steven James;Benjamin Rosman",
        "site": "https://iclr.cc/virtual/2024/poster/17719",
        "summary": "It is desirable for an agent to be able to solve a rich variety of problems that can be specified through language in the same environment. A popular approach towards obtaining such agents is to reuse skills learned in prior tasks to generalise compositionally to new ones. However, this is a challenging problem due to the curse of dimensionality induced by the combinatorially large number of ways high-level goals can be combined both logically and temporally in language. To address this problem, we propose a framework where an agent first learns a sufficient set of skill primitives to achieve all high-level goals in its environment. The agent can then flexibly compose them both logically and temporally to provably achieve temporal logic specifications in any regular language, such as regular fragments of linear temporal logic. This provides the agent with the ability to map from complex temporal logic task specifications to near-optimal behaviours zero-shot. We demonstrate this experimentally in a tabular setting, as well as in a high-dimensional video game and continuous control environment. Finally, we also demonstrate that the performance of skill machines can be improved with regular off-policy reinforcement learning algorithms when optimal behaviours are desired.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Teach LLMs to Phish: Stealing Private Information from Language Models",
        "authors": "Ashwinee Panda;Christopher A. Choquette-Choo;Zhengming Zhang;Yaoqing Yang;Prateek Mittal",
        "site": "https://iclr.cc/virtual/2024/poster/17716",
        "summary": "Abstract:When large language models are trained on private data, it can be a \\textit{significant} privacy risk for them to memorize and regurgitate sensitive information. In this work, we propose a new \\emph{practical} data extraction attack that we call ``neural phishing''. This attack enables an adversary to target and extract sensitive or personally identifiable information (PII), e.g., credit card numbers, from a model trained on user data with upwards of $10\\%$ attack success rates, at times, as high as $50\\%$. Our attack assumes only that an adversary can insert as few as $10$s of benign-appearing sentences into the training dataset using only vague priors on the structure of the user data.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"When large language models are trained on private data, it can be a \\textit{significant} privacy risk for them to memorize and regurgitate sensitive information.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"When large language models are trained on private data, it can be a \\textit{significant} privacy risk for them to memorize and regurgitate sensitive information.\""
    },
    {
        "title": "Demystifying Embedding Spaces using Large Language Models",
        "authors": "Guy Tennenholtz;Yinlam Chow;ChihWei Hsu;Jihwan Jeong;Lior Shani;Azamat Tulepbergenov;Deepak Ramachandran;Martin Mladenov;Craig Boutilier",
        "site": "https://iclr.cc/virtual/2024/poster/17714",
        "summary": "Embeddings have become a pivotal means to represent complex, multi-faceted information about entities, concepts, and relationships in a condensed and useful format. Nevertheless, they often preclude direct interpretation. While downstream tasks make use of these compressed representations, meaningful interpretation usually requires visualization using dimensionality reduction or specialized machine learning interpretability methods. This paper addresses the challenge of making such embeddings more interpretable and broadly useful, by employing large language models (LLMs) to directly interact with embeddings -- transforming abstract vectors into understandable narratives. By injecting embeddings into LLMs, we enable querying and exploration of complex embedding data. We demonstrate our approach on a variety of diverse tasks, including: enhancing concept activation vectors (CAVs), communicating novel embedded entities, and decoding user preferences in recommender systems. Our work couples the immense information potential of embeddings with the interpretative power of LLMs.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the title and abstract mention the use of LLMs to address the challenge of making embeddings more interpretable, implying that LLMs are a solution rather than discussing their limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the title and abstract mention the use of LLMs to address the challenge of making embeddings more interpretable, implying that LLMs are a solution rather than discussing their limitations."
    },
    {
        "title": "UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition",
        "authors": "Wenxuan Zhou;Sheng Zhang;Yu Gu;Muhao Chen;Hoifung Poon",
        "site": "https://iclr.cc/virtual/2024/poster/17704",
        "summary": "Large language models (LLMs) have demonstrated remarkable generalizability, such as understanding arbitrary entities and relations. Instruction tuning has proven effective for distilling LLMs into more cost-efficient models such as Alpaca and Vicuna. Yet such student models still trail the original LLMs by large margins in downstream applications. In this paper, we explore targeted distillation with mission-focused instruction tuning to train student models that can excel in a broad application class such as open information extraction. Using named entity recognition (NER) for case study, we show how ChatGPT can be distilled into much smaller UniversalNER models for open NER. For evaluation, we assemble the largest NER benchmark to date, comprising 43 datasets across 9 diverse domains such as biomedicine, programming, social media, law, finance. Without using any direct supervision, UniversalNER attains remarkable NER accuracy across tens of thousands of entity types, outperforming general instruction-tuned models such as Alpaca and Vicuna by over 30 absolute F1 points in average. With a tiny fraction of parameters, UniversalNER not only acquires ChatGPT's capability in recognizing arbitrary entity types, but also outperforms its NER accuracy by 7-9 absolute F1 points in average. Remarkably, UniversalNER even outperforms by a large margin state-of-the-art multi-task instruction-tuned systems such as InstructUIE, which uses supervised NER examples. We also conduct thorough ablation studies to assess the impact of various components in our distillation approach. We release the distillation recipe, data, and UniversalNER models to facilitate future research on targeted distillation.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Yet such student models still trail the original LLMs by large margins in downstream applications.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Yet such student models still trail the original LLMs by large margins in downstream applications.\""
    },
    {
        "title": "Curriculum reinforcement learning for quantum architecture search under hardware errors",
        "authors": "Yash J. Patel;Akash Kundu;Mateusz Ostaszewski;Xavier Bonet-Monroig;Vedran Dunjko;Onur Danaci",
        "site": "https://iclr.cc/virtual/2024/poster/17697",
        "summary": "The key challenge in the noisy intermediate-scale quantum era is finding useful circuits compatible with current device limitations.Variational quantum algorithms (VQAs) offer a potential solution by fixing the circuit architecture and optimizing individual gate parameters in an external loop. However, parameter optimization can become intractable, and the overall performance of the algorithm depends heavily on the initially chosen circuit architecture. Several quantum architecture search (QAS) algorithms have been developed to design useful circuit architectures automatically. In the case of parameter optimization alone, noise effects have been observed to dramatically influence the performance of the optimizer and final outcomes, which is a key line of study. However, the effects of noise on the architecture search, which could be just as critical, are poorly understood. This work addresses this gap by introducing a curriculum-based reinforcement learning QAS (CRLQAS) algorithm designed to tackle challenges in realistic VQA deployment. The algorithm incorporates (i) a 3D architecture encoding and restrictions on environment dynamics to explore the search space of possible circuits efficiently, (ii) an episode halting scheme to steer the agent to find shorter circuits, and (iii) a novel variant of simultaneous perturbation stochastic approximation as an optimizer for faster convergence. To facilitate studies, we developed an optimized simulator for our algorithm, significantly improving computational efficiency in simulating noisy quantum circuits by employing the Pauli-transfer matrix formalism in the Pauli-Liouville basis. Numerical experiments focusing on quantum chemistry tasks demonstrate that CRLQAS outperforms existing QAS algorithms across several metrics in both noiseless and noisy environments.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Masked Structural Growth for 2x Faster Language Model Pre-training",
        "authors": "Yiqun Yao;Zheng Zhang;Jing Li;Yequan Wang",
        "site": "https://iclr.cc/virtual/2024/poster/17695",
        "summary": "Accelerating large language model pre-training is a critical issue in present research. In this paper, we focus on speeding up pre-training by progressively growing from a small Transformer structure to a large one. There are two main research problems associated with progressive growth: determining the optimal growth schedule, and designing efficient growth operators. In terms of growth schedule, the impact of each single dimension on a schedule’s efficiency is underexplored by existing work. Regarding the growth operators, existing methods rely on the initialization of new weights to inherit knowledge, and achieve only non-strict function preservation, limiting further improvements on training dynamics. To address these issues, we propose Masked Structural Growth (MSG), including (i) growth schedules involving all possible dimensions and (ii) strictly function-preserving growth operators that is independent of the initialization of new weights. Experiments show that MSG is significantly faster than related work: we achieve up to 2.2x speedup in pre-training different types of language models while maintaining comparable or better downstream performances. Code is publicly available at https://github.com/cofe-ai/MSG.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"existing methods rely on the initialization of new weights to inherit knowledge, and achieve only non-strict function preservation, limiting further improvements on training dynamics.\"\n\nThis rating is given because the abstract mentions a limitation of existing methods for accelerating large language model pre-training, but does not extensively analyze or focus on this limitation. The primary focus is on the proposed solution, Masked Structural Growth (MSG",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"existing methods rely on the initialization of new weights to inherit knowledge, and achieve only non-strict function preservation, limiting further improvements on training dynamics.\"\n\nThis rating is given because the abstract mentions a limitation of existing methods for accelerating large language model pre-training, but does not extensively analyze or focus on this limitation. The primary focus is on the proposed solution, Masked Structural Growth (MSG"
    },
    {
        "title": "Functional Interpolation for Relative Positions improves Long Context Transformers",
        "authors": "Shanda Li;Chong You;Guru Guruganesh;Joshua Ainslie;Santiago Ontanon;Manzil Zaheer;Sumit Sanghai;Yiming Yang;Sanjiv Kumar;Srinadh Bhojanapalli",
        "site": "https://iclr.cc/virtual/2024/poster/17693",
        "summary": "Preventing the performance decay of Transformers on inputs longer than those used for training has been an important challenge in extending the context length of these models. Though the Transformer architecture has fundamentally no limits on the input sequence lengths it can process, the choice of position encoding used during training can limit the performance of these models on longer inputs. We propose a novel functional relative position encoding with progressive interpolation, FIRE, to improve Transformer generalization to longer contexts. We theoretically prove that this can represent some of the popular relative position encodings, such as T5's RPE, Alibi, and Kerple. We next empirically show that FIRE models have better generalization to longer contexts on both zero-shot language modeling and long text benchmarks.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Preventing the performance decay of Transformers on inputs longer than those used for training has been an important challenge in extending the context length of these models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Preventing the performance decay of Transformers on inputs longer than those used for training has been an important challenge in extending the context length of these models.\""
    },
    {
        "title": "Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE",
        "authors": "Zeren Chen;Ziqin Wang;Zhen Wang;Huayang Liu;Zhenfei Yin;Si Liu;Lu Sheng;Wanli Ouyang;Jing Shao",
        "site": "https://iclr.cc/virtual/2024/poster/17691",
        "summary": "Recent studies have demonstrated Large Language Models (LLMs) can extend their zero-shot generalization capabilities to multimodal learning through instruction tuning. As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance. While this phenomenon has been overlooked in previous work, we propose a novel and extensible framework, called Octavius, for comprehensive studies and experimentation on multimodal learning with Multimodal Large Language Models (MLLMs). Specifically, to mitigate the interference, we combine the concept of Mixture-of-Experts (MoE) with LoRA and design a multimodal LoRA-MoE decoder for task- and modality-specific learning. To the best of our knowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to address this problem. The experimental results (about 20% improvement) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks. Code and corresponding dataset will be availablesoon.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance.\""
    },
    {
        "title": "Gen-Z: Generative Zero-Shot Text Classification with Contextualized Label Descriptions",
        "authors": "Sachin Kumar;Chan Young Park;Yulia Tsvetkov",
        "site": "https://iclr.cc/virtual/2024/poster/17686",
        "summary": "Language model (LM) prompting—a popular paradigm for solving NLP tasks—has been shown to be susceptible to miscalibration and brittleness to slight prompt variations, caused by its discriminative prompting approach, i.e., predicting the label given the input. To address these issues, we propose Gen-Z—a generative prompting framework for zero-shot text classification. GEN-Z is generative, as it measures the LM likelihood of input text, conditioned on natural language descriptions of labels. The framework is multivariate, as label descriptions allow us to seamlessly integrate additional contextual information about the labels to improve task performance. On various standard classification benchmarks, with six open-source LM families, we show that zero-shot classification with simple contextualization of the data source of the evaluation set consistently outperforms both zero-shot and few-shot baselines while improving robustness to prompt variations. Further, our approach enables personalizing classification in a zero-shot manner by incorporating author, subject, or reader information in the label descriptions.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Language model (LM) prompting—a popular paradigm for solving NLP tasks—has been shown to be susceptible to miscalibration and brittleness to slight prompt variations, caused by its discriminative prompting approach, i.e., predicting the label given the input.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Language model (LM) prompting—a popular paradigm for solving NLP tasks—has been shown to be susceptible to miscalibration and brittleness to slight prompt variations, caused by its discriminative prompting approach, i.e., predicting the label given the input.\""
    },
    {
        "title": "DistillSpec: Improving Speculative Decoding via Knowledge Distillation",
        "authors": "Yongchao Zhou;Kaifeng Lyu;Ankit Singh Rawat;Aditya Krishna Menon;Afshin Rostamizadeh;Sanjiv Kumar;Jean-François Kagy;Rishabh Agarwal",
        "site": "https://iclr.cc/virtual/2024/poster/17680",
        "summary": "Abstract:Speculative decoding~(SD) accelerates large language model inference by employing a faster {\\em draft} model for generating multiple tokens, which are then verified in parallel by the larger {\\em target} model, resulting in the text generated according to the target model distribution. However, identifying a compact draft model that is well-aligned with the target model is challenging. To tackle this issue, we propose {\\em DistillSpec} that uses knowledge distillation to better align the draft model with the target model, before applying SD. DistillSpec makes two key design choices, which we demonstrate via systematic study to be crucial to improve the draft and target alignment: utilizing \\emph{on-policy} data generation from the draft model, and \\emph{tailoring the divergence function} to the task and decoding strategy. Notably, DistillSpec yields impressive $10 - 45\\%$ speedups over standard SD on a range of standard benchmarks, using both greedy and non-greedy sampling. Furthermore, we combine DistillSpec with lossy SD to achieve fine-grained control over the latency vs. task performance trade-off. Finally, in practical scenarios with models of varying sizes, first using distillation to boost the performance of the target model and then applying DistillSpec to train a well-aligned draft model can reduce decoding latency by $6 - 10\\times$ with minimal performance drop, compared to standard decoding without distillation.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, identifying a compact draft model that is well-aligned with the target model is challenging.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, identifying a compact draft model that is well-aligned with the target model is challenging.\""
    },
    {
        "title": "Accurate Retraining-free Pruning for Pretrained Encoder-based Language Models",
        "authors": "Seungcheol Park;Hojun Choi;U Kang",
        "site": "https://iclr.cc/virtual/2024/poster/17667",
        "summary": "Given a pretrained encoder-based language model, how can we accurately compress it without retraining? Retraining-free structured pruning algorithms are crucial in pretrained language model compression due to their significantly reduced pruning cost and capability to prune large language models. However, existing retraining-free algorithms encounter severe accuracy degradation, as they fail to handle pruning errors, especially at high compression rates. In this paper, we propose KPrune (Knowledge-preserving pruning), an accurate retraining-free structured pruning algorithm for pretrained encoder-based language models.KPrune focuses on preserving the useful knowledge of the pretrained model to minimize pruning errors through a carefully designed iterative pruning process composed of knowledge measurement, knowledge-preserving mask search, and knowledge-preserving weight-tuning. As a result, KPrune shows significant accuracy improvements up to 58.02%p higher F1 score compared to existing retraining-free pruning algorithms under a high compression rate of 80% on the SQuAD benchmark without any retraining process.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing retraining-free algorithms encounter severe accuracy degradation, as they fail to handle pruning errors, especially at high compression rates.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, existing retraining-free algorithms encounter severe accuracy degradation, as they fail to handle pruning errors, especially at high compression rates.\""
    },
    {
        "title": "Branch-GAN: Improving Text Generation with (not so) Large Language Models",
        "authors": "Fredrik Carlsson;Johan Broberg;Erik Hillbom;Magnus Sahlgren;Joakim Nivre",
        "site": "https://iclr.cc/virtual/2024/poster/17658",
        "summary": "The current advancements in open domain text generation have been spearheaded by Transformer-based large language models. Leveraging efficient parallelization and vast training datasets, these models achieve unparalleled text generation capabilities. Even so, current models are known to suffer from  deficiencies such as repetitive texts, looping issues, and lack of robustness. While adversarial training through generative adversarial networks (GAN) is a proposed solution, earlier research in this direction has predominantly focused on older architectures, or narrow tasks. As a result, this approach is not yet compatible with modern language models for open-ended text generation, leading to diminished interest within the broader research community. We propose a computationally efficient GAN approach for sequential data that utilizes the parallelization capabilities of Transformer models. Our method revolves around generating multiple branching sequences from each training sample, while also incorporating the typical next-step prediction loss on the original data. In this way, we achieve a dense reward and loss signal for both the generator and the discriminator, resulting in a stable training dynamic. We apply our training method to pre-trained language models, using data from their original training set but less than 0.01% of the available data.  A comprehensive human evaluation shows that our method significantly improves the quality of texts generated by the model while avoiding the previously reported sparsity problems of GAN approaches. Even our smaller models outperform larger original baseline models with more than 16 times the number of parameters. Finally, we corroborate previous claims that perplexity on held-out data is not a sufficient metric for measuring the quality of generated texts.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"current models are known to suffer from deficiencies such as repetitive texts, looping issues, and lack of robustness.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"current models are known to suffer from deficiencies such as repetitive texts, looping issues, and lack of robustness.\""
    },
    {
        "title": "Let Models Speak Ciphers: Multiagent Debate through Embeddings",
        "authors": "Chau Pham;Boyi Liu;Yingxiang Yang;Zhengyu Chen;Tianyi Liu;Jianbo Yuan;Bryan A. Plummer;Zhaoran Wang;Hongxia Yang",
        "site": "https://iclr.cc/virtual/2024/poster/17640",
        "summary": "Discussion and debate among Large Language Models (LLMs) have gained considerable attention due to their potential to enhance the reasoning ability of LLMs. Although natural language is an obvious choice for communication due to LLM's language understanding capability, the token sampling step needed when generating natural language poses a potential risk of information loss, as it uses only one token to represent the model's belief across the entire vocabulary. In this paper, we introduce a communication regime named CIPHER (Communicative Inter-Model Protocol Through Embedding Representation) to address this issue. Specifically, we remove the token sampling step from LLMs and let them communicate their beliefs across the vocabulary through the expectation of the raw transformer output embeddings. Remarkably, by deviating from natural language, CIPHER offers an advantage of encoding a broader spectrum of information without any modification to the model weights, outperforming the state-of-the-art LLM debate methods using natural language by 0.5-5.0% across five reasoning tasks and multiple open-source LLMs of varying sizes. This showcases the superiority and robustness of embeddings as an alternative \"language\" for communication among LLMs. We anticipate that CIPHER will inspire further exploration for the design of interactions within LLM agent systems, offering a new direction that could significantly influence future developments in the field.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the token sampling step needed when generating natural language poses a potential risk of information loss, as it uses only one token to represent the model's belief across the entire vocabulary.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the token sampling step needed when generating natural language poses a potential risk of information loss, as it uses only one token to represent the model's belief across the entire vocabulary.\""
    },
    {
        "title": "ARGS: Alignment as Reward-Guided Search",
        "authors": "Maxim Khanov;Jirayu Burapacheep;Yixuan Li",
        "site": "https://iclr.cc/virtual/2024/poster/17639",
        "summary": "Aligning large language models with human objectives is paramount, yet common approaches including RLHF suffer from unstable and resource-intensive training. In response to this challenge, we introduce ARGS, Alignment as Reward-Guided Search, a novel framework that integrates alignment into the decoding process, eliminating the need for expensive RL training. By adjusting the model’s probabilistic predictions using a reward signal, ARGS generates texts with semantic diversity while being aligned with human preferences, offering a promising and flexible solution for aligning language models. Notably, our method demonstrates consistent enhancements in average reward compared to baselines across diverse alignment tasks and various model dimensions. For example, under the same greedy-based decoding strategy, our method improves the average reward by 19.56% relative to the baseline and secures a preference or tie score of 64.33% in GPT-4 evaluation. We believe that our framework, emphasizing test-time alignment, paves the way for more responsive language models in the future. Code is publicly available at: https://github.com/deeplearning-wisc/args.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Aligning large language models with human objectives is paramount, yet common approaches including RLHF suffer from unstable and resource-intensive training.\"\n\nThis paper mentions a limitation of LLMs, specifically the difficulty in aligning them with human objectives and the instability of common approaches, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Aligning large language models with human objectives is paramount, yet common approaches including RLHF suffer from unstable and resource-intensive training.\"\n\nThis paper mentions a limitation of LLMs, specifically the difficulty in aligning them with human objectives and the instability of common approaches, but it is not the primary focus of the paper."
    },
    {
        "title": "Measuring Vision-Language STEM Skills of Neural Models",
        "authors": "Jianhao Shen;Ye Yuan;Srbuhi Mirzoyan;Ming Zhang;Chenguang Wang",
        "site": "https://iclr.cc/virtual/2024/poster/17631",
        "summary": "Abstract:We introduce a new challenge to test the STEM skills of neural models. The problems in the real world often require solutions, combining knowledge from STEM (science, technology, engineering, and math). Unlike existing datasets, our dataset requires the understanding of multimodal vision-language information of STEM. Our dataset features one of the largest and most comprehensive datasets for the challenge. It includes $448$ skills and $1,073,146$ questions spanning all STEM subjects. Compared to existing datasets that often focus on examining expert-level ability, our dataset includes fundamental skills and questions designed based on the K-12 curriculum. We also add state-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our benchmark. Results show that the recent model advances only help master a very limited number of lower grade-level skills ($2.5$% in the third grade) in our dataset. In fact, these models are still well below (averaging $54.7$%) the performance of elementary students, not to mention near expert-level performance. To understand and increase the performance on our dataset, we teach the models on a training split of our dataset.Even though we observe improved performance, the model performance remains relatively low compared to average elementary students. To solve STEM problems, we will need novel algorithmic innovations from the community.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Results show that the recent model advances only help master a very limited number of lower grade-level skills ($2.5$% in the third grade) in our dataset. In fact, these models are still well below (averaging $54.7$%) the performance of elementary students, not to mention near expert-level performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Results show that the recent model advances only help master a very limited number of lower grade-level skills ($2.5$% in the third grade) in our dataset. In fact, these models are still well below (averaging $54.7$%) the performance of elementary students, not to mention near expert-level performance.\""
    },
    {
        "title": "Controlling Vision-Language Models for Multi-Task Image Restoration",
        "authors": "Ziwei Luo;Fredrik K. Gustafsson;Zheng Zhao;Jens Sjölund;Thomas B. Schön",
        "site": "https://iclr.cc/virtual/2024/poster/17626",
        "summary": "Vision-language models such as CLIP have shown great impact on diverse downstream tasks for zero-shot or label-free predictions. However, when it comes to low-level vision such as image restoration their performance deteriorates dramatically due to corrupted inputs. In this paper, we present a degradation-aware vision-language model (DA-CLIP) to better transfer pretrained vision-language models to low-level vision tasks as a multi-task framework for image restoration. More specifically, DA-CLIP trains an additional controller that adapts the fixed CLIP image encoder to predict high-quality feature embeddings. By integrating the embedding into an image restoration network via cross-attention, we are able to pilot the model to learn a high-fidelity image reconstruction. The controller itself will also output a degradation feature that matches the real corruptions of the input, yielding a natural classifier for different degradation types. In addition, we construct a mixed degradation dataset with synthetic captions for DA-CLIP training. Our approach advances state-of-the-art performance on both degradation-specific and unified image restoration tasks, showing a promising direction of prompting image restoration with large-scale pretrained vision-language models. Our code is available at https://github.com/Algolzw/daclip-uir.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, when it comes to low-level vision such as image restoration their performance deteriorates dramatically due to corrupted inputs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, when it comes to low-level vision such as image restoration their performance deteriorates dramatically due to corrupted inputs.\""
    },
    {
        "title": "Tailoring Self-Rationalizers with Multi-Reward Distillation",
        "authors": "Sahana Ramnath;Brihi Joshi;Skyler Hallinan;Ximing Lu;Liunian Harold Li;Aaron Chan;Jack Hessel;Yejin Choi;Xiang Ren",
        "site": "https://iclr.cc/virtual/2024/poster/17624",
        "summary": "Large language models (LMs) are capable of generating free-text rationales to aid question answering. However, prior work 1) suggests that useful self-rationalization is emergent only at significant scales (e.g., 175B parameter GPT-3); and 2) focuses largely on downstream performance, ignoring the semantics of the rationales themselves, e.g., are they faithful, true, and helpful for humans? In this work, we enable small-scale LMs (∼200x smaller than GPT-3) to generate rationales that not only improve downstream task performance, but are also more plausible, consistent, and diverse, assessed both by automatic and human evaluation. Our method, MaRio (Multi-rewArd RatIOnalization), is a multi-reward conditioned self-rationalization algorithm that optimizes multiple distinct properties like plausibility, diversity and consistency. Results on three difficult question-answering datasets StrategyQA, QuaRel and OpenBookQA show that not only does MaRio improve task accuracy, but it also improves the self-rationalization quality of small LMs across the aforementioned axes better than a supervised fine-tuning (SFT) baseline. Extensive human evaluations confirm that MaRio rationales are preferred vs. SFT rationales, as well as qualitative improvements in plausibility and consistency.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, prior work 1) suggests that useful self-rationalization is emergent only at significant scales (e.g., 175B parameter GPT-3); and 2) focuses largely on downstream performance, ignoring the semantics of the rationales themselves, e.g., are they faithful, true, and helpful for humans?\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, prior work 1) suggests that useful self-rationalization is emergent only at significant scales (e.g., 175B parameter GPT-3); and 2) focuses largely on downstream performance, ignoring the semantics of the rationales themselves, e.g., are they faithful, true, and helpful for humans?\""
    },
    {
        "title": "Language Model Inversion",
        "authors": "John Xavier Morris;Wenting Zhao;Justin T Chiu;Vitaly Shmatikov;Alexander M Rush",
        "site": "https://iclr.cc/virtual/2024/poster/17623",
        "summary": "Abstract:Given a prompt, language models produce a distribution over all possible next tokens; when the prompt is unknown, can we use this distributional information to recover the prompt? We consider the problem of anguage model inversion and show that next-token probabilities contain a surprising amount of information about the preceding text. Often we can recover the text in cases where it is hidden from the user, motivating a method for recovering unknown prompts given only the model's current distribution output. We consider a variety of model access scenarios, and show how even without predictions for every token in the vocabulary we can recover the probability vector through search and reconstruction of the input. On LLAMA-7B, our inversion method reconstructs prompts with a BLEU of $59$ and token-level F1 of $77$ and recovers $23\\%$ of prompts exactly",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Often we can recover the text in cases where it is hidden from the user, motivating a method for recovering unknown prompts given only the model's current distribution output.\"\n\nThis evidence is brief and does not go into detail, it mentions a limitation of the model in the sense that it can be inverted to recover the prompt, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Often we can recover the text in cases where it is hidden from the user, motivating a method for recovering unknown prompts given only the model's current distribution output.\"\n\nThis evidence is brief and does not go into detail, it mentions a limitation of the model in the sense that it can be inverted to recover the prompt, but it is not the primary focus of the paper."
    },
    {
        "title": "GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs",
        "authors": "Pengcheng Jiang;Cao Xiao;Adam Richard Cross;Jimeng Sun",
        "site": "https://iclr.cc/virtual/2024/poster/17612",
        "summary": "Clinical predictive models often rely on patients’ electronic health records (EHR), but integrating medical knowledge to enhance predictions and decision-making is challenging. This is because personalized predictions require personalized knowledgegraphs (KGs), which are difficult to generate from patient EHR data. To address this, we propose GraphCare, an open-world framework that uses external KGs to improve EHR-based predictions. Our method extracts knowledge from large language models (LLMs) and external biomedical KGs to build patient-specific KGs, which are then used to train our proposed Bi-attention AugmenTed(BAT) graph neural network (GNN) for healthcare predictions. On two public datasets, MIMIC-III and MIMIC-IV, GraphCare surpasses baselines in four vital healthcare prediction tasks: mortality, readmission, length of stay (LOS), and drug recommendation. On MIMIC-III, it boosts AUROC by 17.6% and 6.6% for mortality and readmission, and F1-score by 7.9% and 10.8% for LOS and drug recommendation, respectively. Notably, GraphCare demonstrates a substantial edge in scenarios with limited data availability. Our findings highlight the potential of using external KGs in healthcare prediction tasks and demonstrate the promise of GraphCare in generating personalized KGs for promoting personalized medicine.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our method extracts knowledge from large language models (LLMs) and external biomedical KGs to build patient-specific KGs\"\n\nThis paper mentions LLMs but does not discuss any limitations of LLMs. It uses LLMs as a component of the proposed framework to extract knowledge and build patient-specific knowledge graphs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Our method extracts knowledge from large language models (LLMs) and external biomedical KGs to build patient-specific KGs\"\n\nThis paper mentions LLMs but does not discuss any limitations of LLMs. It uses LLMs as a component of the proposed framework to extract knowledge and build patient-specific knowledge graphs."
    },
    {
        "title": "Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization",
        "authors": "Kun LEI;Zhengmao He;Chenhao Lu;Kaizhe Hu;Yang Gao;Huazhe Xu",
        "site": "https://iclr.cc/virtual/2024/poster/17610",
        "summary": "Combining offline and online reinforcement learning (RL) is crucial for efficient and safe learning. However, previous approaches treat offline and online learning as separate procedures, resulting in redundant designs and limited performance. We ask:Can we achieve straightforward yet effective offline and online learning without introducing extra conservatism or regularization?In this study, we propose Uni-O4, which utilizes an on-policy objective for both offline and online learning. Owning to the alignment of objectives in two phases, the RL agent can transfer between offline and online learning seamlessly. This property enhances the flexibility of the learning paradigm, allowing for arbitrary combinations of pretraining, fine-tuning, offline, and online learning. In the offline phase, specifically, Uni-O4 leverages diverse ensemble policies to address the mismatch issues between the estimated behavior policy and the offline dataset. Through a simple offline policy evaluation (OPE) approach, Uni-O4 can achieve multi-step policy improvement safely. We demonstrate that by employing the method above, the fusion of these two paradigms can yield superior offline initialization as well as stable and rapid online fine-tuning capabilities. Through real-world robot tasks, we highlight the benefits of this paradigm for rapid deployment in challenging, previously unseen real-world environments. Additionally, through comprehensive evaluations using numerous simulated benchmarks, we substantiate that our method achieves state-of-the-art performance in both offline and offline-to-online fine-tuning learning.Our website",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Compositional Preference Models for Aligning LMs",
        "authors": "Dongyoung Go;Tomasz Korbak;Germán Kruszewski;Jos Rozen;Marc Dymetman",
        "site": "https://iclr.cc/virtual/2024/poster/17607",
        "summary": "As language models (LMs) become more capable, it is increasingly important to align them with human preferences. However, the dominant paradigm for training Preference Models (PMs) for that purpose suffers from fundamental limitations, such as lack of transparency and scalability, along with susceptibility to overfitting the preference dataset.We propose Compositional Preference Models (CPMs), a novel PM framework that decomposes one global preference assessment into several interpretable features, obtains scalar scores for these features from a prompted LM, and aggregates these scores using a logistic regression classifier. Through these simple steps, CPMs allow to control which properties of the preference data are used to train the preference model and to build it based on features that are believed to underlie the human preference judgment.Our experiments show that CPMs not only improve generalization and are more robust to overoptimization than standard PMs, but also that best-of-n samples obtained using CPMs tend to be preferred over samples obtained using conventional PMs.Overall, our approach demonstrates the benefits of endowing PMs with priors about which features determine human preferences while relying on LM capabilities to extract those features in a scalable and robust way.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the dominant paradigm for training Preference Models (PMs) for that purpose suffers from fundamental limitations, such as lack of transparency and scalability, along with susceptibility to overfitting the preference dataset.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the dominant paradigm for training Preference Models (PMs) for that purpose suffers from fundamental limitations, such as lack of transparency and scalability, along with susceptibility to overfitting the preference dataset.\""
    },
    {
        "title": "Motif: Intrinsic Motivation from Artificial Intelligence Feedback",
        "authors": "Martin Klissarov;Pierluca D'Oro;Shagun Sodhani;Roberta Raileanu;Pierre-Luc Bacon;Pascal Vincent;Amy Zhang;Mikael Henaff",
        "site": "https://iclr.cc/virtual/2024/poster/17604",
        "summary": "Exploring rich environments and evaluating one's actions without prior knowledge is immensely challenging. In this paper, we propose Motif, a general method to interface such prior knowledge from a Large Language Model (LLM) with an agent. Motif is based on the idea of grounding LLMs for decision-making without requiring them to interact with the environment: it elicits preferences from an LLM over pairs of captions to construct an intrinsic reward, which is then used to train agents with reinforcement learning. We evaluate Motif's performance and behavior on the challenging, open-ended and procedurally-generated NetHack game. Surprisingly, by only learning to maximize its intrinsic reward, Motif achieves a higher game score than an algorithm directly trained to maximize the score itself. When combining Motif's intrinsic reward with the environment reward, our method significantly outperforms existing approaches and makes progress on tasks where no advancements have ever been made without demonstrations. Finally, we show that Motif mostly generates intuitive human-aligned behaviors which can be steered easily through prompt modifications, while scaling well with the LLM size and the amount of information given in the prompt.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs, only mentions using LLMs as a component of the proposed method.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No evidence of discussion of limitations of LLMs, only mentions using LLMs as a component of the proposed method."
    },
    {
        "title": "Dissecting learning and forgetting in language model finetuning",
        "authors": "Xiao Zhang;Ji Wu",
        "site": "https://iclr.cc/virtual/2024/poster/17602",
        "summary": "Finetuning language models on domain-specific corpus is a common approach to enhance their domain knowledge and capability. While improving performance on domain tasks, it often brings a side-effect of forgetting of the model's general abilities. In this study, we analyze the effects of finetuning on language models by dissecting its impacts on the modeling of topic, style, and factual knowledge in text. Our method uses instruction-following LLMs such as ChatGPT to auto-generate controlled-variable text examples which we use to probe the model. Our findings reveal that finetuning results in significant shifts in the language model's topic and style priors, while actual knowledge learning only contributes to a small fraction of the total probability change. Analysis shows that the adaptation of topic and style priors behave akin to learning simple features: they are learned rapidly and require little model capacity. They are also learned independently and primarily at the beginning of a text sequence. In contrast, factual knowledge is learned stably but slowly and requires significant model capacity to learn. The research offers insights and understanding into the finer dynamics of learning and forgetting in language models, and can potentially inform future research on improving domain adaptation and addressing the challenges of forgetting in continual learning of language models.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While improving performance on domain tasks, it often brings a side-effect of forgetting of the model's general abilities.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"While improving performance on domain tasks, it often brings a side-effect of forgetting of the model's general abilities.\""
    },
    {
        "title": "Evaluating Large Language Models at Evaluating Instruction Following",
        "authors": "Zhiyuan Zeng;Jiatong Yu;Tianyu Gao;Yu Meng;Tanya Goyal;Danqi Chen",
        "site": "https://iclr.cc/virtual/2024/poster/17598",
        "summary": "As research in large language models (LLMs) continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evaluations for comparing the ever increasing list of models. This paper investigates the efficacy of these “LLM evaluators”, particularly in using them to assess instruction following, a metric that gauges how closely generated text adheres to the given instruction. We introduce a challenging meta-evaluation benchmark, LLMBar, designed to test the ability of an LLM evaluator in discerning instruction-following outputs. The authors manually curated 419 pairs of outputs, one adhering to instructions while the other diverging, yet may possess deceptive qualities that mislead an LLM evaluator, e.g., a more engaging tone. Contrary to existing meta-evaluation, we discover that different evaluators (i.e., combinations of LLMs and prompts) exhibit distinct performance on LLMBar and even the highest-scoring ones have substantial room for improvement. We also present a novel suite of prompting strategies that further close the gap between LLM and human evaluators. With LLMBar, we hope to offer more insight into LLM evaluators and foster future research in developing better instruction-following models.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Contrary to existing meta-evaluation, we discover that different evaluators (i.e., combinations of LLMs and prompts) exhibit distinct performance on LLMBar and even the highest-scoring ones have substantial room for improvement.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Contrary to existing meta-evaluation, we discover that different evaluators (i.e., combinations of LLMs and prompts) exhibit distinct performance on LLMBar and even the highest-scoring ones have substantial room for improvement.\""
    },
    {
        "title": "Towards Understanding Sycophancy in Language Models",
        "authors": "Mrinank Sharma;Meg Tong;Tomasz Korbak;David Duvenaud;Amanda Askell;Samuel R. Bowman;Esin DURMUS;Zac Hatfield-Dodds;Scott R Johnston;Shauna M Kravec;Timothy Maxwell;Sam McCandlish;Kamal Ndousse;Oliver Rausch;Nicholas Schiefer;Da Yan;Miranda Zhang;Ethan Perez",
        "site": "https://iclr.cc/virtual/2024/poster/17593",
        "summary": "Reinforcement learning from human feedback (RLHF) is a popular technique for training high-quality AI assistants. However, RLHF may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models and whether human preference judgments are responsible. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophancy behavior across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of RLHF models, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results indicate that sycophancy is a general behavior of RLHF models, likely driven in part by human preference judgments favoring sycophantic responses.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Reinforcement learning from human feedback (RLHF) may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Reinforcement learning from human feedback (RLHF) may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy.\""
    },
    {
        "title": "Large Language Models as Generalizable Policies for Embodied Tasks",
        "authors": "Andrew Szot;Max Schwarzer;Harsh Agrawal;Bogdan Mazoure;Rin Metcalf;Walter Talbott;Natalie Mackraz;R Devon Hjelm;Alexander T Toshev",
        "site": "https://iclr.cc/virtual/2024/poster/17588",
        "summary": "We show that large language models (LLMs) can be adapted to be generalizable policies for embodied visual tasks. Our approach, called Large LAnguage model Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take as input text instructions and visual egocentric observations and output actions directly in the environment. Using reinforcement learning, we train LLaRP to see and act solely through environmental interactions. We show that LLaRP is robust to complex paraphrasings of task instructions and can generalize to new tasks that require novel optimal behavior. In particular, on 1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other common learned baselines or zero-shot applications of LLMs. Finally, to aid the community in studying language conditioned, massively multi-task, embodied AI problems we release a novel benchmark, Language Rearrangement, consisting of 150,000 training and 1,000 testing tasks for language-conditioned rearrangement.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs, only mentions the success rate of LLaRP compared to \"other common learned baselines or zero-shot applications of LLMs\", which implies a limitation of other approaches rather than LLMs itself.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No evidence of discussion of limitations of LLMs, only mentions the success rate of LLaRP compared to \"other common learned baselines or zero-shot applications of LLMs\", which implies a limitation of other approaches rather than LLMs itself."
    },
    {
        "title": "In-context Exploration-Exploitation for Reinforcement Learning",
        "authors": "Zhenwen Dai;Federico Tomasi;Sina Ghiassian",
        "site": "https://iclr.cc/virtual/2024/poster/17580",
        "summary": "In-context learning is a promising approach for online policy learning of offline reinforcement learning (RL) methods, which can be achieved at inference time without gradient optimization. However, this method is hindered by significant computational costs resulting from the gathering of large training trajectory sets and the need to train large Transformer models. We address this challenge by introducing an In-context Exploration-Exploitation (ICEE) algorithm, designed to optimize the efficiency of in-context policy learning. Unlike existing models, ICEE performs an exploration-exploitation trade-off at inference time within a Transformer model, without the need for explicit Bayesian inference. Consequently, ICEE can solve Bayesian optimization problems as efficiently as Gaussian process biased methods do, but in significantly less time. Through experiments in grid world environments, we demonstrate that ICEE can learn to solve new RL tasks using only tens of episodes, marking a substantial improvement over the hundreds of episodes needed by the previous in-context learning method.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "BEND: Benchmarking DNA Language Models on Biologically Meaningful Tasks",
        "authors": "Frederikke Isa Marin;Felix Teufel;Marc Horlacher;Dennis Madsen;Dennis Pultz;Ole Winther;Wouter Boomsma",
        "site": "https://iclr.cc/virtual/2024/poster/17578",
        "summary": "The genome sequence contains the blueprint for governing cellular processes.   While the availability of genomes has vastly increased over the last decades, experimental annotation of the various functional, non-coding and regulatory elements encoded in the DNA sequence remains both expensive and challenging. This has sparked interest in unsupervised language modeling of genomic DNA, a paradigm that has seen great success for protein sequence data.   Although various DNA language models have been proposed, evaluation tasks often differ between individual works, and might not fully recapitulate the fundamental challenges of genome annotation, including the length, scale and sparsity of the data. In this study, we introduceBEND, aBENchmark forDNA language models, featuring  a collection of realistic and biologically meaningful downstream tasks defined on the human genome.  We find that embeddings from current DNA LMs can approach performance of expert methods on some tasks, but only capture limited information about long-range features.  BEND is available at https://github.com/frederikkemarin/BEND.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"evaluation tasks often differ between individual works, and might not fully recapitulate the fundamental challenges of genome annotation, including the length, scale and sparsity of the data\"; \"We find that embeddings from current DNA LMs can approach performance of expert methods on some tasks, but only capture limited information about long-range features.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"evaluation tasks often differ between individual works, and might not fully recapitulate the fundamental challenges of genome annotation, including the length, scale and sparsity of the data\"; \"We find that embeddings from current DNA LMs can approach performance of expert methods on some tasks, but only capture limited information about long-range features.\""
    },
    {
        "title": "In-context Autoencoder for Context Compression in a Large Language Model",
        "authors": "Tao Ge;Hu Jing;Lei Wang;Xun Wang;Si-Qing Chen;Furu Wei",
        "site": "https://iclr.cc/virtual/2024/poster/17573",
        "summary": "Abstract:We propose the In-context Autoencoder (ICAE), leveraging the power of a large language model (LLM) to compress a long context into short compact memory slots that can be directly conditioned on by the LLM for various purposes. ICAE is first pretrained using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, it is fine-tuned on instruction data for producing desirable responses to various prompts. Experiments demonstrate that our lightweight ICAE, introducing about 1% additional parameters, effectively achieves $4\\times$ context compression based on Llama, offering advantages in both improved latency and GPU memory cost during inference, and showing an interesting insight in memorization as well as potential for scalability. These promising results imply a novel perspective on the connection between working memory in cognitive science and representation learning in LLMs, revealing ICAE's significant implications in addressing the long context problem and suggesting further research in LLM context management. Our data, code and models are available at https://github.com/getao/icae.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"revealing ICAE's significant implications in addressing the long context problem\"\n\nThe paper mentions a limitation of LLMs (the long context problem) but only briefly and in passing, without elaborating on it.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"revealing ICAE's significant implications in addressing the long context problem\"\n\nThe paper mentions a limitation of LLMs (the long context problem) but only briefly and in passing, without elaborating on it."
    },
    {
        "title": "Mixture of LoRA Experts",
        "authors": "Xun Wu;Shaohan Huang;Furu Wei",
        "site": "https://iclr.cc/virtual/2024/poster/17571",
        "summary": "LoRA has gained widespread acceptance in the fine-tuning of large pre-trained models to cater to a diverse array of downstream tasks, showcasing notable effectiveness and efficiency, thereby solidifying its position as one of the most prevalent fine-tuning techniques. Due to the modular nature of LoRA's plug-and-play plugins, researchers have delved into the amalgamation of multiple LoRAs to empower models to excel across various downstream tasks. Nonetheless, extant approaches for LoRA fusion grapple with inherent challenges. Direct arithmetic merging may result in the loss of the original pre-trained model's generative capabilities or the distinct identity of LoRAs, thereby yielding suboptimal outcomes. On the other hand, Reference tuning-based fusion exhibits limitations concerning the requisite flexibility for the effective combination of multiple LoRAs. In response to these challenges, this paper introduces the Mixture of LoRA Experts (MoLE) approach, which harnesses hierarchical control and unfettered branch selection. The MoLE approach not only achieves superior LoRA fusion performance in comparison to direct arithmetic merging but also retains the crucial flexibility for combining LoRAs effectively. Extensive experimental evaluations conducted in both the Natural Language Processing (NLP) and Vision \\& Language (V\\&L) domains substantiate the efficacy of MoLE.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Direct arithmetic merging may result in the loss of the original pre-trained model's generative capabilities or the distinct identity of LoRAs, thereby yielding suboptimal outcomes. On the other hand, Reference tuning-based fusion exhibits limitations concerning the requisite flexibility for the effective combination of multiple LoRAs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Direct arithmetic merging may result in the loss of the original pre-trained model's generative capabilities or the distinct identity of LoRAs, thereby yielding suboptimal outcomes. On the other hand, Reference tuning-based fusion exhibits limitations concerning the requisite flexibility for the effective combination of multiple LoRAs.\""
    },
    {
        "title": "Hierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMs",
        "authors": "Woomin Song;Seunghyuk Oh;Sangwoo Mo;Jaehyung Kim;Sukmin Yun;Jung-Woo Ha;Jinwoo Shin",
        "site": "https://iclr.cc/virtual/2024/poster/17565",
        "summary": "Large language models (LLMs) have shown remarkable performance in various natural language processing tasks. However, a primary constraint they face is the context limit, i.e., the maximum number of tokens they can process. Previous works have explored architectural changes and modifications in positional encoding to relax the constraint, but they often require expensive training or do not address the computational demands of self-attention. In this paper, we present Hierarchical cOntext MERging (HOMER), a new training-free scheme designed to overcome the limitations. HOMER uses a divide-and-conquer algorithm, dividing long inputs into manageable chunks. Each chunk is then processed collectively, employing a hierarchical strategy that merges adjacent chunks at progressive transformer layers. A token reduction technique precedes each merging, ensuring memory usage efficiency. We also propose an optimized computational order reducing the memory requirement to logarithmically scale with respect to input length, making it especially favorable for environments with tight memory restrictions.  Our experiments demonstrate the proposed method's superior performance and memory efficiency, enabling the broader use of LLMs in contexts requiring extended context. Code is available at https://github.com/alinlab/HOMER.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, a primary constraint they face is the context limit, i.e., the maximum number of tokens they can process.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, a primary constraint they face is the context limit, i.e., the maximum number of tokens they can process.\""
    },
    {
        "title": "LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors",
        "authors": "Sheng Jin;Xueying Jiang;Jiaxing Huang;Lewei Lu;Shijian Lu",
        "site": "https://iclr.cc/virtual/2024/poster/17561",
        "summary": "Inspired by the outstanding zero-shot capability of vision language models (VLMs) in image classification tasks, open-vocabulary object detection has attracted increasing interest by distilling the broad VLM knowledge into detector training. However, most existing open-vocabulary detectors learn by aligning region embeddings with categorical labels (e.g., bicycle) only, disregarding the capability of VLMs on aligning visual embeddings with fine-grained text descriptions of object parts (e.g., pedals and bells). This paper presents DVDet, a Descriptor-Enhanced Open Vocabulary Detector that introduces conditional context prompts and hierarchical textual descriptors that enable precise region-text alignment as well as open-vocabulary detection training in general. Specifically, the conditional context prompt transforms regional embeddings into image-like representations that can be directly integrated into general open vocabulary detection training. In addition, we introduce large language models as an interactive and implicit knowledge repository which enables iterative mining and refining visually oriented textual descriptors for precise region-text alignment. Extensive experiments over multiple large-scale benchmarks show that DVDet outperforms the state-of-the-art consistently by large margins.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs, but mentions \"most existing open-vocabulary detectors learn by aligning region embeddings with categorical labels only, disregarding the capability of VLMs\".",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No evidence of discussion of limitations of LLMs, but mentions \"most existing open-vocabulary detectors learn by aligning region embeddings with categorical labels only, disregarding the capability of VLMs\"."
    },
    {
        "title": "AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ",
        "authors": "Jonas Belouadi;Anne Lauscher;Steffen Eger",
        "site": "https://iclr.cc/virtual/2024/poster/17553",
        "summary": "Generating bitmap graphics from text has gained considerable attention, yet for scientific figures, vector graphics are often preferred. Given that vector graphics are typically encoded using low-level graphics primitives, generating them directly is difficult. To address this, we propose the use of TikZ, a well-known abstract graphics language that can be compiled to vector graphics, as an intermediate representation of scientific figures. TikZ offers human-oriented, high-level commands, thereby facilitating conditional language modeling with any large language model. To this end, we introduce DaTikZ the first large-scale TikZ dataset, consisting of 120k TikZ drawings aligned with captions. We fine-tune LLaMA on DaTikZ, as well as our new model CLiMA, which augments LLaMA with multimodal CLIP embeddings. In both human and automatic evaluation, CLiMA and LLaMA outperform commercial GPT-4 and Claude 2 in terms of similarity to human-created figures, with CLiMA additionally improving text-image alignment. Our detailed analysis shows that all models generalize well and are not susceptible to memorization. GPT-4 and Claude 2, however, tend to generate more simplistic figures compared to both humans and our models. We make our framework, AutomaTikZ, along with model weights and datasets, publicly available.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"GPT-4 and Claude 2, however, tend to generate more simplistic figures compared to both humans and our models.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of specific LLMs (GPT-4 and Claude 2) in passing, but does not elaborate on this limitation or make it a major focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"GPT-4 and Claude 2, however, tend to generate more simplistic figures compared to both humans and our models.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of specific LLMs (GPT-4 and Claude 2) in passing, but does not elaborate on this limitation or make it a major focus of the paper."
    },
    {
        "title": "RLCD: Reinforcement Learning from Contrastive Distillation for LM Alignment",
        "authors": "Kevin Yang;Dan Klein;Asli Celikyilmaz;Nanyun Peng;Yuandong Tian",
        "site": "https://iclr.cc/virtual/2024/poster/17552",
        "summary": "We propose Reinforcement Learning from Contrastive Distillation (RLCD), a method for aligning language models to follow principles expressed in natural language (e.g., to be more harmless) without using human feedback. RLCD creates preference pairs from two contrasting model outputs, one using a positive prompt designed to encourage following the given principles, and one using a negative prompt designed to encourage violating them. Using two different prompts causes model outputs to be more differentiated on average, resulting in cleaner preference labels in the absence of human annotations. We then use the preference pairs to train a preference model, which is in turn used to improve a base unaligned language model via reinforcement learning. Empirically, RLCD outperforms RLAIF (Bai et al., 2022b) and context distillation (Huang et al., 2022) baselines across three diverse alignment tasks—harmlessness, helpfulness, and story outline generation—and when using both 7B and 30B model scales for simulating preference data",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"aligning language models to follow principles expressed in natural language (e.g., to be more harmless)\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"aligning language models to follow principles expressed in natural language (e.g., to be more harmless)\""
    },
    {
        "title": "Let's Verify Step by Step",
        "authors": "Hunter Lightman;Vineet Kosaraju;Yuri Burda;Harrison Edwards;Bowen Baker;Teddy Lee;Jan Leike;John Schulman;Ilya Sutskever;Karl Cobbe",
        "site": "https://iclr.cc/virtual/2024/poster/17549",
        "summary": "In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, even state-of-the-art models still regularly produce logical mistakes.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, even state-of-the-art models still regularly produce logical mistakes.\""
    },
    {
        "title": "Multilingual Jailbreak Challenges in Large Language Models",
        "authors": "Yue Deng;Wenxuan Zhang;Sinno Jialin Pan;Lidong Bing",
        "site": "https://iclr.cc/virtual/2024/poster/17543",
        "summary": "While large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the ``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to exhibit undesirable behavior. Although several preventive measures have been developed to mitigate the potential risks associated with LLMs, they have primarily focused on English. In this study, we reveal the presence of multilingual jailbreak challenges within LLMs and consider two potential risky scenarios: unintentional and intentional. The unintentional scenario involves users querying LLMs using non-English prompts and inadvertently bypassing the safety mechanisms, while the intentional scenario concerns malicious users combining malicious instructions with multilingual prompts to deliberately attack LLMs. The experimental results reveal that in the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases. Specifically, low-resource languages exhibit about three times the likelihood of encountering harmful content compared to high-resource languages, with both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts can exacerbate the negative impact of malicious instructions, with astonishingly high rates of unsafe output: 80.92\\% for ChatGPT and 40.71\\% for GPT-4. To handle such a challenge in the multilingual context, we propose a novel \\textsc{Self-Defense} framework that automatically generates multilingual training data for safety fine-tuning. Experimental results show that ChatGPT fine-tuned with such data can achieve a substantial reduction in unsafe content generation.  Data is available at \\url{https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs}.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"they pose potential safety concerns, such as the ``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to exhibit undesirable behavior\"; \"The experimental results reveal that in the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases. Specifically, low-resource languages exhibit about three times the likelihood of encountering harmful content compared to high-resource languages, with",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"they pose potential safety concerns, such as the ``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to exhibit undesirable behavior\"; \"The experimental results reveal that in the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases. Specifically, low-resource languages exhibit about three times the likelihood of encountering harmful content compared to high-resource languages, with"
    },
    {
        "title": "Intriguing Properties of Data Attribution on Diffusion Models",
        "authors": "Xiaosen Zheng;Tianyu Pang;Chao Du;Jing Jiang;Min Lin",
        "site": "https://iclr.cc/virtual/2024/poster/17540",
        "summary": "Data attribution seeks to trace model outputs back to training data. With the recent development of diffusion models, data attribution has become a desired module to properly assign valuations for high-quality or copyrighted training samples, ensuring that data contributors are fairly compensated or credited. Several theoretically motivated methods have been proposed to implement data attribution, in an effort to improve the trade-off between computational scalability and effectiveness. In this work, we conduct extensive experiments and ablation studies on attributing diffusion models, specifically focusing on DDPMs trained on CIFAR-10 and CelebA, as well as a Stable Diffusion model LoRA-finetuned on ArtBench. Intriguingly, we report counter-intuitive observations that theoretically unjustified design choices for attribution empirically outperform previous baselines by a large margin, in terms of both linear datamodeling score and counterfactual evaluation. Our work presents a significantly more efficient approach for attributing diffusion models, while the unexpected findings suggest that at least in non-convex settings, constructions guided by theoretical assumptions may lead to inferior attribution performance. The code is available at https://github.com/sail-sg/D-TRAK.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Fine-Tuned Language Models Generate Stable Inorganic Materials as Text",
        "authors": "Nate Gruver;Anuroop Sriram;Andrea Madotto;Andrew Gordon Wilson;C. Lawrence Zitnick;Zachary Ward Ulissi",
        "site": "https://iclr.cc/virtual/2024/poster/17538",
        "summary": "We propose fine-tuning large language models for generation of stable materials. While unorthodox, fine-tuning large language models on text-encoded atomistic data is simple to implement yet reliable, with around 90\\% of sampled structures obeying physical constraints on atom positions and charges. Using energy above hull calculations from both learned ML potentials and gold-standard DFT calculations, we show that our strongest model (fine-tuned  LLaMA-2 70B) can generate materials predicted to be metastable at about twice the rate (49\\% vs 28\\%) of CDVAE, a competing diffusion model. Because of text prompting's inherent flexibility, our models can simultaneously be used for unconditional generation of stable material, infilling of partial structures and text-conditional generation. Finally, we show that language models' ability to capture key symmetries of crystal structures improves with model scale, suggesting that the biases of pretrained LLMs are surprisingly well-suited for atomistic data.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs, only mentions that \"the biases of pretrained LLMs are surprisingly well-suited for atomistic data\", which is actually a positive aspect.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No evidence of discussion of limitations of LLMs, only mentions that \"the biases of pretrained LLMs are surprisingly well-suited for atomistic data\", which is actually a positive aspect."
    },
    {
        "title": "Provable Memory Efficient Self-Play Algorithm for Model-free Reinforcement Learning",
        "authors": "Na Li;Yuchen Jiao;Hangguan Shan;Shefeng Yan",
        "site": "https://iclr.cc/virtual/2024/poster/17537",
        "summary": "Abstract:The thriving field of multi-agent reinforcement learning (MARL) studies how a group of interacting agents make decisions autonomously in a shared dynamic environment. Existing theoretical studies in this area suffer from at least two of the following obstacles: memory inefficiency, the heavy dependence of sample complexity on the long horizon and the large state space, the high computational complexity, non-Markov policy, non-Nash policy, and high burn-in cost. In this work, we take a step towards settling this problem by designing a model-free self-play algorithm \\emph{Memory-Efficient Nash Q-Learning (ME-Nash-QL)} for two-player zero-sum Markov games, which is a specific setting of MARL. We prove that ME-Nash-QL can output an $\\varepsilon$-approximate Nash policy with remarkable space complexity $O(SABH)$, sample complexity $\\widetilde{O}(H^4SAB/\\varepsilon^2)$, and computational complexity $O(T\\mathrm{poly}(AB))$, where $S$ is the number of states, $\\{A, B\\}$ is the number of actions for the two players, $H$ is the horizon length, and $T$ is the number of samples. Notably, our approach outperforms in terms of space complexity compared to existing algorithms for tabular cases. It achieves the lowest computational complexity while preserving Markov policies, setting a new standard. Furthermore, our algorithm outputs a Nash policy and achieves the best sample complexity compared with the existing guarantee for long horizons, i.e. when $\\min \\\\{ A, B \\\\} \\ll H^2$. Our algorithm also achieves the best burn-in cost $O(SAB\\,\\mathrm{poly}(H))$, whereas previous algorithms need at least $O(S^3 AB\\,\\mathrm{poly}(H))$ to attain the same level of sample complexity with ours.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns",
        "authors": "Saleh Ashkboos;Maximilian L. Croci;Marcelo Gennari do Nascimento;Torsten Hoefler;James Hensman",
        "site": "https://iclr.cc/virtual/2024/poster/17531",
        "summary": "Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA-2 70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA-2 70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources.\""
    },
    {
        "title": "CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules",
        "authors": "Hung Le;Hailin Chen;Amrita Saha;Akash Gokul;Doyen Sahoo;Shafiq Joty",
        "site": "https://iclr.cc/virtual/2024/poster/17529",
        "summary": "Large Language Models (LLMs) have already become quite proficient at solving simpler programming tasks like those in HumanEval or MBPP benchmarks. However, solving more complex and competitive programming tasks is still quite challenging for these models - possibly due to their tendency to generate solutions as monolithic code blocks instead of decomposing them into logical sub-tasks and sub-modules. On the other hand, experienced programmers instinctively write modularized code with abstraction for solving complex tasks, often reusing previously developed modules. To address this gap, we propose CodeChain, a novel framework for inference that elicits modularized code generation through a chain of self-revisions, each being guided by some representative sub-modules generated in previous iterations. Concretely, CodeChain first instructs the LLM to generate modularized codes through chain-of-thought prompting. Then it applies a chain of self-revisions by iterating the two steps: 1) extracting and clustering the generated sub-modules and selecting the cluster representatives as the more generic and re-usable implementations, and 2) augmenting the original chain-of-thought prompt with these selected module-implementations and instructing the LLM to re-generate new modularized solutions. We find that by naturally encouraging the LLM to reuse the previously developed and verified sub-modules, CodeChain can significantly boost both modularity as well as correctness of the generated solutions, achieving relative pass@1 improvements of 35\\% on APPS and 76\\% on CodeContests. It is shown to be effective on both OpenAI LLMs as well as open-sourced LLMs like WizardCoder. We also conduct comprehensive ablation studies with different methods of prompting, number of clusters, model sizes, program qualities, etc., to provide useful insights that underpin CodeChain's success.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, solving more complex and competitive programming tasks is still quite challenging for these models - possibly due to their tendency to generate solutions as monolithic code blocks instead of decomposing them into logical sub-tasks and sub-modules.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, solving more complex and competitive programming tasks is still quite challenging for these models - possibly due to their tendency to generate solutions as monolithic code blocks instead of decomposing them into logical sub-tasks and sub-modules.\""
    },
    {
        "title": "Learning Multi-Agent Communication with Contrastive Learning",
        "authors": "Yat Long Lo;Biswa Sengupta;Jakob Nicolaus Foerster;Michael Noukhovitch",
        "site": "https://iclr.cc/virtual/2024/poster/17527",
        "summary": "Communication is a powerful tool for coordination in multi-agent RL. But inducing an effective, common language is a difficult challenge, particularly in the decentralized setting. In this work, we introduce an alternative perspective where communicative messages sent between agents are considered as different incomplete views of the environment state. By examining the relationship between messages sent and received, we propose to learn to communicate using contrastive learning to maximize the mutual information between messages of a given trajectory. In communication-essential environments, our method outperforms previous work in both performance and learning speed. Using qualitative metrics and representation probing, we show that our method induces more symmetric communication and captures global state information from the environment. Overall, we show the power of contrastive learning and the importance of leveraging messages as encodings for effective communication.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "CODE REPRESENTATION LEARNING AT SCALE",
        "authors": "Dejiao Zhang;Wasi Uddin Ahmad;Ming Tan;Hantian Ding;Ramesh Nallapati;Dan Roth;Xiaofei Ma;Bing Xiang",
        "site": "https://iclr.cc/virtual/2024/poster/17524",
        "summary": "Recent studies have shown that code language model at scale demonstrate significant performance gains on downstream tasks, i.e., code generation. However, most of the existing works on code representation learning train models at a hundred million parameter scale using very limited pretraining corpora. In this work, we fuel code representation learning with a vast amount of code data via a two-stage pretraining scheme. We first train the encoders via a mix that leverages both randomness in masking language modeling and implicit structure and semantic aspects of programming language. We then enhance the representations via contrastive learning with hard negative and hard positive constructed in an unsupervised manner. We establish an off-the-shelf encoder model that persistently outperforms the existing models on a wide variety of downstream tasks by large margins. To comprehend the factors contributing to successful code representation learning, we conduct detailed ablations and share our findings on (i) a customized and effective token-level denoising scheme for source code; (ii) the importance of hard negatives and hard positives; (iii) how the proposed bimodal contrastive learning boost the cross-lingual semantic search performance; and (iv) how the pretraining schemes decide the downstream task performance scales with the model size.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, most of the existing works on code representation learning train models at a hundred million parameter scale using very limited pretraining corpora.\"\n\nThis paper mentions a limitation of existing code representation learning models (limited pretraining corpora), but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, most of the existing works on code representation learning train models at a hundred million parameter scale using very limited pretraining corpora.\"\n\nThis paper mentions a limitation of existing code representation learning models (limited pretraining corpora), but it is not the primary focus of the paper."
    },
    {
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": "Zhijing Jin;Jiarui Liu;Zhiheng LYU;Spencer Poff;Mrinmaya Sachan;Rada Mihalcea;Mona T. Diab;Bernhard Schölkopf",
        "site": "https://iclr.cc/virtual/2024/poster/17518",
        "summary": "Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 200K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fail to generalize – they can only perform causal inference in in-distribution settings when variable names and textual expressions used in the queries are similar to those in the training set, but fail in out-of-distribution settings generated by perturbing these queries. Corr2Cause is a challenging task for LLMs, and can be helpful in guiding future research on improving LLMs’ pure reasoning skills and generalizability. Our data is at https://huggingface.co/datasets/causalnlp/corr2cause. Our code is at https://github.com/causalNLP/corr2cause.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fail to generalize – they can only perform",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fail to generalize – they can only perform"
    },
    {
        "title": "Are Models Biased on Text without Gender-related Language?",
        "authors": "Catarina G Belém;Preethi Seshadri;Yasaman Razeghi;Sameer Singh",
        "site": "https://iclr.cc/virtual/2024/poster/17511",
        "summary": "Gender bias research has been pivotal in revealing undesirable behaviors in large language models, exposing serious gender stereotypes associated with occupations, and emotions. A key observation in prior work is that models reinforce stereotypes as a consequence of the gendered correlations that are present in the training data. In this paper, we focus on bias where the effect from training data is unclear, and instead address the question:Do language models still exhibit gender bias in non-stereotypical settings?To do so, we introduceUnStereoEval (USE), a novel framework tailored for investigating gender bias in stereotype-free scenarios. USE defines a sentence-level score based on pretraining data statistics to determine if the sentence contain minimal word-gender associations. To systematically benchmark the fairness of popular language models in stereotype-free scenarios, we utilize USE to automatically generate benchmarks without any gender-related language.  By leveraging USE's sentence-level score, we also repurpose prior gender bias benchmarks (Winobias and Winogender) for non-stereotypical evaluation. Surprisingly, we find low fairness across all 28 tested models.  Concretely, models demonstrate fair behavior in only 9%-41% of  stereotype-free sentences, suggesting that bias does not solely stem from the presence of gender-related words. These results raise important questions about where underlying model biases come from and highlight the need for more systematic and comprehensive bias evaluation. We release the full dataset and code atucinlp.github.io/unstereo-eval.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Surprisingly, we find low fairness across all 28 tested models. Concretely, models demonstrate fair behavior in only 9%-41% of stereotype-free sentences, suggesting that bias does not solely stem from the presence of gender-related words.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Surprisingly, we find low fairness across all 28 tested models. Concretely, models demonstrate fair behavior in only 9%-41% of stereotype-free sentences, suggesting that bias does not solely stem from the presence of gender-related words.\""
    },
    {
        "title": "MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training",
        "authors": "Yizhi LI;Ruibin Yuan;Ge Zhang;Yinghao Ma;Xingran Chen;Hanzhi Yin;Chenghao Xiao;Chenghua Lin;Anton Ragni;Emmanouil Benetos;Norbert Gyenge;Roger Dannenberg;Ruibo Liu;Wenhu Chen;Gus Xia;Yemin Shi;Wenhao Huang;Zili Wang;Yike Guo;Jie Fu",
        "site": "https://iclr.cc/virtual/2024/poster/17510",
        "summary": "Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is partially due to the distinctive challenges associated with modelling musical knowledge, particularly tonal and pitched characteristics of music.To address this research gap, we propose an acousticMusic undERstanding model with large-scale self-supervisedTraining (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training.In our exploration, we identified an effective combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a musical teacher based on the Constant-Q Transform (CQT). Furthermore, we explore a wide range of settings to overcome the instability in acoustic language model pre-training, which allows our designed paradigm to scale from 95M to 330M parameters.Experimental results indicate that our model can generalise and perform well on 14 music understanding tasks and attain state-of-the-art (SOTA) overall scores.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "SuRe: Summarizing Retrievals using Answer Candidates for Open-domain QA of LLMs",
        "authors": "Jaehyung Kim;Jaehyun Nam;Sangwoo Mo;Jongjin Park;Sang-Woo Lee;Minjoon Seo;Jung-Woo Ha;Jinwoo Shin",
        "site": "https://iclr.cc/virtual/2024/poster/17509",
        "summary": "Large language models (LLMs) have made significant advancements in various natural language processing tasks, including question answering (QA) tasks. While incorporating new information with the retrieval of relevant passages is a promising way to improve QA with LLMs, the existing methods often require additional fine-tuning which becomes infeasible with recent LLMs. Augmenting retrieved passages via prompting has the potential to address this limitation, but this direction has been limitedly explored. To this end, we design a simple yet effective framework to enhance open-domain QA (ODQA) with LLMs, based on the summarized retrieval (SuRe). SuRe helps LLMs predict more accurate answers for a given question, which are well-supported by the summarized retrieval that could be viewed as an explicit rationale extracted from the retrieved passages. Specifically, SuRe first constructs summaries of the retrieved passages for each of the multiple answer candidates. Then, SuRe confirms the most plausible answer from the candidate set by evaluating the validity and ranking of the generated summaries. Experimental results on diverse ODQA benchmarks demonstrate the superiority of SuRe, with improvements of up to 4.6\\% in exact match (EM) and 4.0\\% in F1 score over standard prompting approaches. SuRe also can be integrated with a broad range of retrieval methods and LLMs. Finally, the generated summaries from SuRe show additional advantages to measure the importance of retrieved passages and serve as more preferred rationales by models and humans.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the existing methods often require additional fine-tuning which becomes infeasible with recent LLMs.\"\n\nThis abstract mentions a limitation of LLMs (the need for additional fine-tuning) but does not elaborate on it and focuses on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the existing methods often require additional fine-tuning which becomes infeasible with recent LLMs.\"\n\nThis abstract mentions a limitation of LLMs (the need for additional fine-tuning) but does not elaborate on it and focuses on the proposed solution."
    },
    {
        "title": "Remote Sensing Vision-Language Foundation Models without Annotations via Ground Remote Alignment",
        "authors": "Utkarsh Mall;Cheng Perng Phoo;Meilin Kelsey Liu;Carl Vondrick;Bharath Hariharan;Kavita Bala",
        "site": "https://iclr.cc/virtual/2024/poster/17503",
        "summary": "We introduce a method to train vision-language models for remote-sensing images without using any textual annotations. Our key insight is to use co-located internet imagery taken on the ground as an intermediary for connecting remote-sensing images and language.  Specifically, we train an image encoder for remote sensing images to align with the image encoder of CLIP using a large amount of paired internet and satellite images.  Our unsupervised approach enables the training of a first-of-its-kind large scale VLM for remote sensing images at two different resolutions. We show that these VLMs enable zero-shot, open-vocabulary image classification, retrieval, segmentation and visual question answering for satellite images. On each of these tasks, our VLM trained without textual annotations outperforms existing VLMs trained with supervision, with gains of up to 20\\% for classification and 80\\% for segmentation.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "On the Hardness of Constrained Cooperative Multi-Agent Reinforcement Learning",
        "authors": "Ziyi Chen;Yi Zhou;Heng Huang",
        "site": "https://iclr.cc/virtual/2024/poster/17502",
        "summary": "Constrained cooperative multi-agent reinforcement learning (MARL) is an emerging learning framework that has been widely applied to manage multi-agent systems, and many primal-dual type algorithms have been developed for it. However, the convergence of primal-dual algorithms crucially relies on strong duality -- a condition that has not been formally proved in constrained cooperative MARL. In this work, we prove that strong duality fails to hold in constrained cooperative MARL, by revealing a nonconvex quadratic type constraint on the occupation measure induced by the product policy. Consequently, our reanalysis of the primal-dual algorithm shows that its convergence rate is hindered by the nonzero duality gap. Then, we propose a decentralized primal approach for constrained cooperative MARL to avoid the duality gap, and our analysis shows that its convergence is hindered by another gap induced by the advantage functions. Moreover, we compare these two types of algorithms via concrete examples, and show that neither of them always outperforms the other one. Our study reveals that constrained cooperative MARL is generally a challenging and highly nonconvex problem, and its fundamental structure is very different from that of single-agent constrained RL.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": "Bowen Peng;Jeffrey Quesnelle;Honglu Fan;Enrico Shippole",
        "site": "https://iclr.cc/virtual/2024/poster/17499",
        "summary": "Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. The models fine-tuned using YaRN has been made available and reproduced online up to 128k context length.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these models fail to generalize past the sequence length they were trained on.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these models fail to generalize past the sequence length they were trained on.\""
    },
    {
        "title": "AutoVP: An Automated Visual Prompting Framework and Benchmark",
        "authors": "Hsi-Ai Tsao;Lei Hsiung;Pin-Yu Chen;Sijia Liu;Tsung-Yi Ho",
        "site": "https://iclr.cc/virtual/2024/poster/17494",
        "summary": "Visual prompting (VP) is an emerging parameter-efficient fine-tuning approach to adapting pre-trained vision models to solve various downstream image-classification tasks. However, there has hitherto been little systematic study of the design space of VP and no clear benchmark for evaluating its performance. To bridge this gap, we propose AutoVP, an end-to-end expandable framework for automating VP design choices, along with 12 downstream image-classification tasks that can serve as a holistic VP-performance benchmark. Our design space covers 1) the joint optimization of the prompts; 2) the selection of pre-trained models, including image classifiers and text-image encoders; and 3) model output mapping strategies, including nonparametric and trainable label mapping. Our extensive experimental results show that AutoVP outperforms the best-known current VP methods by a substantial margin, having up to 6.7% improvement in accuracy; and attains a maximum performance increase of 27.5% compared to linear-probing (LP) baseline. AutoVP thus makes a two-fold contribution: serving both as an efficient tool for hyperparameter tuning on VP design choices, and as a comprehensive benchmark that can reasonably be expected to accelerate VP’s development. The source code is available at https://github.com/IBM/AutoVP.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs."
    },
    {
        "title": "CLEX: Continuous Length Extrapolation for Large Language Models",
        "authors": "Guanzheng Chen;Xin Li;Zaiqiao Meng;Shangsong Liang;Lidong Bing",
        "site": "https://iclr.cc/virtual/2024/poster/17492",
        "summary": "Transformer-based Large Language Models (LLMs) are pioneering advances in many natural language processing tasks, however, their exceptional capabilities are restricted within the preset context window of Transformer. Position Embedding (PE) scaling methods, while effective in extending the context window to a specific length, demonstrate either notable limitations in their extrapolation abilities or sacrificing partial performance within the context window. Length extrapolation methods, although theoretically capable of extending the context window beyond the training sequence length, often underperform in practical long-context applications. To address these challenges, we propose Continuous Length EXtrapolation (CLEX) for LLMs. We generalise the PE scaling approaches to model the continuous dynamics by ordinary differential equations over the length scaling factor, thereby overcoming the constraints of current PE scaling methods designed for specific lengths. Moreover, by extending the dynamics to desired context lengths beyond the training sequence length, CLEX facilitates the length extrapolation with impressive performance in practical tasks. We demonstrate that CLEX can be seamlessly incorporated into LLMs equipped with Rotary Position Embedding, such as LLaMA and GPT-NeoX, with negligible impact on training and inference latency. Experimental results reveal that CLEX can effectively extend the context window to over 4× or almost 8× training length, with no deterioration in performance. Furthermore, when evaluated on the practical LongBench benchmark, our model trained on a 4k length exhibits competitive performance against state-of-the-art open-source models trained on context lengths up to 32k. Our code is available at https://github.com/DAMO-NLP-SG/CLEX.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"however, their exceptional capabilities are restricted within the preset context window of Transformer. Position Embedding (PE) scaling methods, while effective in extending the context window to a specific length, demonstrate either notable limitations in their extrapolation abilities or sacrificing partial performance within the context window.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"however, their exceptional capabilities are restricted within the preset context window of Transformer. Position Embedding (PE) scaling methods, while effective in extending the context window to a specific length, demonstrate either notable limitations in their extrapolation abilities or sacrificing partial performance within the context window.\""
    },
    {
        "title": "Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation",
        "authors": "SHIH-YING YEH;Yu-Guan Hsieh;Zhidong Gao;Bernard B W Yang;Giyeong Oh;Yanmin Gong",
        "site": "https://iclr.cc/virtual/2024/poster/17484",
        "summary": "Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts.  Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field.  However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation.  Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion), an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion.  Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Consistency-guided Prompt Learning for Vision-Language Models",
        "authors": "Shuvendu Roy;Ali Etemad",
        "site": "https://iclr.cc/virtual/2024/poster/17475",
        "summary": "We propose Consistency-guided Prompt learning (CoPrompt), a new fine-tuning method for vision-language models. Our approach improves the generalization of large foundation models when fine-tuned on downstream tasks in a few-shot setting. The basic idea of CoPrompt is to enforce a consistency constraint in the prediction of the trainable and pre-trained models to prevent overfitting on the downstream task. Additionally, we introduce the following two components into our consistency constraint to further boost the performance: enforcing consistency on two perturbed inputs and combining two dominant paradigms of tuning, prompting and adapter. Enforcing consistency on perturbed input serves to further regularize the consistency constraint, thereby improving generalization. Moreover, the integration of adapters and prompts not only enhances performance on downstream tasks but also offers increased tuning flexibility in both input and output spaces. This facilitates more effective adaptation to downstream tasks in a few-shot learning setting. Experiments show that CoPrompt outperforms existing methods on a range of evaluation suites, including base-to-novel generalization, domain generalization, and cross-dataset evaluation. On generalization, CoPrompt improves the state-of-the-art on zero-shot tasks and the overall harmonic mean over 11 datasets. Detailed ablation studies show the effectiveness of each of the components in CoPrompt. We make our code available at https://github.com/ShuvenduRoy/CoPrompt.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"to prevent overfitting on the downstream task.\"\n\nThis abstract mentions a limitation of large foundation models (overfitting) but does not elaborate on it and primarily focuses on the proposed solution, CoPrompt.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"to prevent overfitting on the downstream task.\"\n\nThis abstract mentions a limitation of large foundation models (overfitting) but does not elaborate on it and primarily focuses on the proposed solution, CoPrompt."
    },
    {
        "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
        "authors": "Bill Yuchen Lin;Abhilasha Ravichander;Ximing Lu;Nouha Dziri;Melanie Sclar;Khyathi Chandu;Chandra Bhagavatula;Yejin Choi",
        "site": "https://iclr.cc/virtual/2024/poster/17473",
        "summary": "Alignment tuning has become the de facto standard practice for enabling base large language models (LLMs) to serve as open-domain AI assistants. The alignment tuning process typically involves instruction learning through supervised fine-tuning (SFT) and preference tuning via reinforcement learning from human feedback (RLHF). A recent study, LIMA (Zhou et al., 2023), shows that using merely 1K examples for SFT can achieve significant alignment performance as well, suggesting that the effect of alignment tuning might be \"superficial.\" This raises questions about how exactly the alignment tuning transforms a base LLM. We analyze the effect of alignment tuning by examining the token distribution shift between base LLMs and their aligned counterparts (e.g., Llama-2 and Llama-2-chat). Our findings reveal that base LLMs and their alignment-tuned versions perform nearly identically in decoding on the majority of token positions (i.e., they share the top-ranked tokens). Most distribution shifts occur with stylistic tokens (e.g., discourse markers, safety disclaimers). This direct evidence strongly supports the hypothesis that alignment tuning primarily learns to adopt the language style of AI assistants, and that the knowledge required for answering user queries predominantly comes from the base LLMs themselves. Based on these findings, we rethink the alignment of LLMs by posing the research question: how effectively can we align base LLMs without SFT or RLHF? To address this, we introduce a simple, tuning-free alignment method, URIAL (Untuned LLMs with Restyled In-context Alignment). URIAL achieves effective alignment purely through in-context learning (ICL) with base LLMs, requiring as few as three constant stylistic examples and a system prompt. We conduct a fine-grained and interpretable evaluation on a diverse set of examples, named just-eval-instruct. Results demonstrate that base LLMs with URIAL can match or even surpass the performance of LLMs aligned with SFT (Mistral-7b-Instruct) or SFT+RLHF (Llama-2-70b-chat). We show that the gap between tuning-free and tuning-based alignment methods can be significantly reduced through strategic prompting and ICL. Our findings on the superficial nature of alignment tuning and results with URIAL suggest that deeper analysis and theoretical understanding of alignment is crucial to future LLM research.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This raises questions about how exactly the alignment tuning transforms a base LLM.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"This raises questions about how exactly the alignment tuning transforms a base LLM.\""
    },
    {
        "title": "Privacy-Preserving In-Context Learning for Large Language Models",
        "authors": "Tong Wu;Ashwinee Panda;Jiachen T. Wang;Prateek Mittal",
        "site": "https://iclr.cc/virtual/2024/poster/17471",
        "summary": "In-context learning (ICL) is an important capability of Large Language Models (LLMs), enabling these models to dynamically adapt based on specific, in-context exemplars, thereby improving accuracy and relevance.However, LLM's responses may leak the sensitive private information contained in in-context exemplars. To address this challenge, we propose Differentially Private In-context Learning (DP-ICL), a general paradigm for privatizing ICL tasks. The key idea for DP-ICL paradigm is generating differentially private responses through a noisy consensus among an ensemble of LLM's responses based on disjoint exemplar sets. Based on the general paradigm of DP-ICL, we instantiate several techniques showing how to privatize ICL for text classification and language generation. We experiment on four text classification benchmarks and two language generation tasks, and our empirical findings suggest that our DP-ICL achieves a strong utility-privacy tradeoff.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, LLM's responses may leak the sensitive private information contained in in-context exemplars.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, LLM's responses may leak the sensitive private information contained in in-context exemplars.\""
    },
    {
        "title": "Tag2Text: Guiding Vision-Language Model via Image Tagging",
        "authors": "Xinyu Huang;Youcai Zhang;Jinyu Ma;Weiwei Tian;Rui Feng;Yuejie Zhang;Yaqian Li;Yandong Guo;Lei Zhang",
        "site": "https://iclr.cc/virtual/2024/poster/17468",
        "summary": "This paper presents Tag2Text, a vision language pre-training (VLP) framework, which introduces image tagging into vision-language models to guide the learning of visual-linguistic features. In contrast to prior works which utilize object tags either manually labeled or automatically detected with a limited detector, our approach utilizes tags parsed from its paired text to learn an image tagger and meanwhile provides guidance to vision-language models. Given that, Tag2Text can utilize large-scale annotation-free image tags in accordance with image-text pairs, and provides more diverse tag categories beyond objects. Strikingly, Tag2Text showcases the ability of a foundational image tagging model, with superior zero-shot performance even comparable to full supervision manner. Moreover, by leveraging tagging guidance, Tag2Text effectively enhances the performance of vision-language models on both generation-based and alignment-based tasks. Across a wide range of downstream benchmarks, Tag2Text achieves state-of-the-art results with similar model sizes and data scales, demonstrating the efficacy of the proposed tagging guidance.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the abstract does not mention any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the abstract does not mention any limitations of LLMs."
    },
    {
        "title": "Are Bert Family Good Instruction Followers? A Study on Their Potential And Limitations",
        "authors": "yisheng xiao;Juntao Li;Zechen Sun;Zechang Li;Qingrong Xia;Xinyu Duan;Zhefeng Wang;Min Zhang",
        "site": "https://iclr.cc/virtual/2024/poster/17466",
        "summary": "Language modeling at scale has proven very effective and brought unprecedented success to natural language models. Many typical representatives, especially decoder-only models, e.g., BLOOM and LLaMA, and encoder-decoder models, e.g., Flan-T5 and AlexaTM, have exhibited incredible instruction-following capabilities while keeping strong task completion ability. These large language models can achieve superior performance in various tasks and even yield emergent capabilities, e.g., reasoning and universal generalization. Though the above two paradigms are mainstream and well explored, the potential of the BERT family, which are encoder-only based models and have ever been one of the most representative pre-trained models, also deserves attention, at least should be discussed. In this work, we adopt XML-R to explore the effectiveness of the BERT family for instruction following and zero-shot learning. We first design a simple yet effective strategy to utilize the encoder-only models for generation tasks and then conduct multi-task instruction tuning.  Experimental results demonstrate that our fine-tuned model, Instruct-XMLR, outperforms Bloomz on all evaluation tasks and achieves comparable performance with mT0 on most tasks. Surprisingly, Instruct-XMLR also possesses strong task and language generalization abilities, indicating that Instruct-XMLR can also serve as a good instruction follower and zero-shot learner. Besides, Instruct-XMLR can accelerate decoding due to its non-autoregressive generation manner, achieving around 3 times speedup compared with current autoregressive large language models. Although we also witnessed several limitations through our experiments, such as the performance decline in long-generation tasks and the shortcoming of length prediction, Instruct-XMLR can still become a good member of the family of current large language models.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although we also witnessed several limitations through our experiments, such as the performance decline in long-generation tasks and the shortcoming of length prediction,\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although we also witnessed several limitations through our experiments, such as the performance decline in long-generation tasks and the shortcoming of length prediction,\""
    },
    {
        "title": "Towards 3D Molecule-Text Interpretation in Language Models",
        "authors": "Sihang Li;Zhiyuan Liu;Yanchen Luo;Xiang Wang;Xiangnan He;Kenji Kawaguchi;Tat-Seng Chua;Qi Tian",
        "site": "https://iclr.cc/virtual/2024/poster/17458",
        "summary": "Language Models (LMs) have greatly influenced diverse domains. However, their inherent limitation in comprehending 3D molecular structures has considerably constrained their potential in the biomolecular domain. To bridge this gap, we focus on 3D molecule-text interpretation, and propose 3D-MoLM: 3D-Molecular Language Modeling. Specifically, 3D-MoLM enables an LM to interpret and analyze 3D molecules by equipping the LM with a 3D molecular encoder. This integration is achieved by a 3D molecule-text projector, bridging the 3D molecular encoder’s representation space and the LM’s input space. Moreover, to enhance 3DMoLM’s ability of cross-modal molecular understanding and instruction following, we meticulously curated a 3D molecule-centric instruction tuning dataset – 3D-MoIT. Through 3D molecule-text alignment and 3D molecule-centric instruction tuning, 3D-MoLM establishes an integration of 3D molecular encoder and LM. It significantly surpasses existing baselines on downstream tasks, including moleculetext retrieval, molecule captioning, and more challenging open-text molecular QA tasks, especially focusing on 3D-dependent properties. We will release our codes and datasets at https://github.com/lsh0520/3D-MoLM.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, their inherent limitation in comprehending 3D molecular structures has considerably constrained their potential in the biomolecular domain.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, their inherent limitation in comprehending 3D molecular structures has considerably constrained their potential in the biomolecular domain.\""
    },
    {
        "title": "SALMON: Self-Alignment with Instructable Reward Models",
        "authors": "Zhiqing Sun;Yikang Shen;Hongxin Zhang;Qinhong Zhou;Zhenfang Chen;David Daniel Cox;Yiming Yang;Chuang Gan",
        "site": "https://iclr.cc/virtual/2024/poster/17454",
        "summary": "Supervised Fine-Tuning (SFT) on response demonstrations combined with Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful paradigm for aligning LLM-based AI agents. However, a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences. This paper presents a novel approach, namely SALMON, to align base language models with minimal human supervision, using only a small set of human-defined principles, yet achieving superior performance. Central to our approach is an instructable reward model. Trained on synthetic preference data, this model can generate reward scores based on arbitrary human-defined principles. By merely adjusting these principles during the RL training phase, we gain full control over the preferences with the instructable reward model, subsequently influencing the behavior of the RL-trained policy models, and reducing the reliance on the collection of online human preferences. Applying our method to the LLaMA-2-70b base language model, we developed an AI assistant named Dromedary-2. With only 6 exemplars for in-context learning and 31 human-defined principles, Dromedary-2 significantly surpasses the performance of several state-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have open-sourced the code and model weights to encourage further research into aligning LLM-based AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences.\""
    },
    {
        "title": "Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs",
        "authors": "Qingru Zhang;Chandan Singh;Liyuan Liu;Xiaodong Liu;Bin Yu;Jianfeng Gao;Tuo Zhao",
        "site": "https://iclr.cc/virtual/2024/poster/17451",
        "summary": "In human-written articles, we often leverage the subtleties of text style, such as bold and italics, to guide the attention of readers. These textual emphases are vital for the readers to grasp the conveyed information.  When interacting with large language models (LLMs), we have a similar need -- steering the model to pay closer attention to user-specified information, e.g., an instruction. Existing methods, however, are constrained to process plain text and do not support such a mechanism. This motivates us to introduce PASTA -- Post-hoc Attention STeering Approach, a method that allows LLMs to read text with user-specified emphasis marks. To this end, PASTA identifies a small subset of attention heads and applies precise attention reweighting on them, directing the model attention to user-specified parts. Like prompting, PASTA is applied at inference time and does not require changing any model parameters. Experiments demonstrate that PASTA can substantially enhance an LLM's ability to follow user instructions or integrate new knowledge from user inputs, leading to a significant performance improvement on a variety of tasks, e.g., an average accuracy improvement of 22\\% for LLAMA-7B. Our code is publicly available at https://github.com/QingruZhang/PASTA .",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing methods, however, are constrained to process plain text and do not support such a mechanism.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing methods, however, are constrained to process plain text and do not support such a mechanism.\""
    },
    {
        "title": "Statistical Rejection Sampling Improves Preference Optimization",
        "authors": "Tianqi Liu;Yao Zhao;Rishabh Joshi;Misha Khalman;Mohammad Saleh;Peter J Liu;Jialu Liu",
        "site": "https://iclr.cc/virtual/2024/poster/17450",
        "summary": "Improving the alignment of language models with human preferences remains an active research challenge. Previous approaches have primarily utilized online Reinforcement Learning from Human Feedback (RLHF). Recently, offline methods such as Sequence Likelihood Calibration (SLiC) and Direct Preference Optimization (DPO) have emerged as attractive alternatives, offering improvements in stability and scalability while maintaining competitive performance. SLiC refines its loss function using sequence pairs sampled from a supervised fine-tuned (SFT) policy, while DPO directly optimizes language models based on preference data, foregoing the need for a separate reward model. However, the maximum likelihood estimator (MLE) of the target optimal policy requires labeled preference pairs sampled from that policy. The absence of a reward model in DPO constrains its ability to sample preference pairs from the optimal policy. Meanwhile, SLiC can only sample preference pairs from the SFT policy. To address these limitations, we introduce a novel approach called Statistical Rejection Sampling Optimization (RSO) designed to source preference data from the target optimal policy using rejection sampling, enabling a more accurate estimation of the optimal policy. We also propose a unified framework that enhances the loss functions used in both SLiC and DPO from a preference modeling standpoint. Through extensive experiments across diverse tasks, we demonstrate that RSO consistently outperforms both SLiC and DPO as evaluated by both Large Language Models (LLMs) and human raters.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the maximum likelihood estimator (MLE) of the target optimal policy requires labeled preference pairs sampled from that policy. The absence of a reward model in DPO constrains its ability to sample preference pairs from the optimal policy. Meanwhile, SLiC can only sample preference pairs from the SFT policy.\"\n\nNote that the limitations mentioned are related to the methods used with LLMs",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the maximum likelihood estimator (MLE) of the target optimal policy requires labeled preference pairs sampled from that policy. The absence of a reward model in DPO constrains its ability to sample preference pairs from the optimal policy. Meanwhile, SLiC can only sample preference pairs from the SFT policy.\"\n\nNote that the limitations mentioned are related to the methods used with LLMs"
    },
    {
        "title": "Teaching Language Models to Hallucinate Less with Synthetic Tasks",
        "authors": "Erik Jones;Hamid Palangi;Clarisse Simões Ribeiro;Varun Chandrasekaran;Subhabrata Mukherjee;Arindam Mitra;Ahmed Hassan Awadallah;Ece Kamar",
        "site": "https://iclr.cc/virtual/2024/poster/17443",
        "summary": "Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context. However, optimizing to make LLMs hallucinate less is challenging, as hallucination is hard to efficiently, cheaply, and reliably evaluate at each optimization step. In this work, we show that reducing hallucination on asynthetic taskcan also reduce hallucination on real-world downstream tasks. Our method, SynTra, first designs a synthetic task where hallucinations are easy to elicit and measure. It next optimizes the LLM's system message via prefix tuning on the synthetic task, then uses the system message on realistic, hard-to-optimize tasks. Across three realistic abstractive summarization tasks, we reduce hallucination for two 13B-parameter LLMs using supervision signal from only a synthetic retrieval task. We also find that optimizing the system message rather than the model weights can be critical; fine-tuning the entire model on the synthetic task can counterintuitivelyincreasehallucination. Overall, SynTra demonstrates that the extra flexibility of working with synthetic data can help mitigate undesired behaviors in practice.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context.\""
    },
    {
        "title": "LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning",
        "authors": "Han Guo;Philip Greengard;Eric Xing;Yoon Kim",
        "site": "https://iclr.cc/virtual/2024/poster/17436",
        "summary": "We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative  algorithm  to decompose each  pretrained  matrix into a high-precision low-rank component  and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the  low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic  configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget.  We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the  reconstruction objective during matrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables aggressive quantization to sub-3 bits with only minor performance degradations. When finetuned on a language modeling calibration dataset, LQ-LoRA can also be used for model compression; in this setting our 2.75-bit LLaMA-2-70B model (which has 2.85 bits on average when including the low-rank components and requires 27GB of GPU memory) performs respectably compared to the 16-bit baseline.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"with only minor performance degradations.\"\n\nThis paper mentions LLMs and briefly mentions a limitation (performance degradation due to quantization) but does not explore it in depth. The primary focus is on the proposed solution and its performance.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"with only minor performance degradations.\"\n\nThis paper mentions LLMs and briefly mentions a limitation (performance degradation due to quantization) but does not explore it in depth. The primary focus is on the proposed solution and its performance."
    },
    {
        "title": "Retrieval meets Long Context Large Language Models",
        "authors": "Peng Xu;Wei Ping;Xianchao Wu;Lawrence McAfee;Chen Zhu;Zihan Liu;Sandeep Subramanian;Evelina Bakhturina;Mohammad Shoeybi;Bryan Catanzaro",
        "site": "https://iclr.cc/virtual/2024/poster/17435",
        "summary": "Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-turbo-16k and Davinci003 in terms of average score on nine long context tasks including question answering, query-based summarization, and in-context few-shot learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a margin, while being much faster at generation. Our study provides general insights on the choice of retrieval-augmentation versus long context extension of LLM for practitioners.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years.\""
    },
    {
        "title": "Is Self-Repair a Silver Bullet for Code Generation?",
        "authors": "Theo X. Olausson;Jeevana Priya Inala;Chenglong Wang;Jianfeng Gao;Armando Solar-Lezama",
        "site": "https://iclr.cc/virtual/2024/poster/17429",
        "summary": "Large language models have shown remarkable aptitude in code generation, but still struggle to perform complex tasks. Self-repair---in which the model debugs and repairs its own code---has recently become a popular way to boost performance in these settings. However, despite its increasing popularity, existing studies of self-repair have been limited in scope; in many settings, its efficacy thus remains poorly understood. In this paper, we analyze Code Llama, GPT-3.5 and GPT-4's ability to perform self-repair on problems taken from HumanEval and APPS. We find that when the cost of carrying out repair is taken into account, performance gains are often modest, vary a lot between subsets of the data, and are sometimes not present at all. We hypothesize that this is because self-repair is bottlenecked by the model's ability to provide feedback on its own code; using a stronger model to artificially boost the quality of the feedback, we observe substantially larger performance gains. Similarly, a small-scale study in which we provide GPT-4 with feedback from human participants suggests that even for the strongest models, self-repair still lags far behind what can be achieved with human-level debugging.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models have shown remarkable aptitude in code generation, but still struggle to perform complex tasks.\"; \"We find that when the cost of carrying out repair is taken into account, performance gains are often modest, vary a lot between subsets of the data, and are sometimes not present at all.\"; \"Similarly, a small-scale study in which we provide GPT-4 with feedback",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Large language models have shown remarkable aptitude in code generation, but still struggle to perform complex tasks.\"; \"We find that when the cost of carrying out repair is taken into account, performance gains are often modest, vary a lot between subsets of the data, and are sometimes not present at all.\"; \"Similarly, a small-scale study in which we provide GPT-4 with feedback"
    },
    {
        "title": "Adapting Large Language Models via Reading Comprehension",
        "authors": "Daixuan Cheng;Shaohan Huang;Furu Wei",
        "site": "https://iclr.cc/virtual/2024/poster/17426",
        "summary": "We explore how continued pre-training on domain-specific corpora influences large language models, revealing that training on the raw corpora endows the model with domain knowledge, but drastically hurts its prompting ability for question answering. Taken inspiration from human learning via reading comprehension--practice after reading improves the ability to answer questions based on the learned knowledge--we propose a simple method for transforming raw corpora into reading comprehension texts. Each raw text is enriched with a series of tasks related to its content. Our method, highly scalable and applicable to any pre-training corpora, consistently enhances performance across various tasks in three different domains: biomedicine, finance, and law. Notably, our 7B language model achieves competitive performance with domain-specific models of much larger scales, such as BloombergGPT-50B. Furthermore, we demonstrate that domain-specific reading comprehension texts can improve the model's performance even on general benchmarks, showing the potential to develop a general model across even more domains. Our model, code, and data are available at https://github.com/microsoft/LMOps.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"revealing that training on the raw corpora endows the model with domain knowledge, but drastically hurts its prompting ability for question answering.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (hurting prompting ability for question answering) but does not explore it in detail and focuses on the proposed solution to address this limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"revealing that training on the raw corpora endows the model with domain knowledge, but drastically hurts its prompting ability for question answering.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (hurting prompting ability for question answering) but does not explore it in detail and focuses on the proposed solution to address this limitation."
    },
    {
        "title": "Bandits with Replenishable Knapsacks: the Best of both Worlds",
        "authors": "Martino Bernasconi;Matteo Castiglioni;Andrea Celli;Federico Fusco",
        "site": "https://iclr.cc/virtual/2024/poster/17425",
        "summary": "Abstract:The bandits with knapsacks (BwK) framework models online decision-making problems in which an agent makes a sequence of decisions subject to resource consumption constraints. The traditional model assumes that each action consumes a non-negative amount of resources and the process ends when the initial budgets are fully depleted. We study a natural generalization of the BwK framework which allows non-monotonic resource utilization, i.e., resources can be replenished by a positive amount. We propose a best-of-both-worlds primal-dual template that can handle any online learning problem with replenishment for which a suitable primal regret minimizer exists. In particular, we provide the first positive results for the case of adversarial inputs by showing that our framework guarantees a constant competitive ratio $\\alpha$ when $B=\\Omega(T)$ or when the possible per-round replenishment is a positive constant. Moreover, under a stochastic input model, our algorithm yields an instance-independent $\\tilde{\\mathcal{O}}(T^{1/2})$ regret bound which complements existing instance-dependent bounds for the same setting. Finally, we provide applications of our framework to some economic problems of practical relevance.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Conversational Drug Editing Using Retrieval and Domain Feedback",
        "authors": "Shengchao Liu;Jiongxiao Wang;Yijin Yang;Chengpeng Wang;Ling Liu;Hongyu Guo;Chaowei Xiao",
        "site": "https://iclr.cc/virtual/2024/poster/17419",
        "summary": "Recent advancements in conversational large language models (LLMs), such as ChatGPT, have demonstrated remarkable promise in various domains, including drug discovery. However, existing works mainly focus on investigating the capabilities of conversational LLMs on chemical reactions and retrosynthesis. While drug editing, a critical task in the drug discovery pipeline, remains largely unexplored. To bridge this gap, we propose ChatDrug, a framework to facilitate the systematic investigation of drug editing using LLMs. ChatDrug jointly leverages a prompt module, a retrieval and domain feedback module, and a conversation module to streamline effective drug editing. We empirically show that ChatDrug reaches the best performance on all 39 drug editing tasks, encompassing small molecules, peptides, and proteins. We further demonstrate, through 10 case studies, that ChatDrug can successfully identify the key substructures for manipulation, generating diverse and valid suggestions for drug editing. Promisingly, we also show that ChatDrug can offer insightful explanations from a domain-specific perspective, enhancing interpretability and enabling informed decision-making.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations of LLMs are mentioned in the abstract, but it mentions that drug editing \"remains largely unexplored\" which might imply that LLMs have not been fully utilized or effective in this area, however, this is not a direct limitation of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations of LLMs are mentioned in the abstract, but it mentions that drug editing \"remains largely unexplored\" which might imply that LLMs have not been fully utilized or effective in this area, however, this is not a direct limitation of LLMs."
    },
    {
        "title": "VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal Large Language Models",
        "authors": "Zihao Zhu;Mingda Zhang;Shaokui Wei;Bingzhe Wu;Baoyuan Wu",
        "site": "https://iclr.cc/virtual/2024/poster/17409",
        "summary": "The role of data in building AI systems has recently been emphasized by the emerging concept of data-centric AI. Unfortunately, in the real-world, datasets may contain dirty samples, such as poisoned samples from backdoor attack, noisy labels in crowdsourcing, and even hybrids of them. The presence of such dirty samples makes the DNNs vunerable and unreliable.Hence, it is critical to detect dirty samples to improve the quality and realiability of dataset. Existing detectors only focus on detecting poisoned samples or noisy labels, that are often prone to weak generalization when dealing with dirty samples from other fields.In this paper, we find a commonality of various dirty samples is visual-linguistic inconsistency between images and associated labels. To capture the semantic inconsistency between modalities, we propose versatile data cleanser (VDC) leveraging the surpassing capabilities of multimodal large language models (MLLM) in cross-modal alignment and reasoning.It consists of three consecutive modules: the visual question generation module to generate insightful questions about the image; the visual question answering module to acquire the semantics of the visual content by answering the questions with MLLM; followed by the visual answer evaluation module to evaluate the inconsistency.Extensive experiments demonstrate its superior performance and generalization to various categories and types of dirty samples.The code is available athttps://github.com/zihao-ai/vdc.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No direct evidence of discussing limitations of LLMs, but the abstract mentions that existing detectors are \"often prone to weak generalization when dealing with dirty samples from other fields\", which could be indirectly related to LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: No direct evidence of discussing limitations of LLMs, but the abstract mentions that existing detectors are \"often prone to weak generalization when dealing with dirty samples from other fields\", which could be indirectly related to LLMs."
    },
    {
        "title": "Achieving Fairness in Multi-Agent MDP Using Reinforcement Learning",
        "authors": "Peizhong Ju;Arnob Ghosh;Ness Shroff",
        "site": "https://iclr.cc/virtual/2024/poster/17403",
        "summary": "Fairness plays a crucial role in various multi-agent systems (e.g., communication networks, financial markets, etc.). Many multi-agent dynamical interactions can be cast as Markov Decision Processes (MDPs). While existing research has focused on studying fairness in known environments, the exploration of fairness in such systems for unknown environments remains open. In this paper, we propose a  Reinforcement Learning (RL) approach to achieve fairness in multi-agent finite-horizon episodic MDPs. Instead of maximizing the sum of individual agents' value functions, we introduce a fairness function that ensures equitable rewards across agents. Since the classical Bellman's equation does not hold when the sum of individual value functions is not maximized, we cannot use traditional approaches. Instead, in order to explore, we maintain a confidence bound of the unknown environment and then propose an online convex optimization based approach to obtain a policy constrained to this confidence region. We show that such an approach achieves sub-linear regret in terms of the number of episodes. Additionally, we provide a probably approximately correct (PAC) guarantee based on the obtained regret bound. We also propose an offline RL algorithm and bound the optimality gap with respect to the optimal fair solution. To mitigate computational complexity, we introduce a policy-gradient type method for the fair objective. Simulation experiments also demonstrate the efficacy of our approach.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Image Translation as Diffusion Visual Programmers",
        "authors": "Cheng Han;James Chenhao Liang;Qifan Wang;MAJID RABBANI;Sohail Dianat;Raghuveer Rao;Ying Nian Wu;Dongfang Liu",
        "site": "https://iclr.cc/virtual/2024/poster/17402",
        "summary": "Abstract:We introduce the novel Diffusion Visual Programmer (DVP), a neuro-symbolic image translation framework. Our proposed DVP seamlessly embeds a condition-flexible diffusion model within the GPT architecture, orchestrating a coherent sequence of visual programs ($i.e.$, computer vision models) for various pro-symbolic steps, which span RoI identification, style transfer, and position manipulation, facilitating transparent and controllable image translation processes. Extensive experiments demonstrate DVP’s remarkable performance, surpassing concurrent arts. This success can be attributed to several key features of DVP: First, DVP achieves condition-flexible translation via instance normalization, enabling the model to eliminate sensitivity caused by the manual guidance and optimally focus on textual descriptions for high-quality content generation. Second, the frame work enhances in-context reasoning by deciphering intricate high-dimensional concepts in feature spaces into more accessible low-dimensional symbols ($e.g.$, [Prompt], [RoI object]), allowing for localized, context-free editing while maintaining overall coherence. Last but not least, DVP improves systemic controllability and explainability by offering explicit symbolic representations at each programming stage, empowering users to intuitively interpret and modify results. Our research marks a substantial step towards harmonizing artificial image translation processes with cognitive intelligence, promising broader applications.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our proposed DVP seamlessly embeds a condition-flexible diffusion model within the GPT architecture\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Our proposed DVP seamlessly embeds a condition-flexible diffusion model within the GPT architecture\""
    },
    {
        "title": "Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching",
        "authors": "Yang Liu;Muzhi Zhu;Hengtao Li;Hao Chen;Xinlong Wang;Chunhua Shen",
        "site": "https://iclr.cc/virtual/2024/poster/17396",
        "summary": "Abstract:Powered by large-scale pre-training, vision foundation models exhibit significant potential in open-world image understanding. However, unlike large language models that excel at directly tackling various language tasks, vision foundation models require a task-specific model structure followed by fine-tuning on specific tasks. In this work, we present $\\textbf{Matcher}$, a novel perception paradigm that utilizes off-the-shelf vision foundation models to address various perception tasks. Matcher can segment anything by using an in-context example without training. Additionally, we design three effective components within the Matcher framework to collaborate with these foundation models and unleash their full potential in diverse perception tasks. Matcher demonstrates impressive generalization performance across various segmentation tasks, all without training. For example, it achieves 52.7% mIoU on COCO-20$^i$ with one example, surpassing the state-of-the-art specialist model by 1.6%. In addition, Matcher achieves 33.0% mIoU on the proposed LVIS-92$^i$ for one-shot semantic segmentation, outperforming the state-of-the-art generalist model by 14.4%. Our visualization results further showcase the open-world generality and flexibility of Matcher when applied to images in the wild.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, unlike large language models that excel at directly tackling various language tasks, vision foundation models require a task-specific model structure followed by fine-tuning on specific tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, unlike large language models that excel at directly tackling various language tasks, vision foundation models require a task-specific model structure followed by fine-tuning on specific tasks.\""
    },
    {
        "title": "Byzantine Robust Cooperative Multi-Agent Reinforcement Learning as a Bayesian Game",
        "authors": "Simin Li;Jun Guo;Jingqiao Xiu;Ruixiao Xu;Xin Yu;Jiakai Wang;Aishan Liu;Yaodong Yang;Xianglong Liu",
        "site": "https://iclr.cc/virtual/2024/poster/17392",
        "summary": "In this study, we explore the robustness of cooperative multi-agent reinforcement learning (c-MARL) against Byzantine failures, where any agent can enact arbitrary, worst-case actions due to malfunction or adversarial attack. To address the uncertainty that any agent can be adversarial, we propose a Bayesian Adversarial Robust Dec-POMDP (BARDec-POMDP) framework, which views Byzantine adversaries as nature-dictated types, represented by a separate transition. This allows agents to learn policies grounded on their posterior beliefs about the type of other agents, fostering collaboration with identified allies and minimizing vulnerability to adversarial manipulation. We define the optimal solution to the BARDec-POMDP as an ex interim robust Markov perfect Bayesian equilibrium, which we proof to exist and the corresponding policy weakly dominates previous approaches as time goes to infinity. To realize this equilibrium, we put forward a two-timescale actor-critic algorithm with almost sure convergence under specific conditions. Experiments on matrix game, Level-based Foraging and StarCraft II indicate that, our method successfully acquires intricate micromanagement skills and adaptively aligns with allies under worst-case perturbations, showing resilience against non-oblivious adversaries, random allies, observation-based attacks, and transfer-based attacks.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning",
        "authors": "Ke Wang;Houxing Ren;Aojun Zhou;Zimu Lu;Sichun Luo;Weikang Shi;Renrui Zhang;Linqi Song;Mingjie Zhan;Hongsheng Li",
        "site": "https://iclr.cc/virtual/2024/poster/17389",
        "summary": "Abstract:The recently released GPT-4 Code Interpreter has demonstrated remarkable proficiency in solving challenging math problems, primarily attributed to its ability to seamlessly reason with natural language, generate code, execute code, and continue reasoning based on the execution output. In this paper, we present a method to fine-tune open-source language models, enabling them to use code for modeling and deriving math equations and, consequently, enhancing their mathematical reasoning abilities. We propose a method of generating novel and high-quality datasets with math problems and their code-based solutions, referred to as MathCodeInstruct. Each solution interleaves $\\textit{natural language}$, $\\textit{code}$, and $\\textit{execution results}$. We also introduce a customized supervised fine-tuning and inference approach. This approach yields the MathCoder models, a family of models capable of generating code-based solutions for solving challenging math problems. Impressively, the MathCoder models achieve state-of-the-art scores among open-source LLMs on the MATH (45.2%) and GSM8K (83.9%) datasets, substantially outperforming other open-source alternatives. Notably, the MathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8K and MATH but also outperforms GPT-4 on the competition-level MATH dataset. The proposed dataset and models will be released upon acceptance.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations of LLMs are mentioned in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations of LLMs are mentioned in the abstract."
    },
    {
        "title": "AgentBench: Evaluating LLMs as Agents",
        "authors": "Xiao Liu;Hao Yu;Hanchen Zhang;Yifan Xu;Xuanyu Lei;Hanyu Lai;Yu Gu;Hangliang Ding;Kaiwen Men;Kejuan Yang;Shudan Zhang;Xiang Deng;Aohan Zeng;Zhengxiao Du;Chenhui Zhang;Sheng Shen;Tianjun Zhang;Yu Su;Huan Sun;Minlie Huang;Yuxiao Dong;Jie Tang",
        "site": "https://iclr.cc/virtual/2024/poster/17388",
        "summary": "The potential of Large Language Model (LLM) as agents has been widely acknowledged recently.Thus, there is an urgent need to quantitatively evaluate LLMs as agents on challenging tasks in interactive environments.We present AgentBench, a multi-dimensional benchmark that consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities.Our extensive test over 29 API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and many OSS competitors that are no larger than 70B.We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents.Improving instruction following and training on high quality multi-round alignment data could improve agent performance.And different from existing assumptions, training on code present ambivalent impacts on different agent tasks.Datasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents.\""
    },
    {
        "title": "Detecting Pretraining Data from Large Language Models",
        "authors": "Weijia Shi;Anirudh Ajith;Mengzhou Xia;Yangsibo Huang;Daogao Liu;Terra Blevins;Danqi Chen;Luke Zettlemoyer",
        "site": "https://iclr.cc/virtual/2024/poster/17381",
        "summary": "Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method MIN-K PROB based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities. MIN-K PROB can be applied without any knowledge about the pretrainig corpus or any additional training, departing from previous detection methods that require training a reference model on data that is similar to the pretraining data. Moreover, our experiments demonstrate that MIN-K PROB achieves a 7.4% improvement on WIKIMIA over these previous methods. We apply MIN-K PROB to two real-world scenarios, copyrighted book detection and contaminated downstream example detection, and find that it to be a consistently effective solution.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the data used to train them is rarely disclosed... it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks.\"\n\nThis paper mentions limitations of LLMs in passing, specifically the lack of transparency in the data used to train them and the potential inclusion of problematic text. However, it does not elaborate",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the data used to train them is rarely disclosed... it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks.\"\n\nThis paper mentions limitations of LLMs in passing, specifically the lack of transparency in the data used to train them and the potential inclusion of problematic text. However, it does not elaborate"
    },
    {
        "title": "How do Language Models Bind Entities in Context?",
        "authors": "Jiahai Feng;Jacob Steinhardt",
        "site": "https://iclr.cc/virtual/2024/poster/17378",
        "summary": "Language models (LMs) can recall facts mentioned in context, as shown by their performance on reading comprehension tasks. When the context describes facts about more than one entity, the LM has to correctly bind attributes to their corresponding entity. We show, via causal experiments, that LMs' internal activations represent binding information by exhibiting appropriate binding ID vectors at the entity and attribute positions. We further show that binding ID vectors form a subspace and often transfer across tasks. Our results demonstrate that LMs learn interpretable strategies for representing symbolic knowledge in context, and that studying context activations is a fruitful direction for understanding LM cognition.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper discusses the binding of entities in context, which might be seen as a limitation in certain situations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper discusses the binding of entities in context, which might be seen as a limitation in certain situations."
    },
    {
        "title": "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
        "authors": "Xinlu Zhang;Shiyang Li;Xianjun Yang;Chenxin Tian;Yao Qin;Linda Ruth Petzold",
        "site": "https://iclr.cc/virtual/2024/poster/17369",
        "summary": "Abstract:Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments.\""
    },
    {
        "title": "Out-of-Variable Generalisation for Discriminative Models",
        "authors": "Siyuan Guo;Jonas Bernhard Wildberger;Bernhard Schölkopf",
        "site": "https://iclr.cc/virtual/2024/poster/17368",
        "summary": "Abstract:The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
        "authors": "Christian Fabian;Kai Cui;Heinz Koeppl",
        "site": "https://iclr.cc/virtual/2024/poster/17367",
        "summary": "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "On the generalization capacity of neural networks during generic multimodal reasoning",
        "authors": "Takuya Ito;Soham Dan;Mattia Rigotti;James Kozloski;Murray Campbell",
        "site": "https://iclr.cc/virtual/2024/poster/17366",
        "summary": "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provideGeneric COG(gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization.\""
    },
    {
        "title": "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
        "authors": "Wanru Zhao;Yihong Chen;Royson Lee;Xinchi Qiu;Yan Gao;Hongxiang Fan;Nicholas Donald Lane",
        "site": "https://iclr.cc/virtual/2024/poster/17365",
        "summary": "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.To overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
        "source": "iclr2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges stemming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges stemming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border).\""
    }
]